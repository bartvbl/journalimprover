<paperCache><paper><title>Detecting symmetries in polynomial Bezier curves</title><abstract>In a recent article, Alcazar (2014) presents algorithms for detecting
   central and mirror symmetries in planar polynomial curves, expressed
   with proper parameterization in the monomial (i.e., power) basis.
   However, for practical purposes in Computer Graphics and CAGD, the usual
   choice is the Bernstein-Bezier representation, because of its superior
   numerical and geometric characteristics. We point out that, in this form
   and for properly parameterized curves, detecting symmetry amounts to
   simply checking that the Bezier points exhibit pairwise symmetry. This
   result is a direct consequence of well-known properties of the Bezier
   representation, namely its symmetry, affine invariance, and uniqueness
   for proper parameterizations. Detecting the existence of a symmetric
   segment in a Bezier curve also amounts to a simple task, by analysing
   the last non-vanishing derivatives. Finally, these results carry over in
   a straightforward manner to symmetries in Euclidean space. (C) 2015
   Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Sanchez-Reyes, J.</author></paper><paper><title>Characterizing the performance of an image-based recognizer for planar
   mechanical linkages in textbook graphics and hand-drawn sketches</title><abstract>In this work, we present a computational framework for automatically
   generating kinematic models of planar mechanical linkages from raw
   images. The hallmark of our approach is a novel combination of
   supervised learning methods for detecting mechanical parts (e.g. joints,
   rigid bodies) with the optimizing power of a multiobjective evolutionary
   algorithm, which concurrently maximizes image consistency and mechanical
   feasibility. A rigorous set of experiments was conducted to
   systematically evaluate the performance of each phase in our framework,
   comparing various combinations of joint and body detection schemes and
   feasibility constraints. Precision-recall curves are used to assess
   object detection performance. For the optimization, in addition to
   standard accuracy measures such as top-N accuracy, we introduce a new
   performance metric called user effort ratio that quantifies the amount
   of user interaction required to correct an inaccurate optimization
   solution. Current state-of-the-art performance is achieved with (i) one
   (or a cascade of) support vector machines for joint detection, (ii)
   foreground extraction to reduce false positives, (iii) supervised body
   detection using normalized geodesic time, distance, and detected joint
   confidence, and (iv) feasibility constraints derived from graph theory.
   The proposed framework generalizes moderately well from textbook
   graphics to hand-drawn sketches, and user effort ratio results
   demonstrate the potential power of an interactive system in which simple
   user interactions complement computer recognition for fast kinematic
   modeling. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>NOV 2015</date><author>Eicholtz, Matthew
   Kara, Levent Burak</author></paper><paper><title>A Study on Value of Artistic Presentation on Motion Graphics -Focused on
   F5 and Semi-Permanent-</title><abstract>This paper presents the historical development process and expressive
   style of motion graphics which are intended to artistic expression, and
   discusses the value of artistic expressions in motion graphics through
   the works and activities of F5 and Semi-Permanent which are the
   representative organization of that. Commercial use of digital motion
   graphics has been rapidly widespread and used in most of the picture
   contents by changes in the digital video production environment with the
   development of the computer. Motion graphics begins with a graphic
   design applied to the motion graphic content and is transferred from the
   abstract representation of the animation at the same time. Since then
   motion graphics have been developed as an independent genre on the basis
   of graphic design and animation, and take the important position for the
   understanding and development of the contemporary design and content
   industry. Moreover, the motion graphics which present the experiment of
   various techniques and the aspect of mixed genres contribute to the
   development and diversification of the visual image. This paper is
   covered on not only F5 and Semi-Permanent that have published
   experimental artistic motion graphics but also the analysis of the
   developed ways and expressive patters to produce experimental motion
   graphics of their works. Furthermore, it discusses the experimental
   artistic value of these works.</abstract><date>2016</date><author>ì†?êµ­í™˜</author></paper><paper><title>Real-time multi-view deconvolution</title><abstract>In light-sheet microscopy, overall image content and resolution are
   improved by acquiring and fusing multiple views of the sample from
   different directions. State-of-the-art multi-view (MV) deconvolution
   simultaneously fuses and deconvolves the images in 3D, but processing
   takes a multiple of the acquisition time and constitutes the bottleneck
   in the imaging pipeline. Here, we show that MV deconvolution in 3D can
   finally be achieved in real-time by processing cross-sectional planes
   individually on the massively parallel architecture of a graphics
   processing unit (GPU). Our approximation is valid in the typical case
   where the rotation axis lies in the imaging plane.</abstract><date>OCT 15 2015</date><author>Schmid, Benjamin
   Huisken, Jan</author></paper><paper><title>Structural diversity of the epigenetics pocketome</title><abstract>Protein families involved in chromatin-templated events are emerging as
   novel target classes in oncology and other disease areas. The ability to
   discover selective inhibitors against chromatin factors depends on the
   presence of structural features that are unique to the targeted sites.
   To evaluate challenges and opportunities toward the development of
   selective inhibitors, we calculated all pair wise structural distances
   between 575 structures from the protein databank representing 163 unique
   binding pockets found in protein domains that write, read or erase
   post-translational modifications on histones, DNA, and RNA. We find that
   the structural similarity of binding sites does not always follow the
   sequence similarity of protein domains. Our analysis reveals increased
   risks of activity across target-class for compounds competing with the
   cofactor of protein arginine methyltransferases, lysine
   acetyltransferases, and sirtuins, while exploiting the conformational
   plasticity of a protein target is a path toward selective inhibition.
   The structural diversity landscape of the epigenetics pocketome can be
   explored via an open-access graphic user interface at . Proteins 2015;
   83:1316-1326. (c) 2015 Wiley Periodicals, Inc.</abstract><date>JUL 2015</date><author>Cabaye, Alexandre
   Nguyen, Kong T.
   Liu, Lihua
   Pande, Vineet
   Schapira, Matthieu</author></paper><paper><title>Towards in vivo estimation of reaction kinetics using high-throughput
   metabolomics data: a maximum likelihood approach</title><abstract>Background: High-throughput assays such as mass spectrometry have opened
   up the possibility for large-scale in vivo measurements of the
   metabolome. This data could potentially be used to estimate kinetic
   parameters for many metabolic reactions. However, high-throughput in
   vivo measurements have special properties that are not taken into
   account in existing methods for estimating kinetic parameters, including
   significant relative errors in measurements of metabolite concentrations
   and reaction rates, and reactions with multiple substrates and products,
   which are sometimes reversible. A new method is needed to estimate
   kinetic parameters taking into account these factors.Results: A new
   method, InVEst (In Vivo Estimation), is described for estimating
   reaction kinetic parameters, which addresses the specific challenges of
   in vivo data. InVEst uses maximum likelihood estimation based on a model
   where all measurements have relative errors. Simulations show that
   InVEst produces accurate estimates for a reversible enzymatic reaction
   with multiple reactants and products, that estimated parameters can be
   used to predict the effects of genetic variants, and that InVEst is more
   accurate than general least squares and graphic methods on data with
   relative errors. InVEst uses the bootstrap method to evaluate the
   accuracy of its estimates.Conclusions: InVEst addresses several
   challenges of in vivo data, which are not taken into account by existing
   methods. When data have relative errors, InVEst produces more accurate
   and robust estimates. InVEst also provides useful information about
   estimation accuracy using bootstrapping. It has potential applications
   of quantifying the effects of genetic variants, inference of the target
   of a mutation or drug treatment and improving flux estimation.</abstract><date>OCT 5 2015</date><author>Zhang, Weiruo
   Kolte, Ritesh
   Dill, David L.</author></paper><paper><title>Fast phase processing in off-axis holography by CUDA including parallel
   phase unwrapping.</title><abstract>We present parallel processing implementation for rapid extraction of
   the quantitative phase maps from off-axis holograms on the Graphics
   Processing Unit (GPU) of the computer using computer unified device
   architecture (CUDA) programming. To obtain efficient implementation, we
   parallelized both the wrapped phase map extraction algorithm and the
   two-dimensional phase unwrapping algorithm. In contrast to previous
   implementations, we utilized unweighted least squares phase unwrapping
   algorithm that better suits parallelism. We compared the proposed
   algorithm run times on the CPU and the GPU of the computer for various
   sizes of off-axis holograms. Using the GPU implementation, we extracted
   the unwrapped phase maps from the recorded off-axis holograms at 35
   frames per second (fps) for 4 mega pixel holograms, and at 129 fps for 1
   mega pixel holograms, which presents the fastest processing framerates
   obtained so far, to the best of our knowledge. We then used common-path
   off-axis interferometric imaging to quantitatively capture the phase
   maps of a micro-organism with rapid flagellum movements. </abstract><date>2016-Feb-22</date><author>Backoach, Ohad
   Kariv, Saar
   Girshovitz, Pinhas
   Shaked, Natan T</author></paper><paper><title>Optical tomography with discretized path integral.</title><abstract>We present a framework for optical tomography based on a path integral.
   Instead of directly solving the radiative transport equations, which
   have been widely used in optical tomography, we use a path integral that
   has been developed for rendering participating media based on the volume
   rendering equation in computer graphics. For a discretized
   two-dimensional layered grid, we develop an algorithm to estimate the
   extinction coefficients of each voxel with an interior point method.
   Numerical simulation results are shown to demonstrate that the proposed
   method works well. </abstract><date>2015-Jul</date><author>Yuan, Bingzhi
   Tamaki, Toru
   Kushida, Takahiro
   Mukaigawa, Yasuhiro
   Kubo, Hiroyuki
   Raytchev, Bisser
   Kaneda, Kazufumi</author></paper><paper><title>Redefining the Autonomic Nerve Distribution of the Bladder Using
   3-Dimensional Image Reconstruction</title><abstract>Purpose: We sought to create a 3-dimensional reconstruction of the
   autonomic nervous tissue innervating the bladder using male and female
   cadaver histopathology.Materials and Methods: We obtained bladder tissue
   from a male and a female cadaver. Axial cross sections of the bladder
   were generated at 3 to 5 mm intervals and stained with S100 protein. We
   recorded the distance between autonomic nerves and bladder mucosa. We
   manually demarcated nerve tracings using ImageScope software (Aperio,
   Vista, California), which we imported into BlenderTM graphics software
   to generate 3-dimensional reconstructions of autonomic nerve
   anatomy.Results: Mean nerve density ranged from 0.099 to 0.602 and 0.012
   to 0.383 nerves per mm(2) in female and male slides, respectively. The
   highest concentrations of autonomic innervation were located in the
   posterior aspect of the bladder neck in the female specimen and in the
   posterior region of the prostatic urethra in the male specimen. Nerve
   density at all levels of the proximal urethra and bladder neck was
   significantly higher in posterior vs anterior regions in female
   specimens (0.957 vs 0.169 nerves per mm(2), p &lt;0.001) and male specimens
   (0.509 vs 0.206 nerves per mm(2), p = 0.04).Conclusions: Novel
   3-dimensional reconstruction of the bladder is feasible and may help
   redefine our understanding of human bladder innervation. Autonomic
   innervation of the bladder is highly focused in the posterior aspect of
   the proximal urethra and bladder neck in male and female bladders.</abstract><date>DEC 2015</date><author>Spradling, Kyle
   Khoyilar, Cyrus
   Abedi, Garen
   Okhunov, Zhamshid
   Wikenheiser, Jamie
   Yoon, Renai
   Huang, Jiaoti
   Youssef, Ramy F.
   Ghoniem, Gamal
   Landman, Jaime</author></paper><paper><title>Motion Aware Exposure Bracketing for HDR Video</title><abstract>Mobile phones and tablets are rapidly gaining significance as
   omnipresent image and video capture devices. In this context we present
   an algorithm that allows such devices to capture high dynamic range
   (HDR) video. The design of the algorithm was informed by a perceptual
   study that assesses the relative importance of motion and dynamic range.
   We found that ghosting artefacts are more visually disturbing than a
   reduction in dynamic range, even if a comparable number of pixels is
   affected by each. We incorporated these findings into a real-time,
   adaptive metering algorithm that seamlessly adjusts its settings to take
   exposures that will lead to minimal visual artefacts after recombination
   into an HDR sequence. It is uniquely suitable for real-time selection of
   exposure settings. Finally, we present an off-line HDR reconstruction
   algorithm that is matched to the adaptive nature of our real-time
   metering approach.</abstract><date>JUL 2015</date><author>Gryaditskaya, Yulia
   Pouli, Tania
   Reinhard, Erik
   Myszkowski, Karol
   Seidel, Hans-Peter</author></paper><paper><title>Large Scale Tissue Morphogenesis Simulation on Heterogenous Systems
   Based on a Flexible Biomechanical Cell Model</title><abstract>The complexity of biological tissue morphogenesis makes in silico
   simulations of such system very interesting in order to gain a better
   understanding of the underlying mechanisms ruling the development of
   multicellular tissues. This complexity is mainly due to two elements:
   firstly, biological tissues comprise a large amount of cells; secondly,
   these cells exhibit complex interactions and behaviors. To address these
   two issues, we propose two tools: the first one is a virtual cell model
   that comprise two main elements: firstly, a mechanical structure
   (membrane, cytoskeleton, and cortex) and secondly, the main behaviors
   exhibited by biological cells, i. e., mitosis, growth, differentiation,
   molecule consumption, and production as well as the consideration of the
   physical constraints issued from the environment. An artificial
   chemistry is also included in the model. This virtual cell model is
   coupled to an agent-based formalism. The second tool is a simulator that
   relies on the OpenCL framework. It allows efficient parallel simulations
   on heterogenous devices such as micro-processors or graphics processors.
   We present two case studies validating the implementation of our model
   in our simulator: cellular proliferation controlled by cell signalling
   and limb growth in a virtual organism.</abstract><date>SEP-OCT 2015</date><author>Jeannin-Girardon, Anne
   Ballet, Pascal
   Rodin, Vincent</author></paper><paper><title>hybridcheck: software for the rapid detection, visualization and dating
   of recombinant regions in genome sequence data</title><abstract>hybridcheck is a software package to visualize the recombination signal
   in large DNA sequence data set, and it can be used to analyse
   recombination, genetic introgression, hybridization and horizontal gene
   transfer. It can scan large (multiple kb) contigs and whole-genome
   sequences of three or more individuals. hybridcheck is written in the r
   software for OS X, Linux and Windows operating systems, and it has a
   simple graphical user interface. In addition, the r code can be readily
   incorporated in scripts and analysis pipelines. hybridcheck implements
   several ABBA-BABA tests and visualizes the effects of hybridization and
   the resulting mosaic-like genome structure in high-density graphics. The
   package also reports the following: (i) the breakpoint positions, (ii)
   the number of mutations in each introgressed block, (iii) the
   probability that the identified region is not caused by recombination
   and (iv) the estimated age of each recombination event. The divergence
   times between the donor and recombinant sequence are calculated using a
   JC, K80, F81, HKY or GTR correction, and the dating algorithm is
   exceedingly fast. By estimating the coalescence time of introgressed
   blocks, it is possible to distinguish between hybridization and
   incomplete lineage sorting. hybridcheck is libre software and it and its
   manual are free to download from .</abstract><date>MAR 2016</date><author>Ward, Ben J.
   van Oosterhout, Cock</author></paper><paper><title>A Survey of Algorithmic Shapes</title><abstract>In the context of computer-aided design, computer graphics and geometry
   processing, the idea of generative modeling is to allow the generation
   of highly complex objects based on a set of formal construction rules.
   Using these construction rules, a shape is described by a sequence of
   processing steps, rather than just by the result of all applied
   operations: shape design becomes rule design. Due to its very general
   nature, this approach can be applied to any domain and to any shape
   representation that provides a set of generating functions. The aim of
   this survey is to give an overview of the concepts and techniques of
   procedural and generative modeling, as well as their applications with a
   special focus on archeology and architecture.</abstract><date>OCT 2015</date><author>Krispel, Ulrich
   Schinko, Christoph
   Ullrich, Torsten</author></paper><paper><title>An efficient FPGA architecture for integer nth root computation</title><abstract>In embedded computing, it is common to find applications such as signal
   processing, image processing, computer graphics or data compression that
   might benefit from hardware implementation for the computation of
   integer roots of order[GRAPHICS]. However, the scientific literature
   lacks architectural designs that implement such operations for different
   values of N, using a low amount of resources. This article presents a
   parameterisable field programmable gate array (FPGA) architecture for an
   efficient Nth root calculator that uses only adders/subtractors
   and[GRAPHICS]location memory elements. The architecture was tested for
   different values of[GRAPHICS], using 64-bit number representation. The
   results show a consumption up to 10% of the logical resources of a
   Xilinx XC6SLX45-CSG324C device, depending on the value of N. The
   hardware implementation improved the performance of its corresponding
   software implementations in one order of magnitude. The architecture
   performance varies from several thousands to seven millions of root
   operations per second.</abstract><date>OCT 3 2015</date><author>Rangel-Valdez, Nelson
   Hugo Barron-Zambrano, Jose
   Torres-Huitzil, Cesar
   Torres-Jimenez, Jose</author></paper><paper><title>Development of fast patient position verification software using 2D-3D
   image registration and its clinical experience</title><abstract>To improve treatment workflow, we developed a graphic processing unit
   (GPU)-based patient positional verification software application and
   integrated it into carbon-ion scanning beam treatment. Here, we
   evaluated the basic performance of the software. The algorithm provides
   2D/3D registration matching using CT and orthogonal X-ray flat panel
   detector (FPD) images. The participants were 53 patients with tumors of
   the head and neck, prostate or lung receiving carbon-ion beam treatment.
   2D/3D-ITchi-Gime (ITG) calculation accuracy was evaluated in terms of
   computation time and registration accuracy. Registration calculation was
   determined using the similarity measurement metrics gradient difference
   (GD), normalized mutual information (NMI), zero-mean normalized
   cross-correlation (ZNCC), and their combination. Registration accuracy
   was dependent on the particular metric used. Representative examples
   were determined to have target registration error (TRE) = 0.45 +/- 0.23
   mm and angular error (AE) = 0.35 +/- 0.18 degrees with ZNCC + GD for a
   head and neck tumor; TRE = 0.12 +/- 0.07 mm and AE = 0.16 +/- 0.07
   degrees with ZNCC for a pelvic tumor; and TRE = 1.19 +/- 0.78 mm and AE
   = 0.83 +/- 0.61 degrees with ZNCC for lung tumor. Calculation time was
   less than 7.26 s.The new registration software has been successfully
   installed and implemented in our treatment process. We expect that it
   will improve both treatment workflow and treatment accuracy.</abstract><date>SEP 2015</date><author>Mori, Shinichiro
   Kumagai, Motoki
   Miki, Kentaro
   Fukuhara, Riki
   Haneishi, Hideaki</author></paper><paper><title>Identification of minimal eukaryotic introns through GeneBase, a
   user-friendly tool for parsing the NCBI Gene databank</title><abstract>We have developed GeneBase, a full parser of the National Center for
   Biotechnology Information (NCBI) Gene database, which generates a fully
   structured local database with an intuitive user-friendly graphic
   interface for personal computers. Features of all the annotated
   eukaryotic genes are accessible through three main software tables,
   including for each entry details such as the gene summary, the gene
   exon/intron structure and the specific Gene Ontology attributions. The
   structuring of the data, the creation of additional calculation fields
   and the integration with nucleotide sequences allow users to make many
   types of comparisons and calculations that are useful for data retrieval
   and analysis. We provide an original example analysis of the existing
   introns across all the available species, through which the classic
   biological problem of the 'minimal intron' may find a solution using
   available data. Based on all currently available data, we can define the
   shortest known eukaryotic GT-AG intron length, setting the physical
   limit at the 30 base pair intron belonging to the human MST1L gene. This
   'model intron' will shed light on the minimal requirement elements of
   recognition used for conventional splicing functioning. Remarkably, this
   size is indeed consistent with the sum of the splicing consensus
   sequence lengths.</abstract><date>DEC 2015</date><author>Piovesan, Allison
   Caracausi, Maria
   Ricci, Marco
   Strippoli, Pierluigi
   Vitale, Lorenza
   Pelleri, Maria Chiara</author></paper><paper><title>RViz: a toolkit for real domain data visualization</title><abstract>In computational science and computer graphics, there is a strong
   requirement to represent and visualize information in the real domain,
   and many visualization data structures and algorithms have been proposed
   to achieve this aim. Unfortunately, the dataflow model that is often
   selected to address this issue in visualization systems is not flexible
   enough to visualize newly invented data structures and algorithms
   because this scheme can accept only specific data structures. To address
   this problem, we propose a new visualization tool, RViz, which is
   independent of the input information data structures. Since there is no
   requirement for additional efforts to manage the flow networks and the
   interface to abstracted information is simple in RViz, any scientific
   information visualization algorithms are easier to implement than the
   dataflow model. In this paper, we provide case studies in which we have
   successfully implemented new data structures and related algorithms
   using RViz, including geometry synthesis, distance field representation,
   and implicit surface reconstruction. Through these cases, we show how
   RViz helps users visualize and understand any hidden insights in input
   information.</abstract><date>OCT 2015</date><author>Kam, Hyeong Ryeol
   Lee, Sung-Ho
   Park, Taejung
   Kim, Chang-Hun</author></paper><paper><title>Quantitative multi-spectral oxygen saturation measurements independent
   of tissue optical properties</title><abstract>Imaging of tissue oxygenation is important in several applications
   associated with patient care. Optical sensing is commonly applied for
   assessing oxygen saturation but is often restricted to local
   measurements or else it requires spectral and spatial information at the
   expense of time. Many methods proposed so far require assumptions on the
   properties of measured tissue. In this study we investigated a
   computational method that uses only multispectral information and
   quantitatively computes tissue oxygen saturation independently of tissue
   optical properties. The method is based on linear transformations of
   measurements in three isosbestic points. We investigated the ideal
   isosbestic point combination out of six isosbestic points available for
   measurement in the visible and near-infrared region that enable accurate
   oxygen saturation computation. We demonstrate this method on controlled
   tissue mimicking phantoms having different optical properties and
   validated the measurements using a gas analyzer. A mean error of 2.9 +/-
   2.8% O(2)Sat was achieved. Finally, we performed pilot studies in
   tissues in-vivo by measuring dynamic changes in fingers subjected to
   vascular occlusion, the vasculature of mouse ears and exposed mouse
   organs.[GRAPHICS]Selected steps of spectral transformations applied to
   oxygenation spectra. The original reflectance spectrum M(lambda) is
   transformed in step 1 to overlap with reference spectra (grey) in three
   isosbestic points, resulting in M ''(lambda). In step 2, the gradient of
   M ''(lambda) is computed resulting in M-grad ''(lambda), which can be
   used for quantitative oxygenation computation.</abstract><date>JAN 2016</date><author>Radrich, Karin
   Ntziachristos, Vasilis</author></paper><paper><title>Q&amp;A: The dinosaur doctor.</title><abstract></abstract><date>2015-Jun-4</date><author>Horner, Jack
   Hoffman, Jascha</author></paper><paper><title>MULTEM: A new multislice program to perform accurate and fast electron
   diffraction and imaging simulations using Graphics Processing Units with
   CUDA</title><abstract>The main features and the GPU implementation of the MULTEM program are
   presented and described. This new program performs accurate and fast
   multislice simulations by including higher order expansion of the
   multislice solution of the high energy Schrodinger equation, the correct
   subslicing of the three-dimensional potential and top-bottom surfaces.
   The program implements different kinds of simulation for CTEM, STEM, ED,
   PED, CBED, ADF-TEM and ABF-HC with proper treatment of the spatial and
   temporal incoherences. The multislice approach described here treats the
   specimen as amorphous material which allows a straightforward
   implementation of the frozen phonon approximation. The generalized
   transmission function for each slice is calculated when is needed and
   then discarded. This allows us to perform large simulations that can
   include millions of atoms and keep the computer memory requirements to a
   reasonable level. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>SEP 2015</date><author>Lobato, I.
   Van Dyck, D.</author></paper><paper><title>Aurore: new software for neutron reflectivity data analysis</title><abstract>Aurore is a free software application based on MATLAB scripts designed
   for the graphical analysis, inspection and simulation of neutron
   reflectivity data. Its architecture, combined with graphics and other
   advantages of the MATLAB environment, should allow continued development
   of this software and inclusion of new features and analysis methods. The
   development of the software was driven by the necessity for a
   non-commercial open-source application for the analysis of neutron
   reflectivity data. Aurore provides a robust and reliable method for
   evaluation of parameter uncertainty, a feature almost absent in similar
   software applications. In the present paper the main functionalities of
   the software are presented, together with a comprehensive description of
   the modeling approaches available at the moment. The code is released
   under a Creative Commons Attribution Non-Commercial License V2.0. The
   software application can be downloaded at
   http://aurorenr.sourceforge.net/.</abstract><date>FEB 2016</date><author>Gerelli, Yuri</author></paper><paper><title>Rare plants distribution modeling using indirect environmental gradients</title><abstract>Knowledge of geographic and ecological pattern of rare and endangered
   species distribution is of essential importance for understanding human
   activities threatening biodiversity. This study aims to show how
   quantitative data for the abundance of rare plant species in a localized
   territory can be modeled successfully. Ordination and species
   distribution models (SDMs) combine field observation of species
   abundance data with environmental gradients, also measured on the field,
   to produce statistically tested for their accuracy, graphic (biplots,
   response surfaces, etc.) and numerical models which can then be used for
   conservation or management of rare plants. Species distribution
   modeling, shown in this study, can be successfully used for probability
   assessment for finding unknown populations of rare plants as well as
   guiding tool in their search, saving considerable resources and time.</abstract><date>JUL 2015</date><author>Dyakov, N. R.</author></paper><paper><title>Theoretical studies on the reaction of mono- and ditriflate derivatives
   of 1,4:3,6-dianhydro-D-mannitol with trimethylamine-Can a quaternary
   ammonium salt be a source of the methyl group?</title><abstract>DFT studies on the mechanism of the formation of "gemini" quaternary
   ammonium salts in the reaction of 1,4:3,6-dianhydro-d-mannitol
   ditriflate derivative with trimethylamine and its subsequent conversion
   to tertiary amine through the methyl-transfer reaction are discussed.
   Two alternative reaction pathways are presented in the gas phase and in
   ethanol. Additionally, the transformation of the monotriflate derivative
   of 1,4:3,6-dianhydro-d-mannitol into the single quaternary ammonium salt
   is presented. Two functionals (B3LYP, M062X) and two basis sets
   (6-31+G** and 6-311++G**) were used for the calculations. The effect of
   the substituent attached to the five-membered rings at the C2 (and/or
   C5) carbon atom on the activation barrier is described. The
   trimethylammonium group bond to the five-membered ring greatly reduces
   the activation barrier height. The preferred reaction pathway for the
   conversions was established. Including the London dispersion in the
   calculations increases the stabilization of all the points on the
   potential energy surface in relation to individual reactants.[GRAPHICS].</abstract><date>JAN 2016</date><author>Bednarko, Justyna
   Wielinska, Justyna
   Sikora, Karol
   Liberek, Beata
   Nowacki, Andrzej</author></paper><paper><title>Spherical Fibonacci Mapping</title><abstract>Spherical Fibonacci point sets yield nearly uniform point distributions
   on the unit sphere S-2 subset of R-3. The forward generation of these
   point sets has been widely researched and is easy to implement, such
   that they have been used in various applications.Unfortunately, the lack
   of an efficient mapping from points on the unit sphere to their closest
   spherical Fibonacci point set neighbors rendered them impractical for a
   wide range of applications, especially in computer graphics. Therefore,
   we introduce an inverse mapping from points on the unit sphere which
   yields the nearest neighbor in an arbitrarily sized spherical Fibonacci
   point set in constant time, without requiring any precomputations or
   table lookups.We show how to implement this inverse mapping on GPUs
   while addressing arising floating point precision problems. Further, we
   demonstrate the use of this mapping and its variants, and show how to
   apply it to fast unit vector quantization. Finally, we illustrate the
   means by which to modify this inverse mapping for texture mapping with
   smooth filter kernels and showcase its use in the field of procedural
   modeling.</abstract><date>NOV 2015</date><author>Keinert, Benjamin
   Innmann, Matthias
   Saenger, Michael
   Stamminger, Marc</author></paper><paper><title>GRAPHICS PROCESSING UNIT COMPUTATIONS FOR FINITE ELEMENT OPTIMIZATION: A
   REVIEW AND SOME ISSUES TO BE ADDRESSED</title><abstract>In finite element optimization, the computational load is a limiting
   issue. Parallelization has been the preferred route to overcome this
   problem but was again limited by the cost of computers and the number of
   processors available. The graphics processing unit (GPU) on a PC
   provides a means of implementing the massive computations on numerous
   parallel threads cheaply on PCs. The purpose of this is to review finite
   element matrix equation solution on the GPU and point out areas where
   further investigation is warranted. Our intention is to direct
   computational research and computer architecture development so that we
   may use the GPU better for more effective computational parallelization
   in finite element field computation.</abstract><date>JUL-SEP 2015</date><author>Sivasuthan, Sivamayam
   Karthik, Victor U.
   Mathialakan, Thavappiragasam
   Rawashdeh, Mohammad R.
   Jayakumar, Paramsothy
   Thyagarajan, Ravi S.
   Hoole, S. Ratnajeevan. H.</author></paper><paper><title>Sub-second pencil beam dose calculation on GPU for adaptive proton
   therapy</title><abstract>Although proton therapy delivered using scanned pencil beams has the
   potential to produce better dose conformity than conventional
   radiotherapy, the created dose distributions are more sensitive to
   anatomical changes and patient motion. Therefore, the introduction of
   adaptive treatment techniques where the dose can be monitored as it is
   being delivered is highly desirable. We present a GPU-based dose
   calculation engine relying on the widely used pencil beam algorithm,
   developed for on-line dose calculation. The calculation engine was
   implemented from scratch, with each step of the algorithm parallelized
   and adapted to run efficiently on the GPU architecture. To ensure fast
   calculation, it employs several application-specific modifications and
   simplifications, and a fast scatter-based implementation of the
   computationally expensive kernel superposition step. The calculation
   time for a skull base treatment plan using two beam directions was 0.22s
   on an Nvidia Tesla K40 GPU, whereas a test case of a cubic target in
   water from the literature took 0.14s to calculate. The accuracy of the
   patient dose distributions was assessed by calculating the gamma-index
   with respect to a gold standard Monte Carlo simulation. The passing
   rates were 99.2% and 96.7%, respectively, for the 3%/3 mm and 2%/2 mm
   criteria, matching those produced by a clinical treatment planning
   system.</abstract><date>JUN 21 2015</date><author>da Silva, Joakim
   Ansorge, Richard
   Jena, Rajesh</author></paper><paper><title>A new teiid lizard from the Late Cretaceous of the Hateg Basin, Romania
   and its phylogenetic and palaeobiogeographical relationships</title><abstract>A new lizard genus and species is described based on a
   three-dimensionally preserved partial skull and associated lower jaws
   from the Pui Islaz locality (Late Cretaceous, early Maastrichtian) in
   the Hateg Basin, western Romania. Barbatteius vremiri gen. et sp. nov.
   is diagnosed by a unique combination of symplesiomorphies and
   synapomorphies. A nested set of synapomorphies support assigning
   Barbatteius to Teiidae as the first unambiguous Late Cretaceous record
   of this family from Laurasia. Barbatteius differs from other teiids by
   having more extensive osteodermal sculpture on the skull roof and
   suspensorium, and by a pentagonal occipital osteoscute exhibiting more
   or less parallel lateral margins. Barbatteius is a large-bodied lizard,
   estimated to be up to 800 mm in total length. It has weakly heterodont
   dentition, but without enlarged posterior crushing teeth, suggesting
   that it fed on arthropods, small vertebrates and plants. The mix of taxa
   with affinities to Euramerica (paramacellodid and borioteiioid lizards)
   and Gondwana (madtsoiid snakes and the teiid Barbatteius) currently
   known for the Maastrichtian squamate assemblage from Hateg Basin
   supports the growing realization that 'Hateg Island' has a complex
   palaeobiogeographical history.[GRAPHICS]</abstract><date>MAR 3 2016</date><author>Venczel, Marton
   Codrea, Vlad A.</author></paper><paper><title>Medical student preferences for self-directed study resources in gross
   anatomy.</title><abstract>Gross anatomy instruction in medical curricula involve a range of
   resources and activities including dissection, prosected specimens,
   anatomical models, radiological images, surface anatomy, textbooks,
   atlases, and computer-assisted learning (CAL). These resources and
   activities are underpinned by the expectation that students will
   actively engage in self-directed study (SDS) to enhance their knowledge
   and understanding of anatomy. To gain insight into preclinical versus
   clinical medical students' preferences for SDS resources for learning
   gross anatomy, and whether these vary on demographic characteristics and
   attitudes toward anatomy, students were surveyed at two Australian
   medical schools, one undergraduate-entry and the other graduate-entry.
   Lecture/tutorial/practical notes were ranked first by 33% of 156
   respondents (mean rank+-SD, 2.48+-1.38), textbooks by 26% (2.62+-1.35),
   atlases 20% (2.80+-1.44), videos 10% (4.34+-1.68), software 5%
   (4.78+-1.50), and websites 4% (4.24+-1.34). Among CAL resources, Wikipedia
   was ranked highest. The most important factor in selecting CAL resources
   was cost (ranked first by 46%), followed by self-assessment, ease of
   use, alignment with curriculum, and excellent graphics (each 6-9%).
   Compared with preclinical students, clinical students ranked software
   and Acland's Video Atlas of Human Anatomy higher and felt radiological
   images were more important in selecting CAL resources. Along with other
   studies reporting on the quality, features, and impact on learning of
   CAL resources, the diversity of students' preferences and opinions on
   usefulness and ease of use reported here can help guide faculty in
   selecting and recommending a range of CAL and other resources to their
   students to support their self-directed study. Anat Sci Educ 9: 150-160.
    2015 American Association of Anatomists. </abstract><date>2016-Mar</date><author>Choi-Lundberg, Derek L
   Low, Tze Feng
   Patman, Phillip
   Turner, Paul
   Sinha, Sankar N</author></paper><paper><title>A Hybrid CPU/GPU Pattern-Matching Algorithm for Deep Packet Inspection</title><abstract>The large quantities of data now being transferred via high-speed
   networks have made deep packet inspection indispensable for security
   purposes. Scalable and low-cost signature-based network intrusion
   detection systems have been developed for deep packet inspection for
   various software platforms. Traditional approaches that only involve
   central processing units (CPUs) are now considered inadequate in terms
   of inspection speed. Graphic processing units (GPUs) have superior
   parallel processing power, but transmission bottlenecks can reduce
   optimal GPU efficiency. In this paper we describe our proposal for a
   hybrid CPU/GPU pattern-matching algorithm (HPMA) that divides and
   distributes the packet-inspecting workload between a CPU and GPU. All
   packets are initially inspected by the CPU and filtered using a simple
   pre-filtering algorithm, and packets that might contain malicious
   content are sent to the GPU for further inspection. Test results
   indicate that in terms of random payload traffic, the matching speed of
   our proposed algorithm was 3.4 times and 2.7 times faster than those of
   the AC-CPU and AC-GPU algorithms, respectively. Further, HPMA achieved
   higher energy efficiency than the other tested algorithms.</abstract><date>OCT 5 2015</date><author>Lee, Chun-Liang
   Lin, Yi-Shan
   Chen, Yaw-Chung</author></paper><paper><title>Finite element modelling of elastic wave scattering within a
   polycrystalline material in two and three dimensions</title><abstract>Finite element modelling is a promising tool for further progressing the
   development of ultrasonic non-destructive evaluation of polycrystalline
   materials. Yet its widespread adoption has been held back due to a high
   computational cost, which has restricted current works to relatively
   small models and to two dimensions. However, the emergence of
   sufficiently powerful computing, such as highly efficient solutions on
   graphics processors, is enabling a step improvement in possibilities.
   This article aims to realise those capabilities to simulate ultrasonic
   scattering of longitudinal waves in an equiaxed polycrystalline material
   in both two (2D) and three dimensions (3D). The modelling relies on an
   established Voronoi approach to randomly generate a representative grain
   morphology. It is shown that both 2D and 3D numerical data show good
   agreement across a range of scattering regimes in comparison to
   well-established theoretical predictions for attenuation and phase
   velocity. In addition, 2D parametric studies illustrate the mesh
   sampling requirements for two different types of mesh to ensure
   modelling accuracy and present useful guidelines for future works.
   Modelling limitations are also shown. It is found that 2D models reduce
   the scattering mechanism in the Rayleigh regime. (C) 2015 Acoustical
   Society of America.</abstract><date>OCT 2015</date><author>Van Pamel, Anton
   Brett, Colin R.
   Huthwaite, Peter
   Lowe, Michael J. S.</author></paper><paper><title>NBODY6++GPU: ready for the gravitational million-body problem</title><abstract>Accurate direct N-body simulations help to obtain detailed information
   about the dynamical evolution of star clusters. They also enable
   comparisons with analytical models and Fokker-Planck or Monte Carlo
   methods. NBODY6 is a well-known direct N-body code for star clusters,
   and NBODY6++ is the extended version designed for large particle number
   simulations by supercomputers. We present NBODY6++ GPU, an optimized
   version of NBODY6++ with hybrid parallelization methods (MPI, GPU,
   OpenMP, and AVX/SSE) to accelerate large direct N-body simulations, and
   in particular to solve the million-body problem. We discuss the new
   features of the NBODY6++ GPU code, benchmarks, as well as the first
   results from a simulation of a realistic globular cluster initially
   containing a million particles. For million-body simulations, NBODY6++
   GPU is 400-2000 times faster than NBODY6 with 320 CPU cores and 32
   NVIDIA K20X GPUs. With this computing cluster specification, the
   simulations of million-body globular clusters including 5 per cent
   primordial binaries require about an hour per half-mass crossing time.</abstract><date>JUL 11 2015</date><author>Wang, Long
   Spurzem, Rainer
   Aarseth, Sverre
   Nitadori, Keigo
   Berczik, Peter
   Kouwenhoven, M. B. N.
   Naab, Thorsten</author></paper><paper><title>Synthesis, antiproliferative activity, and molecular docking studies of
   curcumin analogues bearing pyrazole ring</title><abstract>Several curcumin analogues bearing pyrazole were synthesized and
   characterized by IR, NMR, and mass spectral data. There were four tested
   compounds among 11 synthesized compounds, which were evaluated for
   antiproliferative activity and showed significant activity in both
   one-dose and five-dose assays. The antiproliferative effects were tested
   on a panel of 60 cell lines, according to the National Cancer Institute
   screening protocol. The most active compounds among the series were
   3,5-bis(4-hydroxy-3-methylstyryl)-1H-pyrazole-1-carboxamide (3k) which
   showed mean percent growth inhibition of 116.09 in one-dose assay at 10
   A mu M, and GI(50) values were ranging between 0.0912 and 2.36 A mu M in
   five-dose assay. The best results were recorded on the leukaemia cell
   lines with value ranging from 0.0912 to 0.365 A mu M. All the tested
   compounds showed broad-spectrum antiproliferative activity over
   different cancer cell lines. When compared with the standard drug
   paclitaxel, the compound 3k showed superior activity on nearly 42 cell
   lines. The molecular docking study was performed to explore the binding
   interaction of these curcumin analogues with the active site of EGFR
   tyrosine kinase (EGFR-TK). The hydroxyl group of both phenyl rings was
   important for the rein-geminated hydrogen bonding by either side chain
   or backbone with the active site of EGFR-TK.Four curcumin analogues were
   evaluated for their antiproliferative activity and showed promising
   results. The molecular docking studies showed that all the compounds
   (3a-k) were well accommodated in the EGFR tyrosine kinase.[GRAPHICS].</abstract><date>DEC 2015</date><author>Ahsan, Mohamed Jawed
   Choudhary, Kavita
   Jadav, Surender Singh
   Yasmin, Sabina
   Ansari, Md. Yousuf
   Sreenivasulu, Reddymasu</author></paper><paper><title>Elastic moduli of simple mass spring models</title><abstract>Mass spring models (MSMs) are a popular choice for representation of
   soft bodies in computer graphics and virtual reality applications. In
   this paper, we investigate physical properties of the simplest MSMs
   composed of mass points and linear springs. The nodes are either placed
   on a cubic lattice or positioned randomly within the system. We
   calculate the elastic moduli for such models and relate the results to
   other studies. We show that there is a well-defined relationship between
   the geometric characteristics of the MSM systems and physical properties
   of the modeled materials. It is also demonstrated that these models
   exhibit a proper convergence to a unique solution upon mesh refinement
   and thus can represent elastic materials with a high precision.</abstract><date>OCT 2015</date><author>Kot, Maciej
   Nagahashi, Hiroshi
   Szymczak, Piotr</author></paper><paper><title>Cytopathology whole slide images and adaptive tutorials for postgraduate
   pathology trainees: a randomized crossover tirial</title><abstract>To determine whether cytopathology whole slide images and virtual
   microscopy adaptive tutorials aid learning by postgraduate trainees, we
   designed a randomized crossover trial to evaluate the quantitative and
   qualitative impact of whole slide images and virtual microscopy adaptive
   tutorials compared with traditional glass slide and textbook methods of
   learning cytopathology. Forty-three anatomical pathology registrars were
   recruited from Australia, New Zealand, and Malaysia. Online assessments
   were used to determine efficacy, whereas user experience and perceptions
   of efficiency were evaluated using online Likert scales and open-ended
   questions. Outcomes of online assessments indicated that, with respect
   to performance, learning with whole slide images and virtual microscopy
   adaptive tutorials was equivalent to using traditional methods.
   High-impact learning, efficiency, and equity of learning from virtual
   microscopy adaptive tutorials were strong themes identified in
   open-ended responses. Participants raised concern about the lack of
   z-axis capability in the cytopathology whole slide images, suggesting
   that delivery of z-stacked whole slide images online may be important
   for future educational development. In this trial, learning
   cytopathology with whole slide images and virtual microscopy adaptive
   tutorials was found to be as effective as and perceived as more
   efficient than learning from glass slides and textbooks. The use of
   whole slide images and virtual microscopy adaptive tutorials has the
   potential to provide equitable access to effective learning from
   teaching material of consistently high quality. It also has broader
   implications for continuing professional development and maintenance of
   competence and quality assurance in specialist practice. (C) 2015
   Elsevier Inc. All rights reserved.</abstract><date>SEP 2015</date><author>Van Es, Simone L.
   Kumar, Rakesh K.
   Pryor, Wendy M.
   Salisbury, Elizabeth L.
   Velan, Gary M.</author></paper><paper><title>Improving Performance of Image Retrieval Based on Fuzzy Colour
   Histograms by Using Hybrid Colour Model and Genetic Algorithm</title><abstract>A hybrid colour model is a colour descriptor formed by combining
   channels from several different colour models. Although rarely used in
   computer graphics applications due to redundancy, hybrid colour models
   may be of interest for the Content-Based Image Retrieval (CBIR). Best
   features of each colour model can be combined to obtain optimal
   retrieval performance. This paper evaluates several approaches to the
   construction of a hybrid colour model that is used to construct a fuzzy
   colour histogram of image as a compact feature for retrieval. By
   evaluating each channel separately, a colour model named HSY is
   proposed. Various parameters of fuzzy histogram are further improved
   using Genetic algorithm (GA). Using standard data sets and the Average
   Normalized Modified Retrieval Rank (ANMRR) as a metric for retrieval
   performance, it is shown that this novel approach can give an improved
   retrieval performance.</abstract><date>DEC 2015</date><author>Ljubovic, V.
   Supic, H.</author></paper><paper><title>Particle-based shallow water simulation for irregular and sparse
   simulation domains</title><abstract>We propose a shallow water simulation using a Lagrangian technique.
   Smoothed particle hydrodynamics are used to solve the shallow water
   equation, so we avoid discretization of the entire simulation domain and
   easily handle sparse and irregular simulation domains. In the context of
   shallow water equations, much less attention has been paid to Lagrangian
   simulation methods than to Eulerian methods. Therefore, many problems
   remained unsolved, which prevents the practical use of Lagrangian
   shallow water simulations in computer graphics. We concentrate on
   several issues associated with the simulation. First, we increase the
   accuracy of the smoothed particle hydrodynamics approximation by
   applying a correction to the kernel function that is used in the
   simulation. Second, we introduce a novel boundary handling algorithm
   that can handle arbitrary boundary domains; even irregular and
   complicated boundaries do not pose a problem and introduce only small
   computational overhead. Third, with the increased accuracy, we use the
   fluid height to generate a flat fluid surface. All the proposed methods
   can easily be integrated into the smoothed particle hydrodynamics
   framework. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>DEC 2015</date><author>Chladek, Michal
   Durikovic, Roman</author></paper><paper><title>Computer Modeling and Simulation of Fruit Sunscald</title><abstract>Although the simulation of many kinds of natural phenomena has been
   studied in the field of computer simulation and graphics, the
   reproduction of natural fruit diseases processes has not received much
   attention. Sunscald is the representative of physiological diseases.
   This paper presented a novel computer modeling and simulation method for
   fruit sunscald. We mainly focus on the morphology change of fruit
   appearance affected by sunscald disease under the condition of water
   loss. An improved mass spring model is proposed, by combining cell
   turgor pressure with mass spring. We adopt this physical deformation
   model for dynamic simulation of fruit sunscald disease. We calculate the
   cell turgor pressure variation due to water loss and thereby get the
   displacement changing of every mass particle in our simulation system.
   Finally, the software of Maya is adopted to render the deform model to
   render the simulation result. Experiments demonstrate the effectiveness
   of our method.</abstract><date>JUL 2015</date><author>Liu, Shiguang
   Fan, Dongfang</author></paper><paper><title>Applying and exploring a new modeling approach of functional
   connectivity regarding ecological network: A case study on the dynamic
   lines of space syntax</title><abstract>The construction of ecological network or other continuous habitat is
   essential for urban eco-system; however, to quantify the heterogeneous
   functional connectivity for eco-network is academically attractive and
   challenging. The dynamic lines of space syntax, tenable to simulate
   perception and navigation flows in network-configured human settlements,
   is introduced to inspire idea and approach to modeling connectivity in
   eco-network, while the classical graphic notions and variables are
   assumed functional to new relationship between other species and
   eco-network. After mapping continuous functional components among land
   layout into free space and then into dynamic lines that influence
   bio-flows, the paper conducts an exploration on functional connectivity
   of Singapore's green network. Conclusions involves the distribution
   heterogeneity of basic variables, Connectivity, Control, Mean Depth and
   Integration, demonstrating each meaning for functional connectivity in
   the network, with a step-wise Integration further comparing the
   connectivity patterns under different behavioral ranges. Moreover, a
   scale robustness determined via linear regression between Integration
   and Connectivity reveals network functionality as behavioral scale
   varying. The analogical modeling of space syntax raised in this paper is
   adaptive and instructive, particularly if original essential traits
   remain valid between the substituted species and space, because several
   principles and characteristics of conventional connectivity models can
   be logically inherited, while the graphic notions of dynamic line shares
   unique advantages. (C) 2014 Elsevier B.V. All rights reserved.</abstract><date>DEC 24 2015</date><author>Yang Tianxiang
   Jing Dong
   Wang Shoubing</author></paper><paper><title>Graphics processing unit-based alignment of protein interaction networks</title><abstract>Network alignment is an important bridge to understanding human
   protein-protein interactions (PPIs) and functions through model
   organisms. However, the underlying subgraph isomorphism problem
   complicates and increases the time required to align protein interaction
   networks (PINs). Parallel computing technology is an effective solution
   to the challenge of aligning large-scale networks via sequential
   computing. In this study, the typical Hungarian-Greedy Algorithm (HGA)
   is used as an example for PIN alignment. The authors propose a HGA with
   2-nearest neighbours (HGA-2N) and implement its graphics processing unit
   (GPU) acceleration. Numerical experiments demonstrate that HGA-2N can
   find alignments that are close to those found by HGA while dramatically
   reducing computing time. The GPU implementation of HGA-2N optimises the
   parallel pattern, computing mode and storage mode and it improves the
   computing time ratio between the CPU and GPU compared with HGA when
   large-scale networks are considered. By using HGA-2N in GPUs, conserved
   PPIs can be observed, and potential PPIs can be predicted. Among the
   predictions based on 25 common Gene Ontology terms, 42.8% can be found
   in the Human Protein Reference Database. Furthermore, a new method of
   reconstructing phylogenetic trees is introduced, which shows the same
   relationships among five herpes viruses that are obtained using other
   methods.</abstract><date>AUG 2015</date><author>Xie, Jiang
   Zhou, Zhonghua
   Ma, Jin
   Xiang, Chaojuan
   Nie, Qing
   Zhang, Wu</author></paper><paper><title>Affective surfing in the visualized interface of a digital library for
   children</title><abstract>The uncertainty children experience when searching for information
   influences their information seeking behavior by stimulating curiosity
   or hindering their search efforts. This study explored the interactions
   and the usability of various search interfaces, and the enjoyment or
   uncertainty experienced by children when using them. Structural Equation
   Modeling was used to determine whether children feel uncertainty or a
   sense of control when using virtual game-like interfaces to search for
   information associated with entertainment or as a means to satisfy an
   assigned learning task. We then analyzed the weight relationships among
   three latent variables (information needs, interface media, and
   affective state) using statistical (path) analysis. Our results indicate
   that children prefer using a retrieval interface with situated
   affordance to satisfy entertainment-related information needs, as
   opposed to searching for information to solve specific problems.
   Furthermore, their perceptions of text and graphic icons determined the
   degree to which they experienced a sense of uncertainty or control. When
   searching for entertainment-related information, they were better able
   to deal with uncertainty and sought greater control in their search
   interface, compared to when they were searching for information related
   to assigned tasks. According to their information needs, children may
   regard a game-like interface as a toy or a tool for learning. The
   results of this study can serve as reference for the future development
   of information search interfaces aimed at arousing the interest of
   children. The use of virtual game-like interfaces to guide the IS
   behavior of children warrants further study. (C) 2015 Elsevier Ltd. All
   rights reserved.</abstract><date>JUL 2015</date><author>Wu, Ko-Chiu</author></paper><paper><title>MIA: Mutual Information Analyzer, a graphic user interface program that
   calculates entropy, vertical and horizontal mutual information of
   molecular sequence sets</title><abstract>Background: Short and long range correlations in biological sequences
   are central in genomic studies of covariation. These correlations can be
   studied using mutual information because it measures the amount of
   information one random variable contains about the other. Here we
   present MIA (Mutual Information Analyzer) a user friendly graphic
   interface pipeline that calculates spectra of vertical entropy (VH),
   vertical mutual information (VMI) and horizontal mutual information
   (HMI), since currently there is no user friendly integrated platform
   that in a single package perform all these calculations. MIA also
   calculates Jensen-Shannon Divergence (JSD) between pair of different
   species spectra, herein called informational distances. Thus, the
   resulting distance matrices can be presented by distance histograms and
   informational dendrograms, giving support to discrimination of closely
   related species.Results: In order to test MIA we analyzed sequences from
   Drosophila Adh locus, because the taxonomy and evolutionary patterns of
   different Drosophila species are well established and the gene Adh is
   extensively studied. The search retrieved 959 sequences of 291 species.
   From the total, 450 sequences of 17 species were selected. With this
   dataset MIA performed all tasks in less than three hours: gathering,
   storing and aligning fasta files; calculating VH, VMI and HMI spectra;
   and calculating JSD between pair of different species spectra. For each
   task MIA saved tables and graphics in the local disk, easily accessible
   for future analysis.Conclusions: Our tests revealed that the
   "informational model free" spectra may represent species signatures.
   Since JSD applied to Horizontal Mutual Information spectra resulted in
   statistically significant distances between species, we could calculate
   respective hierarchical clusters, herein called Informational
   Dendrograms (ID). When compared to phylogenetic trees all Informational
   Dendrograms presented similar taxonomy and species clusterization.</abstract><date>DEC 10 2015</date><author>Lichtenstein, Flavio
   Antoneli, Fernando, Jr.
   Briones, Marcelo R. S.</author></paper><paper><title>Doppler Time-of-Flight Imaging</title><abstract>Over the last few years, depth cameras have become increasingly popular
   for a range of applications, including human-computer interaction and
   gaming, augmented reality, machine vision, and medical imaging. Many of
   the commercially-available devices use the time-of-flight principle,
   where active illumination is temporally coded and analyzed in the camera
   to estimate a per-pixel depth map of the scene. In this paper, we
   propose a fundamentally new imaging modality for all time-of-flight
   (ToF) cameras: per-pixel radial velocity measurement. The proposed
   technique exploits the Doppler effect of objects in motion, which shifts
   the temporal illumination frequency before it reaches the camera. Using
   carefully coded illumination and modulation frequencies of the ToF
   camera, object velocities directly map to measured pixel intensities. We
   show that a slight modification of our imaging system allows for color,
   depth, and velocity information to be captured simultaneously. Combining
   the optical flow computed on the RGB frames with the measured metric
   radial velocity allows us to further estimate the full 3D metric
   velocity field of the scene. The proposed technique has applications in
   many computer graphics and vision problems, for example motion tracking,
   segmentation, recognition, and motion deblurring.</abstract><date>AUG 2015</date><author>Heide, Felix
   Heidrich, Wolfgang
   Hullin, Matthias
   Wetzstein, Gordon</author></paper><paper><title>Real-time finite element structural analysis in augmented reality</title><abstract>Conventional finite element analysis (FEA) is usually carried out in
   offsite and virtual environments, i.e., computer-generated graphics,
   which does not promote a user's perception and interaction, and limits
   its applications. With the purpose of enhancing structural analysis with
   augmented reality (AR) technologies, the paper presents a system which
   integrates sensor measurement and real-time FEA simulation into an
   AR-based environment. By incorporating scientific visualization
   technologies, this system superimposes FEA results directly on
   real-world objects, and provides intuitive interfaces for enhanced data
   exploration. A wireless sensor network has been integrated into the
   system to acquire spatially distributed loads, and a method to register
   the sensors onsite has been developed. Real-time PEA methods are
   employed to generate fast solutions in response to load variations. As a
   case study, this system is applied to monitor the stresses of a step
   ladder under actual loading conditions. The relationships among
   accuracy, mesh resolution and frame rate are investigated. (C) 2015
   Elsevier Ltd. All rights reserved.</abstract><date>SEP 2015</date><author>Huang, J. M.
   Ong, S. K.
   Nee, A. Y. C.</author></paper><paper><title>Visualization of multi-property landscapes for compound selection and
   optimization.</title><abstract>Compound optimization generally requires considering multiple properties
   in concert and reaching a balance between them. Computationally, this
   process can be supported by multi-objective optimization methods that
   produce numerical solutions to an optimization task. Since a variety of
   comparable multi-property solutions are usually obtained further
   prioritization is required. However, the underlying multi-dimensional
   property spaces are typically complex and difficult to rationalize.
   Herein, an approach is introduced to visualize multi-property landscapes
   by adapting the concepts of star and parallel coordinates from computer
   graphics. The visualization method is designed to complement
   multi-objective compound optimization. We show that visualization makes
   it possible to further distinguish between numerically equivalent
   optimization solutions and helps to select drug-like compounds from
   multi-dimensional property spaces. The methodology is intuitive,
   applicable to a wide range of chemical optimization problems, and made
   freely available to the scientific community. </abstract><date>2015-Aug</date><author>de la Vega de Leon, Antonio
   Kayastha, Shilva
   Dimova, Dilyana
   Schultz, Thomas
   Bajorath, Jurgen</author></paper><paper><title>More Efficient Virtual Shadow Maps for Many Lights</title><abstract>Recently, several algorithms have been introduced that enable real-time
   performance for many lights in applications such as games. In this
   paper, we explore the use of hardware-supported virtual cube-map shadows
   to efficiently implement high-quality shadows from hundreds of light
   sources in real time and within a bounded memory footprint. In addition,
   we explore the utility of ray tracing for shadows from many lights and
   present a hybrid algorithm combining ray tracing with cube maps to
   exploit their respective strengths. Our solution supports real-time
   performance with hundreds of lights in fully dynamic high-detail scenes.</abstract><date>JUN 2015</date><author>Olsson, Ola
   Billeter, Markus
   Sintorn, Erik
   Kampe, Viktor
   Assarsson, Ulf</author></paper><paper><title>MASH Suite Pro: A Comprehensive Software Tool for Top-Down Proteomics</title><abstract>Top-down mass spectrometry (MS)-based proteomics is arguably a
   disruptive technology for the comprehensive analysis of all proteoforms
   arising from genetic variation, alternative splicing, and
   posttranslational modifications (PTMs). However, the complexity of
   top-down high-resolution mass spectra presents a significant challenge
   for data analysis. In contrast to the well-developed software packages
   available for data analysis in bottom-up proteomics, the data analysis
   tools in top-down proteomics remain underdeveloped. Moreover, despite
   recent efforts to develop algorithms and tools for the deconvolution of
   top-down high-resolution mass spectra and the identification of proteins
   from complex mixtures, a multifunctional software platform, which allows
   for the identification, quantitation, and characterization of
   proteoforms with visual validation, is still lacking. Herein, we have
   developed MASH Suite Pro, a comprehensive software tool for top-down
   proteomics with multifaceted functionality. MASH Suite Pro is capable of
   processing high-resolution MS and tandem MS (MS/MS) data using two
   deconvolution algorithms to optimize protein identification results. In
   addition, MASH Suite Pro allows for the characterization of PTMs and
   sequence variations, as well as the relative quantitation of multiple
   proteoforms in different experimental conditions. The program also
   provides visualization components for validation and correction of the
   computational outputs. Furthermore, MASH Suite Pro facilitates data
   reporting and presentation via direct output of the graphics. Thus, MASH
   Suite Pro significantly simplifies and speeds up the interpretation of
   high-resolution top-down proteomics data by integrating tools for
   protein identification, quantitation, characterization, and visual
   validation into a customizable and user-friendly interface. We envision
   that MASH Suite Pro will play an integral role in advancing the
   burgeoning field of topdown proteomics.</abstract><date>FEB 2016</date><author>Cai, Wenxuan
   Guner, Huseyin
   Gregorich, Zachery R.
   Chen, Albert J.
   Ayaz-Guner, Serife
   Peng, Ying
   Valeja, Santosh G.
   Liu, Xiaowen
   Ge, Ying</author></paper><paper><title>A hybrid parallel cellular automata model for urban growth simulation
   over GPU/CPU heterogeneous architectures</title><abstract>As an important spatiotemporal simulation approach and an effective tool
   for developing and examining spatial optimization strategies (e.g., land
   allocation and planning), geospatial cellular automata (CA) models often
   require multiple data layers and consist of complicated algorithms in
   order to deal with the complex dynamic processes of interest and the
   intricate relationships and interactions between the processes and their
   driving factors. Also, massive amount of data may be used in CA
   simulations as high-resolution geospatial and non-spatial data are
   widely available. Thus, geospatial CA models can be both computationally
   intensive and data intensive, demanding extensive length of computing
   time and vast memory space. Based on a hybrid parallelism that combines
   processes with discrete memory and threads with global memory, we
   developed a parallel geospatial CA model for urban growth simulation
   over the heterogeneous computer architecture composed of multiple
   central processing units (CPUs) and graphics processing units (GPUs).
   Experiments with the datasets of California showed that the overall
   computing time for a 50-year simulation dropped from 13,647seconds on a
   single CPU to 32seconds using 64 GPU/CPU nodes. We conclude that the
   hybrid parallelism of geospatial CA over the emerging heterogeneous
   computer architectures provides scalable solutions to enabling complex
   simulations and optimizations with massive amount of data that were
   previously infeasible, sometimes impossible, using individual computing
   approaches.</abstract><date>MAR 3 2016</date><author>Guan, Qingfeng
   Shi, Xuan
   Huang, Miaoqing
   Lai, Chenggang</author></paper><paper><title>Optimization Integrator for Large Time Steps</title><abstract>Practical time steps in today's state-of-the-art simulators typically
   rely on Newton's method to solve large systems of nonlinear equations.
   In practice, this works well for small time steps but is unreliable at
   large time steps at or near the frame rate, particularly for difficult
   or stiff simulations. We show that recasting backward Euler as a
   minimization problem allows Newton's method to be stabilized by standard
   optimization techniques with some novel improvements of our own. The
   resulting solver is capable of solving even the toughest simulations at
   the 24 Hz frame rate and beyond. We show how simple collisions can be
   incorporated directly into the solver through constrained minimization
   without sacrificing efficiency. We also present novel penalty collision
   formulations for self collisions and collisions against scripted bodies
   designed for the unique demands of this solver. Finally, we show that
   these techniques improve the behavior of Material Point Method (MPM)
   simulations by recasting it as an optimization problem.</abstract><date>OCT 2015</date><author>Gast, Theodore F.
   Schroeder, Craig
   Stomakhin, Alexey
   Jiang, Chenfanfu
   Teran, Joseph M.</author></paper><paper><title>Computer simulation and image guidance for individualised dynamic spinal
   stabilization</title><abstract>Dynamic implants for the human spine are used to re-establish regular
   segmental motion. However, the results have often been unsatisfactory
   and complications such as screw loosening are common. Individualisation
   of appliances and precision implantation are needed to improve the
   outcome of this procedure. Computer simulation, virtual implant
   optimisation and image guidance were used to improve the technique.A
   human lumbar spine computer model was developed using multi-body
   simulation software. The model simulates spinal motion under load and
   degenerative changes. After virtual degeneration of a L4/5 segment,
   virtual pedicle screw-based implants were introduced. The implants'
   positions and properties were iteratively optimised. The resulting
   implant positions were used as operative plan for image guidance and
   finally implemented in a physical spine model.In the simulation, the
   introduction and optimisation of virtually designed dynamic implants
   could partly compensate for the effects of virtual lumbar segment
   degeneration. The optimised operative plan was exported to two different
   image-guidance systems for transfer to a physical spine
   model.Three-dimensional computer graphic simulation is a feasible means
   to develop operative plans for dynamic spinal stabilization. These
   operative plans can be transferred to commercially available
   image-guidance systems for use in implantation of physical implants in a
   spine model. This concept has important potential in the design of
   operative plans and implants for individualised dynamic spine
   stabilization surgery.</abstract><date>AUG 2015</date><author>Kantelhardt, S. R.
   Hausen, U.
   Kosterhon, M.
   Amr, A. N.
   Gruber, K.
   Giese, A.</author></paper><paper><title>Statistical simulation of SAR variability with geometric and tissue
   property changes by using the unscented transform</title><abstract>PurposeThe local specific absorption rate (SAR) is critical to the
   safety of radio frequency transmit coils. A statistical simulation
   approach is introduced to address the local SAR variability related to
   tissue property and geometric variations.MethodsThe local SAR is modeled
   as the output of a nonlinear transformation with factors that may affect
   its value being treated as random input variables. Instead of using the
   Monte Carlo method with a large number of sample points, the unscented
   transform is applied with a small set of deterministic sample points. A
   sensitivity analysis is further performed to determine the significance
   of each input variable. Electromagnetic simulations are carried out by
   the finite-difference time-domain method implemented on graphic
   processing unit.ResultsThe local SAR variability of a 7 Tesla square
   loop coil for spine imaging and a 16-element brain imaging array as the
   result of tissue property and geometric changes were examined
   respectively. SAR limits were determined based on their means and
   standard deviations.ConclusionThe proposed approach is efficient and
   general for the study of local SAR variability. Magn Reson Med
   73:2357-2362, 2015. (c) 2014 Wiley Periodicals, Inc.</abstract><date>JUN 2015</date><author>Shao, Yu
   Zeng, Peng
   Wang, Shumin</author></paper><paper><title>High dynamic range imaging pipeline on the GPU</title><abstract>Use of high dynamic range (HDR) images and video in image processing and
   computer graphics applications is rapidly gaining popularity. However,
   creating and displaying high resolution HDR content on CPUs is a time
   consuming task. Although some previous work focused on real-time tone
   mapping, implementation of a full HDR imaging (HDRI) pipeline on the GPU
   has not been detailed. In this article we aim to fill this gap by
   providing a detailed description of how the HDRI pipeline, from HDR
   image assembly to tone mapping, can be implemented exclusively on the
   GPU. We also explain the trade-offs that need to be made for improving
   efficiency and show timing comparisons for CPU versus GPU
   implementations of the HDRI pipeline.</abstract><date>JUN 2015</date><author>Akyuz, Ahmet Oguz</author></paper><paper><title>Reciprocal Drag-and-Drop</title><abstract>Drag-and-drop has become ubiquitous, both on desktop computers and
   touch-sensitive surfaces. It is used to move and edit the geometry of
   elements in graphics editors, to adjust parameters using controllers
   such as sliders, or to manage views (e.g., moving and resizing windows,
   panning maps). Reverting changes made via a drag-and-drop usually
   entails performing the reciprocal drag-and-drop action. This can be
   costly as users have to remember the previous position of the object and
   put it back precisely. We introduce the DND-1 model that handles all
   past locations of graphical objects. We redesign the Dwell-and-Spring
   widget to interact with this history, and explain how applications can
   implement DND-1 to enable users to perform reciprocal drag-and-drop to
   any past location for both individual objects and groups of objects. We
   report on two user studies, whose results show that users understand
   DND-1, and that Dwell-and-Spring enables them to interact with this
   model effectively.</abstract><date>DEC 2015</date><author>Appert, Caroline
   Chapuis, Olivier
   Pietriga, Emmanuel
   Lobo, Maria-Jesus</author></paper><paper><title>Beyond the Bezel: Coin-Op Arcade Video Game Cabinets as Design History</title><abstract>This article studies the surfaces and shapes of coin-op arcade video
   game machine cabinets produced within the USA between 1971 and 1979 to
   demonstrate how design played a constitutive role in defining 'the
   game'. Such a focus highlights how the cabinet designs of Nutting
   Associate's Computer Space (1971) and Atari's Pong (1972) differentiated
   the nascent medium from presiding forms of public amusement, namely
   pinball and electromechanical games, to expand the market for coin-op
   machines. In addition, restricting the article's interest to a period
   before the arcade video game 'craze' hit full swing with its major
   'stars' on the near horizon and with the design paradigms of older games
   still prevalent, provides a look into machines for which cabinets played
   a much larger role in 'filling in the gaps' when color monitors and
   multicolored graphics were not standard. My disassembly of Boot Hill
   (Midway, 1977) and Warrior (Vectorbeam, 1979) offers concrete examples
   of the cabinet's role in shaping the game when the modified televisions
   behind the bezel still radiated in black and white. The article closes
   by drawing a brief parallel between disciplinary debates that have
   shaped Design History with those that are currently forming the emergent
   area of Game History.</abstract><date>NOV 2015</date><author>Guins, Raiford</author></paper><paper><title>A Photometric Sampling Strategy for Reflectance Characterization and
   Transference</title><abstract>Rendering 3D models with real world reflectance properties is an open
   research problem with significant applications in the field of computer
   graphics and image understanding. In this paper, our interest is in the
   characterization and transference of appearance from a source object
   onto a target 3D shape. To this end, a three-step strategy is proposed.
   In the first step, reflectance is sampled by rotating a light source in
   concentric circles around the source object. Singular value
   decomposition is then used for describing, in a pixel-wise manner,
   appearance features such as color, texture, and specular regions. The
   second step introduces a Markov random field transference method based
   on surface normal correspondence between the source object and a
   synthetic sphere. The aim of this step is to generate a sphere whose
   appearance emulates that of the source material. In the third step,
   final transference of properties is performed from the surface normals
   of the generated sphere to the surface normals of the target 3D model.
   Experimental evaluation validates the suitability of the proposed
   strategy for transferring appearance from a variety of materials between
   diverse shapes. </abstract><date>2015-06</date><author>Castela´n, Mario
   Cruz-Pe´rez, Elier
   Torres-Me´ndez, Luz Abril</author></paper><paper><title>Integrating rotation and angular velocity from curvature</title><abstract>The problem of integrating the rotational vector from a given angular
   velocity vector is met in such diverse fields as the navigation,
   robotics, computer graphics, optical tracking and non-linear dynamics of
   flexible beams. For example, if the numerical formulation of non-linear
   dynamics of flexible beams is based on the interpolation of curvature,
   one needs to derive the rotation from the assumed curvature field. The
   relation between the angular velocity and the rotation is described by
   the first-order quasilinear differential equation. If the rotation is
   given, the related angular velocity is obtained by the differentiation.
   By contrast, if the angular velocity is given, the related rotations are
   obtained by the integration. The exact closed-form solution for the
   rotation is only possible if the angular velocity is constant in time.
   In dynamics of non-linear flexible spatial. beams, the problem of
   integrating rotations from a given angular velocity becomes even more
   complex because both the angular velocity and the curvature need
   simultaneously be integrated and are both functions of space and time.
   As the angular velocity and the curvature are assumed to be analytic
   functions, they must satisfy certain integrability conditions to assure
   the unique rotation is obtained from the two differential equations. The
   objective of the present paper is to derive approximate, yet closed-form
   solutions of the following problem: for a given curvature vector,
   determine both the rotation and the angular velocity. In order to avoid
   the singularity of kinematic relations, the quatemions are used for the
   parametrization of rotations, and the integrations are partly performed
   in the four-dimensional quatemion space. The resulting closed-form
   expressions for the rotational and angular velocity quatemions are ready
   to be used in the finite-element formulations of the dynamics of
   flexible spatial beams as interpolating functions. The present novel
   solution is assessed by comparisons of the numerical results with
   analytical solutions for variety of oscillating curvature functions, as
   well as with the solutions of the quaternion-based midpoint integrator
   and the Runge-Kutta-based Crouch-Grossman geometrical methods CG3 and
   CG4. (C) 2015 The Authors. Published by Elsevier Ltd.</abstract><date>JUL 2015</date><author>Treven, A.
   Saje, M.</author></paper><paper><title>Electromagnetic metamaterial simulations using a GPU-accelerated FDTD
   method</title><abstract>Metamaterials composed of artificial subwavelength structures exhibit
   extraordinary properties that cannot be found in nature. Designing
   artificial structures having exceptional properties plays a pivotal role
   in current metamaterial research. We present a new numerical simulation
   scheme for metamaterial research. The scheme is based on a graphic
   processing unit (GPU)-accelerated finite-difference time-domain (FDTD)
   method. The FDTD computation can be significantly accelerated when GPUs
   are used instead of only central processing units (CPUs). We explain how
   the fast FDTD simulation of large-scale metamaterials can be achieved
   through communication optimization in a heterogeneous CPU/GPU-based
   computer cluster. Our method also includes various advanced FDTD
   techniques: the non-uniform grid technique, the
   total-field/scattered-field (TFSF) technique, the auxiliary field
   technique for dispersive materials, the running discrete Fourier
   transform, and the complex structure setting. We demonstrate the power
   of our new FDTD simulation scheme by simulating the negative refraction
   of light in a coaxial waveguide metamaterial.</abstract><date>DEC 2015</date><author>Seok, Myung-Su
   Lee, Min-Gon
   Yoo, SeokJae
   Park, Q-Han</author></paper><paper><title>BrowserGenome.org: web-based RNA-seq data analysis and visualization</title><abstract></abstract><date>NOV 2015</date><author>Schmid-Burgk, Jonathan L.
   Hornung, Veit</author></paper><paper><title>An integrated environmental model for a surface flow constructed
   wetland: Water quality processes</title><abstract>It is a challenge to apply coupled hydrodynamic, sediment, water
   quality, submerged aquatic vegetation (SAV), and emergent aquatic
   vegetation (EAV) models to the studies of constructed wetlands (CWs). So
   far, there are few published studies on CWs that included comprehensive
   SAV, EAV and nutrient cycling processes, had detailed model-data
   comparisons for multiple years, and were applied to support CWs
   management. Stormwater Treatment Areas (STAs) in south Florida are CWs
   that have been built to reduce phosphorus (P) concentrations from
   agricultural drainage waters and Lake Okeechobee discharges. The scale
   of this CWs project is unprecedented in terms of size, cost, and
   scientific challenges. The STA management needs models/tools to provide
   detailed spatial and temporal information to optimize the P removal
   efficiency and to predict the dynamic response of STAs under a variety
   of management conditions. Based on the Lake Okeechobee Environment Model
   (LOEM) developed in the past 15 years, the LOEM-CW water quality model
   is developed for simulating water quality processes in CWs. The coupled
   interactions of SAV and EAV with P variables are included in the LOEM-CW
   to ensure that the P cycling processes are represented realistically.
   The LOEM-CW is calibrated and verified with 6 years of measured data
   (2008-2013) at 6 locations in STA-314 Cells 3A13B. Through graphic and
   statistical comparisons, it is shown that the model simulated P,
   nitrogen, and dissolved oxygen processes in the STA well. The model
   results illustrate that increasing water depth increases the retention
   time and decreases effluent TP concentration. By adjusting the existing
   outflow rates, the STA can be more efficient in removing P. It has been
   shown that the LOEM-CW can serve as a useful tool in wetland management.
   (C) 2015 Published by Elsevier B.V.</abstract><date>JAN 2016</date><author>Ji, Zhen-Gang
   Jin, Kang-Ren</author></paper><paper><title>GPU-based MapReduce for large-scale near-duplicate video retrieval</title><abstract>With the exponential growth of multimedia data, people are overwhelmed
   with massive amount of online videos, of which Near-Duplicate Videos
   (NDVs) occupy a large portion. In this paper, we present a novel
   framework for NDV retrieval, which explores the parallel power of two
   promising techniques: Graphics Processing Unit (GPU) and MapReduce. With
   the power of the proposed framework, various key algorithms in the field
   of computer vision, such as K-Means clustering, bag of features,
   inverted file index with hamming embedding and weak geometric
   consistency, are applied to NDV retrieval. Experimental results on the
   benchmark CC_WEB_VIDEO NDV dataset demonstrate that the proposed
   framework can significantly speed up processing huge amounts of video
   repositories.</abstract><date>DEC 2015</date><author>Wang, Hanli
   Zhu, Fengkuangtian
   Xiao, Bo
   Wang, Lei
   Jiang, Yu-Gang</author></paper><paper><title>Parallel Key Frame Extraction for Surveillance Video Service in a Smart
   City</title><abstract>Surveillance video service (SVS) is one of the most important services
   provided in a smart city. It is very important for the utilization of
   SVS to provide design efficient surveillance video analysis techniques.
   Key frame extraction is a simple yet effective technique to achieve this
   goal. In surveillance video applications, key frames are typically used
   to summarize important video content. It is very important and essential
   to extract key frames accurately and efficiently. A novel approach is
   proposed to extract key frames from traffic surveillance videos based on
   GPU (graphics processing units) to ensure high efficiency and accuracy.
   For the determination of key frames, motion is a more salient feature in
   presenting actions or events, especially in surveillance videos. The
   motion feature is extracted in GPU to reduce running time. It is also
   smoothed to reduce noise, and the frames with local maxima of motion
   information are selected as the final key frames. The experimental
   results show that this approach can extract key frames more accurately
   and efficiently compared with several other methods.</abstract><date>AUG 18 2015</date><author>Zheng, Ran
   Yao, Chuanwei
   Jin, Hai
   Zhu, Lei
   Zhang, Qin
   Deng, Wei</author></paper><paper><title>Gctf: Real-time CTF determination and correction</title><abstract>Accurate estimation of the contrast transfer function (CTF) is critical
   for a near-atomic resolution cryo electron microscopy (cryoEM)
   reconstruction. Here, a GPU-accelerated computer program, Gctf, for
   accurate and robust, real-time CTF determination is presented. The main
   target of Gctf is to maximize the cross-correlation of a simulated CTF
   with the logarithmic amplitude spectra (LAS) of observed micrographs
   after background subtraction. Novel approaches in Gctf improve both
   speed and accuracy. In addition to GPU acceleration (e.g. 10-50x), a
   fast '1-dimensional search plus 2-dimensional refinement (1S2R)'
   procedure further speeds up Gctf. Based on the global CTF determination,
   the local defocus for each particle and for single frames of movies is
   accurately refined, which improves CTF parameters of all particles for
   subsequent image processing. Novel diagnosis method using equiphase
   averaging (EPA) and self-consistency verification procedures have also
   been implemented in the program for practical use, especially for aims
   of near-atomic reconstruction. Gctf is an independent program and the
   outputs can be easily imported into other cryoEM software such as Relion
   (Scheres, 2012) and Frealign (Grigorieff, 2007). The results from
   several representative datasets are shown and discussed in this paper.
   (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>JAN 2016</date><author>Zhang, Kai</author></paper><paper><title>ACTIVATING ROLE OF INTERACTIVE DIDACTIC MATERIALS IN TEACHING COMPUTER
   SUBJECTS</title><abstract>In the days of the visual culture a manner of the transmission of
   information plays a very important role. Adopting a technique of the
   join of text, graphics, sound and animation in frames of the uniform
   structure of presenting data, particularly in the education, it is
   possible to achieve good results in handing over of knowledge than at
   using only one of the media. The article presents the results of
   research devoted to the influence of visual and textual teaching
   materials, on the level of assimilation of knowledge subjects and their
   involvement in the assimilation of content. The analysis of the results
   showed that the visualization of teaching content is a factor
   significantly activating the educational process and affecting the level
   of knowledge assimilation.</abstract><date>DEC 2015</date><author>Lis, Renata</author></paper><paper><title>The Relative Odds of Progressing by Structural and Functional Tests in
   Glaucoma</title><abstract></abstract><date>JUN 2015</date><author>Marvasti, Amir
   Zangwill, Linda M.
   Abe, Ricardo Y.
   Diniz-Filho, Alberto
   Gracitelli, Carolina
   Weinreb, Robert N.
   Girkin, Christopher A.
   Liebmann, Jeffrey M.
   Medeiros, Felipe A.</author></paper><paper><title>Efficient 3D Transpositions in Graphics Processing Units</title><abstract>Matrix transposition is a basic operation for several computing tasks.
   Hence, transposing a matrix in a computer's main memory has been well
   studied since many years ago. More recently, the out-of-place matrix
   transposition has been performed efficiently in graphical processing
   units (GPU), which are broadly used today for general purpose computing.
   However, due to the particular architecture of GPUs, the adaptation of
   the matrix transposition operation to 3D arrays is not straightforward.
   In this paper, we describe efficient implementations for graphical
   processing units of the 5 possible out-of-place 3D transpositions.
   Moreover, we also include the transposition of the most basic in-place
   3D transpositions. The results show that the achieved bandwidth is close
   to a simple array copy and is similar to the 2D transposition.</abstract><date>OCT 2015</date><author>Jodra, Jose L.
   Gurrutxaga, Ibai
   Muguerza, Javier</author></paper><paper><title>Escher: A Web Application for Building, Sharing, and Embedding Data-Rich
   Visualizations of Biological Pathways</title><abstract>Escher is a web application for visualizing data on biological pathways.
   Three key features make Escher a uniquely effective tool for pathway
   visualization. First, users can rapidly design new pathway maps. Escher
   provides pathway suggestions based on user data and genome-scale models,
   so users can draw pathways in a semi-automated way. Second, users can
   visualize data related to genes or proteins on the associated reactions
   and pathways, using rules that define which enzymes catalyze each
   reaction. Thus, users can identify trends in common genomic data types
   (e.g. RNA-Seq, proteomics, ChIP)-in conjunction with metabolite-and
   reaction-oriented data types (e.g. metabolomics, fluxomics). Third,
   Escher harnesses the strengths of web technologies (SVG, D3, developer
   tools) so that visualizations can be rapidly adapted, extended, shared,
   and embedded. This paper provides examples of each of these features and
   explains how the development approach used for Escher can be used to
   guide the development of future visualization tools.</abstract><date>AUG 2015</date><author>King, Zachary A.
   Draeger, Andreas
   Ebrahim, Ali
   Sonnenschein, Nikolaus
   Lewis, Nathan E.
   Palsson, Bernhard O.</author></paper><paper><title>Parallel Implementation of Polarimetric Synthetic Aperture Radar Data
   Processing for Unsupervised Classification Using the Complex Wishart
   Classifier</title><abstract>This work investigates the parallel implementation of target
   decomposition and unsupervised classification algorithms for
   polarimetric synthetic aperture radar (POLSAR) data processing. The
   algorithms are implemented using two different parallel programming
   models: 1) clusters of CPUs, using message passing interface (MPI), and
   2) commodity graphic processing units (GPUs), using the compute device
   unified architecture (CUDA). POLSAR data processing generally involves a
   large amount of computations as the full polarimetric information needs
   to be decomposed and analyzed. Our experiments reveal that GPU
   architectures provide a good framework for massive parallelization of
   POLSAR data processing. For instance, it is found that a single GPU can
   be more efficient than a cluster of 128 nodes with speedups of more than
   100x in comparison with the single processor times. The proposed
   implementation makes the best use of low-level features in the GPU
   architecture such as shared memories, while also providing coalesced
   accesses to memory in order to achieve maximum performance.</abstract><date>NOV 2015</date><author>Sanchez, Sergio
   Marpu, Prashanth R.
   Plaza, Antonio
   Paz-Gallardo, Abel</author></paper><paper><title>Optimal Diagnostic indices for idiopathic Normal Pressure Hydrocephalus
   Based on the 3D Quantitative Volumetric Analysis for the Cerebral
   Ventricle and Subarachnoid Space</title><abstract>BACKGROUND AND PURPOSE: Despite the remarkable progress of 3D graphics
   technology, the Evans index has been the most popular index for
   ventricular enlargement. We investigated a novel reliable index for the
   MR imaging features specified in idiopathic normal pressure
   hydrocephalus, rather than the Evans index.MATERIALS AND METHODS: The
   patients with suspected idiopathic normal pressure hydrocephalus on the
   basis of the ventriculomegaly and a triad of symptoms underwent the CSF
   tap test. CSF volumes were extracted from a T2-weighted 3D spin-echo
   sequence named "sampling perfection with application-optimized contrasts
   by using different flip angle evolutions (SPACE)" on 3T MR imaging and
   were quantified semiautomatically. Subarachnoid spaces were divided as
   follows: upper and lower parts and 4 compartments of frontal convexity,
   parietal convexity, Sylvian fissure and basal cistern, and posterior
   fossa. The maximum length of 3 axial directions in the bilateral
   ventricles and their frontal horns was measured. The "z-Evans Index" was
   defined as the maximum z-axial length of the frontal horns to the
   maximum cranial z-axial length. These parameters were evaluated for the
   predictive accuracy for the tap-positive groups compared with the
   tap-negative groups and age-adjusted odds ratios at the optimal
   thresholds.RESULTS: In this study, 24 patients with tap-positive
   idiopathic normal pressure hydrocephalus, 25 patients without response
   to the tap test, and 23 age-matched controls were included. The frontal
   horns of the bilateral ventricles were expanded, with the most excessive
   expansion being toward the z-direction. The CSF volume of the parietal
   convexity had the highest area under the receiver operating
   characteristic curve (0.768), the z-Evans Index was the second (0.758),
   and the upper-to-lower subarachnoid space ratio index was the third
   (0.723), to discriminate the tap-test response.CONCLUSIONS: The CSF
   volume of the parietal convexity of &lt;38 mL, upper-to-lower subarachnoid
   space ratio of &lt;0.33, and the z-Evans Index of &gt;0.42 were newly proposed
   useful indices for the idiopathic normal pressure hydrocephalus
   diagnosis, an alternative to the Evans Index.</abstract><date>DEC 2015</date><author>Yamada, S.
   Ishikawa, M.
   Yamamoto, K.</author></paper><paper><title>Real-time HD image distortion correction in heterogeneous parallel
   computing systems using efficient memory access patterns</title><abstract>High-definition video is becoming a standard in clinical endoscopy.
   State-of-the-art systems for medical endoscopy provide 1080p video
   streams at 60 Hz. For such high resolutions and frame rates, the
   real-time execution of image-processing tasks is far from trivial,
   requiring careful algorithm design and development. In this article, we
   propose a fully functional software-based solution for correcting the
   radial distortion (RD) of HD video that runs in real time in a personal
   computer (PC) equipped with a conventional graphics processing unit
   (GPU) and a video acquisition card. Our system acquires the video feed
   directly from the digital output of the endoscopic camera control unit,
   warps each frame using a heterogeneous parallel computing architecture,
   and outputs the result back to the display. Although we target the
   particular problem of correcting geometric distortion in medical
   endoscopy, the concepts and framework herein described can be extended
   to other image-processing tasks with hard real-time requirements. We
   show that a heterogeneous approach, as well as efficient memory access
   patterns in the GPU, improve the performance of this highly memory-bound
   algorithm, leading to frame rates above 250 fps.</abstract><date>JAN 2016</date><author>Melo, Rui
   Falcao, Gabriel
   Barreto, Joao P.</author></paper><paper><title>Graphic Warning Labels Elicit Affective and Thoughtful Responses from
   Smokers: Results of a Randomized Clinical Trial</title><abstract>ObjectiveObservational research suggests that placing graphic images on
   cigarette warning labels can reduce smoking rates, but field studies
   lack experimental control. Our primary objective was to determine the
   psychological processes set in motion by naturalistic exposure to
   graphic vs. text-only warnings in a randomized clinical trial involving
   exposure to modified cigarette packs over a 4-week period. Theories of
   graphic-warning impact were tested by examining affect toward smoking,
   credibility of warning information, risk perceptions, quit intentions,
   warning label memory, and smoking risk knowledge.MethodsAdults who
   smoked between 5 and 40 cigarettes daily (N = 293; mean age = 33.7), did
   not have a contra-indicated medical condition, and did not intend to
   quit were recruited from Philadelphia, PA and Columbus, OH. Smokers were
   randomly assigned to receive their own brand of cigarettes for four
   weeks in one of three warning conditions: text only, graphic images plus
   text, or graphic images with elaborated text.ResultsData from 244
   participants who completed the trial were analyzed in
   structural-equation models. The presence of graphic images (compared to
   text-only) caused more negative affect toward smoking, a process that
   indirectly influenced risk perceptions and quit intentions (e.g.,
   image-&gt;negative affect-&gt;risk perception-&gt;quit intention). Negative
   affect from graphic images also enhanced warning credibility including
   through increased scrutiny of the warnings, a process that also
   indirectly affected risk perceptions and quit intentions (e.g.,
   image-&gt;negative affect-&gt;risk scrutiny-&gt;warning credibility-&gt;risk
   perception-&gt;quit intention). Unexpectedly, elaborated text reduced
   warning credibility. Finally, graphic warnings increased
   warning-information recall and indirectly increased smoking-risk
   knowledge at the end of the trial and one month later.ConclusionsIn the
   first naturalistic clinical trial conducted, graphic warning labels are
   more effective than text-only warnings in encouraging smokers to
   consider quitting and in educating them about smoking's risks. Negative
   affective reactions to smoking, thinking about risks, and perceptions of
   credibility are mediators of their impact.</abstract><date>DEC 16 2015</date><author>Evans, Abigail T.
   Peters, Ellen
   Strasser, Andrew A.
   Emery, Lydia F.
   Sheerin, Kaitlin M.
   Romer, Daniel</author></paper><paper><title>An exact general remeshing scheme applied to physically conservative
   voxelization</title><abstract>We present an exact general remeshing scheme to compute analytic
   integrals of polynomial functions over the intersections between convex
   polyhedral cells of old and new meshes. In physics applications this
   allows one to ensure global mass, momentum, and energy conservation
   while applying higher-order polynomial interpolation. We elaborate on
   applications of our algorithm arising in the analysis of cosmological
   N-body data, computer graphics, and continuum mechanics problems.We
   focus on the particular case of remeshing tetrahedral cells onto a
   Cartesian grid such that the volume integral of the polynomial density
   function given on the input mesh is guaranteed to equal the
   corresponding integral over the output mesh. We refer to this as
   "physically conservative voxelization."At the core of our method is an
   algorithm for intersecting two convex polyhedra by successively clipping
   one against the faces of the other. This algorithm is an implementation
   of the ideas presented abstractly by Sugihara [48], who suggests using
   the planar graph representations of convex polyhedra to ensure
   topological consistency of the output. This makes our implementation
   robust to geometric degeneracy in the input. We employ a simplicial
   decomposition to calculate moment integrals up to quadratic order over
   the resulting intersection domain.We also address practical issues
   arising in a software implementation, including numerical stability in
   geometric calculations, management of cancellation errors, and extension
   to two dimensions. In a comparison to recent work, we show substantial
   performance gains. We provide a C implementation intended to be a fast,
   accurate, and robust tool for geometric calculations on polyhedral mesh
   elements. (C) 2015 The Authors. Published by Elsevier Inc.</abstract><date>SEP 15 2015</date><author>Powell, Devon
   Abel, Tom</author></paper><paper><title>A new technique for ultimate limit state design of arbitrary shape RC
   sections under biaxial bending</title><abstract>The design of reinforced concrete sections of arbitrary shape, namely
   with variable geometry, holes as well as with arbitrary distribution of
   reinforcing steel bars, is a very common task in civil engineering,
   reinforced concrete structures. The design of these sections requires
   the integration of non-linear stress fields on complex shapes, because
   of the non-linear behavior of concrete in compression. In this paper, a
   novel algorithm is proposed to compute the ultimate strength of
   reinforced concrete sections under biaxial bending. The algorithm
   includes section subdivision into trapezoidal elements using the
   techniques of polygon clipping algorithm proposed by Weiler-Atherton.
   Exact numerical integration for normal strength concrete (f(ck) &lt;= 50
   MPa) is achieved, for each trapezoid, using the change of variables
   theorem followed by Gauss-Legendre integration. The proposed technique
   is hereafter referred to as WAGL (Weiler-Atherton, Gauss-Legendre). The
   verification of the proposed algorithm is performed by comparing
   analytical results between the WAGL technique and methods proposed by
   other authors (five examples). Additionally, the results obtained are
   also compared with experimental results available in the literature. The
   application of the WAGL technique is illustrated with two RC
   cross-section design examples. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>DEC 1 2015</date><author>Rodrigues, R. Vaz</author></paper><paper><title>Physically Meaningful Rendering using Tristimulus Colours</title><abstract>In photorealistic image synthesis the radiative transfer equation is
   often not solved by simulating every wavelength of light, but instead by
   computing tristimulus transport, for instance using sRGB primaries as a
   basis. This choice is convenient, because input texture data is usually
   stored in RGB colour spaces. However, there are problems with this
   approach which are often overlooked or ignored. By comparing to spectral
   reference renderings, we show how rendering in tristimulus colour spaces
   introduces colour shifts in indirect light, violation of energy
   conservation, and unexpected behaviour in participating media.
   Furthermore, we introduce a fast method to compute spectra from almost
   any given XYZ input colour. It creates spectra that match the input
   colour precisely. Additionally, like in natural reflectance spectra,
   their energy is smoothly distributed over wide wavelength bands. This
   method is both useful to upsample RGB input data when spectral transport
   is used and as an intermediate step for corrected tristimulus-based
   transport. Finally, we show how energy conservation can be enforced in
   RGB by mapping colours to valid reflectances.</abstract><date>JUL 2015</date><author>Meng, Johannes
   Simon, Florian
   Hanika, Johannes
   Dachsbacher, Carsten</author></paper><paper><title>Screening of a library of 4-aryl/heteroaryl-4H-fused pyrans for xanthine
   oxidase inhibition: synthesis, biological evaluation and docking studies</title><abstract>A series of 4-aryl/heteroaryl-4H-fused pyrans was synthesized via
   multicomponent reaction in a microwave synthesizer. All the pyrans were
   evaluated for in vitro xanthine oxidase inhibition. Structure-activity
   relationship was also established. Among the series of 108 compounds,
   Compound 5n was the most potent displaying remarkable inhibition against
   the enzyme with an IC50 value of 0.59 mu M. Enzyme kinetic study was
   carried out for the compound 5n to determine the type of inhibition. The
   study revealed that the compound 5n was a mixed-type inhibitor.
   Molecular modelling studies were also performed to figure out the
   interactions of both the enantiomers of 5n with the amino acid residues
   of the enzyme.[GRAPHICS].</abstract><date>AUG 2015</date><author>Kaur, Ramandeep
   Naaz, Fatima
   Sharma, Sahil
   Mehndiratta, Samir
   Gupta, Manish Kumar
   Bedi, Preet Mohinder Singh
   Nepali, Kunal</author></paper><paper><title>Using Adobe Flash Animations of electron transport chain to teach and
   learn biochemistry</title><abstract>Teaching the subject of the electron transport chain is one of the most
   challenging aspects of the chemistry curriculum at the high school
   level. This article presents an educational program called Electron
   Transport Chain which consists of 14 visual animations including a
   biochemistry quiz. The program was created in the Adobe Flash CS3
   Professional animation program and is designed for high school chemistry
   students. Our goal is to develop educational materials that facilitate
   the comprehension of this complex subject through dynamic animations
   which show the course of the electron transport chain and simultaneously
   explain its nature. We record the process of the electron transport
   chain, including connections with oxidative phosphorylation, in such a
   way as to minimize the occurrence of discrepancies in interpretation.
   The educational program was evaluated in high schools through the
   administration of a questionnaire, which contained 12 opened-ended items
   and which required participants to evaluate the graphics of the
   animations, chemical content, student preferences, and its suitability
   for high school biochemistry teaching. (c) 2015 by the International
   Union of Biochemistry and Molecular Biology, 43(4):294-299, 2015.</abstract><date>JUL-AUG 2015</date><author>Tepla, Milada
   Klimova, Helena</author></paper><paper><title>Using the comprehensive patent citation network (CPC) to evaluate patent
   value</title><abstract>Most approaches to patent citation network analysis are based on
   single-patent direct citation relation, which is an incomplete
   understanding of the nature of knowledge flow between patent pairs,
   which are incapable of objectively evaluating patent value. In this
   paper, four types of patent citation networks (direct citation, indirect
   citation, coupling and co-citation networks) are combined, filtered and
   recomposed based on relational algebra. Then, a method based on
   comprehensive patent citation (CPC) network for patent value evaluation
   is proposed, and empirical study of optical disk technology related
   patents has been conducted based on this method. The empirical study was
   carried out in two steps: observation of network characteristics over
   the entire process (citation time lag and topological and graphics
   characteristics), and measurement verification by independent proxies of
   patent value (patent family and patent duration). Our results show that
   the CPC network retains the advantages of patent direct citation, and
   performs better on topological structure, graphics features, centrality
   distribution, citation lag and sensitivity than a direct citation
   network; The verified results by the patent family and maintenance show
   that the proposed method covers more valuable patents than the
   traditional method.</abstract><date>DEC 2015</date><author>Yang, Guan-Can
   Li, Gang
   Li, Chun-Ya
   Zhao, Yun-Hua
   Zhang, Jing
   Liu, Tong
   Chen, Dar-Zen
   Huang, Mu-Hsuan</author></paper><paper><title>A rapid GPU-based heat transfer and solidification model for dynamic
   computer simulations of continuous steel casting</title><abstract>The paper presents a GPU-based model for continuous casting of steel.
   The model provides rapid computation capabilities required for real-time
   use in the casting control and optimization. The fully three-dimensional
   formulation of the heat transfer and solidification model is based on
   the control volume method and it allows for very fast transient
   simulations of the thermal behaviour of cast strands. The developed
   model has been verified on Stefan problem and validated with industry
   measurements. Heat transfer conditions in the mould and secondary
   cooling were determined experimentally in lab-scale experiments. The
   computational model is implemented as highly-parallel with the use of
   the NVIDIA CUDA architecture, which enables to launch the model on
   graphics processing units (GPUs) allowing for its great acceleration.
   The acceleration can be evaluated with the use of the relative
   computational time, which is the dimensionless ratio between the
   computational time that the model needs to compute the simulation and
   the wall-clock time of the real casting process being simulated. The
   relative computational time of the presented GPU-based computational
   model is between 0.0016 for a coarse mesh and 0.27 for a very fine mesh.
   The corresponding multiple of the GPU-acceleration, which is the ratio
   between the computational time of the GPU-based model and of the
   CPU-based model for the identical simulation, is between 33 and 68. (C)
   2015 Elsevier B.V. All rights reserved.</abstract><date>DEC 2015</date><author>Klimes, Lubomir
   Stetina, Josef</author></paper><paper><title>Evaluation and Acceleration of High-Throughput Fixed-Point Object
   Detection on FPGAs</title><abstract>Reliance on object or people detection is rapidly growing beyond
   surveillance to industrial and social applications. The histogram of
   oriented gradients (HOG), one of the most popular object detection
   algorithms, achieves high detection accuracy but delivers just under 1
   frame/s on a high-end CPU. Field-programmable gate array (FPGA)
   accelerations of this algorithm are limited by the intensive
   floating-point computations. All current fixed-point HOG implementations
   use large bit width to maintain detection accuracy, or perform poorly at
   reduced data precision. In this paper, we introduce the full-image
   evaluation methodology to explore the FPGA implementation of HOG using
   reduced bit width. This approach lessens the required area resources on
   the FPGA, and increases the clock frequency and hence the throughput per
   device through increased parallelism. We evaluate the detection accuracy
   of the fixed-point HOG by applying state-of-the-art computer vision
   pedestrian detection evaluation metrics and show it performs as well as
   the original floating-point code from OpenCV. We then show our single
   FPGA implementation achieves a 68.7 x higher throughput than a high-end
   CPU, 5.1 x higher than a high-end graphics processing unit (GPU), and
   7.8 x higher than the same implementation using floating-point on the
   same FPGA. A power consumption comparison for different platforms shows
   our fixed-point FPGA implementation uses 130 x less power than CPU, and
   31 x less energy than GPU to process one image.</abstract><date>JUN 2015</date><author>Ma, Xiaoyin
   Najjar, Walid A.
   Roy-Chowdhury, Amit K.</author></paper><paper><title>GPU-accelerated adjoint algorithmic differentiation</title><abstract>Many scientific problems such as classifier training or medical image
   reconstruction can be expressed as minimization of differentiable
   real-valued cost functions and solved with iterative gradient-based
   methods. Adjoint algorithmic differentiation (AAD) enables automated
   computation of gradients of such cost functions implemented as computer
   programs. To backpropagate adjoint derivatives, excessive memory is
   potentially required to store the intermediate partial derivatives on a
   dedicated data structure, referred to as the "tape". Parallelization is
   difficult because threads need to synchronize their accesses during
   taping and backpropagation. This situation is aggravated for many-core
   architectures, such as Graphics Processing Units (GPUs), because of the
   large number of light-weight threads and the limited memory size in
   general as well as per thread. We show how these limitations can be
   mediated if the cost function is expressed using GPU-accelerated vector
   and matrix operations which are recognized as intrinsic functions by our
   AAD software. We compare this approach with naive and vectorized
   implementations for CPUs. We use four increasingly complex cost
   functions to evaluate the performance with respect to memory consumption
   and gradient computation times. Using vectorization, CPU and GPU memory
   consumption could be substantially reduced compared to the naive
   reference implementation, in some cases even by an order of complexity.
   The vectorization allowed usage of optimized parallel libraries during
   forward and reverse passes which resulted in high speedups for the
   vectorized CPU version compared to the naive reference implementation.
   The GPU version achieved an additional speedup of 7.5 +/- 4.4, showing
   that the processing power of GPUs can be utilized for AAD using this
   concept. Furthermore, we show how this software can be systematically
   extended for more complex problems such as nonlinear absorption
   reconstruction for fluorescence-mediated tomography.Program
   summaryProgram title: AD-GPUCatalogue identifier: AEYX_v1_0Program
   summary URL: http://cpc.cs.qub.ac.uk/summaries/AEYX_v1_0.htmlProgram
   obtainable from: CPC Program Library, Queen's University, Belfast, N.
   IrelandLicensing provisions: Standard CPC licence,
   http://cpc.cs.qub.ac.uk/licence/licence.htmlNo. of lines in distributed
   program, including test data, etc.: 16715No. of bytes in distributed
   program, including test data, etc.: 143683Distribution format:
   tar.gzProgramming language: C++ and CUDA.Computer: Any computer with a
   compatible C++ compiler and a GPU with CUDA capability 3.0 or
   higher.Operating system: Windows 7 or Linux.RAM: 16 GbyteClassification:
   4.9, 4.12, 6.1, 6.5.External routines: CUDA 6.5, Intel MKL (optional)
   and routines from BLAS, LAPACK and CUBLASNature of problem: Gradients
   are required for many optimization problems, e.g. classifier training or
   nonlinear image reconstruction. Often, the function, of which the
   gradient is required, can be implemented as a computer program. Then,
   algorithmic differentiation methods can be used to compute the gradient.
   Depending on the approach this may result in excessive requirements of
   computational resources, i.e. memory and arithmetic computations. GPUs
   provide massive computational resources but require special
   considerations to distribute the workload onto many light-weight
   threads.Solution method: Adjoint algorithmic differentiation allows
   efficient computation of gradients of cost functions given as computer
   programs. The gradient can be theoretically computed using a similar
   amount of arithmetic operations as one function evaluation. Optimal
   usage of parallel processors and limited memory is a major challenge
   which can be mediated by the use of vectorization.Restrictions: To use
   the GPU-accelerated adjoint algorithmic differentiation method, the cost
   function must be implemented using the provided AD-GPU intrinsics for
   matrix and vector operations. Unusual
   features:GPU-acceleration.Additional comments: The code uses some
   features of C++11, e.g. std::shared ptr. Alternatively, the boost
   library can be used.Running time: The time to run the example program is
   a few minutes or up to a few hours to reproduce the performance
   measurements. (C) 2015 The Authors. Published by Elsevier B.V.</abstract><date>MAR 2016</date><author>Gremse, Felix
   Hoefter, Andreas
   Razik, Lukas
   Kiessling, Fabian
   Naumann, Uwe</author></paper><paper><title>Linear solvation energy relationship for the adsorption of synthetic
   organic compounds on single-walled carbon nanotubes in water</title><abstract>The linear solvation energy relationship (LSER) was applied to predict
   the adsorption coefficient (K) of synthetic organic compounds (SOCs) on
   single-walled carbon nanotubes (SWCNTs). A total of 40 log K values were
   used to develop and validate the LSER model. The adsorption data for 34
   SOCs were collected from 13 published articles and the other six were
   obtained in our experiment. The optimal model composed of four
   descriptors was developed by a stepwise multiple linear regression (MLR)
   method. The adjusted r(2) (r(adj)(2)) and root mean square error (RMSE)
   were 0.84 and 0.49, respectively, indicating good fitness. The
   leave-one-out cross-validation Q(2) ([GRAPHICS]) was 0.79, suggesting
   the robustness of the model was satisfactory. The external Q(2)
   ([GRAPHICS]) and RMSE (RMSEext) were 0.72 and 0.50, respectively,
   showing the model's strong predictive ability. Hydrogen bond donating
   interaction (bB) and cavity formation and dispersion interactions (vV)
   stood out as the two most influential factors controlling the adsorption
   of SOCs onto SWCNTs. The equilibrium concentration would affect the
   fitness and predictive ability of the model, while the coefficients
   varied slightly.</abstract><date>JAN 2 2016</date><author>Ding, H.
   Chen, C.
   Zhang, X.</author></paper><paper><title>Development of a Second-Generation Underwater Acoustic Ambient Noise
   Imaging Camera</title><abstract>A nominally circular 2-D broadband acoustic array of 1.3-m diameter,
   comprising 508 sensors and associated electronics, was designed, built,
   and tested for ambient noise imaging (ANI) potential in Singapore
   waters. The system, named Remotely Operated Mobile Ambient Noise Imaging
   System (ROMANIS), operates over 25-85 kHz, streaming real-time data at
   1.6 Gb/s over a fiber optic link. By using sensors that are much larger
   than half-wavelength at the highest frequency of interest, so with some
   directionality, good beamforming performance is obtained with a small
   number of sensors compared to a conventional half-wavelength-spaced
   array. A data acquisition system consisting of eight single-board
   computers enables synchronous data collection from all 508 sensors. A
   dry-coupled neoprene cover is used to encapsulate the ceramic elements
   as an alternative to potting or oil filling, for easier maintenance.
   Beamforming is performed in real-time using parallel computing on a
   graphics processing unit (GPU). Experiments conducted in Singapore
   waters yielded images of underwater objects at much larger ranges and
   with better resolution than any previous ANI system. Although ROMANIS
   was designed for ANI, the array may be valuable in many other
   applications requiring a broadband underwater acoustic receiving array.</abstract><date>JAN 2016</date><author>Pallayil, Venugopalan
   Chitre, Mandar
   Kuselan, Subash
   Raichur, Amogh
   Ignatius, Manu
   Potter, John R.</author></paper><paper><title>Key-layered normal distributions transform for point cloud registration</title><abstract>A new scan matching algorithm is proposed using the concept of key
   layers. In the conventional multi-layered normal distributions transform
   (MLNDT), the number of layers and iterations per layer are fixed and
   mismatches in point clouds occur due to the limited number of optimising
   iterations per layer. Moreover, the accuracy of registration is low and
   the number of layers is heuristically determined in MLNDT. The proposed
   key-layered normal distributions transform (KLNDT) works well with both
   enhanced success rate and accuracy. It is also possible for KLNDT to
   register in higher layers than the traditional MLNDT.</abstract><date>NOV 19 2015</date><author>Hong, Hyunki
   Lee, B. H.</author></paper><paper><title>MSI.R scripts reveal volatile and semi-volatile features in
   low-temperature plasma mass spectrometry imaging (LTP-MSI) of chilli
   (Capsicum annuum)</title><abstract>In cartography, the combination of colour and contour lines is used to
   express a three-dimensional landscape on a two-dimensional map. We
   transferred this concept to the analysis of mass spectrometry imaging
   (MSI) data and developed a collection of R scripts for the efficient
   evaluation of .imzML archives in a four-step strategy: (1) calculation
   of the density distribution of mass-to-charge ratio (m/z) signals in the
   .imzML file and assembling of a pseudo-master spectrum with peak list,
   (2) automated generation of mass images for a defined scan range and
   subsequent visual inspection, (3) visualisation of individual ion
   distributions and export of relevant .mzML spectra and (4) creation of
   overlay graphics of ion images and photographies. The use of a
   Hue-Chroma-Luminance (HCL) colour model in MSI graphics takes into
   account the human perception for colours and supports the correct
   evaluation of signal intensities. Further, readers with colour blindness
   are supported. Contour maps promote the visual recognition of patterns
   in MSI data, which is particularly useful for noisy data sets. We
   demonstrate the scalability of MSI.R scripts by running them on
   different systems: on a personal computer, on Amazon Web Services (AWS)
   instances and on an institutional cluster. By implementing a parallel
   computing strategy, the execution speed for .imzML data scanning with
   image generation could be improved by more than an order of magnitude.
   Applying our MSI.R scripts to low-temperature plasma (LTP)-MSI data
   shows the localisation of volatile and semi-volatile compounds in the
   cross-cut of a chilli (Capsicum annuum) fruit. The subsequent
   identification of compounds by gas and liquid chromatography coupled to
   mass spectrometry (GC-MS, LC-MS) proves that LTP-MSI enables the direct
   measurement of volatile organic compound (VOC) distributions from
   biological tissues.</abstract><date>JUL 2015</date><author>Gamboa-Becerra, Roberto
   Ramirez-Chavez, Enrique
   Molina-Torres, Jorge
   Winkler, Robert</author></paper><paper><title>Determining protein structures by combining semireliable data with
   atomistic physical models by Bayesian inference</title><abstract>More than 100,000 protein structures are now known at atomic detail.
   However, far more are not yet known, particularly among large or complex
   proteins. Often, experimental information is only semireliable because
   it is uncertain, limited, or confusing in important ways. Some
   experiments give sparse information, some give ambiguous or nonspecific
   information, and others give uncertain information-where some is right,
   some is wrong, but we don't know which. We describe a method called
   Modeling Employing Limited Data (MELD) that can harness such problematic
   information in a physics-based, Bayesian framework for improved
   structure determination. We apply MELD to eight proteins of known
   structure for which such problematic structural data are available,
   including a sparse NMR dataset, two ambiguous EPR datasets, and four
   uncertain datasets taken from sequence evolution data. MELD gives
   excellent structures, indicating its promise for experimental
   biomolecule structure determination where only semireliable data are
   available.</abstract><date>JUN 2 2015</date><author>MacCallum, Justin L.
   Perez, Alberto
   Dill, Ken A.</author></paper><paper><title>Dynamic response of a frame-foundation-soil system: a coupled BEM-FEM
   procedure and a GPU implementation</title><abstract>Graphics processing units have experienced an increasing demand for
   general-purpose computer applications such as engineering, science,
   finance, among other areas. This study uses such devices to analyze the
   performance of a code designed to evaluate the dynamic response of
   frame-foundation-soil system. In the present article, a direct version
   of the boundary element method (BEM) is applied to synthesize the 3D
   dynamic compliance matrix of a rigid and massless foundation interacting
   with unbounded soil profiles. The foundation compliance matrix is
   coupled to a frame, modeled by the finite element method (FEM) leading
   to the dynamic response of a coupled frame-foundation-soil system. The
   direct version of the 3D boundary element method is built based on the
   stationary fundamental solution of an elastic full-space. Viscoelastic
   effects are incorporated by means of the elastic-viscoelastic
   correspondence principle. The article describes the methodology applied
   to couple rigid bodies with the BEM mesh. The soil-foundation
   interaction result is given in terms of a dynamic compliance matrix for
   a rigid and massless foundation. The frame structural analysis is based
   on FEM where bar and beam elements are coupled. This strategy allows
   performing frequency domain analysis of such structural systems. The
   performance analysis is done by comparing two codes: one was exclusively
   developed in C language and the other one in CUDA C (Compute Unified
   Device Architecture). The level of speedup was achieved by implementing
   some functions in the frame model, i.e., on the FEM based code.</abstract><date>JUL 2015</date><author>Carrion, Ronaldo
   Mesquita, Euclides
   Ansoni, Jonas Laerte</author></paper><paper><title>Evaluation of a Modified User Guide for Hearing Aid Management</title><abstract>Objectives: This study investigated if a hearing aid user guide modified
   using best practice principles for health literacy resulted in superior
   ability to perform hearing aid management tasks, compared with the user
   guide in the original form.Design: This research utilized a two-arm
   study design to compare the original manufacturer's user guide with a
   modified user guide for the same hearing aidan Oticon Acto
   behind-the-ear aid with an open dome. The modified user guide had a
   lower reading grade level (4.2 versus 10.5), used a larger font size,
   included more graphics, and had less technical information. Eighty-nine
   adults ages 55 years and over were included in the study; none had
   experience with hearing aid use or management. Participants were
   randomly assigned either the modified guide (n = 47) or the original
   guide (n = 42). All participants were administered the Hearing Aid
   Management test, designed for this study, which assessed their ability
   to perform seven management tasks (e.g., change battery) with their
   assigned user guide.Results: The regression analysis indicated that the
   type of user guide was significantly associated with performance on the
   Hearing Aid Management test, adjusting for 11 potential covariates. In
   addition, participants assigned the modified guide required
   significantly fewer prompts to perform tasks and were significantly more
   likely to perform four of the seven tasks without the need for prompts.
   The median time taken by those assigned the modified guide was also
   significantly shorter for three of the tasks. Other variables associated
   with performance on the Hearing Aid Management test were health literacy
   level, finger dexterity, and age.Conclusions: Findings indicate that the
   need to design hearing aid user guides in line with best practice
   principles of health literacy as a means of facilitating improved
   hearing aid management in older adults.</abstract><date>JAN-FEB 2016</date><author>Caposecco, Andrea
   Hickson, Louise
   Meyer, Carly
   Khan, Asaduzzaman</author></paper><paper><title>Genetic annealing with efficient strategies to improve the performance
   for the NP-hard and routing problems</title><abstract>&lt;inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink"
   xlink:href="teta_a_1020624_ilm0001.gif"&gt;&lt;/inline-graphic&gt; problem which
   cannot be solved in polynomial time for asymptotically large values of
   &lt;inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink"
   xlink:href="teta_a_1020624_ilm0002.gif"&gt;&lt;/inline-graphic&gt; and travelling
   salesman problem (TSP) is important in operations research and
   theoretical computer science. In this paper a balanced combination of
   genetic algorithm and simulated annealing has been applied. To improve
   the performance of finding an optimal solution from huge search space,
   we have incorporated the use of tournament and rank as selection
   operators, and inver-over operator mechanism for crossover and mutation.
   This proposed technique is applied for some routing resource problems in
   a chip design process and a best optimal solution was obtained, and the
   TSP appears as a sub-problem in many areas and is used as a benchmark
   for many optimisation methods.</abstract><date>NOV 2 2015</date><author>Eswarawaka, Rajesh
   Mahammad, S. K. Noor
   Reddy, B. Eswara</author></paper><paper><title>Comparative visualization of genotype-Phenotype relationships</title><abstract></abstract><date>AUG 2015</date><author>Yaikhom, Gagarine
   Morgan, Hugh
   Sneddon, Duncan
   Retha, Ahmad
   Atienza-Herrero, Julian
   Blake, Andrew
   Brown, James
   Di Fenza, Armida
   Fiegel, Tanja
   Horner, Neil
   Ring, Natalie
   Santos, Luis
   Westerberg, Henrik
   Brown, Steve D. M.
   Mallon, Ann-Marie</author></paper><paper><title>An investigation into inter- and intragenomic variations of graphic
   genomic signatures</title><abstract>Background: Motivated by the general need to identify and classify
   species based on molecular evidence, genome comparisons have been
   proposed that are based on measuring mostly Euclidean distances between
   Chaos Game Representation (CGR) patterns of genomic DNA
   sequences.Results: We provide, on an extensive dataset and using several
   different distances, confirmation of the hypothesis that CGR patterns
   are preserved along a genomic DNA sequence, and are different for DNA
   sequences originating from genomes of different species. This finding
   lends support to the theory that CGRs of genomic sequences can act as
   graphic genomic signatures. In particular, we compare the CGR patterns
   of over five hundred different 150,000 bp genomic sequences spanning one
   complete chromosome from each of six organisms, representing all
   kingdoms of life: H. sapiens (Animalia; chromosome 21), S. cerevisiae
   (Fungi; chromosome 4), A. thaliana (Plantae; chromosome 1), P.
   falciparum (Protista; chromosome 14), E. coli (Bacteria - full genome),
   and P. furiosus (Archaea - full genome). To maximize the diversity
   within each species, we also analyze the interrelationships within a set
   of over five hundred 150,000 bp genomic sequences sampled from the
   entire aforementioned genomes. Lastly, we provide some preliminary
   evidence of this method's ability to classify genomic DNA sequences at
   lower taxonomic levels by comparing sequences sampled from the entire
   genome of H. sapiens (class Mammalia, order Primates) and of M. musculus
   (class Mammalia, order Rodentia), for a total length of approximately
   174 million basepairs analyzed. We compute pairwise distances between
   CGRs of these genomic sequences using six different distances, and
   construct Molecular Distance Maps, which visualize all sequences as
   points in a two-dimensional or three-dimensional space, to
   simultaneously display their interrelationships.Conclusion: Our analysis
   confirms, for this dataset, that CGR patterns of DNA sequences from the
   same genome are in general quantitatively similar, while being different
   for DNA sequences from genomes of different species. Our assessment of
   the performance of the six distances analyzed uses three different
   quality measures and suggests that several distances outperform the
   Euclidean distance, which has so far been almost exclusively used for
   such studies.</abstract><date>AUG 7 2015</date><author>Karamichalis, Rallis
   Kari, Lila
   Konstantinidis, Stavros
   Kopecki, Steffen</author></paper><paper><title>Seeing is believing: good graphic design principles for medical research</title><abstract>Have you noticed when you browse a book, journal, study report, or
   product label how your eye is drawn to figures more than to words and
   tables? Statistical graphs are powerful ways to transparently and
   succinctly communicate the key points of medical research. Furthermore,
   the graphic design itself adds to the clarity of the messages in the
   data. The goal of this paper is to provide a mechanism for selecting the
   appropriate graph to thoughtfully construct quality deliverables using
   good graphic design principles. Examples are motivated by the efforts of
   a Safety Graphics Working Group that consisted of scientists from the
   pharmaceutical industry, Food and Drug Administration, and academic
   institutions. Copyright (c) 2015 John Wiley &amp; Sons, Ltd.</abstract><date>SEP 30 2015</date><author>Duke, Susan P.
   Bancken, Fabrice
   Crowe, Brenda
   Soukup, Mat
   Botsis, Taxiarchis
   Forshee, Richard</author></paper><paper><title>User-drawn sketch-based 3D object retrievalusing sparse coding</title><abstract>3D object retrieval from user-drawn (sketch) queries is one of the
   important research issues in the areas of pattern recognition and
   computer graphics for simulation, visualization, and Computer Aided
   Design. The performance of any content-based 3D object retrieval system
   crucially depends on the availability of effective descriptors and
   similarity measures for this kind of data. We present a sketch-based
   approach for improving 3D object retrieval effectiveness by optimizing
   the representation of one particular type of features (oriented
   gradients) using a sparse coding approach. We perform experiments, the
   results of which show that the retrieval quality improves over
   alternative features and codings. Based our findings, the coding can be
   proposed for sketch-based 3D object retrieval systems relying on
   oriented gradient features.</abstract><date>JUN 2015</date><author>Yoon, Sang Min
   Yoon, Gang-Joon
   Schreck, Tobias</author></paper><paper><title>Statistical shape analysis of temporal lobe in mesial temporal sclerosis
   patients</title><abstract>Surgery is regarded as a common treatment option for patients with
   mesial temporal lobe epilepsy due to hippocampal sclerosis but sometimes
   deciding this diagnosis can be very difficult. We aim to investigate the
   shape differences in the temporal lobe of mesial temporal sclerosis
   epilepsy patients compared with healthy controls, investigating the side
   difference and, if present, assessing the clinical application of this
   situation.The MRI scans of mesial TLE patients and controls were
   retrospectively reviewed. Temporal lobe data were collected from the
   two-dimensional digital images. Standard anthropometric landmarks were
   selected and marked on each digital image using TPSDIG 2.04 software.
   Eight anatomic landmarks were marked on images. A generalized Procrustes
   analysis was used to evaluate the shape difference. The shape
   deformation of the temporal lobe from control to patient was evaluated
   using the TPS method.There were statistically significant TL shape
   differences between groups. High level deformations for the left and
   right side from the control to patient group were seen in the TPS
   graphic. The highest deformation was determined at the inferior lateral
   temporal midpoint of the middle temporal gyri and superior temporal
   landmark points of both the right and left sides.Our study for the first
   time demonstrated temporal shape differences in TLE patients using a
   landmark-based geometrical morphometric method by taking into
   consideration the topographic distribution of TL.</abstract><date>NOV 2015</date><author>Ocakoglu, Gokhan
   Taskapilioglu, M. Ozgur
   Ercan, Ilker
   Demir, Aylin Bican
   Hakyemez, Bahattin
   Bekar, Ahmet
   Bora, Ibrahim</author></paper><paper><title>Scale-space feature extraction on digital surfaces</title><abstract>A classical problem in many computer graphics applications consists in
   extracting significant zones or points on an object surface, like loci
   of tangent discontinuity (edges), maxima or minima of curvatures,
   inflection points, etc. These places have specific local geometrical
   properties and often called generically features. An important problem
   is related to the scale, or range of scales, for which a feature is
   relevant. We propose a new robust method to detect features on digital
   data (surface of objects in Z(3), which exploits asymptotic properties
   of recent digital curvature estimators. In Coeurjolly et al [1] and
   Levallois et al. [1,2], authors have proposed curvature estimators
   (mean, principal and Gaussian) on 2D and 3D digitized shapes and have
   demonstrated their multigrid convergence (for C-3-smooth surfaces).
   Since such approaches integrate local information within a ball around
   points of interest the radius is a crucial parameter. In this paper, we
   consider the radius as a scale-space parameter. By analyzing the
   behavior of such curvature estimators as the ball radius tends to zero,
   we propose a tool to efficiently characterize and extract several
   relevant features (edges, smooth and flat parts) on digital surfaces.
   (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>OCT 2015</date><author>Levallois, Jeremy
   Coeurjolly, David
   Lachaud, Jacques-Olivier</author></paper><paper><title>Video segmentation with L-0 gradient minimization</title><abstract>Video segmentation is an important preprocessing step for many computer
   vision and graphics tasks. Its main goal is to group the voxels in the
   video volume with similar appearance and motion into spatio-temporally
   consistent supervoxels. In this paper, we formulate video segmentation
   as an L-0 gradient minimization problem, so that the spatio-temporal
   coherence can be effectively enforced through a gradient sparsity
   pursuit way. In our method, the appearance and motion descriptor space
   is first built for over-segmented image patches of each video frame.
   Then the L-0 gradient minimization is performed in the descriptor space,
   for both spatial and temporal dimensions. To solve the non-convex L-0
   norm minimization problem, we extend the fused coordinate descent
   algorithm from 2D image grids to 3D video volume. We conduct
   quantitative evaluation of our method in a public video segmentation
   benchmark LIBSVX. The experimental results demonstrate our superior
   performance to state-of-the-arts in segmentation accuracy and
   undersegmentation error, and comparable performance in boundary recall
   and explained variation. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>FEB 2016</date><author>Cheng, Xuan
   Feng, Yuanli
   Zeng, Ming
   Liu, Xinguo</author></paper><paper><title>Chinese herbal medicine network and core treatments for allergic skin
   diseases: Implications from a nationwide database</title><abstract>Ethno-pharmacological relevance: Chinese herbal medicine (CHM) is
   commonly used to treat skin diseases, but CHM prescription patterns are
   difficult to understand due to their complexity and interconnections.
   This study aimed to demonstrate CHM core treatments and network for
   treatment of allergic skin diseases by analyzing a nationwide
   prescription database.Materials and methods: All CHM prescriptions made
   for atopic dermatitis (with age limitation &lt;= 12 years) and urticaria
   for the entire year of 2011 were included. Association rule mining (ARM)
   combined with social network analysis (SNA) were used to analyze CHM
   prescriptions and explore the CHM prescription pattern and
   network.Results: A total of 27,350 and 97,188 prescriptions for atopic
   dermatitis and urticaria, respectively, were analyzed. Xiao-Feng-San
   (XFS) was the most commonly used CHM (32% of prescriptions for atopic
   dermatitis and 47.4% for urticaria) and was the core treatment for both
   diseases. Moreover, 42 and 82 important CHM-CHM combinations were
   identified to establish the CHM network, and XFS with Dictamnus
   dasycarpus Turcz was the most prevalent (6.4% for atopic dermatitis and
   9.1% for urticaria). Traditional Chinese Medicine heat syndrome was most
   prevalent cause. Extensive anti-inflammation, anti-allergy,
   anti-oxidation, and anti-bacterial effects were also found among the
   CHMs.Conclusions: Network analysis on CHM prescriptions provides graphic
   and comprehensive illustrations regarding CHM treatment for atopic
   dermatitis and urticaria. The CHM network analysis of prescriptions is
   essential to realize the CHM treatments and to select suitable
   candidates for clinical use or further studies. (C) 2015 Elsevier
   Ireland Ltd. All rights reserved.</abstract><date>JUN 20 2015</date><author>Chen, Hsing-Yu
   Lin, Yi-Hsuan
   Huang, Jen-Wu
   Chen, Yu-Chun</author></paper><paper><title>Potential Application Areas of GIS in Preconstruction Planning</title><abstract>The construction industry is experiencing changes in the tools and
   techniques used in the planning for high productivity, safety, and
   sustainability. These changes have not been seen much in the
   preconstruction planning (PCP) stage. The PCP is highly individualistic
   in nature, in which planners/contractors use their past experience to
   plan a project within the constraints of time, cost, and quality. The
   use of planning techniques in the PCP stage has been the subject of
   research, development, and implementation for a long time; however, at
   present, importance is being placed on the use of different tools. Tools
   like knowledge-based systems, computer graphics, and a combination of
   the two have been reported in the literature. Literature also suggests
   the use of geographic information systems (GIS) as a recent tool in the
   PCP stage. A structure that is to be developed becomes an important
   component of its neighborhood. It is closely related to all other
   existing facilities in its surroundings; hence, its construction should
   not be planned in isolation but in reference to its locality, existing
   facilities/utilities, topography, and so on. In the PCP stage,
   consideration should be given to its surroundings, in which the use of
   GIS has been found to be helpful. Therefore, the primary objective of
   the present study is to explore the potential application areas of GIS,
   as a tool, in the PCP stage. The present study informs practitioners in
   the construction industry about the application areas of GIS for their
   wider use in real-life projects. Finally, research areas worthy for
   further investigation are highlighted.(C) 2015 American Society of Civil
   Engineers.</abstract><date>JAN 2016</date><author>Bansal, V. K.</author></paper><paper><title>Analyses of pupils' polygonal shape drawing strategy with respect to
   handwriting performance</title><abstract>Polygonal shape drawing tasks are commonly used in psychological,
   clinical and standard handwriting tests to evaluate children's
   development. Early detection of physical/mental disorders within
   subjects therefore requires objective analysis of the drawing tasks.
   This analysis would help to identify specific rehabilitation needs and
   accurate detection of disorders. Herein, the aim is to determine the
   correlation between the performance of polygonal shape drawing and
   levels in handwriting performance. In the reported experimentation two
   groups of participants aged between 6 and 7 were studied. The first
   group was identified by educational experts as being below-average
   writers within their age group whilst the second group was age-matched
   controls of average and above. Subjects were required to draw an
   isosceles triangle within a novel computer-based framework founded on a
   pen-based graphic tablet capture device. Subsequently, a sequential
   feature vector containing performance values relating to the order in
   which they drew the triangle was extracted from tablet data and compared
   against one another when presented in constructional strategy models.
   Statistical analyses and automated classification were applied to
   sequences to infer handwriting level based on the triangle drawing
   strategy. From our experiments drawing strategies showed significant
   differences in drawing end-point position, number of strokes used, and
   the frequency of particular drawing strategies amongst average and
   below-average handwriting groups. Additionally, a support vector machine
   classifier was used to detect group membership based on the triangle
   drawing strategy. From this exemplar polygonal shape drawing study it is
   revealed that there are details in children's drawing strategy which
   considerably differs in grouping based on handwriting performance.</abstract><date>AUG 2015</date><author>Tabatabaey-Mashadi, Narges
   Sudirman, Rubita
   Guest, Richard M.
   Khalid, Puspa Inayat</author></paper><paper><title>Improved L-0 Gradient Minimization with L-1 Fidelity for Image Smoothing</title><abstract>Edge-preserving image smoothing is one of the fundamental tasks in the
   field of computer graphics and computer vision. Recently, L-0 gradient
   minimization (LGM) has been proposed for this purpose. In contrast to
   the total variation (TV) model which employs the L-1 norm of the image
   gradient, the LGM model adopts the L-0 norm and yields much better
   results for the piecewise constant image. However, as an improvement of
   the total variation (TV) model, the LGM model also suffers, even more
   seriously, from the staircasing effect and is not robust to noise. In
   order to overcome these drawbacks, in this paper, we propose an
   improvement of the LGM model by prefiltering the image gradient and
   employing the L-1 fidelity. The proposed improved LGM (ILGM) behaves
   robustly to noise and overcomes the staircasing artifact effectively.
   Experimental results show that the ILGM is promising as compared with
   the existing methods.</abstract><date>SEP 18 2015</date><author>Pang, Xueshun
   Zhang, Suqi
   Gu, Junhua
   Li, Lingling
   Liu, Boying
   Wang, Huaibin</author></paper><paper><title>Efficient urban flood simulation using a GPU-accelerated SPH model</title><abstract>Urban flooding may lead to significant losses of properties and lives
   and numerical modelling can facilitate better flood risk management to
   reduce losses. Flood modelling generally involves seeking numerical
   solutions to the shallow water equations (SWEs) or one of the simplified
   forms using the traditional numerical methods including the finite
   difference method (FDM), finite volume method (FVM) and finite element
   method (FEM). Recently, a relatively new approach, smoothed particle
   hydrodynamics (SPH), has also been used to solve the SWEs and
   encouraging results have been reported. However, the SPH method is
   computationally too demanding for efficient simulations, which has been
   one of the major disadvantages dogging its wider applications. This work
   presents an SPH model that is computationally accelerated by modern
   graphic processing units (GPUs) for efficient urban flooding modelling.
   The model's predictive capability and enhanced computational efficiency
   are demonstrated by application to experimental and field-scale
   hypothetic urban flood events.</abstract><date>DEC 2015</date><author>Liang, Qiuhua
   Xia, Xilin
   Hou, Jingming</author></paper><paper><title>Eye gaze technology: a South African perspective.</title><abstract>PURPOSE: Based on the bioecological model by Bronfenbrenner, this paper
   will provide a broad perspective on factors that need to be taken into
   account in order to facilitate communication and participation in
   preliterate children making use of electronic Augmentative and
   Alternative Communication (AAC) systems accessed through eye
   gaze.METHOD: Two case studies of children who have been provided with
   the technology described are presented. The case studies were analysed
   using the four nested systems of the ecology as a framework to describe
   not only the environment, but also the processes and interactions
   between the persons and their context.RESULTS: Risk and opportunity
   factors are evident at all levels of the ecology.CONCLUSIONS: While a
   good fit between the person and the technology is an essential starting
   point, additional factors pertaining to the partner, the immediate
   environment as well as meso-, exo- and macrosystemic issues (such as
   societal attitudes and funding sources) have a significant influence on
   benefits derived. In resource-limited environments, the lack of support
   at more distal levels of the ecology (meso-, exo- and marosystemic
   levels) seems to be a factor that differentiates these environments from
   more resourced ones. Implications for Rehabilitation Within
   resource-limited environments lack of support from wider ecological
   systems pose a risk to the implementation of eye gaze technology.
   Attempts to improve collaboration between all role players could provide
   the opportunity for the establishment of an integrated plan for
   intervention and set the stage for information sharing and multiskilling
   between role players. Intervention should not only be aimed at
   addressing the needs of the individual client and their family, but also
   focus on building community capacity that could provide support to
   others.</abstract><date>2015-Jul</date><author>van Niekerk, Karin
   Tonsing, Kerstin</author></paper><paper><title>The effect of misclassification error on risk estimation in case-control
   studies.</title><abstract>INTRODUCTION: In epidemiological studies, misclassification error,
   especially differential misclassification, has serious
   implications.OBJECTIVE: To illustrate how differential misclassification
   error (DME) and non-differential misclassification error (NDME) occur in
   a case-control design and to describe the trends in DME and
   NDME.METHODS: Different sensitivity levels, specificity levels,
   prevalence rates and odds ratios were simulated. Interaction graphics
   were constructed to study bias in the different settings, and the effect
   of the different factors on bias was described using linear
   models.RESULTS: One hundred per cent of the biases caused by NDME were
   negative. DME biased the association positively more often than it did
   negatively (70 versus 30%), increasing or decreasing the OR estimate
   towards the null hypothesis.CONCLUSIONS: The effect of the sensitivity
   and specificity in classifying exposure, the prevalence of exposure in
   controls and true OR differed between positive and negative biases. The
   use of valid exposure classification instruments with high sensitivity
   and high specificity is recommended to mitigate this type of bias.</abstract><date>2015 Apr-Jun</date><author>Baena, Armando
   Garces-Palacio, Isabel Cristina
   Grisales, Hugo</author></paper><paper><title>Benchmark test of accelerated multi-slice simulation by GPGPU</title><abstract>A fast multi-slice image simulation by parallelized computation using a
   graphics processing unit (CPU) has been developed. The image simulation
   contains multiple sets of computing steps, such as Fourier transform and
   pixel-to-pixel operation. The efficiency of CPU varies depending on the
   type of calculation. In the effective case of utilizing CPU, the
   calculation speed is conducted hundreds of times faster than a central
   processing unit (CPU). The benchmark test of parallelized multi-slice
   was performed, and the results of contents, such as TEM imaging, STEM
   imaging and CBD calculation are reported. Some features of the
   simulation software are also introduced. (C) 2015 Elsevier B.V. All
   rights reserved,</abstract><date>NOV 2015</date><author>Hosokawa, Fumio
   Shinkawa, Takao
   Arai, Yoshihiro
   Sannomiya, Takumi</author></paper><paper><title>Comparison of Neck Screw and Conventional Fixation Techniques in
   Mandibular Condyle Fractures Using 3-Dimensional Finite Element Analysis</title><abstract>Purpose: To compare the mechanical stress on the mandibular condyle
   after the reduction and fixation of mandibular condylar fractures using
   the neck screw and 2 other conventional techniques according to
   3-dimensional finite element analysis.Materials and Methods: A
   3-dimensional finite element model of a mandible was created and
   graphically simulated on a computer screen. The model was fixed with 3
   different techniques: a 2.0-mm plate with 4 screws, 2 plates (1 1.5-mm
   plate and 1 2.0-mm plate) with 4 screws, and a neck screw. Loads were
   applied that simulated muscular action, with restrictions of the upper
   movements of the mandible, differentiation of the cortical and medullary
   bone, and the virtual `` folds'' of the plates and screws so that they
   could adjust to the condylar surface. Afterward, the data were exported
   for graphic visualization of the results and quantitative analysis was
   performed.Results: The 2-plate technique exhibited better stability in
   regard to displacement of fractures, deformity of the synthesis
   materials, and minimum and maximum tension values. The results with the
   neck screw were satisfactory and were similar to those found when a
   miniplate was used.Conclusion: Although the study shows that 2 isolated
   plates yielded better results compared with the other groups using other
   fixation systems and methods, the neck screw could be an option for
   condylar fracture reduction. (C) 2015 American Association of Oral and
   Maxillofacial Surgeons</abstract><date>JUL 2015</date><author>Conci, Ricardo Augusto
   Silveira Tomazi, Flavio Henrique
   Noritomi, Pedro Yoshito
   Lopes da Silva, Jorge Vicente
   Fritscher, Guilherme Genehr
   Heitz, Claiton</author></paper><paper><title>Computational Intelligence-based Entertaining Level Generation for
   Platform Games</title><abstract>With computers becoming ubiquitous and high resolution graphics reaching
   the next level, computer games have become a major source of
   entertainment. It has been a tedious task for game developers to measure
   the entertainment value of the computer games. The entertainment value
   of a game does depend upon the genre of the game in addition to the game
   contents. In this paper, we propose a set of entertainment metrics for
   the platform genre of games. The set of entertainment metrics is
   proposed based upon certain theories on entertainment in computer games.
   To test the metrics, we use an evolutionary algorithm for automated
   generation of game rules which are entertaining. The proposed approach
   starts with an initial set of randomly generated games and, based upon
   the proposed metrics as an objective function, guides the evolutionary
   process. The results produced are counterchecked against the
   entertainment criteria of humans by conducting a human user survey and a
   controller learning ability experiment. The proposed metrics and the
   evolutionary process of generating games can be employed by any platform
   game for the purpose of automatic generation of interesting games
   provided an initial search space is given.</abstract><date>NOV 2 2015</date><author>Halim, Zahid
   Baig, Abdul Rauf
   Abbas, Ghulam</author></paper><paper><title>A Review of Eye Gaze in Virtual Agents, Social Robotics and HCI:
   Behaviour Generation, User Interaction and Perception</title><abstract>A person's emotions and state of mind are apparent in their face and
   eyes. As a Latin proverb states: 'The face is the portrait of the mind;
   the eyes, its informers'. This presents a significant challenge for
   Computer Graphics researchers who generate artificial entities that aim
   to replicate the movement and appearance of the human eye, which is so
   important in human-human interactions. This review article provides an
   overview of the efforts made on tackling this demanding task. As with
   many topics in computer graphics, a cross-disciplinary approach is
   required to fully understand the workings of the eye in the transmission
   of information to the user. We begin with a discussion of the movement
   of the eyeballs, eyelids and the head from a physiological perspective
   and how these movements can be modelled, rendered and animated in
   computer graphics applications. Furthermore, we present recent research
   from psychology and sociology that seeks to understand higher level
   behaviours, such as attention and eye gaze, during the expression of
   emotion or during conversation. We discuss how these findings are
   synthesized in computer graphics and can be utilized in the domains of
   Human-Robot Interaction and Human-Computer Interaction for allowing
   humans to interact with virtual agents and other artificial entities. We
   conclude with a summary of guidelines for animating the eye and head
   from the perspective of a character animator.</abstract><date>SEP 2015</date><author>Ruhland, K.
   Peters, C. E.
   Andrist, S.
   Badler, J. B.
   Badler, N. I.
   Gleicher, M.
   Mutlu, B.
   McDonnell, R.</author></paper><paper><title>Runtime Analysis of GPU-Based Stereo Matching</title><abstract>This paper elaborates on the possibility to leverage the highly parallel
   nature of GPUs to implement more efficient stereo matching algorithms.
   Different algorithms have been implemented and compared on the CPU and
   the GPU in order to show the speedup gained by moving the computation to
   the graphics card. The results were evaluated for accuracy using the
   test available on the Middlebury website for stereo vision. An
   assessment of the runtime performance was done by a script which
   examined the runtime behaviour of the individual steps of the stereo
   matching algorithm.</abstract><date>NOV 2015</date><author>Zentner, Christian
   Liu, Yan</author></paper><paper><title>Blood pressure increases with body size in mammals</title><abstract>In a recent technical comment regarding our analysis of the scaling of
   blood pressure with body mass in mammals (White and Seymour 2014),
   Packard (2015) argues that the trends in our graphs do not accurately
   reflect the relationship between the original variables, and that
   neither the graphics nor the accompanying statistical analyses provide
   strong support for the conclusions from the study, namely that larger
   mammals have higher arterial blood pressures. Here we take the
   opportunity to respond to these criticisms.</abstract><date>DEC 2015</date><author>White, Craig R.
   Seymour, Roger S.</author></paper><paper><title>A Publishing Method of Lightweight Three-Dimensional Assembly
   Instruction for Complex Products</title><abstract>In order to accurately guide on-site workers to quickly accomplish the
   assembly job of complex products, and reduce the deployment cost of
   assembly instruction, we propose a publishing method of lightweight 3D
   assembly instruction for complex products. In this paper, the key frames
   of assembly motion and the 3D technical annotations in the lightweight
   model are mapped to the time-based assembly process. Then, the annotated
   lightweight model and assembly process information are integrated and
   published into a single 3D assembly instruction document. An assembly
   instruction publishing example of satellite antenna feed component shows
   that the lightweight 3D assembly instruction is well instructive and
   affordable because it provides the interactive simulation of assembly
   process and time-based display of assembly technical annotations without
   using expensive computer-aided design (CAD) systems, graphics
   workstations, or virtual reality equipments. This method gives a full
   play to the advantages of model-based definition technology and
   lightweight model, and fills the gap between the process planning and
   the instruction publishing in the 3D virtual manufacturing environment.</abstract><date>SEP 2015</date><author>Geng, Junhao
   Zhang, Sumei
   Yang, Bin</author></paper><paper><title>Real-time time-division color electroholography using a single GPU and a
   USB module for synchronizing reference light</title><abstract>We propose real-time time-division color electroholography using a
   single graphics processing unit (GPU) and a simple synchronization
   system of reference light. To facilitate real-time time-division color
   electroholography, we developed a light emitting diode (LED) controller
   with a universal serial bus (USB) module and the drive circuit for
   reference light. A one-chip RGB LED connected to a personal computer via
   an LED controller was used as the reference light. A single GPU
   calculates three computer-generated holograms (CGHs) suitable for red,
   green, and blue colors in each frame of a three-dimensional (3D) movie.
   After CGH calculation using a single GPU, the CPU can synchronize the
   CGH display with the color switching of the one-chip RGB LED via the LED
   controller. Consequently, we succeeded in real-time time-division color
   electroholography for a 3D object consisting of around 1000 points per
   color when an NVIDIA GeForce GTX TITAN was used as the GPU. Furthermore,
   we implemented the proposed method in various GPUs. The experimental
   results showed that the proposed method was effective for various GPUs.
   (C) 2015 Optical Society of America</abstract><date>DEC 1 2015</date><author>Araki, Hiromitsu
   Takada, Naoki
   Niwase, Hiroaki
   Ikawa, Shohei
   Fujiwara, Masato
   Nakayama, Hirotaka
   Kakue, Takashi
   Shimobaba, Tomoyoshi
   Ito, Tomoyoshi</author></paper><paper><title>Distinct roles for GABA across multiple timescales in mammalian
   circadian timekeeping</title><abstract>The suprachiasmatic nuclei (SCN), the central circadian pacemakers in
   mammals, comprise a multiscale neuronal system that times daily events.
   We use recent advances in graphics processing unit computing to generate
   a multiscale model for the SCN that resolves cellular electrical
   activity down to the timescale of individual action potentials and the
   intracellular molecular events that generate circadian rhythms. We use
   the model to study the role of the neurotransmitter GABA in
   synchronizing circadian rhythms among individual SCN neurons, a topic of
   much debate in the circadian community. The model predicts that GABA
   signaling has two components: phasic (fast) and tonic (slow). Phasic
   GABA postsynaptic currents are released after action potentials, and can
   both increase or decrease firing rate, depending on their timing in the
   interspike interval, a modeling hypothesis we experimentally validate;
   this allows flexibility in the timing of circadian output signals.
   Phasic GABA, however, does not significantly affect molecular
   timekeeping. The tonic GABA signal is released when cells become very
   excited and depolarized; it changes the excitability of neurons in the
   network, can shift molecular rhythms, and affects SCN synchrony. We
   measure which neurons are excited or inhibited by GABA across the day
   and find GABA-excited neurons are synchronized by-and GABA-inhibited
   neurons repelled from-this tonic GABA signal, which modulates the
   synchrony in the SCN provided by other signaling molecules. Our
   mathematical model also provides an important tool for circadian
   research, and a model computational system for the many multiscale
   projects currently studying brain function.</abstract><date>JUL 21 2015</date><author>DeWoskin, Daniel
   Myung, Jihwan
   Belle, Mino D. C.
   Piggins, Hugh D.
   Takumi, Toru
   Forger, Daniel B.</author></paper><paper><title>Interactive Near-Field Illumination for Photorealistic Augmented Reality
   with Varying Materials on Mobile Devices</title><abstract>At present, photorealistic augmentation is not yet possible since the
   computational power of mobile devices is insufficient. Even streaming
   solutions from stationary PCs cause a latency that affects user
   interactions considerably. Therefore, we introduce a differential
   rendering method that allows for a consistent illumination of the
   inserted virtual objects on mobile devices, avoiding delays. The
   computation effort is shared between a stationary PC and the mobile
   devices to make use of the capacities available on both sides. The
   method is designed such that only a minimum amount of data has to be
   transferred asynchronously between the participants. This allows for an
   interactive illumination of virtual objects with a consistent appearance
   under both temporally and spatially varying real illumination
   conditions. To describe the complex near-field illumination in an indoor
   scenario, HDR video cameras are used to capture the illumination from
   multiple directions. In this way, sources of illumination can be
   considered that are not directly visible to the mobile device because of
   occlusions and the limited field of view. While our method focuses on
   Lambertian materials, we also provide some initial approaches to
   approximate non-diffuse virtual objects and thereby allow for a wider
   field of application at nearly the same cost.</abstract><date>DEC 2015</date><author>Rohmer, Kai
   Bueschel, Wolfgang
   Dachselt, Raimund
   Grosch, Thorsten</author></paper><paper><title>Working with the HL7 metamodel in a Model Driven Engineering context</title><abstract>HL7 (Health Level 7) International is an organization that defines
   health information standards. Most HL7 domain information models have
   been designed according to a proprietary graphic language whose domain
   models are based on the HL7 metamodel. Many researchers have considered
   using HL7 in the MDE (Model-Driven Engineering) context. A limitation
   has been identified: all MDE tools support UML (Unified Modeling
   Language), which is a standard model language, but most do not support
   the HL7 proprietary model language. We want to support software
   engineers without HL7 experience, thus realworld problems would be
   modeled by them by defining system requirements in UML that are
   compliant with HL7 domain models transparently. The objective of the
   present research is to connect HL7 with software analysis using a
   generic model-based approach. This paper introduces a first approach to
   an HL7 MDE solution that considers the MIF (Model Interchange Format)
   metamodel proposed by HL7 by making use of a plug-in developed in the EA
   (Enterprise Architect) tool. (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>OCT 2015</date><author>Martinez-Garcia, A.
   Garcia-Garcia, J. A.
   Escalona, M. J.
   Parra-Calderon, C. L.</author></paper><paper><title>PteroTerra: a searchable pterosaur database web application that
   interfaces with Google Earth</title><abstract>Recently, paleontologists have begun using Internet databases and Google
   Earth((R)) as new tools to share data with the scientific community. The
   newly developed web application PteroTerra ([GRAPHICS]), implemented
   using the Ruby on Rails((R)) web framework, is a specimen-based
   pterosaur database that interfaces with Google Earth((R)). This database
   currently catalogues over 1300 pterosaur specimens from all over the
   world and includes information about each specimen such as taxon name,
   classification, geologic age, geographic location of discovery, geologic
   formation, rock type, paleoenvironment, articulation, wingspan, proposed
   diet and housing institution. The application allows users to search for
   specimens based on keywords and to create groups of pterosaurs based on
   shared characteristics. Groups can then be downloaded as a.kml file,
   which can be automatically uploaded into Google Earth((R)) in order to
   study geographic patterns of pterosaur specimens based on any criteria
   of interest. The use and continuous updating of PteroTerra will provide
   pterosaurologists and other paleontologists with a central location for
   storing and obtaining information about particular pterosaur specimens,
   as well as a way for researchers to observe pterosaur patterns on a
   worldwide scale. The principles behind this program can easily be
   expanded to other fields of study.</abstract><date>AUG 18 2015</date><author>McLain, Matthew A.
   Chase, Brad
   Bryant, Eric</author></paper><paper><title>An Iterative Process for Developing and Evaluating a Computer-Based
   Prostate Cancer Decision Aid for African American Men</title><abstract>Background. The disproportionate burden of prostate cancer (PrCA) among
   African American (AA) men amplifies the need for informed decisions
   about PrCA screening. To create a computer-based decision aid (CBDA) for
   increasing prostate knowledge, decision self-efficacy, and intention to
   make an informed decision, the study implemented an iterative approach
   to develop a culturally appropriate CBDA. Method. A short CBDA prototype
   containing PrCA information and interactive activities was developed. A
   sample of 21 AA men aged 37 to 66 years in South Carolina participated
   in one of seven 90-minute focus groups and completed a 36-item survey.
   Updates were made to the CBDA based on participant feedback. The CBDA
   and heuristic evaluation surveys were then distributed to six expert
   reviewers. Ten men were also randomly selected from our sample
   population to participate in interviews regarding usability of the CBDA.
   Results. Participants and expert reviewers expressed consensus on many
   features of the CBDA, but some suggested changes to the format and
   graphics in order to enhance the CBDA's effectiveness. Development and
   evaluation processes and implications are discussed. Conclusions. Using
   CBDAs for informed decision making may be appropriate for AA men. It is
   important to engage the community and experts in an iterative
   development process to ensure that a CBDA is relevant for priority
   populations.</abstract><date>SEP 2015</date><author>Owens, Otis L.
   Friedman, Daniela B.
   Brandt, Heather M.
   Bernhardt, Jay M.
   Hebert, James R.</author></paper><paper><title>On geodesics of the rotation group SO(3)</title><abstract>Geodesics on SO(3) are characterized by constant angular velocity
   motions and as great circles on a three-sphere. The former
   interpretation is widely used in optometry and the latter features in
   the interpolation of rotations in computer graphics. The simplicity of
   these two disparate interpretations belies the complexity of the
   corresponding rotations. Using a quaternion representation for a
   rotation, we present a simple proof of the equivalence of the
   aforementioned characterizations and a straightforward method to
   establish features of the corresponding rotations.</abstract><date>NOV 2015</date><author>Novelia, Alyssa
   O'Reilly, Oliver M.</author></paper><paper><title>A new 2D graphical representation of protein sequence and its
   application</title><abstract>Graphical representation is a very efficient tool for visual analysis of
   protein sequences. In this paper, a novel 2D graphical representation
   scheme is proposed on the basis of a newly introduced concept, named
   characteristic model of the protein sequences. After obtaining the 2D
   graphics of protein sequences, two numerical characterizations of them
   is designed as descriptors to analyze the nine DN5 protein sequences,
   simulation and analysis results show that, comparing with existing
   methods, our method is not only visible, intuitional, and simple, but
   also has no circuit or degeneracy, and even more important, since the
   storage space required by our method is constant and has nothing to do
   with the length of protein sequences, then it can keep excellent visual
   inspection for long protein sequences.</abstract><date>SEP 2015</date><author>Wang, Lei
   Peng, Hui
   Zheng, Jinhua
   Qiu, Yanzi</author></paper><paper><title>Aerobic biodegradation of organic compounds in hydraulic fracturing
   fluids</title><abstract>Little is known of the attenuation of chemical mixtures created for
   hydraulic fracturing within the natural environment. A synthetic
   hydraulic fracturing fluid was developed from disclosed industry
   formulas and produced for laboratory experiments using commercial
   additives in use by Marcellus shale field crews. The experiments
   employed an internationally accepted standard method (OECD 301A) to
   evaluate aerobic biodegradation potential of the fluid mixture by
   monitoring the removal of dissolved organic carbon (DOC) from an aqueous
   solution by activated sludge and lake water microbial consortia for two
   substrate concentrations and four salinities. Microbial degradation
   removed from 57 % to more than 90 % of added DOC within 6.5 days, with
   higher removal efficiency at more dilute concentrations and little
   difference in overall removal extent between sludge and lake microbe
   treatments. The alcohols isopropanol and octanol were degraded to levels
   below detection limits while the solvent acetone accumulated in
   biological treatments through time. Salinity concentrations of 40 g/L or
   more completely inhibited degradation during the first 6.5 days of
   incubation with the synthetic hydraulic fracturing fluid even though
   communities were pre-acclimated to salt. Initially diverse microbial
   communities became dominated by 16S rRNA sequences affiliated with
   Pseudomonas and other Pseudomonadaceae after incubation with the
   synthetic fracturing fluid, taxa which may be involved in acetone
   production. These data expand our understanding of constraints on the
   biodegradation potential of organic compounds in hydraulic fracturing
   fluids under aerobic conditions in the event that they are accidentally
   released to surface waters and shallow soils.[GRAPHICS].</abstract><date>JUL 2015</date><author>Kekacs, Daniel
   Drollette, Brian D.
   Brooker, Michael
   Plata, Desiree L.
   Mouser, Paula J.</author></paper><paper><title>Computing Locally Injective Mappings by Advanced MIPS</title><abstract>Computing locally injective mappings with low distortion in an efficient
   way is a fundamental task in computer graphics. By revisiting the
   well-known MIPS (Most-Isometric ParameterizationS) method, we introduce
   an advanced MIPS method that inherits the local injectivity of MIPS,
   achieves as low as possible distortions compared to the state-of-the-art
   locally injective mapping techniques, and performs one to two orders of
   magnitude faster in computing a mesh-based mapping. The success of our
   method relies on two key components. The first one is an enhanced MIPS
   energy function that penalizes the maximal distortion significantly and
   distributes the distortion evenly over the domain for both mesh-based
   and meshless mappings. The second is a use of the inexact block
   coordinate descent method in mesh-based mapping in a way that
   efficiently minimizes the distortion with the capability not to be
   trapped early by the local minimum. We demonstrate the capability and
   superiority of our method in various applications including mesh
   parameterization, mesh-based and meshless deformation, and mesh
   improvement.</abstract><date>AUG 2015</date><author>Fu, Xiao-Ming
   Liu, Yang
   Guo, Baining</author></paper><paper><title>Formative questioning in computer learning environments: a course for
   pre-service mathematics teachers</title><abstract>This paper focuses on a specific aspect of formative assessment, namely
   questioning. Given that computers have gained widespread use in learning
   and teaching, specific attention should be made when organizing
   formative assessment in computer learning environments (CLEs). A course
   including various workshops was designed to develop knowledge and skills
   of questioning in CLEs. This study investigates how pre-service
   mathematics teachers used formative questioning with technological tools
   such as Geogebra and Graphic Calculus software. Participants are 35
   pre-service mathematics teachers. To analyse formative questioning, two
   types of questions are investigated: mathematical questions and
   technical questions. Data were collected through lesson plans, teaching
   notes, interviews and observations. Descriptive statistics of the number
   of questions in the lesson plans before and after the workshops are
   presented. Examples of two types of questions are discussed using the
   theoretical framework. One pre-service teacher was selected and a deeper
   analysis of the way he used questioning during his three lessons was
   also investigated. The findings indicated an improvement in using
   technical questions for formative purposes and that the course provided
   a guideline in planning and using mathematical and technical questions
   in CLEs.</abstract><date>NOV 17 2015</date><author>Akkoc, Hatice</author></paper><paper><title>Computational performance of a smoothed particle hydrodynamics
   simulation for shared-memory parallel computing</title><abstract>The computational performance of a smoothed particle hydrodynamics (SPH)
   simulation is investigated for three types of current shared-memory
   parallel computer devices: many integrated core (MIC) processors,
   graphics processing units (GPUs), and multi-core CPUs. We are especially
   interested in efficient shared-memory allocation methods for each
   chipset, because the efficient data access patterns differ between
   compute unified device architecture (CUDA) programming for GPUs and
   OpenMP programming for MIC processors and multi-core CPUs. We first
   introduce several parallel implementation techniques for the SPH code,
   and then examine these on our target computer architectures to determine
   the most effective algorithms for each processor unit. In addition, we
   evaluate the effective computing performance and power efficiency of the
   SPH simulation on each architecture, as these are critical metrics for
   overall performance in a multi-device environment. In our benchmark
   test, the GPU is found to produce the best arithmetic performance as a
   standalone device unit, and gives the most efficient power consumption.
   The multi-core CPU obtains the most effective computing performance. The
   computational speed of the MIC processor on Xeon Phi approached that of
   two Xeon CPUs. This indicates that using MICs is an attractive choice
   for existing SPH codes on multi-core CPUs parallelized by OpenMP, as it
   gains computational acceleration without the need for significant
   changes to the source code. (C) 2015 The Authors. Published by Elsevier
   B.V.</abstract><date>SEP 2015</date><author>Nishiura, Daisuke
   Furuichi, Mikito
   Sakaguchi, Hide</author></paper><paper><title>Accelerating the Smith-Waterman algorithm with interpair pruning and
   band optimization for the all-pairs comparison of base sequences</title><abstract>Background: The Smith-Waterman algorithm is known to be a more sensitive
   approach than heuristic algorithms for local sequence alignment
   algorithms. Despite its sensitivity, a greater time complexity
   associated with the Smith-Waterman algorithm prevents its application to
   the all-pairs comparisons of base sequences, which aids in the
   construction of accurate phylogenetic trees. The aim of this study is to
   achieve greater acceleration using the Smith-Waterman algorithm (by
   realizing interpair block pruning and band optimization) compared with
   that achieved using a previous method that performs intrapair block
   pruning on graphics processing units (GPUs).Results: We present an
   interpair optimization method for the Smith-Waterman algorithm with the
   aim of accelerating the all-pairs comparison of base sequences. Given
   the results of the pairs of sequences, our method realizes efficient
   block pruning by computing a lower bound for other pairs that have not
   yet been processed. This lower bound is further used for band
   optimization. We integrated our interpair optimization method into SW#,
   a previous GPU-based implementation that employs variants of a banded
   Smith-Waterman algorithm and a banded Myers-Miller algorithm. Evaluation
   using the six genomes of Bacillus anthracis shows that our method pruned
   88 % of the matrix cells on a single GPU and 73 % of the matrix cells on
   two GPUs. For the genomes of the human chromosome 21, the alignment
   performance reached 202 giga-cell updates per second (GCUPS) on two
   Tesla K40 GPUs.Conclusions: Efficient interpair pruning and band
   optimization makes it possible to complete the all-pairs comparisons of
   the sequences of the same species 1.2 times faster than the intrapair
   pruning method. This acceleration was achieved at the first phase of
   SW#, where our method significantly improved the initial lower bound.
   However, our interpair optimization was not effective for the comparison
   of the sequences of different species such as comparing human,
   chimpanzee, and gorilla. Consequently, our method is useful in
   accelerating the applications that require optimal local alignments
   scores for the same species. The source code is available for download
   from http://www-hagi.ist.osaka-u.ac.jp/research/code/.</abstract><date>OCT 6 2015</date><author>Okada, Daiki
   Ino, Fumihiko
   Hagihara, Kenichi</author></paper><paper><title>Indian scorpions collected in Karnataka: maintenance in captivity, venom
   extraction and toxicity studies</title><abstract>Background: Maintenance of scorpions under laboratory conditions is
   ideal for long-term venom collection to explore the therapeutic
   applications of scorpion venom. Collection of venom by electrical
   stimulation requires a reliable stimulator and effective restrainer.
   Thus, the present study was conducted to develop a convenient method to
   maintain scorpions and to extract their venom for toxicity studies via a
   modified restrainer and stimulator.Methods: Four different scorpion
   species were collected, among which three species were maintained in the
   laboratory in containers that mimic their natural habitat. Venom was
   extracted from Hottentotta rugiscutis by electrical stimulation at 8 V
   for 18 months and LD50 was estimated by the graphic method of Miller and
   Tainter.Results: A total of 373 scorpions including Hottentotta
   rugiscutis, Hottentotta tamulus, Lychas tricarinatus and Heterometrus
   swammerdami were collected, identified and maintained successfully,
   achieving a 97 % survival rate. Hottentotta rugiscutis yielded 6.0 mL of
   venom by electrical stimulation. The LD50 of H. rugiscutis venom was
   estimated to be 3.02 mg/kg of body weight in female Swiss albino
   mice.Conclusions: Scorpions were successfully maintained for 18 months.
   Herein we have also documented a simple, cost-effective method of venom
   extraction by electrical stimulation using a modified restrainer.
   Furthermore, Hottentotta rugiscutis was reported for the first time in
   Karnataka.</abstract><date>DEC 4 2015</date><author>Nagaraj, Santhosh Kambaiah
   Dattatreya, Pavana
   Boramuthi, Thippeswamy Nayaka</author></paper><paper><title>A new algorithm for design, operation and cost assessment of struvite
   (MgNH4PO4) precipitation processes</title><abstract>Deliberate struvite (MgNH4PO4) precipitation from wastewater streams has
   been the topic of extensive research in the last two decades and is
   expected to gather worldwide momentum in the near future as a P-reuse
   technique. A wide range of operational alternatives has been reported
   for struvite precipitation, including the application of various Mg(II)
   sources, two pH elevation techniques and several Mg:P ratios and pH
   values. The choice of each operational parameter within the struvite
   precipitation process affects process efficiency, the overall cost and
   also the choice of other operational parameters. Thus, a comprehensive
   simulation program that takes all these parameters into account is
   essential for process design. This paper introduces a systematic
   decision-supporting tool which accepts a wide range of possible
   operational parameters, including unconventional Mg(II) sources (i.e.
   seawater and seawater nanofiltration brines). The study is supplied with
   a free-of-charge computerized tool ([GRAPHICS]) which links two computer
   platforms (Python and PHREEQC) for executing thermodynamic calculations
   according to predefined kinetic considerations. The model can be (inter
   alia) used for optimizing the struvite-fluidized bed reactor process
   operation with respect to P removal efficiency, struvite purity and
   economic feasibility of the chosen alternative. The paper describes the
   algorithm and its underlying assumptions, and shows results (i.e.
   effluent water quality, cost breakdown and P removal efficiency) of
   several case studies consisting of typical wastewaters treated at
   various operational conditions.</abstract><date>AUG 3 2015</date><author>Birnhack, Liat
   Nir, Oded
   Telzhenski, Marina
   Lahav, Ori</author></paper><paper><title>NGL Viewer: a web application for molecular visualization</title><abstract>The NGL Viewer (http://proteinformatics.charite.de/ngl) is a web
   application for the visualization of macromolecular structures. By fully
   adopting capabilities of modern web browsers, such as WebGL, for
   molecular graphics, the viewer can interactively display large molecular
   complexes and is also unaffected by the retirement of third-party
   plug-ins like Flash and Java Applets. Generally, the web application
   offers comprehensive molecular visualization through a graphical user
   interface so that life scientists can easily access and profit from
   available structural data. It supports common structural file-formats
   (e.g. PDB, mmCIF) and a variety of molecular representations (e.g.
   'cartoon, spacefill, licorice'). Moreover, the viewer can be embedded in
   other web sites to provide specialized visualizations of entries in
   structural databases or results of structure-related calculations.</abstract><date>JUL 1 2015</date><author>Rose, Alexander S.
   Hildebrand, Peter W.</author></paper><paper><title>A Low-cost System for Generating Near-realistic Virtual Actors</title><abstract>Generating virtual actors is one of the most challenging fields in
   computer graphics. The reconstruction of a realistic virtual actor has
   been paid attention by the academic research and the film industry to
   generate human-like virtual actors. Many movies were acted by human-like
   virtual actors, where the audience cannot distinguish between real and
   virtual actors. The synthesis of realistic virtual actors is considered
   a complex process. Many techniques are used to generate a realistic
   virtual actor; however they usually require expensive hardware
   equipment. In this paper, a low-cost system that generates
   near-realistic virtual actors is presented. The facial features of the
   real actor are blended with a virtual head that is attached to the
   actor's body. Comparing with other techniques that generate virtual
   actors, the proposed system is considered a low-cost system that
   requires only one camera that records the scene without using any
   expensive hardware equipment. The results of our system show that the
   system generates good near-realistic virtual actors that can be used on
   many applications.</abstract><date>JUN 2015</date><author>Afifi, Mahmoud
   Hussain, Khaled F.
   Ibrahim, Hosny M.
   Omar, Nagwa M.</author></paper><paper><title>Performance improvement of data mining in Weka through multi-core and
   GPU acceleration: opportunities and pitfalls</title><abstract>Data mining tools may be computationally demanding, which leads to an
   increasing interest on parallel computing strategies in order to improve
   their performance. While multi-core processors and Graphics Processing
   Units (GPUs) accelerators increased the computing power of current
   desktop computers, we observe that desktop-based data mining tools do
   not take full advantage of these architectures yet. This paper
   investigates strategies to improve the performance of Weka, a popular
   data mining tool, through multi-core and GPU acceleration. Using
   performance profiling of Weka, we identify operations that could improve
   the data mining performance when parallelized. We selected two of these
   operations, and analyze the impact of their parallel execution on Weka's
   performance. These experiments demonstrate that while significant
   speedups can be achieved, all operations are not prone to be
   parallelized, which reinforces the need for a careful and well-studied
   selection of the candidates.</abstract><date>AUG 2015</date><author>Engel, Tiago Augusto
   Charao, Andrea Schwertner
   Kirsch-Pinheiro, Manuele
   Steffenel, Luiz-Angelo</author></paper><paper><title>Active learning for sketch recognition</title><abstract>The increasing availability of pen-based tablets, and pen-based
   interfaces opened the avenue for computer graphics applications that can
   utilize sketch recognition technologies for natural interaction. This
   has led to an increasing interest in sketch recognition algorithms
   within the computer graphics community. However, a key problem getting
   in the way of building accurate sketch recognizers has been the
   necessity of creating large amounts of annotated training data. Several
   authors have attempted to address this issue by creating synthetic data,
   or by building easy-to-use annotation tools. In this paper, we take a
   different approach, and demonstrate that the active learning technology
   can be used to reduce the amount of manual annotation required to
   achieve a target recognition accuracy. In particular, we show that by
   annotating few, but carefully selected examples, we can surpass
   accuracies achievable with equal number of arbitrarily selected
   examples. This work is the first comprehensive study on the use of
   active learning for sketch recognition. We present results of extensive
   analyses and show that the utility of active learning depends on a
   number of practical factors that require careful consideration. These
   factors include the choices of informativeness measures, batch selection
   strategies, seed size, and domain-specific factors such as feature
   representation and the choice of database. Our results imply that the
   Margin based informativeness measure consistently outperforms other
   measures. We also show that active learning brings definitive advantages
   in challenging databases when accompanied with powerful feature
   representations. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>NOV 2015</date><author>Yanik, Erelcan
   Sezgin, Tevfik Metin</author></paper><paper><title>A rational cubic clipping method for computing real roots of a
   polynomial</title><abstract>Many problems in computer aided geometric design and computer graphics
   can be turned into a root-finding problem of a polynomial equation.
   Among various solutions, clipping methods based on the Bernstein-Bezier
   form usually have good numerical stability. A traditional clipping
   method using polynomials of degree r can achieve a convergence rate of r
   + 1 for a single root. It utilizes two polynomials of degree r to bound
   the given polynomial f(t) of degree n, where r = 2, 3, and the roots of
   the bounding polynomials are used for clipping off the subintervals
   containing no roots of f(t). This paper presents a rational cubic
   clipping method for finding the roots of a polynomial f(t) within an
   interval. The bounding rational cubics can achieve an approximation
   order of 7 and the corresponding convergence rate for finding a single
   root is also 7. In addition, differently from the traditional cubic
   clipping method solving the two bounding polynomials in O (n(2)), the
   new method directly constructs the two rational cubics in O(n) which can
   be used for bounding f (t) in many cases. Some examples are provided to
   show the efficiency, the approximation effect and the convergence rate
   of the new method. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>OCT 2015</date><author>Chen, Xiao-Diao
   Ma, Weiyin
   Ye, Yangtian</author></paper><paper><title>I-COMS: Interprotein-COrrelated Mutations Server</title><abstract>Interprotein contact prediction using multiple sequence alignments
   (MSAs) is a useful approach to help detect protein-protein interfaces.
   Different computational methods have been developed in recent years as
   an approximation to solve this problem. However, as there are
   discrepancies in the results provided by them, there is still no
   consensus on which is the best performing methodology. To address this
   problem, I-COMS (interprotein COrrelated Mutations Server) is presented.
   I-COMS allows to estimate covariation between residues of different
   proteins by four different covariation methods. It provides a graphical
   and interactive output that helps compare results obtained using
   different methods. I-COMS automatically builds the required MSA for the
   calculation and produces a rich visualization of either intraprotein
   and/or interprotein covariating positions in a circos representation.
   Furthermore, comparison between any two methods is available as well as
   the overlap between any or all four methodologies. In addition, as a
   complementary source of information, a matrix visualization of the
   corresponding scores is made available and the density plot distribution
   of the inter, intra and inter+intra scores are calculated. Finally, all
   the results can be downloaded (including MSAs, scores and graphics) for
   comparison and visualization and/or for further analysis.</abstract><date>JUL 1 2015</date><author>Iserte, Javier
   Simonetti, Franco L.
   Zea, Diego J.
   Teppa, Elin
   Marino-Buslje, Cristina</author></paper><paper><title>A new approach for optical assessment of directional anisotropy in
   turbid media</title><abstract>A study of polarized light transport in scattering media exhibiting
   directional anisotropy or linear birefringence is presented in this
   paper. Novel theoretical and experimental methodologies for the
   quantification of birefringent alignment based on out-of-plane polarized
   light transport are presented here. A polarized Monte Carlo model and a
   polarimetric imaging system were devised to predict and measure the
   impact of birefringence on an impinging linearly polarized light beam.
   Ex-vivo experiments conducted on bovine tendon, a biological sample
   consisting of highly packed type I collagen fibers with birefringent
   property, showed good agreement with the analytical
   results.[GRAPHICS]Top view geometry of the in-plane (a) and the
   out-of-plane (b) detection. Letter C indicates the location of the
   detection arm.</abstract><date>JAN 2016</date><author>Ghassemi, Pejhman
   Moffatt, Lauren T.
   Shupp, Jeffrey W.
   Ramella-Roman, Jessica C.</author></paper><paper><title>A Dataset for Visual Navigation with Neuromorphic Methods</title><abstract>Standardized benchmarks in Computer Vision have greatly contributed to
   the advance of approaches to many problems in the field. If we want to
   enhance the visibility of event-driven vision and increase its impact,
   we will need benchmarks that allow comparison among different
   neuromorphic methods as well as comparison to Computer Vision
   conventional approaches. We present datasets to evaluate the accuracy of
   frame-free and frame-based approaches for tasks of visual navigation.
   Similar to conventional Computer Vision datasets, we provide synthetic
   and real scenes, with the synthetic data created with graphics packages,
   and the real data recorded using a mobile robotic platform carrying a
   dynamic and active pixel vision sensor (DAVIS) and an RGB+Depth sensor.
   For both datasets the cameras move with a rigid motion in a static
   scene, and the data includes the images, events, optic flow, 3D camera
   motion, and the depth of the scene, along with calibration procedures.
   Finally, we also provide simulated event data generated synthetically
   from well-known frame-based optical flow datasets.</abstract><date>FEB 23 2016</date><author>Barranco, Francisco
   Fermuller, Cornelia
   Aloimonos, Yiannis
   Delbruck, Tobi</author></paper><paper><title>A multimodal parallel architecture: A cognitive framework for multimodal
   interactions</title><abstract>Human communication is naturally multimodal, and substantial focus has
   examined the semantic correspondences in speech-gesture and text-image
   relationships. However, visual narratives, like those in comics, provide
   an interesting challenge to multimodal communication because the words
   and/or images can guide the overall meaning, and both modalities can
   appear in complicated "grammatical" sequences: sentences use a syntactic
   structure and sequential images use a narrative structure. These dual
   structures create complexity beyond those typically addressed by
   theories of multimodality where only a single form uses combinatorial
   structure, and also poses challenges for models of the linguistic system
   that focus on single modalities. This paper outlines a broad theoretical
   framework for multimodal interactions by expanding on Jackendoffs (2002)
   parallel architecture for language. Multimodal interactions are
   characterized in terms of their component cognitive structures: whether
   a particular modality (verbal, bodily, visual) is present, whether it
   uses a grammatical structure (syntax, narrative), and whether it
   "dominates" the semantics of the overall expression. Altogether, this
   approach integrates multimodal interactions into an existing framework
   of language and cognition, and characterizes interactions between
   varying complexity in the verbal, bodily, and graphic domains. The
   resulting theoretical model presents an expanded consideration of the
   boundaries of the "linguistic" system and its involvement in multimodal
   interactions, with a framework that can benefit research on corpus
   analyses, experimentation, and the educational benefits of
   multimodality. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>JAN 2016</date><author>Cohn, Neil</author></paper><paper><title>CyberSoLIM: A cyber platform for digital soil mapping</title><abstract>In recent years, new demands and trends have emerged in the digital soil
   mapping (DSM) field: the range of applications as well as the range of
   users has become much diverse. Users of DSM include not only experts in
   soil science community but also those from relevant domains (e.g.,
   hydrology, ecology). In addition, the rapid expansion of areas for DSM
   and the ever increasing spatial resolution of covariates call for an
   accelerated level of computation. These new trends have raised the bar
   for DSM software platforms. This paper presents CyberSoLIM, a prototype
   system to illustrate an idea of easy-of-use and high performance enabled
   cyber environment for DSM. CyberSoLIM is implemented to have five main
   features: (1) heuristic modeling, which allows digital soil mappers to
   construct DSM workflow easily; (2) visualized modeling with which the
   conceptual workflow of DSM is expressed by graphic icons; (3) workflow
   reuse, which increases the efficiency of DSM deployment; (4) online
   execution and high-performance computing, which can use the advantage of
   cyber infrastructure and high-performance computing; and (5) web service
   enabled, which provides effective and easy means to share and integrate
   models and algorithms. As an illustration of such environment,
   CyberSoLIM was used to infer the silt content in topsoil (0-20 cm) in
   China's Anhui Province. The case study confirms that software platform
   as illustrated by CyberSoLIM is easy to use and efficient in terms of
   mapping productivity. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>FEB 1 2016</date><author>Jiang, Jingchao
   Zhu, A-Xing
   Qin, Cheng-Zhi
   Zhu, Tongxin
   Liu, Junzhi
   Du, Fei
   Liu, Jing
   Zhang, Guiming
   An, Yiming</author></paper><paper><title>Very High Frame Rate Volumetric Integration of Depth Images on Mobile
   Devices</title><abstract>Volumetric methods provide efficient, flexible and simple ways of
   integrating multiple depth images into a full 3D model. They provide
   dense and photorealistic 3D reconstructions, and parallelised
   implementations on GPUs achieve real-time performance on modern graphics
   hardware. To run such methods on mobile devices, providing users with
   freedom of movement and instantaneous reconstruction feedback, remains
   challenging however. In this paper we present a range of modifications
   to existing volumetric integration methods based on voxel block hashing,
   considerably improving their performance and making them applicable to
   tablet computer applications. We present (i) optimisations for the basic
   data structure, and its allocation and integration; (ii) a highly
   optimised raycasting pipeline; and (iii) extensions to the camera
   tracker to incorporate IMU data. In total, our system thus achieves
   frame rates up 47 Hz on a Nvidia Shield Tablet and 910 Hz on a Nvidia
   GTX Titan X GPU, or even beyond 1.1 kHz without visualisation.</abstract><date>NOV 2015</date><author>Kaehler, Olaf
   Prisacariu, Victor Adrian
   Ren, Carl Yuheng
   Sun, Xin
   Torr, Philip
   Murray, David</author></paper><paper><title>A model evaluation study for treatment planning of laser-induced thermal
   therapy</title><abstract>A cross-validation analysis evaluating computer model prediction
   accuracy for a priori planning magnetic resonance-guided laser-induced
   thermal therapy (MRgLITT) procedures in treating focal diseased brain
   tissue is presented. Two mathematical models are considered. (1) A
   spectral element discretisation of the transient Pennes bioheat transfer
   equation is implemented to predict the laser-induced heating in perfused
   tissue. (2) A closed-form algorithm for predicting the steady-state heat
   transfer from a linear superposition of analytic point source heating
   functions is also considered. Prediction accuracy is retrospectively
   evaluated via leave-one-out cross-validation (LOOCV). Modelling
   predictions are quantitatively evaluated in terms of a Dice similarity
   coefficient (DSC) between the simulated thermal dose and thermal dose
   information contained within N=22 MR thermometry datasets. During LOOCV
   analysis, the transient model's DSC mean and median are 0.7323 and
   0.8001 respectively, with 15 of 22 DSC values exceeding the success
   criterion of DSC0.7. The steady-state model's DSC mean and median are
   0.6431 and 0.6770 respectively, with 10 of 22 passing. A one-sample,
   one-sided Wilcoxon signed-rank test indicates that the transient finite
   element method model achieves the prediction success criteria, DSC0.7,
   at a statistically significant level.</abstract><date>OCT 3 2015</date><author>Fahrenholtz, Samuel J.
   Moon, Tim Y.
   Franco, Michael
   Medina, David
   Danish, Shabbar
   Gowda, Ashok
   Shetty, Anil
   Maier, Florian
   Hazle, John D.
   Stafford, Roger J.
   Warburton, Tim
   Fuentes, David</author></paper><paper><title>Untangling polygonal and polyhedral meshes via mesh optimization</title><abstract>We propose simple and efficient optimization-based untangling strategies
   for 2D polygonal and 3D polyhedral meshes. The first approach uses a
   size-based mesh metric, which eliminates inverted elements by averaging
   element size over the entire mesh. The second method uses a hybrid
   quality metric, which untangles inverted elements by simultaneously
   averaging element size and improving element shape. The last method
   using a variant of the hybrid quality metric gives a high penalty for
   inverted elements and employs an adaptive sigmoid function for handling
   various mesh sizes. Numerical experiments are presented to show the
   effectiveness of the proposed untangling strategies for various 2D
   polygonal and 3D polyhedral meshes.</abstract><date>JUL 2015</date><author>Kim, Jibum
   Chung, Jaeyong</author></paper><paper><title>Single particle tomography in EMAN2</title><abstract>Single particle tomography (SPT or subtomogram averaging) offers a
   powerful alternative to traditional 2-D single particle reconstruction
   for studying conformationally or compositionally heterogeneous
   macromolecules. It can also provide direct observation (without labeling
   or staining) of complexes inside cells at nanometer resolution. The
   development of computational methods and tools for SPT remains an area
   of active research. Here we present the EMAN2.1 SPT toolbox, which
   offers a full SPT processing pipeline, from particle picking to
   post-alignment analysis of subtomogram averages, automating most steps.
   Different algorithm combinations can be applied at each step, providing
   versatility and allowing for procedural cross-testing and
   specimen-specific strategies. Alignment methods include all-vs-all,
   binary tree, iterative single-model refinement, multiple-model
   refinement, and self-symmetry alignment. An efficient angular search,
   Graphic Processing Unit (GPU) acceleration and both threaded and
   distributed parallelism are provided to speed up processing. Finally,
   automated simulations, per particle reconstruction of subtiltseries, and
   per-particle Contrast Transfer Function (CTF) correction have been
   implemented. Processing examples using both real and simulated data are
   shown for several structures. (C) 2015 Elsevier Inc. All rights
   reserved.</abstract><date>JUN 2015</date><author>Galaz-Montoya, Jesus G.
   Flanagan, John
   Schmid, Michael F.
   Ludtke, Steven J.</author></paper><paper><title>Fast computation of seabed spherical-wave reflection coefficients in
   geoacoustic inversion</title><abstract>This paper develops a fast numerical approach to computing
   spherical-wave reflection coefficients (SWRCs) for layered seabeds,
   which provides substantial savings in computation time when used as the
   forward model for geoacoustic inversion of broadband seabed reflectivity
   data. The approach exploits the Sommerfeld-integral representation of
   SWRCs as the Hankel transform of a function proportional to the
   plane-wave reflection coefficient (PWRC), and applies Levin integration
   to the rapidly oscillating integrand cast as the product of a
   (pre-computed) media-independent matrix and a vector involving PWRCs at
   a sparse sampling of integration angles. Compared to conventional
   Simpson's rule integration for computation of the SWRC, the Levin
   integration yields speed-up factors of an order of magnitude or more.
   Further, it results in reduced memory requirements for storage of
   pre-computed quantities, a desirable property when a graphics processing
   unit (GPU) is used for parallel computation of SWRCs. The paper applies
   trans-dimensional Bayesian inversion to investigate the impact of
   forward modeling in terms of PWRCs and SWRCs on the estimation of
   geoacoustic parameters and uncertainties. Model comparisons are
   quantified in simulated-and measured-data inversions by comparing the
   estimated geoacoustic parameters to the true parameters or core
   measurements, respectively, and by calculating the deviance information
   criterion for model selection. (C) 2015 Acoustical Society of America.</abstract><date>OCT 2015</date><author>Quijano, Jorge E.
   Dosso, Stan E.
   Dettmer, Jan
   Holland, Charles W.</author></paper><paper><title>A literature review of bounding volumes hierarchy focused on collision
   detection</title><abstract>A bounding volume is a common method to simplify object representation
   by using the composition of geometrical shapes that enclose the object;
   it encapsulates complex objects by means of simple volumes and it is
   widely useful in collision detection applications and ray tracing for
   rendering algorithms. They are popular in computer graphics and
   computational geometry. Most popular bounding volumes are spheres,
   Oriented-Bounding Boxes (OBB's), Axis-Aligned Bounding Boxes (AABB's);
   moreover, the literature review includes ellipsoids, cylinders, sphere
   packing, sphere shells, k-DOP's, convex hulls, cloud of points, and
   minimal bounding boxes, among others. A Bounding Volume Hierarchy is
   usually a tree in which the complete object is represented tighter
   fitting every level of the hierarchy. Additionally, each bounding volume
   has a cost associated to construction, update, and interference tests.
   For instance, spheres are invariant to rotation and translations, then
   they do not require being updated; their constructions and interference
   tests are more straightforward then OBB's; however, their tightness is
   lower than other bounding volumes. Finally, three comparisons between
   two polyhedra; seven different algorithms were used, of which five are
   public libraries for collision detection.</abstract><date>2015</date><author>Dinas, Simena
   Banon, Jose M.</author></paper><paper><title>Portal-Masked Environment Map Sampling</title><abstract>We present a technique to efficiently importance sample distant,
   all-frequency illumination in indoor scenes. Standard environment
   sampling is inefficient in such cases since the distant lighting is
   typically only visible through small openings (e.g. windows). This
   visibility is often addressed by manually placing a portal around each
   window to direct samples towards the openings; however, uniformly
   sampling the portal (its area or solid angle) disregards the possibly
   high frequency environment map. We propose a new portal importance
   sampling technique which takes into account both the environment map and
   its visibility through the portal, drawing samples proportional to the
   product of the two. To make this practical, we propose a novel,
   portal-rectified reparametrization of the environment map with the key
   property that the visible region induced by a rectangular portal
   projects to an axis-aligned rectangle. This allows us to sample
   according to the desired product distribution at an arbitrary shading
   location using a single (precomputed) summed-area table per portal. Our
   technique is unbiased, relevant to many renderers, and can also be
   applied to rectangular light sources with directional emission profiles,
   enabling efficient rendering of non-diffuse light sources with soft
   shadows.</abstract><date>JUL 2015</date><author>Bitterli, Benedikt
   Novak, Jan
   Jarosz, Wojciech</author></paper><paper><title>GGEMS-Brachy: GPU GEant4-based Monte Carlo simulation for brachytherapy
   applications</title><abstract>In brachytherapy, plans are routinely calculated using the AAPM TG43
   formalism which considers the patient as a simple water object. An
   accurate modeling of the physical processes considering patient
   heterogeneity using Monte Carlo simulation (MCS) methods is currently
   too time-consuming and computationally demanding to be routinely used.
   In this work we implemented and evaluated an accurate and fast MCS on
   Graphics Processing Units (GPU) for brachytherapy low dose rate (LDR)
   applications. A previously proposed Geant4 based MCS framework
   implemented on GPU (GGEMS) was extended to include a hybrid GPU
   navigator, allowing navigation within voxelized patient specific images
   and analytically modeled I-125 seeds used in LDR brachytherapy. In
   addition, dose scoring based on track length estimator including
   uncertainty calculations was incorporated. The implemented GGEMS-brachy
   platform was validated using a comparison with Geant4 simulations and
   reference datasets. Finally, a comparative dosimetry study based on the
   current clinical standard (TG43) and the proposed platform was performed
   on twelve prostate cancer patients undergoing LDR brachytherapy.
   Considering patient 3D CT volumes of 400 x 250 x 65 voxels and an
   average of 58 implanted seeds, the mean patient dosimetry study run time
   for a 2% dose uncertainty was 9.35 s (approximate to 500 ms 10(-6)
   simulated particles) and 2.5 s when using one and four GPUs,
   respectively. The performance of the proposed GGEMS-brachy platform
   allows envisaging the use of Monte Carlo simulation based dosimetry
   studies in brachytherapy compatible with clinical practice. Although the
   proposed platform was evaluated for prostate cancer, it is equally
   applicable to other LDR brachytherapy clinical applications. Future
   extensions will allow its application in high dose rate brachytherapy
   applications.</abstract><date>JUL 7 2015</date><author>Lemarechal, Yannick
   Bert, Julien
   Falconnet, Claire
   Despres, Philippe
   Valeri, Antoine
   Schick, Ulrike
   Pradier, Olivier
   Garcia, Marie-Paule
   Boussion, Nicolas
   Visvikis, Dimitris</author></paper><paper><title>NiftySim: A GPU-based nonlinear finite element package for simulation of
   soft tissue biomechanics</title><abstract>NiftySim, an open-source finite element toolkit, has been designed to
   allow incorporation of high-performance soft tissue simulation
   capabilities into biomedical applications. The toolkit provides the
   option of execution on fast graphics processing unit (GPU) hardware,
   numerous constitutive models and solid-element options, membrane and
   shell elements, and contact modelling facilities, in a simple to use
   library.The toolkit is founded on the total Lagrangian explicit dynamics
   (TLEDs) algorithm, which has been shown to be efficient and accurate for
   simulation of soft tissues. The base code is written in C, and GPU
   execution is achieved using the nVidia CUDA framework. In most cases,
   interaction with the underlying solvers can be achieved through a single
   Simulator class, which may be embedded directly in third-party
   applications such as, surgical guidance systems. Advanced capabilities
   such as contact modelling and nonlinear constitutive models are also
   provided, as are more experimental technologies like reduced order
   modelling. A consistent description of the underlying solution
   algorithm, its implementation with a focus on GPU execution, and
   examples of the toolkit's usage in biomedical applications are
   provided.Efficient mapping of the TLED algorithm to parallel hardware
   results in very high computational performance, far exceeding that
   available in commercial packages.The NiftySim toolkit provides
   high-performance soft tissue simulation capabilities using GPU
   technology for biomechanical simulation research applications in medical
   image computing, surgical simulation, and surgical guidance
   applications.</abstract><date>JUL 2015</date><author>Johnsen, Stian F.
   Taylor, Zeike A.
   Clarkson, Matthew J.
   Hipwell, John
   Modat, Marc
   Eiben, Bjoern
   Han, Lianghao
   Hu, Yipeng
   Mertzanidou, Thomy
   Hawkes, David J.
   Ourselin, Sebastien</author></paper><paper><title>Three-directional motion-compensation mask-based novel look-up table on
   graphics processing units for video-rate generation of digital
   holographic videos of three-dimensional scenes</title><abstract>A three-directional motion-compensation mask-based novel look-up table
   method is proposed and implemented on graphics processing units (GPUs)
   for video-rate generation of digital holographic videos of
   three-dimensional (3D) scenes. Since the proposed method is designed to
   be well matched with the software and memory structures of GPUs, the
   number of compute-unified-device-architecture kernel function calls can
   be significantly reduced. This results in a great increase of the
   computational speed of the proposed method, allowing video-rate
   generation of the computer-generated hologram (CGH) patterns of 3D
   scenes. Experimental results reveal that the proposed method can
   generate 39.8 frames of Fresnel CGH patterns with 1920 x 1080 pixels per
   second for the test 3D video scenario with 12,088 object points on dual
   GPU boards of NVIDIA GTX TITANs, and they confirm the feasibility of the
   proposed method in the practical application fields of
   electroholographic 3D displays. (C) 2015 Optical Society of America</abstract><date>JAN 20 2016</date><author>Kwon, Min-Woo
   Kim, Seung-Cheol
   Kim, Eun-Soo</author></paper><paper><title>OVERVIEW OF RISK-ESTIMATION TOOLS FOR PRIMARY PREVENTION OF
   CARDIOVASCULAR DISEASES IN EUROPEAN POPULATIONS</title><abstract>To identify persons with a high risk for cardiovascular diseases (CVD)
   special tools (scores, charts, graphics or computer programs) for
   CVD-risk assessment based on levels of the certain risk factors have
   been constructed. The applicability of these instruments depends on the
   derivation cohorts, considered risk factors and endpoints, applied
   statistical methods as well as used formats.The review addresses the
   risk-estimation tools for primary prevention of CVD potentially relevant
   for European populations. The risk-estimation tools were identified
   using two previously published systematic reviews as well as conducting
   a literature search in MEDLINE and a manual search. Only instruments
   were considered which were derived from cohorts of at least 1,000
   participants of one gender without pre-existing CVD, enable risk
   assessment for a period of at least 5 years, were designed for an
   age-range of at least 25 years and published after the year 2000.A
   number of risk-estimation tools for CVD derived from single European,
   several European and from non-European cohorts were identified. From a
   clinical perspective, seem to be preferable instruments for risk of CVD
   contemporary developed for the population of interest, which use easily
   accessible measures and show a high discriminating ability. Instruments,
   restricting risk-estimation to certain cardiovascular events,
   recalibrated high-accuracy tools or tools derived from European
   populations with similar risk factors distribution and CVD-incidence are
   the second choice. In younger people, calculating the relative risk or
   cardiovascular age equivalence measures may be of more benefit.</abstract><date>JUN 2015</date><author>Gorenoi, Vitali
   Hagen, Anja</author></paper><paper><title>Distortionless segmentation image fusion and coordinate system
   transformation for 3D scene reconstruction on fibre-to-chip coupling</title><abstract>A three-dimensional (3D) scene of fibre-to-chip coupling is
   reconstructed from three captured 2D graphics in which coordinate system
   transformation and distortionless segmentation image fusion algorithm
   are presented to visually guide precise alignment and nanopositioning
   procedures based on the established single lens photometric machine
   vision system. As an instance, the 3D scene on wedge-shaped fibre (WSF)
   coupling with indium phosphide (InP) photonic integrated circuit (PIC)
   chip is demonstrated, where the wedged angle is calculated to be 46.5
   degrees, differing from the nominal value of 45 degrees with the error
   &lt;3.5%, and for the longitudinal displacement, the computation of 94 m
   has the error within 7% compared with the measurement of 88 m. Under the
   guidance of 3D scene reconstruction on WSF-InP PIC, the coupling
   efficiency is 2 dB higher than that without such stereoscopic image.</abstract><date>NOV 19 2015</date><author>Liu, Xu
   Sun, Xiaohan</author></paper><paper><title>Big Data Approaches for the Analysis of Large-Scale fMRI Data Using
   Apache Spark and GPU Processing: A Demonstration on Resting-State fMRI
   Data from the Human Connectome Project</title><abstract>Technologies for scalable analysis of very large datasets have emerged
   in the domain of internet computing, but are still rarely used in
   neuroimaging despite the existence of data and research questions in
   need of efficient computation tools especially in fMRI. In this work, we
   present software tools for the application of Apache Spark and Graphics
   Processing Units (GPUs) to neuroimaging datasets, in particular
   providing distributed file input for 4D NIfTI fMRI datasets in Scala for
   use in an Apache Spark environment. Examples for using this Big Data
   platform in graph analysis of fMRI datasets are shown to illustrate how
   processing pipelines employing it can be developed. With more tools for
   the convenient integration of neuroimaging file formats and typical
   processing steps, big data technologies could find wider endorsement in
   the community, leading to a range of potentially useful applications
   especially in view of the current collaborative creation of a wealth of
   large data repositories including thousands of individual fMRI datasets.</abstract><date>JAN 6 2016</date><author>Boubela, Roland N.
   Kalcher, Klaudius
   Huf, Wolfgang
   Nasel, Christian
   Moser, Ewald</author></paper><paper><title>MRI of the Prostate in Germany: Online Survey among Radiologists</title><abstract>Purpose: To assess structural, technical, and communicative aspects of
   dedicated MR examinations of the prostate (MRP) offered by radiologists
   in Germany.Materials and Methods: We conducted an eight-item online
   survey among members of the German Radiology Society (DRG). Radiological
   institutions were asked about their structure, i.e., either hospital
   department (HD) or private practice (PP), number of board-certified
   radiologists, postal regions, number of MRPs in 2011, MR technology and
   MR sequences applied, ways to communicate results, and feedback from
   referring physicians on results of subsequent tests and procedures.
   Submissions were cleared of redundancies and anonymized. Differences in
   the number of positive replies to each item were statistically
   significant at p &lt; 0.05 for two-tailed testing in 2x2 tables.Results:
   The survey represented board-certified radiologists in 128 institutions
   (63 HDs and 65 PPs) in 67/95 German postal regions (71 %). Almost
   two-thirds of institutions performed 11 to 50 MRPs in 2011, more often
   at 1.5 T (116/128, 91 %) than at 3.0 T (36/128, 28 %), and most
   frequently with surface coils (1.5 T, 88/116, 76 %; 3.0 T, 34/36, 94 %;
   chi-square, 1.9736, 0.1 &lt; p &lt; 0.25). About two-thirds of 1.5 T users and
   90 % of 3.0 T users applied at least one functional MR modality
   (diffusion-weighted imaging, dynamic contrast- enhanced imaging, or MR
   spectroscopy) for MRP. Reports including graphic representations of the
   prostate were applied by 21/128 institutions (16 %). Clinical feedback
   after MRP to radiologists other than upon their own request was
   infrequent (HDs, 32 - 45 %, PPs, 18 - 32 %).Conclusion: MRP was a widely
   available, small-volume examination among radiologists in Germany in
   2011. The technology mainstay was a 1.5 T surface coil examination
   including at least one functional MR modality. Dedicated reporting and
   feedback mechanisms for quality control were underdeveloped.</abstract><date>AUG 2015</date><author>Mueller-Lisse, U. G.
   Lewerich, B.
   Mueller-Lisse, U. L.
   Reiser, M.
   Scherr, M. K.</author></paper><paper><title>The importance of reaming the posterior femoral cortex before inserting
   lengthening nails and calculation of the amount of reaming</title><abstract>Background: Lengthening nails have been used to correct limb length
   discrepancy caused by different etiologies, as well as for
   post-traumatic reasons. Two important lengthening nail-related
   complications are damage to the distraction mechanism and femoral
   fractures around the nail tip. As a result of the curved anatomy of the
   femur, straight nails impinge on the anterior cortex. Therefore, proper
   reshaping of the medullary canal to accommodate straight lengthening
   nails is crucial for the prevention of this problem. Reaming the dense
   posterior cortex is important when aiming to insert a lengthening nail
   without incurring anterior cortex nail tip impingement-related
   complications. Posterior femoral cortex over-reaming is a solution to
   this situation.Methods: Sixty patients received lengthening nails during
   2008-2013, (ISKD, Fitbone, Precice). Posterior cortex rigid-reaming
   technique was used successfully in 45 retrograde femoral lengthening
   cases. The preoperatively planned posterior cortex amount was reamed
   until the impingement was overcome during the operation under
   fluoroscopic control for each case. Since the preoperative determination
   of posterior cortex reaming amount is time consuming and operator
   dependent, we evaluated the X rays of the patients with computer
   software and conventional paper-based measurements. The effect of
   reaming the posterior cortical wall on the inclination of the nail tip
   to the anterior femoral cortex was detected with measurements on the
   preoperative and postoperative lateral femoral X-rays by using the
   CorelDRAW (R) Graphic Suite X6 software package (Corel, Inc., Ottawa,
   Ontario, Canada) software. On the same software, X-rays and the
   posterior reaming amount were also calculated.Results: The mean age of
   the patients was 27 years (11-42), while the mean lengthening was 5.9 cm
   (2-14). The mean consolidation index was 1.05 (0.75-1.62), and the mean
   follow-up period was 31 months (range, 18-45 months). The mean distance
   of the osteotomy site to the intercondylar notch of the femur was 81.2
   mm (+/- 16.92). The mean displacement of the nail tip position was 15.42
   mm (+/- 4.77) on the measurements on the postoperative X-rays after nail
   insertion compared to the preoperative simulations on the templates. The
   mean posterior cortex reaming thickness was 3.68 mm (+/-
   1.02).Conclusions: We derived a formula that allows the required amount
   of optimal posterior cortex reaming to be determined. No
   impingement-related complications or nail damage were observed.</abstract><date>JAN 16 2016</date><author>Kucukkaya, Metin
   Karakoyun, Ozgur
   Erol, Mehmet Fatih</author></paper><paper><title>Common influence region problems</title><abstract>In this paper we propose and solve common influence region problems.
   These problems are related to the simultaneous influence, or the
   capacity to attract customers, of two sets of facilities of different
   types. For instance, while a facility of the first type competes with
   the other facilities of the first type, it cooperates with several
   facilities of the second type. The problems studied can be applied, for
   example, to decision-making support systems for marketing and/or
   locating facilities. We present parallel algorithms, to be run on a
   Graphics Processing Unit, for approximately solving the problems
   considered here. We also provide experimental results and discuss the
   efficiency and scalability of our approach. Finally, we present the
   speedup ratios obtained when the running times of the parallel proposed
   algorithms using a GPU are compared with those obtained from their
   respective efficient sequential CPU versions. (C) 2015 Elsevier Inc. All
   rights reserved.</abstract><date>NOV 10 2015</date><author>Fort, M.
   Sellares, J. A.</author></paper><paper><title>Reconstruction using 'triangular approximation' of bone grafts for
   orbital blowout fractures</title><abstract>There are many orbital wall reconstruction materials that can be used in
   surgery for orbital blowout fractures. We consider autogenous bone
   grafts to have the best overall characteristics among these materials
   and use thinned, inner cortical tables of the ilium. A bone bender is
   normally used to shape the inner iliac table to match the orbital shape.
   Since orbital walls curve three-dimensionally, processing of bone grafts
   is not easy and often requires much time and effort.We applied a
   triangular approximation method to the processing of bone grafts.
   Triangular approximation is a concept used in computer graphics for
   polygon processing. In this method, the shape of an object is
   represented as combinations of polygons, mainly triangles. In this
   study, the inner iliac table was used as a bone graft, and cuts or
   scores were made to create triangular sections. These triangular
   sections were designed three-dimensionally so that the shape of the
   resulting graft approximated to the three-dimensional orbital shape.
   This method was used in 12 patients with orbital blowout fractures,
   which included orbital floor fractures, medial wall fractures, and
   combined inferior and medial wall fractures. In all patients, bone
   grafts conformed to the orbital shape and good results were
   obtained.This simple method uses a reasonable and easy-to-understand
   approach and is useful in the treatment of bone defects in orbital
   blowout fractures when using a hard graft material. (C) 2015 European
   Association for Cranio-Maxillo-Facial Surgery. Published by Elsevier
   Ltd. All rights reserved.</abstract><date>OCT 2015</date><author>Saiga, Atsuomi
   Mitsukawa, Nobuyuki
   Yamaji, Yoshihisa</author></paper><paper><title>Design and Implementation of a Graphic 3D Simulator for the Study of
   Control Techniques Applied to Cooperative Robots</title><abstract>A new methodology is proposed for the design and implementation of
   three-dimensional simulations of manipulator robots, in individual as
   well as cooperative tasks, using several programming and 3D-design
   tools. To obtain easily the dynamic-mathematical models of movement for
   the represented systems, regardless of the kind of manipulator robot to
   be considered, the development of a computer algorithm is also
   presented. Besides, through the use of the Bullet library, we
   incorporated the detection of collisions between constitutive elements
   of the system, considering elements external to the robots, collisions
   between links of the same robot, or between two robots. Using a C++
   programming platform, we also designed and implemented a flexible and
   intuitive graphic user interface. The dynamic-mathematical performance
   of the cooperative interaction between two actual robots, designed and
   implemented in the Departamento de Ingenieria Electrica of the
   Universidad de Santiago de Chile (DIE-UdeSantiago de Chile), was
   modeled, plotted and three-dimensionally simulated, by using the new
   algorithm proposed for this purpose. Finally, the performance results
   are presented and analyzed.</abstract><date>DEC 2015</date><author>Urrea, Claudio
   Paul Coltters, Jean</author></paper><paper><title>Spectral turning bands for efficient Gaussian random fields generation
   on GPUs and accelerators</title><abstract>A random field (RF) is a set of correlated random variables associated
   with different spatial locations. RF generation algorithms are of
   crucial importance for many scientific areas, such as astrophysics,
   geostatistics, computer graphics, and many others. Current approaches
   commonly make use of 3D fast Fourier transform (FFT), which does not
   scale well for RF bigger than the available memory; they are also
   limited to regular rectilinear meshes. We introduce random field
   generation with the turning band method (RAFT), an RF generation
   algorithm based on the turning band method that is optimized for
   massively parallel hardware such as GPUs and accelerators. Our algorithm
   replaces the 3D FFT with a lower-order, one-dimensional FFT followed by
   a projection step and is further optimized with loop unrolling and
   blocking. RAFT can easily generate RF on non-regular (non-uniform)
   meshes and efficiently produce fields with mesh sizes bigger than the
   available device memory by using a streaming, out-of-core approach. Our
   algorithm generates RF with the correct statistical behavior and is
   tested on a variety of modern hardware, such as NVIDIA Tesla, AMD
   FirePro and Intel Phi. RAFT is faster than the traditional methods on
   regular meshes and has been successfully applied to two real case
   scenarios: planetary nebulae and cosmological simulations. Copyright (c)
   2015John Wiley &amp; Sons, Ltd.</abstract><date>NOV 2015</date><author>Hunger, Lars
   Cosenza, Biagio
   Kimeswenger, Stefan
   Fahringer, Thomas</author></paper><paper><title>Fast Wavefront Propagation (FWP) for Computing Exact Geodesic Distances
   on Meshes</title><abstract>Computing geodesic distances on triangle meshes is a fundamental problem
   in computational geometry and computer graphics. To date, two notable
   classes of algorithms, the Mitchell-Mount-Papadimitriou (MMP) algorithm
   and the Chen-Han (CH) algorithm, have been proposed. Although these
   algorithms can compute exact geodesic distances if numerical computation
   is exact, they are computationally expensive, which diminishes their
   usefulness for large-scale models and/or time-critical applications. In
   this paper, we propose the fast wavefront propagation (FWP) framework
   for improving the performance of both the MMP and CH algorithms. Unlike
   the original algorithms that propagate only a single window (a data
   structure locally encodes geodesic information) at each iteration, our
   method organizes windows with a bucket data structure so that it can
   process a large number of windows simultaneously without compromising
   wavefront quality. Thanks to its macro nature, the FWP method is less
   sensitive to mesh triangulation than the MMP and CH algorithms. We
   evaluate our FWP-based MMP and CH algorithms on a wide range of
   large-scale real-world models. Computational results show that our
   method can improve the speed by a factor of 3-10.</abstract><date>JUL 2015</date><author>Xu, Chunxu
   Wang, Tuanfeng Y.
   Liu, Yong-Jin
   Liu, Ligang
   He, Ying</author></paper><paper><title>WorldBrush: Interactive Example-based Synthesis of Procedural Virtual
   Worlds</title><abstract>We present a novel approach for the interactive synthesis and editing of
   virtual worlds. Our method is inspired by painting operations and uses
   methods for statistical example-based synthesis to automate content
   synthesis and deformation. Our real-time approach takes a form of local
   inverse procedural modeling based on intermediate statistical models:
   selected regions of procedurally and manually constructed example scenes
   are analyzed, and their parameters are stored as distributions in a
   palette, similar to colors on a painter's palette. These distributions
   can then be interactively applied with brushes and combined in various
   ways, like in painting systems. Selected regions can also be moved or
   stretched while maintaining the consistency of their content. Our method
   captures both distributions of elements and structured objects, and
   models their interactions. Results range from the interactive editing of
   2D artwork maps to the design of 3D virtual worlds, where constraints
   set by the terrain's slope are also taken into account.</abstract><date>AUG 2015</date><author>Emilien, Arnaud
   Vimont, Ulysse
   Cani, Marie-Paule
   Poulin, Pierre
   Benes, Bedrich</author></paper><paper><title>Quantitative visualization of alternative exon expression from RNA-seq
   data</title><abstract>Motivation: Analysis of RNA sequencing (RNA-Seq) data revealed that the
   vast majority of human genes express multiple mRNA isoforms, produced by
   alternative pre-mRNA splicing and other mechanisms, and that most
   alternative isoforms vary in expression between human tissues. As
   RNA-Seq datasets grow in size, it remains challenging to visualize
   isoform expression across multiple samples.Results: To help address this
   problem, we present Sashimi plots, a quantitative visualization of
   aligned RNA-Seq reads that enables quantitative comparison of exon usage
   across samples or experimental conditions. Sashimi plots can be made
   using the Broad Integrated Genome Viewer or with a stand-alone command
   line program.</abstract><date>JUL 15 2015</date><author>Katz, Yarden
   Wang, Eric T.
   Silterra, Jacob
   Schwartz, Schraga
   Wong, Bang
   Thorvaldsdottir, Helga
   Robinson, James T.
   Mesirov, Jill P.
   Airoldi, Edoardo M.
   Burge, Christopher B.</author></paper><paper><title>Cotton QTLdb: a cotton QTL database for QTL analysis, visualization, and
   comparison between Gossypium hirsutum and G-hirsutum x G-barbadense
   populations</title><abstract>A specialized database currently containing more than 2200 QTL is
   established, which allows graphic presentation, visualization and
   submission of QTL.In cotton quantitative trait loci (QTL), studies are
   focused on intraspecific Gossypium hirsutum and interspecific G.
   hirsutum x G. barbadense populations. These two populations are
   commercially important for the textile industry and are evaluated for
   fiber quality, yield, seed quality, resistance, physiological, and
   morphological trait QTL. With meta-analysis data based on the vast
   amount of QTL studies in cotton it will be beneficial to organize the
   data into a functional database for the cotton community. Here we
   provide a tool for cotton researchers to visualize previously identified
   QTL and submit their own QTL to the Cotton QTLdb database. The database
   provides the user with the option of selecting various QTL trait types
   from either the G. hirsutum or G. hirsutum x G. barbadense populations.
   Based on the user's QTL trait selection, graphical representations of
   chromosomes of the population selected are displayed in publication
   ready images. The database also provides users with trait information on
   QTL, LOD scores, and explained phenotypic variances for all QTL
   selected. The CottonQTLdb database provides cotton geneticist and
   breeders with statistical data on cotton QTL previously identified and
   provides a visualization tool to view QTL positions on chromosomes.
   Currently the database (Release 1) contains 2274 QTLs, and succeeding
   QTL studies will be updated regularly by the curators and members of the
   cotton community that contribute their data to keep the database
   current. The database is accessible from http://www.cottonqtldb.org.</abstract><date>AUG 2015</date><author>Said, Joseph I.
   Knapka, Joseph A.
   Song, Mingzhou
   Zhang, Jinfa</author></paper><paper><title>MultiElec: A MATLAB Based Application for MEA Data Analysis</title><abstract>We present MultiElec, an open source MATLAB based application for data
   analysis of microelectrode array (MEA) recordings. MultiElec displays an
   extremely user-friendly graphic user interface (GUI) that allows the
   simultaneous display and analysis of voltage traces for 60 electrodes
   and includes functions for activation-time determination, the production
   of activation-time heat maps with activation time and isoline display.
   Furthermore, local conduction velocities are semi-automatically
   calculated along with their corresponding vector plots. MultiElec allows
   ad hoc signal suppression, enabling the user to easily and efficiently
   handle signal artefacts and for incomplete data sets to be analysed.
   Voltage traces and heat maps can be simply exported for figure
   production and presentation. In addition, our platform is able to
   produce 3D videos of signal progression over all 60 electrodes.
   Functions are controlled entirely by a single GUI with no need for
   command line input or any understanding of MATLAB code. MultiElec is
   open source under the terms of the GNU General Public License as
   published by the Free Software Foundation, version 3. Both the program
   and source code are available to download from
   http://www.cancer.manchester.ac.uk/MultiElec/.</abstract><date>JUN 15 2015</date><author>Georgiadis, Vassilis
   Stephanou, Anastasis
   Townsend, Paul A.
   Jackson, Thomas R.</author></paper><paper><title>Towards the extrapolation of the valence-valence electron partial
   structure factor for liquid Mg near freezing from a combination of
   theory and experiment</title><abstract>Egelstaff, March, and McGill (1973) proposed the extraction of electron
   correlation functions in liquids from scattering data. Here, we appeal
   to computer simulation by de Wijs et al. (1995) on the partial structure
   factor[GRAPHICS]between ions (i) and valence electrons (v) for liquid Mg
   near freezing, to write the valence-valence partial structure
   factor[GRAPHICS]in terms of[GRAPHICS]and the neutron structure
   factor[GRAPHICS], to high accuracy.</abstract><date>SEP 3 2015</date><author>March, N. H.
   Angilella, G. G. N.</author></paper><paper><title>Dynamics Modeling and Control Simulation of an Autonomous Underwater
   Vehicle</title><abstract>A dynamics model of an open-shelf Autonomous underwater vehicle (AUV) is
   described in this paper. The virtual prototype technology and the
   control simulation software are used to build the virtual prototype
   model of AUV, and AUV dynamic location control arithmetic is simulated
   based on analyzing motion and hydrodynamic mathematical model of the
   virtual prototype. The simulation results indicate that the virtual
   prototype system has the function of simulation demo and performance
   validation, and can provide one kind of new method for AUV graphic
   simulation, and has very important practical meaning on AUV design and
   control research.</abstract><date>WIN 2015</date><author>Liu, Guijie
   Chen, Gong
   Jiao, Jianbo
   Jiang, Ruilin</author></paper><paper><title>3D hybrid-domain full waveform inversion on GPU</title><abstract>The traditional frequency-domain full waveform inversion (FWI) method
   has limited application for 3D case due to its significant computational
   challenges and huge memory cost. The time-frequency hybrid-domain FWI is
   thus utilized to combine the computational efficiency of time-domain FWI
   and multiscale inversion of frequency-domain FWI. We present a
   simplified hybrid-domain FWI method. Compared with the previous
   hybrid-domain FWI, our method can take one less Discrete Fourier
   Transform (DFT) and one less inverse Discrete Fourier Transform (IDFT)
   for single iteration and achieved the equivalent inversion effect.
   Meanwhile, the hybrid-domain FWI is very suitable for fine-grained
   parallel computation, and the application of graphic processing unit
   (GPU) has significantly enhanced the computation efficiency. Modeling,
   boundary absorption and DFT are the most time-consuming modules. The
   occupancy of the three kernels achieved a good level as Compute Unified
   Device Architecture (CUDA) Visual Profile shows and the bandwidth usage
   has been much improved by introducing the tiling method for 3D finite
   difference problem. Finally, our hybrid-domain FWI is applied to a 3D
   model on our personal computer equipped with GTX 680 graphic card, and
   complexity analysis of our algorithm is presented. The results further
   confirm the feasibility of this technique. (C) 2015 Elsevier Ltd. All
   rights reserved.</abstract><date>OCT 2015</date><author>Liu, Lu
   Ding, Renwei
   Liu, Hongwei
   Liu, Hong</author></paper><paper><title>Combination of pharmacophore hypothesis and molecular docking to
   identify novel inhibitors of HCV NS5B polymerase</title><abstract>Hepatitis C virus (HCV) infection or HCV-related liver diseases are now
   shown to cause more than 350,000 deaths every year. Adaptability of HCV
   genome to vary its composition and the existence of multiple strains
   makes it more difficult to combat the emergence of drug-resistant HCV
   infections. Among the HCV polyprotein which has both the structural and
   non-structural regions, the non-structural protein NS5B RNA-dependent
   RNA polymerase (RdRP) mainly mediates the catalytic role of RNA
   replication in conjunction with its viral protein machinery as well as
   host chaperone proteins. Lack of such RNA-dependent RNA polymerase
   enzyme in host had made it an attractive and hotly pursued target for
   drug discovery efforts. Recent drug discovery efforts targeting HCV RdRP
   have seen success with FDA approval for sofosbuvir as a direct-acting
   antiviral against HCV infection. However, variations in drug-binding
   sites induce drug resistance, and therefore targeting allosteric sites
   could delay the emergence of drug resistance. In this study, we focussed
   on allosteric thumb site II of the non-structural protein NS5B
   RNA-dependent RNA polymerase and developed a five-feature pharmacophore
   hypothesis/model which estimated the experimental activity with a strong
   correlation of 0.971 &amp; 0.944 for training and test sets, respectively.
   Further, the Guner-Henry score of 0.6 suggests that the model was able
   to discern the active and inactive compounds and enrich the true
   positives during a database search. In this study, database search and
   molecular docking results supported by experimental HCV viral
   replication inhibition assays suggested ligands with best fitness to the
   pharmacophore model dock to the key residues involved in thumbs site II,
   which inhibited the HCV 1b viral replication in sub-micro-molar
   range.HCV nonstructural protein NS5B RNA-dependent RNA polymerase (RdRP)
   mediates the catalytic role of viral RNA replication. Lack of host
   RNA-dependent RNA polymerase enzyme had made it an attractive and hotly
   pursued target for drug discovery efforts. In this study, we developed a
   five-feature pharmacophore (3D QSAR) model for thumb site inhibitors of
   HCV RdRP, which estimated the experimental activity with a strong
   correlation of 0.971 &amp; 0.944 for training and test sets, respectively.
   Our database search and molecular docking results suggested that the
   compounds 1 and 2 with best fitness to the pharmacophore model were
   predicted to interact with key residues involved in thumbs site II and
   could inhibit the HCV RdRP activity. Further, the compounds 1 and 2
   potently inhibited HCV 1b viral replication in sub-micro-molar
   range.[GRAPHICS]</abstract><date>AUG 2015</date><author>Harikishore, Amaravadhi
   Li, Enlin
   Lee, Jia Jun
   Cho, Nam-Joon
   Yoon, Ho Sup</author></paper><paper><title>A Total Order Heuristic-Based Convex Hull Algorithm for Points in the
   Plane</title><abstract>Computing the convex hull of a set of points is a fundamental operation
   in many research fields, including geometric computing, computer
   graphics, computer vision, robotics, and so forth. This problem is
   particularly challenging when the number of points goes beyond some
   millions. In this article, we describe a very fast algorithm that copes
   with millions of points in a short period of time without using any kind
   of parallel computing. This has been made possible because the algorithm
   reduces to a sorting problem of the input point set, what dramatically
   minimizes the geometric computations (e.g., angles, distances, and so
   forth) that are typical in other algorithms. When compared with popular
   convex hull algorithms (namely, Graham's scan, Andrew's monotone chain,
   Jarvis' gift wrapping, Chan's, and Quickhull), our algorithm is capable
   of generating the convex hull of a point set in the plane much faster
   than those five algorithms without penalties in memory space. (C) 2015
   Elsevier Ltd. All rights reserved.</abstract><date>JAN 2016</date><author>Gomes, Abel J. P.</author></paper><paper><title>Modeling and analysis of performances for concurrent multithread
   applications on multicore and graphics processing unit systems</title><abstract>The capabilities of multicore processors lead them to be widely adopted
   in systems at any scale, since their are able to provide more computing
   power at a lower consumption and dissipation cost. System designers are
   challenged to a deeper understanding of multicore functioning in order
   to fully exploit them while keeping the optimal balance between cores
   utilization and optimal throughput, response time and energy usage.
   Besides the advancement of general purpose CPUs, the same technological
   evolution leads to the rise of GPUs, dramatic evolution of graphical
   coprocessors, that are now affordable, efficient, dedicated computing
   units, capable of parallel computing and equipped with facilities that
   make them suited for supporting the main CPU of a system in running
   ordinary applications. The availability of commercial off-the-shelf
   (COTS) multicore computers, eventually equipped with one or more GPUs,
   makes them the basic building block of data centers devoted to cloud
   applications or scientific computing. The way to optimal exploitation of
   such a wide amount of computing power passes through the ability of
   matching the best scheduling of hardware resources with the software
   characteristics of the applications. This requires appropriate models
   and evaluation methods. Simulation and analytical techniques are
   essential tools to support the design and the management process of such
   architectures, but a sound characterization of the workloads is
   required. Typical workloads consist in multithreaded applications, with
   different characteristics, that dynamically span over the cores of
   multiple machines, connected by fast networks. In this paper we propose
   several parametric performance models for different configurations of
   multicore machines, with or without GPU support, running multiple class
   multithreaded applications, aiming to supply a detailed modeling help
   for complex data centers. Copyright (c) 2015John Wiley &amp; Sons, Ltd.</abstract><date>FEB 2016</date><author>Cerotti, D.
   Gribaudo, M.
   Iacono, M.
   Piazzolla, P.</author></paper><paper><title>Multimodal interaction with virtual worlds XMMVR: eXtensible language
   for MultiModal interaction with virtual reality worlds</title><abstract>Based on a philosophy of integrating components from multimodal
   interaction applications with 3D graphical environments, reusing already
   defined markup language for describing graphics, graphical and spoken
   interactions based on the interactive movie metaphor, a markup language
   for modeling scenes, behavior and interaction is sought. With the
   definition of this language, we hope to have a common framework for
   developing applications that allow multimodal interaction at 3D stages.
   Thus we have defined the basis of an architecture that allows us to
   integrate the components of such multimodal interaction applications in
   3D virtual environments.</abstract><date>SEP 2015</date><author>Olmedo, Hector
   Escudero, David
   Cardenoso, Valentin</author></paper><paper><title>RootGraph: a graphic optimization tool for automated image analysis of
   plant roots</title><abstract>This paper outlines a numerical scheme for accurate, detailed, and
   high-throughput image analysis of plant roots. In contrast to existing
   root image analysis tools that focus on root system-average traits, a
   novel, fully automated and robust approach for the detailed
   characterization of root traits, based on a graph optimization process
   is presented. The scheme, firstly, distinguishes primary roots from
   lateral roots and, secondly, quantifies a broad spectrum of root traits
   for each identified primary and lateral root. Thirdly, it associates
   lateral roots and their properties with the specific primary root from
   which the laterals emerge. The performance of this approach was
   evaluated through comparisons with other automated and semi-automated
   software solutions as well as against results based on manual
   measurements. The comparisons and subsequent application of the
   algorithm to an array of experimental data demonstrate that this method
   outperforms existing methods in terms of accuracy, robustness, and the
   ability to process root images under high-throughput conditions.</abstract><date>NOV 2015</date><author>Cai, Jinhai
   Zeng, Zhanghui
   Connor, Jason N.
   Huang, Chun Yuan
   Melino, Vanessa
   Kumar, Pankaj
   Miklavcic, Stanley J.</author></paper><paper><title>A lightweight and cross-platform Web3D system for casting process based
   on virtual reality technology using WebGL</title><abstract>With the advances in computer technology nowadays, the virtual
   manufacturing technology has provided a new way in casting process
   design. In this study, a virtual reality system of casting production
   process named VR-Casting has been developed. The full range of virtual
   display about the casting process has been revealed under the network
   environment. Several key technologies applied in VR-Casting were
   introduced in this paper, such as a novel network running environment,
   the latest Web3D drawing standard named WebGL, levels of detail
   technology used for rendering on demand, and the detection and updating
   technology in the casting motions. Based on the above technologies,
   different 3D models and the virtual panoramic scene were established.
   Motion schema and user interface of the casting process was delicately
   designed to enable the system a better interactivity. The latest
   internet graphics standard WebGL was used to render the models. As
   VR-Casting is characterized as lightweight and cross-platform, it offers
   access to the visualization for various platforms and devices. The
   demonstration delivers VR-Casting has a broad application prospect like
   exhibition, education, training, and process analysis. Some tests were
   implemented on different devices, and the results demonstrated
   VR-Casting has a splendid performance when conducting tests on rendering
   models. When using VR-Casting, observer has a certain sense of immersion
   with arbitrarily adjusting of the observation angle and even watches
   deeply into the interior of the casts. Thus, observer can master the
   details in a more comprehensive and diverse way during the casting
   process.</abstract><date>SEP 2015</date><author>Sun, Fei
   Zhang, Zhaochuang
   Liao, Dunming
   Chen, Tao
   Zhou, Jianxin</author></paper><paper><title>Sentiment of Emojis</title><abstract>There is a new generation of emoticons, called emojis, that is
   increasingly being used in mobile communications and social media. In
   the past two years, over ten billion emojis were used on Twitter. Emojis
   are Unicode graphic symbols, used as a shorthand to express concepts and
   ideas. In contrast to the small number of well-known emoticons that
   carry clear emotional contents, there are hundreds of emojis. But what
   are their emotional contents? We provide the first emoji sentiment
   lexicon, called the Emoji Sentiment Ranking, and draw a sentiment map of
   the 751 most frequently used emojis. The sentiment of the emojis is
   computed from the sentiment of the tweets in which they occur. We
   engaged 83 human annotators to label over 1.6 million tweets in 13
   European languages by the sentiment polarity (negative, neutral, or
   positive). About 4% of the annotated tweets contain emojis. The
   sentiment analysis of the emojis allows us to draw several interesting
   conclusions. It turns out that most of the emojis are positive,
   especially the most popular ones. The sentiment distribution of the
   tweets with and without emojis is significantly different. The
   inter-annotator agreement on the tweets with emojis is higher. Emojis
   tend to occur at the end of the tweets, and their sentiment polarity
   increases with the distance. We observe no significant differences in
   the emoji rankings between the 13 languages and the Emoji Sentiment
   Ranking. Consequently, we propose our Emoji Sentiment Ranking as a
   European language-independent resource for automated sentiment analysis.
   Finally, the paper provides a formalization of sentiment and a novel
   visualization in the form of a sentiment bar.</abstract><date>DEC 7 2015</date><author>Novak, Petra Kralj
   Smailovic, Jasmina
   Sluban, Borut
   Mozetic, Igor</author></paper><paper><title>FPGA Implementation of Real-Time Compressive Sensing with Partial
   Fourier Dictionary</title><abstract>This paper presents a novel real-time compressive sensing (CS)
   reconstruction which employs high density field-programmable gate array
   (FPGA) for hardware acceleration. Traditionally, CS can be implemented
   using a high-level computer language in a personal computer (PC) or
   multicore platforms, such as graphics processing units (GPUs) and
   Digital Signal Processors (DSPs). However, reconstruction algorithms are
   computing demanding and software implementation of these algorithms is
   extremely slow and power consuming. In this paper, the orthogonal
   matching pursuit (OMP) algorithm is refined to solve the sparse
   decomposition optimization for partial Fourier dictionary, which is
   always adopted in radar imaging and detection application. OMP
   reconstruction can be divided into two main stages: optimization which
   finds the closely correlated vectors and least square problem. For large
   scale dictionary, the implementation of correlation is time consuming
   since it often requires a large number of matrix multiplications. Also
   solving the least square problem always needs a scalable matrix
   decomposition operation. To solve these problems efficiently, the
   correlation optimization is implemented by fast Fourier transform (FFT)
   and the large scale least square problem is implemented by Conjugate
   Gradient (CG) technique, respectively. The proposed method is verified
   by FPGA (Xilinx Virtex-7 XC7VX690T) realization, revealing its
   effectiveness in real-time applications.</abstract><date>2016</date><author>Quan, Yinghui
   Li, Yachao
   Gao, Xiaoxiao
   Xing, Mengdao</author></paper><paper><title>An Aid to Generating Figures for the American Journal of Epidemiology
   Using SAS/GRAPH</title><abstract>Data visualization is an important tool that epidemiologists use to
   communicate with others in the field. The American Journal of
   Epidemiology recently acknowledged the importance of data visualization
   by inaugurating an award for the "Figure of the Year." Yet, creating
   figures that adhere to the standards of the Journal is a challenge. The
   purpose of the present article was to provide helpful hints for creating
   figures in SAS/GRAPH that meet the requirements of the Journal. It
   stresses 3 techniques: properly sizing figures overall, sizing text
   within a figure, and creating acceptable file formats. This information
   will prove useful to authors who create data-driven figures intended to
   be published in the Journal.</abstract><date>NOV 1 2015</date><author>McArdle, Patrick F.</author></paper><paper><title>Microvascular anatomy of spinal dural arteriovenous fistulas:
   arteriovenous connections and their relationships with the dura mater</title><abstract>OBJECT The microvascular anatomy of spinal dural arteriovenous fistulas
   (AVFs), especially the relationships of the vessels with the dura mater,
   has yet to be angiographically demonstrated in detail and proven
   histologically.METHODS From January 2012 through April 2014, a total of
   7 patients with spinal dural AVFs in the thoracic region underwent open
   microsurgical obliteration at Tokyo Metropolitan Neurological Hospital.
   The microvascular anatomy of spinal dural AVFs was comprehensively
   assessed by using advanced microangiography, including 3D computer
   graphics and intraoperative indocyanine green video angiography, and by
   histological findings.RESULTS The 2 microangiography techniques revealed
   the spatial course and in vivo blood flow of the meningeal vessels and
   their relationships with the dura mater in sufficient detail. The
   meningeal branch of the intercostal artery split into multiple meningeal
   vessels on the outer dural surface adjacent to the root sleeve. After
   crossing the dura mater to the inner dural surface, these vessels
   gathered and joined a single intradural draining vessel. On the inner
   dural surface, the single draining vessel was fed by the surrounding
   multiple meningeal vessels, which appeared to be caput medusae.
   Histological findings revealed that the structure of the meningeal
   branch of the intercostal artery corresponded to that of a normal
   artery. The structure of intradural draining vessels corresponded to
   that of a vein modified by retrograde arterial inflow. On the inner
   dural surface, more than 1 meningeal artery gathered and joined with the
   proximal radiculomedullary vein.CONCLUSIONS Spinal dural AVFs are
   located on the inner dural surface, where multiple direct AV connections
   between more than 1 meningeal feeding artery and a single proximal
   radiculomedullary vein occur at the site where the vein connects to the
   dura mater.</abstract><date>OCT 2015</date><author>Takai, Keisuke
   Komori, Takashi
   Taniguchi, Makoto</author></paper><paper><title>Stable Anisotropic Materials</title><abstract>The Finite Element Method (FEM) is commonly used to simulate isotropic
   deformable objects in computer graphics. Several applications (wood,
   plants, muscles) require modeling the directional dependence of the
   material elastic properties in three orthogonal directions. We
   investigate linear orthotropic materials, a special class of linear
   anisotropic materials where the shear stresses are decoupled from normal
   stresses, as well as general linear (non-orthotropic) anisotropic
   materials. Orthotropic materials generalize transversely isotropic
   materials, by exhibiting different stiffness in three orthogonal
   directions. Orthotropic materials are, however, parameterized by nine
   values that are difficult to tune in practice, as poorly adjusted
   settings easily lead to simulation instabilities. We present a
   user-friendly approach to setting these parameters that is guaranteed to
   be stable. Our approach is intuitive as it extends the familiar
   intuition known from isotropic materials. Similarly to linear
   orthotropic materials, we also derive a stability condition for a subset
   of general linear anisotropic materials, and give intuitive approaches
   to tuning them. In order to simulate large deformations, we augment
   linear corotational FEM simulations with our orthotropic and general
   anisotropic materials.</abstract><date>OCT 2015</date><author>Li, Yijing
   Barbic, Jernej</author></paper><paper><title>Ocean Wave Simulation Based on Wind Field</title><abstract>Ocean wave simulation has a wide range of applications in movies, video
   games and training systems. Wind force is the main energy resource for
   generating ocean waves, which are the result of the interaction between
   wind and the ocean surface. While numerous methods to handle simulating
   oceans and other fluid phenomena have undergone rapid development during
   the past years in the field of computer graphic, few of them consider to
   construct ocean surface height field from the perspective of wind force
   driving ocean waves. We introduce wind force to the construction of the
   ocean surface height field through applying wind field data and
   wind-driven wave particles. Continual and realistic ocean waves result
   from the overlap of wind-driven wave particles, and a strategy was
   proposed to control these discrete wave particles and simulate an
   endless ocean surface. The results showed that the new method is capable
   of obtaining a realistic ocean scene under the influence of wind fields
   at real time rates.</abstract><date>JAN 25 2016</date><author>Li, Zhongyi
   Wang, Hao</author></paper><paper><title>A new test and graphical tool to assess the goodness of fit of logistic
   regression models</title><abstract>A prognostic model is well calibrated when it accurately predicts event
   rates. This is first determined by testing for goodness of fit with the
   development dataset. All existing tests and graphic tools designed for
   the purpose suffer several drawbacks, related mainly to the subgrouping
   of observations or to heavy dependence on arbitrary parameters. We
   propose a statistical test and a graphical method to assess the goodness
   of fit of logistic regression models, obtained through an extension of
   similar techniques developed for external validation. We analytically
   computed and numerically verified the distribution of the underlying
   statistic. Simulations on a set of realistic scenarios show that this
   test and the well-known Hosmer-Lemeshow approach have similar type I
   error rates. The main advantage of this new approach is that the
   relationship between model predictions and outcome rates across the
   range of probabilities can be represented in the calibration belt plot,
   together with its statistical confidence. By readily spotting any
   deviations from the perfect fit, this new graphical tool is designed to
   identify, during the process of model development, poorly modeled
   variables that call for further investigation. This is illustrated
   through an example based on real data. Copyright (c) 2015 John Wiley &amp;
   Sons, Ltd.</abstract><date>FEB 28 2016</date><author>Nattino, Giovanni
   Finazzi, Stefano
   Bertolini, Guido</author></paper><paper><title>Eliciting prior distributions for extra parameters in some generalized
   linear models</title><abstract>To elicit an informative prior distribution for a normal linear model or
   a gamma generalized linear model (GLM), expert opinion must be
   quantified about both the regression coefficients and the extra
   parameters of these models. The latter task has attracted comparatively
   little attention. In this article, we introduce two elicitation methods
   that aim to complete the prior structure of the normal and gamma GLMs.
   First, we develop a method of assessing a conjugate prior distribution
   for the error variance in normal linear models. The method quantifies an
   expert's opinions through assessments of a median and conditional
   medians. Second, we propose a novel method for eliciting a lognormal
   prior distribution for the scale parameter of gamma GLMs. Given the mean
   value of a gamma distributed response variable, the method is based on
   conditional quartile assessments. It can also be used to quantify an
   expert's opinion about the prior distribution for the shape parameter of
   any gamma random variable, if the mean of the distribution has been
   elicited or is assumed to be known. In the context of GLMs, the mean
   value is determined by the regression coefficients. Interactive graphics
   is the medium through which assessments for the two proposed methods are
   elicited. Examples illustrating use of the methods are given. Computer
   programs that implement both methods are available.</abstract><date>AUG 2015</date><author>Elfadaly, Fadlalla G.
   Garthwaite, Paul H.</author></paper><paper><title>An adaptive approach for texture enhancement based on a fractional
   differential operator with non-integer step and order</title><abstract>Image texture enhancement is an important topic in computer graphics,
   computer vision and pattern recognition. By applying the fractional
   derivative to analyze texture characteristics, a new fractional
   differential operator mask with adaptive non-integral step and order is
   proposed in this paper to enhance texture images. A non-regular
   self-similar support region is constructed based on a local texture
   similarity measure, which can effectively exclude pixels with low
   correlation and noise. Then, through applying sub-pixel division and
   introducing a local linear piecewise model to estimate the gray value in
   between the pixels, the resulting non-integral steps can improve the
   characterization of self-similarity that is inherent in many image
   types. Moreover, with in-depth understanding of the local texture
   pattern distribution in the support region, adaptive selection of the
   fractional derivative order is also performed to deal with complex
   texture details. Finally, the non-regular fractional differential
   operator mask which incorporates adaptive non-integral step and order is
   constructed. Experimental results show that, for images with rich
   texture contents, the effective characterization of the degree of
   self-similarity in the texture patterns based on our proposed approach
   leads to improved image enhancement results when compared with
   conventional approaches. (C) 2014 Elsevier B.V. All rights reserved.</abstract><date>JUN 22 2015</date><author>Hu, Fuyuan
   Si, Shaohui
   Wong, Hau San
   Fu, Baochuan
   Si, MaoXin
   Luo, Heng</author></paper><paper><title>Rendering Pacioli's rhombicuboctahedron</title><abstract>We analyse the glass rhombicuboctahedron (RCO) appearing in a famous
   painting of Pacioli (1495), considering the extent to which it might
   agree with a physically correct rendering of a corresponding glass
   container half filled with water. This investigation shows that it is
   unlikely that the painter of the RCO was looking at such a physical
   object. We then ask what a proper rendering of such an object might look
   like. Our computer renderings, which take into account multiple internal
   and external reflections and refractions, yield visual effects that
   differ strongly from their depictions in the painting. Nevertheless, the
   painter of the RCO has clearly succeeded in providing a rendering that
   appears plausible and awe-inspiring to almost all observers.</abstract><date>OCT 2 2015</date><author>Sequin, Carlo H.
   Shiau, Raymond</author></paper><paper><title>ASPECTS OF GENERATING 3D SURFACES WITH APPLICATIONS IN DRIVING
   SIMULATORS</title><abstract>Driving simulators are complex technical entities for reproducing the
   phenomena of real driving. One of their essential components is the
   visual simulator. For the purpose of creating the visual simulator it is
   of utmost importance to generate the 3D surfaces such that they simulate
   both the road and the environment. In this paper, the problem of terrain
   generation has been encountered. The generation of such surfaces,
   suitable with real ones, is a natural issue. In the last few decades
   much research was done and many attempts were made in order to generate
   random surfaces, as real looking as possible, theoretically justified
   and easy to model. The purpose of our research is to propose an
   alternative method for generation of 3D surfaces for different roads by
   using two uncommon algorithms which were initially designed for other
   purposes: the Douglas-Peucker algorithm and the so called onion peeling
   algorithm.</abstract><date>OCT-DEC 2015</date><author>Ilea, Lucian
   Munteanu, Ligia
   Dumitriu, Dan
   Dudescu, Mircea
   Brisan, Cornel
   Chiroiu, Veturia</author></paper><paper><title>Presentation and response timing accuracy in Adobe Flash and
   HTML5/JavaScript Web experiments</title><abstract>Web-based research is becoming ubiquitous in the behavioral sciences,
   facilitated by convenient, readily available participant pools and
   relatively straightforward ways of running experiments: most recently,
   through the development of the HTML5 standard. Although in most studies
   participants give untimed responses, there is a growing interest in
   being able to record response times online. Existing data on the
   accuracy and cross-machine variability of online timing measures are
   limited, and generally they have compared behavioral data gathered on
   the Web with similar data gathered in the lab. For this article, we took
   a more direct approach, examining two ways of running experiments
   online-Adobe Flash and HTML5 with CSS3 and JavaScript-across 19
   different computer systems. We used specialist hardware to measure
   stimulus display durations and to generate precise response times to
   visual stimuli in order to assess measurement accuracy, examining
   effects of duration, browser, and system-to-system variability (such as
   across different Windows versions), as well as effects of processing
   power and graphics capability. We found that (a) Flash and JavaScript's
   presentation and response time measurement accuracy are similar; (b)
   within-system variability is generally small, even in low-powered
   machines under high load; (c) the variability of measured response times
   across systems is somewhat larger; and (d) browser type and system
   hardware appear to have relatively small effects on measured response
   times. Modeling of the effects of this technical variability suggests
   that for most within-and between-subjects experiments, Flash and
   JavaScript can both be used to accurately detect differences in response
   times across conditions. Concerns are, however, noted about using some
   correlational or longitudinal designs online.</abstract><date>JUN 2015</date><author>Reimers, Stian
   Stewart, Neil</author></paper><paper><title>Surface trees - Representation of boundary surfaces using a tree
   descriptor</title><abstract>Many applications in fields as diverse as computer graphics, medical
   imaging or pattern recognition require the usage of the boundary of
   digital objects, or discrete surface. A discrete surface is a set of
   orthogonal quadrilaterals connected to each other that is typically
   represented either as a face adjacency graph or as a polygon mesh. In
   this work we propose a new method, named surface trees, to represent
   discrete surfaces. Surface trees allow the representation of any
   discrete surface by coding a tree structure contained in the face
   adjacency graph. This method uses an alphabet of nine symbols, in
   addition to the parenthesis notation, to codify trees of maximum degree
   four. Surface trees are a compact way of representing any discrete
   surface at the same time they preserve geometrical information and
   provide invariance under translation and rotation. We demonstrate our
   method on synthetic surfaces as well as others obtained from real data.
   (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>AUG 2015</date><author>Lemus, Eduardo
   Bribiesca, Ernesto
   Garduno, Edgar</author></paper><paper><title>GPU MrBayes V3.1: MrBayes on Graphics Processing Units for Protein
   Sequence Data</title><abstract>We present a modified GPU (graphics processing unit) version of MrBayes,
   called ta(MC)(3) (GPU MrBayes V3.1), for Bayesian phylogenetic inference
   on protein data sets. Our main contributions are 1) utilizing 64-bit
   variables, thereby enabling ta(MC)(3) to process larger data sets than
   MrBayes; and 2) to use Kahan summation to improve accuracy, convergence
   rates, and consequently runtime. Versus the current fastest software, we
   achieve a speedup of up to around 2.5 (and up to around 90 vs. serial
   MrBayes), and more on multi-GPU hardware. GPU MrBayes V3.1 is available
   from http://sourceforge.net/projects/mrbayes-gpu/.</abstract><date>SEP 2015</date><author>Pang, Shuai
   Stones, Rebecca J.
   Ren, Ming-Ming
   Liu, Xiao-Guang
   Wang, Gang
   Xia, Hong-ju
   Wu, Hao-Yang
   Liu, Yang
   Xie, Qiang</author></paper><paper><title>On spatio-temporal feature point detection for animated meshes</title><abstract>Although automatic feature detection has been a long-sought subject by
   researchers in computer graphics and computer vision, feature extraction
   on deforming models remains a relatively unexplored area. In this paper,
   we develop a new method for automatic detection of spatio-temporal
   feature points on animated meshes. Our algorithm consists of three main
   parts. We first define local deformation characteristics, based on
   strain and curvature values computed for each point at each frame. Next,
   we construct multi-resolution space-time Gaussians and
   difference-of-Gaussian (DoG) pyramids on the deformation characteristics
   representing the input animated mesh, where each level contains 3D
   smoothed and subsampled representation of the previous level. Finally,
   we estimate locations and scales of spatio-temporal feature points by
   using a scale-normalized differential operator. A new, precise
   approximation of spatio-temporal scale-normalized Laplacian has been
   introduced, based on the space-time DoG. We have experimentally verified
   our algorithm on a number of examples and conclude that our technique
   allows to detect spatio and temporal feature points in a reliable
   manner.</abstract><date>NOV 2015</date><author>Mykhalchuk, Vasyl
   Seo, Hyewon
   Cordier, Frederic</author></paper><paper><title>Comparison of Acceleration Techniques for Selected Low-Level
   Bioinformatics Operations</title><abstract>Within the recent years clock rates of modern processors stagnated while
   the demand for computing power continued to grow. This applied
   particularly for the fields of life sciences and bioinformatics, where
   new technologies keep on creating rapidly growing piles of raw data with
   increasing speed. The number of cores per processor increased in an
   attempt to compensate for slight increments of clock rates. This
   technological shift demands changes in software development, especially
   in the field of high performance computing where parallelization
   techniques are gaining in importance due to the pressing issue of large
   sized datasets generated by e.g., modern genomics. This paper presents
   an overview of state-of-the-art manual and automatic acceleration
   techniques and lists some applications employing these in different
   areas of sequence informatics. Furthermore, we provide examples for
   automatic acceleration of two use cases to show typical problems and
   gains of transforming a serial application to a parallel one. The paper
   should aid the reader in deciding for a certain techniques for the
   problem at hand. We compare four different state-of-the-art automatic
   acceleration approaches (OpenMP, PluTo-SICA, PPCG, and OpenACC). Their
   performance as well as their applicability for selected use cases is
   discussed. While optimizations targeting the CPU worked better in the
   complex k-mer use case, optimizers for Graphics Processing Units (GPUs)
   performed better in the matrix multiplication example. But performance
   is only superior at a certain problem size due to data migration
   overhead. We show that automatic code parallelization is feasible with
   current compiler software and yields significant increases in execution
   speed. Automatic optimizers for CPU are mature and usually no additional
   manual adjustment is required. In contrast, some automatic parallelizers
   targeting GPUs still lack maturity and are limited to simple statements
   and structures.</abstract><date>FEB 10 2016</date><author>Langenkaemper, Daniel
   Jakobi, Tobias
   Feld, Dustin
   Jelonek, Lukas
   Goesmann, Alexander
   Nattkemper, Tim W.</author></paper><paper><title>Association between Lamina Cribrosa Position Change and Glaucomatous
   Visual Field Progression</title><abstract></abstract><date>JUN 2015</date><author>Abumasmah, Ramiz
   Ren, Ruojin
   Ghassibi, Mark
   Chien, Jason L.
   Adleyba, Olga
   Tello, Celso
   Liebmann, Jeffrey M.
   Ritch, Robert
   Park, Sung Chul (Sean)</author></paper><paper><title>Mobile Volume Rendering: Past, Present and Future</title><abstract>Volume rendering has been a relevant topic in scientific visualization
   for the last decades. However, the exploration of reasonably big volume
   datasets requires considerable computing power, which has limited this
   field to the desktop scenario. But the recent advances in mobile
   graphics hardware have motivated the research community to overcome
   these restrictions and to bring volume graphics to these ubiquitous
   handheld platforms. This survey presents the past and present work on
   mobile volume rendering, and is meant to serve as an overview and
   introduction to the field. It proposes a classification of the current
   efforts and covers aspects such as advantages and issues of the mobile
   platforms, rendering strategies, performance and user interfaces. The
   paper ends by highlighting promising research directions to motivate the
   development of new and interesting mobile volume solutions.</abstract><date>FEB 2016</date><author>Noguera, Jose M.
   Roberto Jimenez, J.</author></paper><paper><title>Synthesis, molecular docking and biological evaluation of novel
   bis-pyrazole derivatives for analgesic, anti-inflammatory and
   antimicrobial activities</title><abstract>A new series of bis-pyrazoles were synthesized by Michael addition of
   hydrazine to chalcones. The starting-material-substituted acetophenones
   required for the synthesis of chalcones were prepared from itaconic
   anhydride. The newly synthesized compounds were characterized by IR,
   H-1-NMR, C-13-NMR, mass spectral and analytical data. All the
   synthesized compounds were evaluated for in vivo analgesic,
   anti-inflammatory and in vitro antimicrobial activities. Among the
   tested compounds, 5a, 5b and 5d showed potential anti-inflammatory and
   analgesic activities. Further anti-inflammatory results were supported
   by in silico docking study, in which tested bis-pyrazoles were found to
   be more selective toward COX-2 (PDB ID: 1CX2) rather than COX-1 (PDB ID:
   1CQE). The LD50 values for these products 5(a-l) showed a high safety
   margin with a dose level &gt; 2000 mg/kg. Among all synthesized compounds,
   N-[4-(5-(4-bromophenyl)-1-phenyl-1H-pyrazol-3-yl)phenyl-2-(3-hydroxy-1-p
   henyl-1H-pyrazol-4-yl)] acetamide (5b) emerged as most potent molecule
   with anti-inflammatory, analgesic and antimicrobial
   properties.[GRAPHICS].</abstract><date>DEC 2015</date><author>Nayak, Prakash S.
   Narayana, B.
   Sarojini, B. K.
   Fernades, Jennifer
   Bharath, B. R.
   Madhu, L. N.</author></paper><paper><title>A two-tier design space exploration algorithm to construct GPU
   performance model</title><abstract>Graphics Processing Units (GPUs) have a large and complex design space
   that needs to be explored in order to optimize the performance of future
   GPUs. Statistical techniques are useful tools to help computer
   architects to predict performance of complex processors. In this study,
   these methods are utilized to build a model which predicts the GPU
   performance efficiently. The design space of targeted Fermi GPU has more
   than 8 million points which cause exploring this huge design space a
   challenging process. In order to build an accurate model, we propose a
   two-tier algorithm in our algorithm which builds a multiple linear
   regression model from a small set of simulated data. In this algorithm
   the Plackett Burman design is used to find the key parameters of the
   GPU, and further simulations are guided by a fractional factorial design
   for the most important parameters. Our algorithm is able to construct a
   GPU performance predictor which can predict the performance of any point
   in the design space with an average prediction error between 1% and 5%
   for different benchmark applications. In addition, in comparison to
   other methods which need a large number of sampling points, the accuracy
   in our method is achieved by only sampling between 0.0003% and 0.0015%
   of the full design space. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Mirsoleimani, S. Ali
   Khunjush, Farshad
   Karami, Ali</author></paper><paper><title>Path-space Motion Estimation and Decomposition for Robust Animation
   Filtering</title><abstract>Renderings of animation sequences with physics-based Monte Carlo light
   transport simulations are exceedingly costly to generate frame-by-frame,
   yet much of this computation is highly redundant due to the strong
   coherence in space, time and among samples. A promising approach pursued
   in prior work entails subsampling the sequence in space, time, and
   number of samples, followed by image-based spatio-temporal upsampling
   and denoising. These methods can provide significant performance gains,
   though major issues remain: firstly, in a multiple scattering
   simulation, the final pixel color is the composite of many different
   light transport phenomena, and this conflicting information causes
   artifacts in image-based methods. Secondly, motion vectors are needed to
   establish correspondence between the pixels in different frames, but it
   is unclear how to obtain them for most kinds of light paths (e.g. an
   object seen through a curved glass panel). To reduce these ambiguities,
   we propose a general decomposition framework, where the final pixel
   color is separated into components corresponding to disjoint subsets of
   the space of light paths. Each component is accompanied by motion
   vectors and other auxiliary features such as reflectance and surface
   normals. The motion vectors of specular paths are computed using a
   temporal extension of manifold exploration and the remaining components
   use a specialized variant of optical flow. Our experiments show that
   this decomposition leads to significant improvements in three
   image-based applications: denoising, spatial upsampling, and temporal
   interpolation.</abstract><date>JUL 2015</date><author>Zimmer, Henning
   Rousselle, Fabrice
   Jakob, Wenzel
   Wang, Oliver
   Adler, David
   Jarosz, Wojciech
   Sorkine-Hornung, Olga
   Sorkine-Hornung, Alexander</author></paper><paper><title>Localized discrete Laplace-Beltrami operator over triangular mesh</title><abstract>The Laplace-Beltrami operator is the foundation of describing geometric
   partial differential equations, and it also plays an important role in
   the fields of computational geometry, computer graphics and image
   processing, such as surface parameterization, shape analysis, matching
   and interpolation. However, constructing the discretized
   Laplace-Beltrami operator with convergent property has been an open
   problem. In this paper we propose a new discretization scheme of the
   Laplace-Beltrami operator over triangulated surfaces. We prove that our
   discretization of the Laplace-Beltrami operator converges to the
   Laplace-Beltrami operator at every point of an arbitrary smooth surface
   as the size of the triangular mesh over the surface tends to zero.
   Numerical experiments are conducted, which support the theoretical
   analysis. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Li, Xinge
   Xu, Guoliang
   Zhang, Yongjie Jessica</author></paper><paper><title>Fragment-based similarity searching with infinite color space</title><abstract>Fragment-based searching and abstract representation of molecular
   features through reduced graphs have separately been used for virtual
   screening. Here, we combine these two approaches and apply the algorithm
   RedFrag to virtual screens retrospectively and prospectively. It uses a
   new type of reduced graph that does not suffer from information loss
   during its construction and bypasses the necessity of feature
   definitions. Built upon chemical epitopes resulting from molecule
   fragmentation, the reduced graph embodies physico-chemical and
   2D-structural properties of a molecule. Reduced graphs are compared with
   a continuous-similarity-distance-driven maximal common subgraph
   algorithm, which calculates similarity at the fragmental and topological
   levels. The performance of the algorithm is evaluated by retrieval
   experiments utilizing precompiled validation sets. By predicting and
   experimentally testing ligands for endothiapepsin, a challenging model
   protease, the method is assessed in a prospective setting. Here, we
   identified five novel ligands with affinities as low as 2.08 M. (c) 2015
   Wiley Periodicals, Inc.</abstract><date>AUG 5 2015</date><author>Gunera, Jakub
   Kolb, Peter</author></paper><paper><title>3D-Model-Based Video Analysis for Computer Generated Faces
   Identification</title><abstract>Modern computer graphics technologies brought realism in
   computer-generated characters, making them achieve truly natural
   appearance. Besides traditional virtual reality applications such as
   avatars, games, or cinema, these synthetic characters may be used to
   generate realistic fakes, which may lead to improper use of the
   technology. This fact raises the demand for advanced tools able to
   discriminate real and artificial human faces in digital media. In this
   paper, we propose a method to distinguish between computer generated and
   natural faces by modeling and evaluating their dynamic behavior. Because
   of a 3D-model-based video analysis, the proposed technique allows
   identifying synthetic characters by detecting their more limited
   variability over time. Experimental results demonstrate the
   effectiveness of the proposed approach also on very challenging and
   realistic video sequences.</abstract><date>AUG 2015</date><author>Duc-Tien Dang-Nguyen
   Boato, Giulia
   De Natale, Francesco G. B.</author></paper><paper><title>Fast GPU-based Monte Carlo simulations for LDR prostate brachytherapy</title><abstract>The aim of this study was to evaluate the potential of bGPUMCD, a Monte
   Carlo algorithm executed on Graphics Processing Units (GPUs), for fast
   dose calculations in permanent prostate implant dosimetry. It also aimed
   to validate a low dose rate brachytherapy source in terms of TG-43
   metrics and to use this source to compute dose distributions for
   permanent prostate implant in very short times.The physics of bGPUMCD
   was reviewed and extended to include Rayleigh scattering and
   fluorescence from photoelectric interactions for all materials involved.
   The radial and anisotropy functions were obtained for the Nucletron
   SelectSeed in TG-43 conditions. These functions were compared to those
   found in the MD Anderson Imaging and Radiation Oncology Core
   brachytherapy source registry which are considered the TG-43 reference
   values. After appropriate calibration of the source, permanent prostate
   implant dose distributions were calculated for four patients and
   compared to an already validated Geant4 algorithm.The radial function
   calculated from bGPUMCD showed excellent agreement (differences within
   1.3%) with TG-43 accepted values. The anisotropy functions at r = 1 cm
   and r = 4 cm were within 2% of TG-43 values for angles over 17.5
   degrees. For permanent prostate implants, Monte Carlo-based dose
   distributions with a statistical uncertainty of 1% or less for the
   target volume were obtained in 30 s or less for 1 x 1 x 1 mm(3)
   calculation grids. Dosimetric indices were very similar (within 2.7%) to
   those obtained with a validated, independent Monte Carlo code (Geant4)
   performing the calculations for the same cases in a much longer time
   (tens of minutes to more than a hour).bGPUMCD is a promising code that
   lets envision the use of Monte Carlo techniques in a clinical
   environment, with sub-minute execution times on a standard workstation.
   Future work will explore the use of this code with an inverse planning
   method to provide a complete Monte Carlo-based planning solution.</abstract><date>JUL 7 2015</date><author>Bonenfant, Eric
   Magnoux, Vincent
   Hissoiny, Sami
   Ozell, Benoit
   Beaulieu, Luc
   Despres, Philippe</author></paper><paper><title>iDrug-Target: predicting the interactions between drug compounds and
   target proteins in cellular networking via benchmark dataset
   optimization approach</title><abstract>Information about the interactions of drug compounds with proteins in
   cellular networking is very important for drug development.
   Unfortunately, all the existing predictors for identifying drug-protein
   interactions were trained by a skewed benchmark data-set where the
   number of non-interactive drug-protein pairs is overwhelmingly larger
   than that of the interactive ones. Using this kind of highly unbalanced
   benchmark data-set to train predictors would lead to the outcome that
   many interactive drug-protein pairs might be mispredicted as
   non-interactive. Since the minority interactive pairs often contain the
   most important information for drug design, it is necessary to minimize
   this kind of misprediction. In this study, we adopted the neighborhood
   cleaning rule and synthetic minority over-sampling technique to treat
   the skewed benchmark datasets and balance the positive and negative
   subsets. The new benchmark datasets thus obtained are called the
   optimized benchmark datasets, based on which a new predictor called
   iDrug-Target was developed that contains four sub-predictors:
   iDrug-GPCR, iDrug-Chl, iDrug-Ezy, and iDrug-NR, specialized for
   identifying the interactions of drug compounds with GPCRs
   (G-protein-coupled receptors), ion channels, enzymes, and NR (nuclear
   receptors), respectively. Rigorous cross-validations on a set of
   experiment-confirmed datasets have indicated that these new predictors
   remarkably outperformed the existing ones for the same purpose. To
   maximize users' convenience, a public accessible Web server for
   iDrug-Target has been established at[GRAPHICS], by which users can
   easily get their desired results. It has not escaped our notice that the
   aforementioned strategy can be widely used in many other areas as well.</abstract><date>OCT 3 2015</date><author>Xiao, Xuan
   Min, Jian-Liang
   Lin, Wei-Zhong
   Liu, Zi
   Cheng, Xiang
   Chou, Kuo-Chen</author></paper><paper><title>Splicing Express: a software suite for alternative splicing analysis
   using next-generation sequencing data</title><abstract>Motivation. Alternative splicing events (ASEs) are prevalent in the
   transcriptome of eukaryotic species and are known to influence many
   biological phenomena. The identification and quantification of these
   events are crucial for a better understanding of biological processes.
   Next-generation DNA sequencing technologies have allowed deep
   characterization of transcriptomes and made it possible to address these
   issues. ASEs analysis, however, represents a challenging task especially
   when many different samples need to be compared. Some popular tools for
   the analysis of ASEs are known to report thousands of events without
   annotations and/or graphical representations. A new tool for the
   identification and visualization of ASEs is here described, which can be
   used by biologists without a solid bioinformatics background.Results. A
   software suite named Splicing Express was created to perform ASEs
   analysis from transcriptome sequencing data derived from next-generation
   DNA sequencing platforms. Its major goal is to serve the needs of
   biomedical researchers who do not have bioinformatics skills. Splicing
   Express performs automatic annotation of transcriptome data (GTF files)
   using gene coordinates available from the UCSC genome browser and allows
   the analysis of data from all available species. The identification of
   ASEs is done by a known algorithm previously implemented in another tool
   named Splooce. As a final result, Splicing Express creates a set of HTML
   files composed of graphics and tables designed to describe the
   expression profile of ASEs among all analyzed samples. By using RNA-Seq
   data from the Illumina Human Body Map and the Rat Body Map, we show that
   Splicing Express is able to perform all tasks in a straightforward way,
   identifying well-known specific events.Availability and Implementation.
   Splicing Express is written in Perl and is suitable to run only in
   UNIX-like systems.</abstract><date>NOV 19 2015</date><author>Kroll, Jose E.
   Kim, Jihoon
   Ohno-Machado, Lucila
   de Souza, Sandro J.</author></paper><paper><title>Development of genome-wide insertion/deletion markers in rice based on
   graphic pipeline platform</title><abstract>DNA markers play important roles in plant breeding and genetics. The
   Insertion/Deletion (InDel) marker is one kind of co-dominant DNA markers
   widely used due to its low cost and high precision. However, the
   canonical way of searching for InDel markers is time-consuming and
   labor-intensive. We developed an end-to-end computational solution
   (InDel Markers Development Platform, IMDP) to identify genome-wide InDel
   markers under a graphic pipeline environment. IMDP constitutes assembled
   genome sequences alignment pipeline (AGA-pipe) and next-generation
   re-sequencing data mapping pipeline (NGS-pipe). With AGA-pipe we are
   able to identify 12,944 markers between the genome of rice cultivars
   Nipponbare and 93-11. Using NGS-pipe, we reported 34,794 InDels from
   re-sequencing data of rice cultivars Wu-Yun-Geng7 and Guang-Lu-Ai4.
   Combining AGA-pipe and NGS-pipe, we developed 205,659 InDels in eight
   japonica and nine indica cultivars and 2,681 InDels showed a
   subgroup-specific pattern. Polymerase chain reaction (PCR) analysis of
   subgroup-specific markers indicated that the precision reached 90% (86
   of 95). Finally, to make them available to the public, we have
   integrated the InDels/markers information into a website (Rice InDel
   Marker Database, RIMD, ). The application of IMDP in rice will
   facilitate efficiency for development of genome-wide InDel markers, in
   addition it can be used in other species with reference genome sequences
   and NGS data.</abstract><date>NOV 2015</date><author>Lu, Yang
   Cui, Xiao
   Li, Rui
   Huang, Piaopiao
   Zong, Jie
   Yao, Danqing
   Li, Gang
   Zhang, Dabing
   Yuan, Zheng</author></paper><paper><title>Accelerating earthquake simulations on general-purpose graphics
   processors</title><abstract>Parallelization strategies are presented for Virtual Quake, a numerical
   simulation code for earthquakes based on topologically realistic systems
   of interacting earthquake faults. One of the demands placed upon the
   simulation is the accurate reproduction of the observed earthquake
   statistics over three to four decades. This requires the use of a
   high-resolution fault model in computations, which demands computational
   power that is well beyond the scope of off-the-shelf multi-core CPU
   computers. However, the recent advances in general-purpose graphic
   processing units have the potential to address this problem at moderate
   cost increments. A functional decomposition of Virtual Quake is
   performed, and opportunities for parallelization are discussed in this
   work. Computationally intensive modules are identified, and these are
   implemented on graphics processing units, significantly speeding up
   earthquake simulations. In the current best case scenario, a computer
   with six graphics processing units can simulate 500years of fault
   activity in California at 1.5kmx1.5km element resolution in less than
   1hour, whereas a single CPU requires more than 2days to perform the same
   simulation. Copyright (c) 2015 John Wiley &amp; Sons, Ltd.</abstract><date>DEC 10 2015</date><author>Sengupta, Prasenjit
   Nguyen, Jimmy
   Kwan, Jason
   Menon, Padmanabhan K.
   Heien, Eric M.
   Rundle, John B.</author></paper><paper><title>Expanding social mobile games beyond the device screen</title><abstract>Emerging pervasive games use sensors, graphics and networking
   technologies to provide immersive game experiences integrated with the
   real world. Existing pervasive games commonly rely on a device screen
   for providing game-related information, while overlooking opportunities
   to include new types of contextual interactions like jumping, a punching
   gesture, or even voice to be used as game inputs. We present the design
   of Spellbound, a physical mobile team-based game, to help contribute to
   our understanding of how we can design pervasive games that aim to
   nurture a spirit of togetherness. We also briefly touch upon how
   togetherness and playfulness can transform physical movement into a
   desirable activity in the user evaluation section. Spellbound is an
   outdoor pervasive team-based physical game. It takes advantage of the
   above-mentioned opportunities and integrates real-world actions like
   jumping and spinning with a virtual world. It also replaces touch-based
   input with voice interaction and provides glanceable and haptic feedback
   using custom hardware in the true spirit of social play characteristic
   of traditional children's games. We believe Spellbound is a form of
   digital outdoor gaming that anchors enjoyment on physical action, social
   interaction, and tangible feedback. Spellbound was well received in user
   evaluation playtests which confirmed that the main design objective of
   enhancing a sense of togetherness was largely met.</abstract><date>JUL 2015</date><author>Sra, Misha
   Schmandt, Chris</author></paper><paper><title>BCVEGPY2.2: A newly upgraded version for hadronic production of the
   meson B-c and its excited states</title><abstract>A newly upgraded version of the BCVEGPY, a generator for hadronic
   production of the meson Be and its excited states, is available. In
   comparison with the previous one (Chang et al., 2006), the new version
   is to apply an improved hit-and-miss technology to generating the
   un-weighted events much more efficiently under various simulation
   environments. The codes for production of 2S-wave B-c states are also
   given here.New version program summaryTitle of program:
   BCVEGPY2.2Catalogue identifier: ADTJ_v2_3Program obtained from: CPC
   Program Library, Queen's University, Belfast, N. IrelandLicensing
   provisions: Standard CPC licence,
   http://cpc.cs.qub.ac.uk/licence/licence.htmlNo. of lines in distributed
   program, including test data, etc.: 323731No. of bytes in distributed
   program, including test data, etc.: 4498602Distribution format:
   tar.gzComputer: Any LINUX based on PC with FORTRAN 77 or FORTRAN 90 and
   GNU C compiler as well Operating systems: LINUXProgramming language
   used: FORTRAN 77/90Memory required to execute with typical data: About
   2.0 MBClassification: 11.2, 11.5Catalogue identifier of previous
   version: ADTJ_v2_2Journal reference of previous version: Comput. Phys.
   Commun. 183 (2012) 442Does the new version supersede the old version?:
   YesNature of physical problem: Hadronic Production of B-c meson and its
   excited states.Method of solution: To generate un-weighted events of B-c
   meson and its excited states by using an improved hit-and-miss
   technology.Reasons for new version: Responding to the feedback from
   users, such as those from CMS and LHCb groups, we create a new
   hit-and-miss algorithm for generating the un-weighted events.
   Furthermore, the relevant codes for generating the 2S-excited state of
   B-c meson are added, because the excited state production may be sizable
   in the new LHC run.Typical running time: it depends on which option is
   chosen to match PYTHIA when generating the full events and also on which
   state of B-c meson, either its ground state or its excited states, is to
   be generated. Typically on a 2.27GHz Intel Xeon E5520 processor machine,
   for producing the B-c meson ground state: I) If setting [IDWTUP=3 and
   unwght =.true.], it shall adopt the new hit-and-miss technology to
   generate the un-weighted events, and to generate 10(5) events takes 30
   minutes; II) If setting [IDWTUP=3 and unwght =.false.] or [IDWTUP=1 and
   IGENERATE=0], it shall generate the weighted events, and to generate 105
   events takes 2 minutes only (the fastest way, for theoretical purpose
   only); Ill) As a comparison, if setting [IDWTUP=1 and IGENERATE=1], it
   shall, as the same as the previous version, adopt the PYTHIA inner
   hit-and-miss technology to generate the un-weighted events, and to
   generate 1000 events takes about 22 hours. Thus, the efficiency (and
   accuracy also) for generating the un-weighted events obviously is
   greatly increased.Keywords: Event generator; Hadronic production; B-c
   meson; Un-weighted eventsSummary of revisions: 1). We improve the
   approach for generating un-weighted events. 2). Responding to the
   feedback from users, we adjust part of the codes to make it work more
   user-friendly. More specifically, we explain main changes in the
   following :Event generation.If each simulated event comes with a weight,
   it will make the data analysis much more complicated. Thus the
   un-weighted events are usually adopted for Monte Carlo simulations. As
   an external process of PYTHIA, the generator BCVEGPY [1-4] shall call
   the PYTHIA inner hit-and-miss mechanism to generate the un-weighted
   events by setting IDWGTUP=1 and IGENERATE=1 [5], i.e. the Von Neumann
   method is used for generating the un-weighted B-c events.Every events
   bearing a weight (xwgtup) respectively, when inputting them to PYTHIA,
   they are suffered from being accepted or rejected, all the fully
   generated events at the output become to have a common weight. The Von
   Neumann method states that the event should be accepted by the PYTHIA
   subroutine PYEVNT with a probability R = xwgtup/xmaxup. This can be
   achieved by comparing R with a random number that is uniformly
   distributed within the region of [0, 1]. Namely if R is bigger than such
   a random number then the event is accepted, otherwise it should be
   rejected. Here xmaxup stands for the maximum event weight.The von
   Neumann method works effectively for the cases when all the weights of
   input events are moderate in the whole phase-space. However if the input
   events' weights vary greatly, such as varying logarithmically, thee its
   efficiency shall be greatly depressed, since too much time shall be
   wasted for calculating xwgtup of the rejected events. Thus it is helpful
   to find a new method for generating unweighted events.We will adopt the
   new hit-and-miss strategy suggested by Ref.[6] to do the Bc meson
   un-weight simulation. Extra switches for calling this new technology are
   added to BCVEGPY, e.g. the new hit-and-miss technology shall be called
   by setting IDWTUP=3 and unwght =.true.. Details for this new technology
   can be found in Ref. [6]. For self-consistency, we repeat its main idea
   here.To be different from previous versions, BCVEGPY2.2 uses the VEGAS
   [7] and the MINT [8] as a combined way to generate the un-weighted
   events. The whole phase space shall be separated to a multidimensional
   phase-space grid. The main purpose of VEGAS [7] is to perform the
   adaptive Monte Carlo multi-dimensional integration, which uses the
   importance-sampling method to improve the integration efficiency. Each
   event shall generally result in a different weight, recorded by xwgtup,
   and the maximum weight within each grid shall be simultaneously recorded
   into the importance-sampling grid file (with the suffix.grid). Then
   following the idea of MINT, the Von Neumann method is used in each
   phase-space grid. Within this small grid region, the von Neumann
   algorithm works effectively, thus the efficiency for generating
   un-weighted events are greatly increased.To implement the new
   hit-and-miss algorithm into BCVEGPY2.2, we change the original VEGAS
   subroutine as vegas (fxn, ndim, ncall, itmx, nprn, xint xmax,
   imode)Three new variables xint, xmax and imode are added in the VEGAS
   subroutine. The xmax array is used to record the maximum weights in all
   cells and mode is a flag. xint stands for the output cross-section when
   setting imode=0, which shall be used to initialize the xmax array when
   setting imode=1. For convenience, the generated xmax array will be
   stored in the same grid file in which the importance sampling function
   is stored.In the initialization stage, the VEGAS subroutine shall be
   called by the subroutine evntinit twice by setting imode=0 and imode=1
   respectively to generate both the upper bound grid xmax for all cells
   and the importance sampling function.A subroutine gem (fxn, ndim, Xmax,
   i mode) is defined in the file vegas. F with the purpose to generate the
   un-weighted events. Three options for calling gem subroutine are
   defined: jmode=0 is to initializes the parameter; jmode=3 is to print
   the generation statistics; jmode=1 is the key option, which is to use
   the new hit-and-miss technology to generate the un-weighted events. More
   explicitly, by calling gen (fxn, ndim,mmax, jmode=1), three steps shall
   be executed:1. Call the phase_gen subroutine to generate a random
   phase-space point and to calculate its weight xwgtup.2. Judge the point
   locates in which cell and read from the xmax array and get the upper
   bound value xmaxup for this particular cell.3. Judge whether such point
   be kept or not by using the Von Neumann method with the help of the
   probability xwgtup/xmaxup.To be more flexible, we add one parameter
   igenmode for generating or using the existed. grid files. When setting
   igenmode=1, the VEGAS subroutine shall be called to generate the. grid
   files. When setting igenmode=2, the VEGAS subroutine shall be called to
   generate more accurate. grid files from the existed. grid files. When
   setting igenmode=3, one can directly use the existed. grid files to
   generate events without running VEGAS. Importantly, before using the
   existed. grid files, one must ensure all the parameters be the same as
   the previous generation.A script for setting the parameters and a
   cross-check of the un-weighted events.We put an additional file,
   bcvegpy_set_par.nam, in the new version for setting the parameters. This
   way the user does not need to compile the program again if only the
   parameter values are changed.[GRAPHICS]Fig. 1. Comparison of the
   normalized B-c transverse momentum (PT) and rapidity (y) distributions
   derived by setting unwght=.true. (events) and unwght=.false.
   (differential distributions), which are represented by solid and dotted
   lines, respectively.As a cross-check of the new technology, we compare
   the un-weighted Bc event distributions derived by setting unwght=.true.
   with the weighted Bc differential distributions derived by setting
   unwght=.false.. The results are shown in Fig. 1. Those two distributions
   after proper normalization agree well with each other, that shows our
   present scheme for un-weighted events is correct.Bc(2S) generation.In
   2014 the ATLAS collaboration reported an observation about an excited
   state of Bc meson, which most probably is Bc (2S) state [9]. With more
   data being collected at LHC detectors, it is hopeful that more
   observations on the excited Bc states will be issued. Therefore in
   addition to the production via color-singlet B-c(1S), B-c(1P) and
   color-octet B-c(1S) states, the B-c(2S) production is involved in
   BCVEGPY2.2. It is achieved by replacing the 1S-wave bound-state
   parameters pmb, pmc and fbc with those of the 2S-wave one. Here pmb, pmc
   and fbc are for b-quark mass, c-quark mass and the radial wave function
   at the zero (vertical bar R(0)1), respectively. For the 2S-wave case,
   their default values are set as pmb =5.234 GeV, pmc =1.633 GeV and fbc
   =0.991 GeV3/2 [10] if the mass of the 2S-wave Bc state is 6.867 GeV.More
   explicitly, two new values for ibcstate are added: ibcstate =9 is to
   generate 2(1)S(0) state and ibcstate =10 is to generate 2(3)S(1) state.
   Detailed technologies for deriving the production properties of all the
   mentioned ten Bc meson states can be found in Refs.[11-13]. Furthermore,
   the values for mix_type are rearranged. mix_type=1 is to generate the
   mixing events for all mentioned states. mix_type=2 is to generate the
   mixing events for 1(1)S(0) and 1(3)S(1) states. mix_type=3 is to
   generate the mixing events for the four 1P-wave states and the two
   color-octet 1(1)S(0) and 1(3)S(1) states. mix_type=4 is to generate the
   mixing events for 2(1)S(0) and 2(3)S(1) states. (C) 2015 Elsevier B.V.
   All rights reserved.</abstract><date>DEC 2015</date><author>Chang, Chao-Hsi
   Wang, Xian-You
   Wu, Xing-Gang</author></paper><paper><title>Orbifold Tutte Embeddings</title><abstract>Injective parameterizations of surface meshes are vital for many
   applications in Computer Graphics, Geometry Processing and related
   fields. Tutte's embedding, and its generalization to convex combination
   maps, are among the most popular approaches for computing
   parameterizations of surface meshes into the plane, as they guarantee
   injectivity, and their computation only requires solving a sparse linear
   system. However, they are only applicable to disk-type and toric surface
   meshes.In this paper we suggest a generalization of Tutte's embedding to
   other surface topologies, and in particular the common, yet untreated
   case, of sphere-type surfaces. The basic idea is to enforce certain
   boundary conditions on the parameterization so as to achieve a Euclidean
   orbifold structure. The orbifold-Tutte embedding is a seamless, globally
   bijective parameterization that, similarly to the classic Tutte
   embedding, only requires solving a sparse linear system for its
   computation.In case the cotangent weights are used, the orbifold-Tutte
   embedding globally minimizes the Dirichlet energy and is shown to
   approximate conformal and four-point quasiconformal mappings. As far as
   we are aware, this is the first fully-linear method that produces
   bijective approximations to conformal mappings.Aside from
   parameterizations, the orbifold-Tutte embedding can be used to generate
   bijective inter-surface mappings with three or four landmarks and
   symmetric patterns on sphere-type surfaces.</abstract><date>NOV 2015</date><author>Aigerman, Noam
   Lipman, Yaron</author></paper><paper><title>In Acute Myocardial Infarction Liver Parameters Are Associated With
   Stenosis Diameter</title><abstract>Detection of high-risk subjects in acute myocardial infarction (AMI) by
   noninvasive means would reduce the need for intracardiac catheterization
   and associated complications. Liver enzymes are associated with
   cardiovascular disease risk. A potential predictive value for liver
   serum markers for the severity of stenosis in AMI was analyzed.Patients
   with AMI undergoing percutaneous coronary intervention (PCI; n = 437)
   were retrospectively evaluated. Minimal lumen diameter (MLD) and percent
   stenosis diameter (SD) were determined from quantitative coronary
   angiography. Patients were classified according to the severity of
   stenosis (SD &gt;= 50%, n = 357; SD &lt; 50%, n = 80). Routine heart and liver
   parameters were associated with SD using random forests (RF). A
   prediction model (M10) was developed based on parameter importance
   analysis in RF.Age, alkaline phosphatase (AP), aspartate
   aminotransferase (AST), and MLD differed significantly between SD &gt;= 50
   and SD &lt; 50. Age, AST, alanine aminotransferase (ALT), and troponin
   correlated significantly with SD, whereas MLD correlated inversely with
   SD. M10 (age, BMI, AP, AST, ALT, gamma-glutamyltransferase, creatinine,
   troponin) reached an AUC of 69.7% (CI 63.8-75.5%, P &lt; 0.0001).Routine
   liver parameters are associated with SD in AMI. A small set of
   noninvasively determined parameters can identify SD in AMI, and might
   avoid unnecessary coronary angiography in patients with low risk. The
   model can be accessed via[GRAPHICS].</abstract><date>FEB 2016</date><author>Baars, Theodor
   Neumann, Ursula
   Jinawy, Mona
   Hendricks, Stefanie
   Sowa, Jan-Peter
   Kaelsch, Julia
   Riemenschneider, Mona
   Gerken, Guido
   Erbel, Raimund
   Heider, Dominik
   Canbay, Ali</author></paper><paper><title>Vacancy-related diffusion correlation effects in a simple cubic random
   alloy and on the Na-K sublattice of alkali feldspar</title><abstract>Motivated by the need to analyse experimental data on ionic conductivity
   in alkali feldspar, we performed Monte Carlo (MC) simulations of vacancy
   diffusion in random binary systems. We employed an efficient procedure
   for the calculation of the vacancy correlation factor[GRAPHICS], which
   includes the computation of the associated partial correlation factors
   (PCFs)[GRAPHICS]and[GRAPHICS]. Test simulations on a simple cubic
   lattice show the improvements compared to previous MC data and the
   discrepancies with the Manning model. Vacancy correlation factors on the
   Na-K sublattice in the monoclinic structure of alkali feldspar proved to
   be dependent on crystal orientation. For the[GRAPHICS]-direction, PCFs
   related to the four different jump types were calculated. We also
   examined the percolation behaviour for extreme ratios of the atomic jump
   frequencies. The results are found to agree with known data for the
   simple cubic lattice. In the case of feldspar, we provide the first
   useful estimates for the percolation threshold and the associated
   critical exponent using a simplified set of jump frequencies.</abstract><date>JUL 23 2015</date><author>Wilangowski, F.
   Stolwijk, N. A.</author></paper><paper><title>THE TYPES OF COMPUTER GRAPHICS AND THEIR APPLICATION AT DIFFERENT LEVELS
   OF KNOWLEDGE</title><abstract>In this article we introduce the concept of computer graphics and
   graphical application C.a.R (Compasses and Ruler), its basic commands
   and several examples associated with the geometry. This subject is at
   all levels of knowledge in various stages of development. We will
   present the C. a. R possibilities that can be used in secondary schools,
   high schools and colleges. We show function graphs of varying degrees of
   difficulty that are too complicated for the human imagination.</abstract><date>DEC 2015</date><author>Makarewicz, Anna
   Korga, Sylwester
   Rosa, Wojciech</author></paper><paper><title>Visual Perception of Procedural Textures: Identifying Perceptual
   Dimensions and Predicting Generation Models</title><abstract>Procedural models are widely used in computer graphics for generating
   realistic, natural-looking textures. However, these mathematical models
   are not perceptually meaningful, whereas the users, such as artists and
   designers, would prefer to make descriptions using intuitive and
   perceptual characteristics like "repetitive," "directional,"
   "structured," and so on. To make up for this gap, we investigated the
   perceptual dimensions of textures generated by a collection of
   procedural models. Two psychophysical experiments were conducted:
   free-grouping and rating. We applied Hierarchical Cluster Analysis (HCA)
   and Singular Value Decomposition (SVD) to discover the perceptual
   features used by the observers in grouping similar textures. The results
   suggested that existing dimensions in literature cannot accommodate
   random textures. We therefore utilized isometric feature mapping
   (Isomap) to establish a three-dimensional perceptual texture space which
   better explains the features used by humans in texture similarity
   judgment. Finally, we proposed computational models to map perceptual
   features to the perceptual texture space, which can suggest a procedural
   model to produce textures according to user-defined perceptual scales.</abstract><date>JUN 24 2015</date><author>Liu, Jun
   Dong, Junyu
   Cai, Xiaoxu
   Qi, Lin
   Chantler, Mike</author></paper><paper><title>Revised spectral matching algorithm for scenes with mutually
   inconsistent local transformations</title><abstract>Spectral matching (SM) is an efficient and effective greedy algorithm
   for solving the graph matching problem in feature correspondence in
   computer vision and graphics. However, the classic SM algorithm cannot
   extract correspondences well when the affinity matrix is sparse and
   reducible (i.e. its corresponding graph is not connected). This case
   often happens when the geometric deformations consist of transformations
   with local inconsistency. The authors analyse this problem and show how
   the original SM could fail in this scenario. Then, the authors propose a
   revised two-step pipeline to tackle this issue: (1) decompose the
   mutually inconsistent local deformations into several consistent
   transformations which can be solved by individual SM; (2) filter out
   incorrect correspondences through an automatic thresholding. The authors
   perform experiments to demonstrate that this modification can
   effectively handle the coarse correspondence computation in shape or
   image registration where the global transformation consists of multiple
   inconsistent local transformations.</abstract><date>OCT 2015</date><author>Chen, Peizhi
   Li, Xin</author></paper><paper><title>Editing, Publishing and Aggregating Video Articles: Do We Need a
   Scholarly Approach?</title><abstract>The article supports the idea of providing infrastructure and training
   for preparing and publishing quality video articles. Properly edited,
   formatted, and verified video items can present graphic contents of
   interest to the global scientific community. It is suggested to apply
   traditional attributes of scholarly articles to video items and
   aggregate them on a specifically designed editing, publishing, and
   indexing platform, called PubTube. As a mega platform, PubTube may
   provide space for a variety of open-access sources of information,
   ranging from short audio-video presentations to research protocols and
   educational lectures. Video articles on the platform have to pass
   quality checks by skilled reviewers. Global editorial associations
   should be prepared to improving the whole process of publishing and
   aggregating video articles.</abstract><date>SEP 2015</date><author>Assadi, Reza
   Gasparyan, Armen Yuri</author></paper><paper><title>Intraoperative on-the-fly organ-mosaicking for laparoscopic surgery.</title><abstract>The goal of computer-assisted surgery is to provide the surgeon with
   guidance during an intervention, e.g.,using augmented reality. To
   display preoperative data, soft tissue deformations that occur during
   surgery have to be taken into consideration. Laparoscopic sensors, such
   as stereo endoscopes, can be used to create a three-dimensional
   reconstruction of stereo frames for registration. Due to the small field
   of view and the homogeneous structure of tissue, reconstructing just one
   frame, in general, will not provide enough detail to register
   preoperative data, since every frame only contains a part of an organ
   surface. A correct assignment to the preoperative model is possible only
   if the patch geometry can be unambiguously matched to a part of the
   preoperative surface. We propose and evaluate a system that combines
   multiple smaller reconstructions from different viewpoints to segment
   and reconstruct a large model of an organ. Using graphics processing
   unit-based methods, we achieved four frames per second. We evaluated the
   system with in silico, phantom, ex vivo, and in vivo (porcine) data,
   using different methods for estimating the camera pose (optical
   tracking, iterative closest point, and a combination). The results
   indicate that the proposed method is promising for on-the-fly organ
   reconstruction and registration. </abstract><date>2015-Oct</date><author>Reichard, Daniel
   Bodenstedt, Sebastian
   Suwelack, Stefan
   Mayer, Benjamin
   Preukschas, Anas
   Wagner, Martin
   Kenngott, Hannes
   Muller-Stich, Beat
   Dillmann, Rudiger
   Speidel, Stefanie</author></paper><paper><title>CHANGING TECHNIQUES OF ARCHITECTURAL DESIGN PRESENTATION</title><abstract>The aim of this paper is to highlight contemporary grey area in fair
   presentation of architectural project. Historical evolution of depicting
   techniques shows adequacy of architectural "visions" to the
   manifestations of art in subsequent historical periods. A breakthrough
   in the presentation of projects turned out to be the departure from
   realism in Art. Building designs were presented in a similar, abstract
   manner. At first it was a domain of groups of avant-garde artists and
   architects. Situation has changed after World War II followed by the
   division of Europe into two political and economy zones, and formation
   of the so-called people's democracy countries. In state-owned, big
   design offices and contractor companies, design drawings were delivering
   professional information to the professionals. At the same time, in
   capitalist countries, presentations of designs were aimed at private
   investors, majority of whom were not architects. Therefore, for
   practical reasons they took on decidedly realistic forms. A computer
   with graphic software offers now almost unlimited possibilities of
   presenting hiperrealistic and thus seductive images. Flair and
   creativity are sometimes substituted with software manipulations.
   Professional ethics should become the principal virtue in contemporary
   design rendering.</abstract><date>SEP 2015</date><author>Bardzinska-Bonenberg, Teresa
   Swit-Jankowska, Barbara</author></paper><paper><title>Days of Endless Time</title><abstract></abstract><date>JUL-AUG 2015</date><author>Day, Charles</author></paper><paper><title>The Genesis of Electronic Charting</title><abstract>Significant advances in marine navigation have resulted in all major
   vessels being equipped with GPS (Global Positioning System) receivers
   today, capable of providing, highly accurate positions worldwide as an
   input to ECS (Electronic Chart System) and ECDIS (Electronic Chart
   Display:and Information System) a system that was approved by IMO
   (International Maritime Organization) for use on ships in
   1989.Similarly, most major vehicle fleet operations and, in fact, most
   automobiles are now, 5 available With devices referred to as "GPS,"
   although they consist of the same two distinct subsystems : One
   essentially a GPS receiver collects,signals from satellite constellation
   to calculate a position. Most importantly, it provides a latitude and
   longitude position, which is sent to the second Sub-system consisting of
   a computer with a graphics display that Shows this Lat/Lon position on a
   map or chart background.To manufacturers, GPS is primarily an enabling
   technology for electronic charts and mapping systems that provides the
   necessary position information so it can be displayed in relation to
   surrounding hazards. It does this without,delay and often in combination
   With other important navigation sensor data on board.Without charts as a
   background, numbers er coordinates alone can the mariner about his or:
   her relationship to a reef or Shoal up ahead. Without such systems,
   ships could net meet the suggested accuracies for "Harbor Entrance and
   Approach" in the United States .ECDIS has been recognized by many great
   leaders in the industry."It will completely change the way we do
   business... For the first time you will know where you are, not where
   you were."-RADM J. Austin Yeager, NOAA Coast &amp; geodetic survey (A New
   Way to Navigate) Geodetic Survey (A New to Navigate)"ECDIS is the most
   significant improvement in navigtion in the past 100 years."- Captain Ed
   Rollinson, U.S. Coast Guard (ECDIS- A View From the Bridge, issued by
   The Canadian Hydrographic Service and Partners, U.S. Coast Guard R&amp;D
   Center)"ECDIS... Potentially the most significant breakthrough in marine
   navigation that _ has occurred since the advent of radar almost 50 years
   ago"-Dr. Lee Alexander, Chairman of International Electrotechnical
   Commission Working Group (Leading the Way with ECPINS, issued by
   Offshore Systems Ltd. [OSL]/Offshore Systems International
   [OSI])Development and application of the first ECS is presented,
   together sequent advances to level of ECDIS.</abstract><date>NOV-DEC 2015</date><author>Lanziner, Helmut</author></paper><paper><title>Future Directions: How Virtual Reality Can Further Improve the
   Assessment and Treatment of Eating Disorders and Obesity.</title><abstract>Transdisciplinary efforts for further elucidating the etiology of eating
   and weight disorders and improving the effectiveness of the available
   evidence-based interventions are imperative at this time. Recent studies
   indicate that computer-generated graphic environments-virtual reality
   (VR)-can integrate and extend existing treatments for eating and weight
   disorders (EWDs). Future possibilities for VR to improve actual
   approaches include its use for altering in real time the experience of
   the body (embodiment) and as a cue exposure tool for reducing food
   craving. </abstract><date>2016-Feb</date><author>Gutierrez-Maldonado, Jose
   Wiederhold, Brenda K
   Riva, Giuseppe</author></paper><paper><title>Dimensionality reduction and coloured noise removal from hyperspectral
   images</title><abstract>Hyperspectral image (HSI) classification requires spectral
   dimensionality reduction and noise reduction. While common
   dimensionality reduction (DR) and denoising methods are based on linear
   algebra, we propose a multilinear algebra method to jointly achieve
   denoising reduction and DR. Multilinear tools consider multidimensional
   data as whole entity by processing jointly spatial and spectral ways.
   However, it cannot cope with the HSIs distorted by non-white noise which
   is the most realistic case and cannot preserve rare signals. First, we
   propose a new method for whitening the noise (W) in HSI. Then we propose
   a method based on multidimensional wavelet packet transform (MWPT) and
   multiway Wiener filter (MWF) which performs both non-white noise and
   spectral DR, referred to as W-MWPT-MWFdr-([GRAPHICS]). The
   Classification algorithm support vector (SVM) machines are applied to
   the output of the following DR and noise reduction methods to compare
   their efficiency: The proposed W-MWPT-MWFdr-([GRAPHICS]); prewhitening
   method associated with MWF (PMWF), principal component analysis
   (PCA(dr)), minimum noise fraction (MNFdr), PCA(dr) associated with
   Wiener filtering (PCA(dr)-Wiener), and MNFdr associated with Wiener
   filtering (MNFdr-Wiener).</abstract><date>NOV 2 2015</date><author>Bourennane, S.
   Fossati, C.</author></paper><paper><title>IMP 2.0: a multi-species functional genomics portal for integration,
   visualization and prediction of protein functions and networks</title><abstract>IMP (Integrative Multi-species Prediction), originally released in 2012,
   is an interactive web server that enables molecular biologists to
   interpret experimental results and to generate hypotheses in the context
   of a large cross-organism compendium of functional predictions and
   networks. The system provides biologists with a framework to analyze
   their candidate gene sets in the context of functional networks,
   expanding or refining their sets using functional relationships
   predicted from integrated high-throughput data. IMP 2.0 integrates
   updated prior knowledge and data collections from the last three years
   in the seven supported organisms (Homo sapiens, Mus musculus, Rattus
   norvegicus, Drosophila melanogaster, Danio rerio, Caenorhabditis
   elegans, and Saccharomyces cerevisiae) and extends function prediction
   coverage to include human disease. IMP identifies homologs with
   conserved functional roles for disease knowledge transfer, allowing
   biologists to analyze disease contexts and predictions across all
   organisms. Additionally, IMP 2.0 implements a new flexible platform for
   experts to generate custom hypotheses about biological processes or
   diseases, making sophisticated data-driven methods easily accessible to
   researchers. IMP does not require any registration or installation and
   is freely available for use at http://imp.princeton.edu.</abstract><date>JUL 1 2015</date><author>Wong, Aaron K.
   Krishnan, Arjun
   Yao, Victoria
   Tadych, Alicja
   Troyanskaya, Olga G.</author></paper><paper><title>(PS)(2): protein structure prediction server version 3.0</title><abstract>Protein complexes are involved in many biological processes. Examining
   coupling between subunits of a complexwould be useful to understand
   themolecular basis of protein function. Here, our updated (PS)(2) web
   server predicts the three-dimensional structures of protein complexes
   based on comparative modeling; furthermore, this server examines the
   coupling between subunits of the predicted complex by combining
   structural and evolutionary considerations. The predicted complex
   structure could be indicated and visualized by Java-based 3D graphics
   viewers and the structural and evolutionary profiles are shown and
   compared chain-by-chain. For each subunit, considerations with or
   without the packing contribution of other subunits cause the differences
   in similarities between structural and evolutionary profiles, and these
   differences imply which form, complex or monomeric, is preferred in the
   biological condition for the subunit. We believe that the (PS)(2) server
   would be a useful tool for biologists who are interested not only in the
   structures of protein complexes but also in the coupling between
   subunits of the complexes. The (PS)(2) is freely available at
   http://ps2v3.life.nctu.edu.tw/.</abstract><date>JUL 1 2015</date><author>Huang, Tsun-Tsao
   Hwang, Jenn-Kang
   Chen, Chu-Huang
   Chu, Chih-Sheng
   Lee, Chi-Wen
   Chen, Chih-Chieh</author></paper><paper><title>Automated protein motif generation in the structure-based protein
   function prediction tool ProMOL</title><abstract>ProMOL, a plugin for the PyMOL molecular graphics system, is a
   structure-based protein function prediction tool. ProMOL includes a set
   of routines for building motif templates that are used for screening
   query structures for enzyme active sites. Previously, each motif
   template was generated manually and required supervision in the
   optimization of parameters for sensitivity and selectivity. We developed
   an algorithm and workflow for the automation of motif building and
   testing routines in ProMOL. The algorithm uses a set of empirically
   derived parameters for optimization and requires little user
   intervention. The automated motif generation algorithm was first tested
   in a performance comparison with a set of manually generated motifs
   based on identical active sites from the same 112 PDB entries. The two
   sets of motifs were equally effective in identifying alignments with
   homologs and in rejecting alignments with unrelated structures. A second
   set of 296 active site motifs were generated automatically, based on
   Catalytic Site Atlas entries with literature citations, as an expansion
   of the library of existing manually generated motif templates. The new
   motif templates exhibited comparable performance to the existing ones in
   terms of hit rates against native structures, homologs with the same EC
   and Pfam designations, and randomly selected unrelated structures with a
   different EC designation at the first EC digit, as well as in terms of
   RMSD values obtained from local structural alignments of motifs and
   query structures. This research is supported by NIH grant GM078077.</abstract><date>DEC 2015</date><author>Osipovitch, Mikhail
   Lambrecht, Mitchell
   Baker, Cameron
   Madha, Shariq
   Mills, Jeffrey L.
   Craig, Paul A.
   Bernstein, Herbert J.</author></paper><paper><title>Extended graph rotation systems as a model for cyclic weaving on
   orientable surfaces</title><abstract>We present an extension of the theory of graph rotation systems, which
   has been a widely used model for graph imbeddings on topological
   surfaces. The extended model is quite beyond what is needed to specify
   graph imbeddings on surfaces, and it can be used to represent and
   generate link structures immersed on surfaces. Since link structures
   immersed on surfaces can be viewed as woven images in 3D space, the
   extended graph rotation systems provide a well-formulated mathematical
   model for developing a topologically robust graphics system with strong
   interactive operations for the design of woven images in 3D
   mesh-modeling and computer-aided sculpting. (C) 2015 Elsevier B.V. All
   rights reserved.</abstract><date>OCT 1 2015</date><author>Akleman, Ergun
   Chen, Jianer
   Gross, Jonathan L.</author></paper><paper><title>Improving Remote Desktopping through Adaptive Record/Replay</title><abstract>Accessing the display of a computer remotely, is popularly called
   "remote desktopping(1)". Remote desktopping software installs at both
   the user-facing client computer and the remote server computer; it
   simulates user's input events at server, and streams the corresponding
   display changes to client, thus providing an illusion to the user of
   controlling the remote machine using local input devices (e.g.,
   keyboard/mouse). Many such remote desktopping tools are widely used.We
   show that if the remote server is a virtual machine (VM) and the client
   is reasonably powerful (e.g., current laptop and desktop grade
   hardware), VM deterministic replay capabilities can be used adaptively
   to significantly reduce the network bandwidth consumption and
   server-side CPU utilization of a remote desktopping tool. We implement
   these optimizations in a tool based on Qemu/KVM virtualization platform
   and VNC remote desktopping platform. Our tool reduces VNC's network
   bandwidth consumption by up to 9x and server-side CPU utilization by up
   to 56% for popular graphics-intensive applications. On the flip side,
   our techniques consume higher CPU/memory/disk resources at the client.
   The effect of our optimizations on user-perceived latency is negligible.</abstract><date>JUL 2015</date><author>Jaffer, Shehbaz
   Kedia, Piyus
   Bansal, Sorav</author></paper><paper><title>A proxy approach to integrate heterogeneous CAD resources for
   distributed collaborative design</title><abstract>Collaborative product design on heterogeneous computer-aided design
   (CAD) platforms has shown its significance to today's global
   manufacturing industries. This article proposes a novel collaboration
   framework for supporting distributed collaborative design with
   heterogeneous CAD resources. The unique feature of the framework is
   that, instead of directly accessing heterogeneous CAD resources, a
   so-called co-proxy model is defined and created to work as the
   replication agent representing original CAD resources during
   collaborative sessions. The co-proxy model is designed to represent
   corresponding CAD resources with both structured polygonal graphics for
   visualisation and a set of embedded virtual topological elements (VTE),
   so as to support collaborative design manipulations such as virtual
   assembly and annotation. Moreover, the co-proxy model comes with an
   external link mechanism, with which the powerful modelling and editing
   functionalities from heterogeneous CAD systems can be reasonably
   integrated into the distributed collaboration framework. The proposed
   framework together with its enabling technologies has been implemented
   into a prototype system named as Co-DMU, which is generally applicable
   for feature-based CAD modelling systems with automation interfaces
   provided.</abstract><date>JUN 3 2015</date><author>Li, Jing-Rong
   Tang, Cheng
   Wang, Qing-Hui</author></paper><paper><title>Topology optimization design of 3D electrothermomechanical actuators by
   using GPU as a co-processor</title><abstract>The topology optimization method (TOM) requires high computational
   resources to be solved, especially in multiphysics problems. The high
   number of computational requirements is because TOM is an iterative
   technique, in which the iterations go from tens to thousands.
   Furthermore, at each TOM iteration, it is necessary to execute several
   routines such as the finite element method (FEM), the optimizer, the
   calculation of the objective function and filter, and other less
   intensive computations. Consequently, several topology optimization
   problems have been restricted in the dimensionality and/or in the
   complexity considered, or addressed in powerful computers of restricted
   access due to their cost. Hence, in order to deal with complex and
   large-scale problem in standard computers, a methodology based on
   parallel computing on graphics processing unit (GPU) has been proposed
   by the scientific community. However, so far, this kind of approach has
   been mostly investigated for the traditional mean compliance
   optimization problem. In this work, TOM is applied to designing
   three-dimensional (3D) electrothermomechanical (ETM) actuators by using
   GPU as a co-processor in the most intensive and intrinsic parallel tasks
   of the method. The TOM is based on the SIMP material model and solved by
   sequential linear programming. The code is programmed in Matlab and
   CUDA, and is tested for obtaining a 3D microactuator. Considerable
   speedup is gained with the GPU; the whole TOM process is achieved up to
   35 times faster than that obtained with the sequential code version. (C)
   2016 Elsevier B.V. All rights reserved.</abstract><date>APR 15 2016</date><author>Javier Ramirez-Gil, Francisco
   Nelli Silva, Emilio Carlos
   Montealegre-Rubio, Wilfredo</author></paper><paper><title>Interactive image filtering for level-of-abstraction texturing of
   virtual 3D scenes</title><abstract>Texture mapping is a key technology in computer graphics. For the visual
   design of 3D scenes, in particular, effective texturing depends
   significantly on how important contents are expressed, e.g., by
   preserving global salient structures, and how their depiction is
   cognitively processed by the user in an application context.
   Edge-preserving image filtering is one key approach to address these
   concerns. Much research has focused on applying image filters in a
   post-process stage to generate artistically stylized depictions.
   However, these approaches generally do not preserve depth cues, which
   are important for the perception of 3D visualization (e.g., texture
   gradient). To this end, filtering is required that processes texture
   data coherently with respect to linear perspective and spatial
   relationships. In this work, we present an approach for texturing 3D
   scenes with perspective coherence by arbitrary image filters. We propose
   decoupled deferred texturing with (1) caching strategies to
   interactively perform image filtering prior to texture mapping and (2)
   for each mipmap level separately to enable a progressive level of
   abstraction, using (3) direct interaction interfaces to parameterize the
   visualization according to spatial, semantic, and thematic data. We
   demonstrate the potentials of our method by several applications using
   touch or natural language inputs to serve the different interests of
   users in specific information, including illustrative visualization,
   focus+context visualization, geometric detail removal, and semantic
   depth of field. The approach supports frame-to-frame coherence,
   order-independent transparency, multitexturing, and content-based
   filtering. In addition, it seamlessly integrates into real-time
   rendering pipelines and is extensible for custom interaction techniques.
   (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>NOV 2015</date><author>Semmo, Amir
   Doellner, Juergen</author></paper><paper><title>Radiometric Transfer: Example-based Radiometric Linearization of
   Photographs</title><abstract>We present an example-based approach for radiometrically linearizing
   photographs that takes as input a radiometrically linear exemplar image
   and a target regular uncalibrated image of the same scene, possibly from
   a different viewpoint and/or under different lighting. The output of our
   method is a radiometrically linearized version of the target image.
   Modeling the change in appearance of a small image patch seen from a
   different viewpoint and/or under different lighting as a linear 1D
   subspace, allows us to recast radiometric transfer in a form similar to
   classic radiometric calibration from exposure stacks. The resulting
   radiometric transfer method is lightweight and easy to implement. We
   demonstrate the accuracy and validity of our method on a variety of
   scenes.</abstract><date>JUL 2015</date><author>Li, Han
   Peers, Pieter</author></paper><paper><title>Longest-edge n-section algorithms: Properties and open problems</title><abstract>In this paper we survey all known (including own recent results)
   properties of the longest-edge n-section algorithms. These algorithms
   (in classical and recently designed conforming form) are nowadays used
   in many applications, including finite element simulations, computer
   graphics, etc. as a reliable tool for controllable mesh generation. In
   addition, we present a list of open problems arising in and around this
   topic. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>FEB 2016</date><author>Korotov, Sergey
   Plaza, Angel
   Suarez, Jose P.</author></paper><paper><title>Batch-Theta* for path planning to the best goal in a goal set</title><abstract>The development of 3D cameras and many navigation-supporting sensors has
   recently enabled robots to build their working maps and navigate
   accurately, making path planning popular not just on computer graphics,
   but in real environments as well. Pursuing the solutions for robot path
   planning, this paper presents a variant of searching method Theta* for
   choosing the best goal among given goals and the lowest-cost path to it,
   called Batch-Theta*. The novelty lies at the proposed line-of-sight
   checking function during the searching process and the manner that the
   method handles the batch of goals during one search instead of
   repeatedly considering a single goal or blindly doing the exhausted
   search. The analysis and simulations show that the proposed Batch-Theta*
   efficiently finds the lowest-cost path to the best goal in a given goal
   set under Theta*'s mechanism.</abstract><date>DEC 2 2015</date><author>Viet-Hung Dang
   Nguyen Duc Thang
   Hoang Huu Viet
   Le Anh Tuan</author></paper><paper><title>The Hydrological Status Concept: Application at a Temporary River
   (Candelaro, Italy)</title><abstract>In achieving the final objective of the European Water Framework
   Directive, the evaluation of the hydrological status' of a water body in
   a catchment is of the utmost importance. It represents the divergence of
   the actual hydrological regime from its natural' condition and may thus
   provide crucial information about the ecological status of a river. In
   this paper, a new approach in evaluating the hydrological status of a
   temporary river was tested. The flow regime of a river has been
   classified through the analysis of two metrics: the permanence of flow
   and the predictability of no-flow conditions that were evaluated on
   monthly streamflow data. This method was applied to the Candelaro river
   basin (Puglia, Italy) where we had to face the problem of limited data
   availability. The Soil and Water Assessment Tool model was used when
   streamflow data were not available, and a geographic information system
   procedure was applied to estimate potential water abstractions from the
   river. Four types of rivers were identified whose regimes may exert a
   control on aquatic life. By using the two metrics as coordinates in a
   plot, a graphic representation of the regime can be visualized in a
   point. Hydrological perturbations associated with water abstractions,
   point discharges and the presence of a reservoir were assessed by
   comparing the position of the two points representing the regime before
   and after the impacts. The method is intended to be used with biological
   metrics in order to define the ecological status of a stream, and it
   could also be used in planning the measures' aimed at fulfilling the
   Water Framework Directive goals. Copyright (c) 2014 John Wiley &amp; Sons,
   Ltd.</abstract><date>SEP 2015</date><author>De Girolamo, A. M.
   Lo Porto, A.
   Pappagallo, G.
   Tzoraki, O.
   Gallart, F.</author></paper><paper><title>Special Issue: 40 YEARS OF COMPUTER GRAPHICS IN DARMSTADT Foreword</title><abstract></abstract><date>DEC 2015</date><author>Encarnacao, Jose Luis</author></paper><paper><title>Stereoscopic neuroanatomy lectures using a three-dimensional virtual
   reality environment</title><abstract>Introduction: Three-dimensional (3D) computer graphics are increasingly
   used to supplement the teaching of anatomy. While most systems consist
   of a program which produces 3D renderings on a workstation with a
   standard screen, the Dextrobeam virtual reality VR environment allows
   the presentation of spatial neuroanatomical models to larger groups of
   students through a stereoscopic projection system.Materials and methods:
   Second-year medical students (n = 169) were randomly allocated to
   receive a standardised pre-recorded audio lecture detailing the anatomy
   of the third ventricle accompanied by either a two-dimensional (2D)
   PowerPoint presentation (n = 80) or a 3D animated tour of the third
   ventricle with the DextroBeam. Students completed a 10-question
   multiple-choice exam based on the content learned and a subjective
   evaluation of the teaching method immediately after the lecture.Results:
   Students in the 2D group achieved a mean score of 5.19 (+/- 2.12)
   compared to 5.45 (+/- 2.16) in the 3D group, with the results in the 3D
   group statistically non-inferior to those of the 2D group (p &lt; 0.0001).
   The students rated the 3D method superior to 2D teaching in four domains
   (spatial understanding, application in future anatomy classes,
   effectiveness, enjoyableness) (p &lt; 0.01).Conclusion: Stereoscopically
   enhanced 3D lectures are valid methods of imparting neuroanatomical
   knowledge and are well received by students. More research is required
   to define and develop the role of large-group VR systems in modern
   neuroanatomy curricula. (C) 2015 Elsevier GmbH. All rights reserved.</abstract><date>2015</date><author>Kockro, Ralf A.
   Amaxopoulou, Christina
   Killeen, Tim
   Wagner, Wolfgang
   Reisch, Robert
   Schwandt, Eike
   Gutenberg, Angelika
   Giese, Alf
   Stofft, Eckart
   Stadie, Axel T.</author></paper><paper><title>A review of the new HGNC gene family resource</title><abstract>The HUGO Gene Nomenclature Committee (HGNC) approves unique gene symbols
   and names for human loci. As well as naming genomic loci, we manually
   curate genes into family sets based on shared characteristics such as
   function, homology or phenotype. Each HGNC gene family has its own
   dedicated gene family report on our website, www.genenames.org. We have
   recently redesigned these reports to support the visualisation and
   browsing of complex relationships between families and to provide extra
   curated information such as family descriptions, protein domain graphics
   and gene family aliases. Here, we review how our gene families are
   curated and explain how to view, search and download the gene family
   data.</abstract><date>FEB 3 2016</date><author>Gray, Kristian A.
   Seal, Ruth L.
   Tweedie, Susan
   Wright, Mathew W.
   Bruford, Elspeth A.</author></paper><paper><title>A survey on object deformation and decomposition in computer graphics</title><abstract>In a realistic world, objects are expected to change in appearance over
   time when exposed to their environment. Morphology changes can be
   important indicators of an object's make up and the environment in which
   it exists. Simulating changes in an object's appearance over time has
   become increasingly popular over the recent years. In this survey we
   will describe a number of methods used in computer graphics to simulate
   object morphology changes due natural influences, such as cracks,
   fractures, patina, corrosion, erosion, burning, melting, decay, rotting
   and withering. We will focus on approaches that consider effects which
   influence the geometry of the entire object, instead of the surface
   appearance alone. The methods described are categorised according to the
   natural phenomena that drive the appearance changes. We pay particular
   attention to the different object representation and deformation
   techniques used in current approaches. The aim of this article is to
   provide a comprehensive overview of the state of the art in the area of
   object morphology changes driven by the environment. (C) 2015 Elsevier
   Ltd. All rights reserved.</abstract><date>NOV 2015</date><author>Frerichs, Dhana
   Vidler, Andrew
   Gatzidis, Christos</author></paper><paper><title>High performance GPU based optimized feature matching for computer
   vision applications</title><abstract>In this paper, a graphics processing unit (GPU) based matching technique
   is proposed to perform fast feature matching between different images
   under various image conditions with viewpoint changes. Most recently,
   general-purpose graphics processing units (GPGPUs or GPUs) have become
   commonplace within high performance supercomputers. GPUs allow
   developers to effectively exploit the computational power for high
   performance computing. This paper focuses on improving the performance
   of feature matching based on self-organizing map by porting it onto the
   GPUs. GPU optimization has been applied for the fast computation of
   keypoints to make the system fast and efficient. This scheme has
   enhanced the overall performance and is much more efficient compared to
   other methods without degradation of detection results. The proposed
   matching algorithm is partitioned between the CPU and GPU in a way that
   best exploits the parallelism and perform matching between the different
   images. Experimental results demonstrate that fast feature matching is
   achieved using the graphics processing units, and its computational
   efficiency is checked by comparing its results and run times with those
   of some standard software (MATLAB) run on central processing unit (CPU).
   (c) 2015 Elsevier GmbH. All rights reserved.</abstract><date>2016</date><author>Sharma, Kajal</author></paper><paper><title>1D and 2D Shape Descriptors Applied in Fabric Drape Computer Simulation</title><abstract>Fabric drape simulations accomplished by computer graphics software can
   provide the basis for effective communication among designers,
   manufacturers and other players in the apparel industry. The goal of our
   study was to investigate various 1D and 2D shape descriptors used to
   characterize renderings of 3D drape simulations in dependence on the
   geometry of collision objects and fabric type. Image processing routines
   were implemented to extract and compute the shape descriptors while
   principal components analysis was applied to interpret the relationships
   among the parameters studied. Drapes on cube, octahedron and prism were
   found to behave in a distinctively different manner compared to those
   produced using the other six collision objects: cone, cylinder,
   dodecahedron, gen-gon, sphere, and tube. A first principal component can
   be, to a large extent, represented by the following mutually strongly
   correlated 2D shape descriptors: area, major axis length, minor axis
   length, equivalent diameter, and perimeter. Analysis using 1D shape
   descriptors confirms these findings and additionally suggests that
   rubber-based drapes contain the lowest number of folds while those on
   polyester, wool, and sometimes silk and/or satin are characterized by
   the highest number of drape folds. These results were confirmed by
   visual examination of the drapes simulated.</abstract><date>NOV-DEC 2015</date><author>Tomc, Helena Gabrijelcic
   Hladnik, Ales</author></paper><paper><title>Analysis of cholera epidemics with bacterial growth and spatial movement</title><abstract>In this work, we propose novel epidemic models (named,
   susceptible-infected-recovered-susceptible-bacteria) for cholera
   dynamics by incorporating a general formulation of bacteria growth and
   spatial variation. In the first part, a generalized ordinary
   differential equation (ODE) model is presented and it is found that
   bacterial growth contributes to the increase in the basic reproduction
   number, &lt;inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink"
   xlink:href="tjbd_a_974696_ilm0001.gif"&gt;&lt;/inline-graphic&gt;. With the
   derived basic reproduction number, we analyse the local and global
   dynamics of the model. Particularly, we give a rigorous proof on the
   endemic global stability by employing the geometric approach. In the
   second part, we extend the ODE model to a partial differential equation
   (PDE) model with the inclusion of diffusion to capture the movement of
   human hosts and bacteria in a heterogeneous environment. The disease
   threshold of this PDE model is studied again by using the basic
   reproduction number. The results on the threshold dynamics of the ODE
   and PDE models are compared, and verified through numerical simulation.
   Additionally, our analysis shows that incorporating diffusive spatial
   spread does not produce a Turing instability when &lt;inline-graphic
   xmlns:xlink="http://www.w3.org/1999/xlink"
   xlink:href="tjbd_a_974696_ilm0002.gif"&gt;&lt;/inline-graphic&gt; associated with
   the ODE model is less than the unity.</abstract><date>JUN 30 2015</date><author>Wang, Xueying
   Wang, Jin</author></paper><paper><title>Accelerating Fibre Orientation Estimation from Diffusion Weighted
   Magnetic Resonance Imaging Using GPUs (vol 8, e61892, 2013)</title><abstract></abstract><date>JUN 12 2015</date><author>Hernandez, Moises
   Guerrero, Gines D.
   Cecilia, Jose M.
   Garcia, Jose M.
   Inuggi, Alberto
   Jbabdi, Saad
   Behrens, Timothy E. J.
   Sotiropoulos, Stamatios N.</author></paper><paper><title>Redundancy computation analysis and implementation of phase diversity
   based on GPU</title><abstract>Phase diversity method is not only used as an image restoration
   technique, but also as a wavefront sensor. However, its computations
   have been perceived as being too burdensome to achieve its real-time
   applications on a desktop computer platform. In this paper, the
   implementation of the phase diversity algorithm based on graphic
   processing unit (GPU) is presented. The redundancy computations for the
   pupil function, point spread function, and optical transfer function are
   analyzed. Two kinds of implementation methods based on GPU are compared:
   one is the general method which is accomplished by GPU library CUFFT
   without precision loss (method-1) and the other one performed by our own
   custom FFT with little damage of precision considering the redundant
   calculations (method-2). The results show the cost and gradient
   functions can be speeded up by method-2 in contrast with the method-1
   and the overhead of global memory access by kernel fusion can be
   reduced. For the image of 256 x 256 with the sampling factor of 3, the
   results reveal that method-2 achieves speedup of 1.83x compared with
   method-1 when the central 128 x 128 pixels of the point spread function
   are used.</abstract><date>OCT 2015</date><author>Zhang, Quan
   Bao, Hua
   Rao, Changhui
   Peng, Zhenming</author></paper><paper><title>HLog(n)GP: A parallel computation model for GPU clusters</title><abstract>Parallel computation model is an abstraction for the performance
   characteristics of parallel computers, and should evolve with the
   development of computational infrastructure. The heterogeneous
   CPU/Graphics Processing Unit (GPU) systems have been and will be
   important platforms for scientific computing, which introduces an urgent
   demand for new parallel computation models targeting this kind of
   supercomputers. In this research, we propose a parallel computation
   model called HLog(n)GP to abstract the computation and communication
   features of heterogeneous platforms like TH-1A. All the substantial
   parameters of HLog(n)GP are in vector form and deal with the new
   features in GPU clusters. A simplified version HLog(3)GP of the proposed
   model is mapped to a specific GPU cluster and verified with two typical
   benchmarks. Experimental results show that HLog(3)GP outperforms the
   other two evaluated models and can well model the new particularities of
   GPU clusters. Copyright (c) 2015 John Wiley &amp; Sons, Ltd.</abstract><date>DEC 10 2015</date><author>Lu, Fengshun
   Song, Junqiang
   Pang, Yufei</author></paper><paper><title>DRAWING SKILLS IN CHILDREN WITH NEURODEVELOPMENTAL DELAY AGED 2-5 YEARS</title><abstract>In typically developing children, drawing development occurs in stages
   from uncontrolled strokes to complex drawing. In this study, we examined
   drawing development in children with neurodevelopmental delay (NDD). In
   order to do so, we observed the influence of age, intraventricular
   hemorrhage (IVH) and gender on the development of drawing skills. The
   sample consisted of 52 children with NDD, aged 2 years and 6 months to 5
   years. All children were hospitalized for multidisciplinary team
   monitoring and developmental support. The evaluation of drawing
   development was administered by giving each child a blank A4 paper and
   the instruction to draw anything they wanted. All of the drawings were
   scored satisfactory or unsatisfactory. Descriptive statistics was
   employed on all relevant data to show results in frequencies and
   percentages. In order to determine differences between groups, the
   chi(2)-test was administered. The results showed greatest difference in
   drawing in children aged from 3 years to 3 years and 11 months. Children
   with lower IVH had better drawing scores than children with higher IVH
   levels. According to gender dissimilarities, a difference was found
   showing girls to have better drawing skills than boys. All study results
   pointed to the importance of early rehabilitation and continuous
   structured work with children with NDD.</abstract><date>JUN 2015</date><author>Morovic, Maja Lang
   Matijevic, Valentina
   Divljakovic, Kristina
   Kraljevic, Marija
   Dimic, Zdenka</author></paper><paper><title>A GPU Simulation Tool for Training and Optimisation in 2D Digital X-Ray
   Imaging</title><abstract>Conventional radiology is performed by means of digital detectors, with
   various types of technology and different performance in terms of
   efficiency and image quality. Following the arrival of a new digital
   detector in a radiology department, all the staff involved should adapt
   the procedure parameters to the properties of the detector, in order to
   achieve an optimal result in terms of correct diagnostic information and
   minimum radiation risks for the patient. The aim of this study was to
   develop and validate a software capable of simulating a digital Xray
   imaging system, using graphics processing unit computing. All
   radiological image components were implemented in this application: an
   X-ray tube with primary beam, a virtual patient, noise, scatter
   radiation, a grid and a digital detector. Three different digital
   detectors (two digital radiography and a computed radiography systems)
   were implemented. In order to validate the software, we carried out a
   quantitative comparison of geometrical and anthropomorphic phantom
   simulated images with those acquired. In terms of average pixel values,
   the maximum differences were below 15%, while the noise values were in
   agreement with a maximum difference of 20%. The relative trends of
   contrast to noise ratio versus beamenergy and intensity were well
   simulated. Total calculation times were below 3 seconds for clinical
   images with pixel size of actual dimensions less than 0.2 mm. The
   application proved to be efficient and realistic. Short calculation
   times and the accuracy of the results obtained make this software a
   useful tool for training operators and dose optimisation studies.</abstract><date>NOV 6 2015</date><author>Gallio, Elena
   Rampado, Osvaldo
   Gianaria, Elena
   Bianchi, Silvio Diego
   Ropolo, Roberto</author></paper><paper><title>Two Dimensional Yau-Hausdorff Distance with Applications on Comparison
   of DNA and Protein Sequences</title><abstract>Comparing DNA or protein sequences plays an important role in the
   functional analysis of genomes. Despite many methods available for
   sequences comparison, few methods retain the information content of
   sequences. We propose a new approach, the Yau-Hausdorff method, which
   considers all translations and rotations when seeking the best match of
   graphical curves of DNA or protein sequences. The complexity of this
   method is lower than that of any other two dimensional minimum Hausdorff
   algorithm. The Yau-Hausdorff method can be used for measuring the
   similarity of DNA sequences based on two important tools: the
   Yau-Hausdorff distance and graphical representation of DNA sequences.
   The graphical representations of DNA sequences conserve all sequence
   information and the Yau-Hausdorff distance is mathematically proved as a
   true metric. Therefore, the proposed distance can preciously measure the
   similarity of DNA sequences. The phylogenetic analyses of DNA sequences
   by the Yau-Hausdorff distance show the accuracy and stability of our
   approach in similarity comparison of DNA or protein sequences. This
   study demonstrates that Yau-Hausdorff distance is a natural metric for
   DNA and protein sequences with high level of stability. The approach can
   be also applied to similarity analysis of protein sequences by graphic
   representations, as well as general two dimensional shape matching.</abstract><date>SEP 18 2015</date><author>Tian, Kun
   Yang, Xiaoqian
   Kong, Qin
   Yin, Changchuan
   He, Rong L.
   Yau, Stephen S. -T.</author></paper><paper><title>Augment Your Reality.</title><abstract>This article features some of the latest advances and applications in
   computer graphics technology. </abstract><date>2016 Jan-Feb</date><author>[Anonymous]</author></paper><paper><title>Validation of a novel open-source work-flow for the simulation of
   packed-bed reactors</title><abstract>The simulation of flow and transport in packed-bed (catalytic and
   non-catalytic) reactors is of paramount importance in the chemical
   industry. Different tools have been developed in the last decades for
   generating particle packings, such as the Discrete Element Method (DEM),
   whereas Computational Fluid Dynamics (CFD) is generally employed for
   simulating fluid flow and scalar dispersion. This work-flow presents the
   main drawbacks of being computationally expensive, as most packing
   generation algorithms deal with non-convex objects, such as trilobes,
   with cumbersome strategies, and of making use of in-house or commercial
   codes, that are either difficult to access or costly. In this paper a
   novel open-source and easily accessible work-flow based on Blender, a
   rigid-body simulation tool developed for computer graphics applications,
   and OpenFOAM a very well-known CFD code, is presented. The approach,
   which presents the main advantage of being computationally fast, is
   validated by comparison with experimental data for global bulk porosity,
   particle orientation, local porosity and velocity distributions, and
   pressure drop. To our knowledge this is the very first application of
   Blender for the simulation of packed-bed reactors. (C) 2015 Elsevier
   B.V. All rights reserved.</abstract><date>NOV 1 2015</date><author>Boccardo, Gianluca
   Augier, Frederic
   Haroun, Yacine
   Ferre, Daniel
   Marchisio, Daniele L.</author></paper><paper><title>Targeted-Tracking With Pointing Devices</title><abstract>Targeting and tracking in graphical user interfaces have been widely
   studied, but attempts to model targeted-tracking are few.
   Targeted-tracking is essentially a two component task of tracking
   followed by targeting, where either or both components may dominate
   depending on the levels of difficulty in each component. The
   applicability of an empirical model based on computer mouse use is
   unknown with respect to other devices. In order to confirm the model
   validity for other input devices, experiments were carried out using a
   mouse, a pen mouse, a touch screen, and a graphics tablet. Fourteen
   participants were tested on 48 experimental conditions that included
   four difficulty levels and 12 conditions with varying track width (P),
   track length (D), and target width (W). Movement time, error rate, index
   of performance, and throughput were compared. Repeated-measures ANOVA
   indicated that factors in the targeted-tracking model were significant
   (p &lt; 0.05) and movement time data were a good fit (R-2 &gt; 0.8) to the
   model, confirming the generality of the model. A principal component
   analysis showed that a mouse is relatively superior in terms of both
   movement time and error rate. Thus, the targeted-tracking model is an
   effective way to compare and evaluate input devices.</abstract><date>AUG 2015</date><author>Senanayake, Ransalu
   Goonetilleke, Ravindra S.
   Hoffmann, Errol R.</author></paper><paper><title>Fast Disk Conformal Parameterization of Simply-Connected Open Surfaces</title><abstract>Surface parameterizations have been widely used in computer graphics and
   geometry processing. In particular, as simply-connected open surfaces
   are conformally equivalent to the unit disk, it is desirable to compute
   the disk conformal parameterizations of the surfaces. In this paper, we
   propose a novel algorithm for the conformal parameterization of a
   simply-connected open surface onto the unit disk, which significantly
   speeds up the computation, enhances the conformality and stability, and
   guarantees the bijectivity. The conformality distortions at the inner
   region and on the boundary are corrected by two steps, with the aid of
   an iterative scheme using quasi-conformal theories. Experimental results
   demonstrate the effectiveness of our proposed method.</abstract><date>DEC 2015</date><author>Choi, Pui Tung
   Lui, Lok Ming</author></paper><paper><title>Survey of Physically Based Simulation of Cuts in Deformable Bodies</title><abstract>Virtual cutting of deformable bodies has been an important and active
   research topic in physically based modelling and simulation for more
   than a decade. A particular challenge in virtual cutting is the robust
   and efficient incorporation of cuts into an accurate computational model
   that is used for the simulation of the deformable body. This report
   presents a coherent summary of the state of the art in virtual cutting
   of deformable bodies, focusing on the distinct geometrical and
   topological representations of the deformable body, as well as the
   specific numerical discretizations of the governing equations of motion.
   In particular, we discuss virtual cutting based on tetrahedral,
   hexahedral and polyhedral meshes, in combination with standard,
   polyhedral, composite and extended finite element discretizations. A
   separate section is devoted to meshfree methods. Furthermore, we discuss
   cutting-related research problems such as collision detection and haptic
   rendering in the context of interactive cutting scenarios. The report is
   complemented with an application study to assess the performance of
   virtual cutting simulators.</abstract><date>SEP 2015</date><author>Wu, Jun
   Westermann, Ruediger
   Dick, Christian</author></paper><paper><title>Multimodality Neurological Data Visualization With Multi-VOI-Based DTI
   Fiber Dynamic Integration.</title><abstract>Brain lesions are usually located adjacent to critical spinal
   structures, so it is a challenging task for neurosurgeons to precisely
   plan a surgical procedure without damaging healthy tissues and nerves.
   The advancement of medical imaging technologies produces a large amount
   of neurological data, which are capable of showing a wide variety of
   brain properties. Advanced algorithms of medical data computing and
   visualization are critically helpful in efficiently utilizing the
   acquired data for disease diagnosis and brain function and structure
   exploration, which is helpful for treatment planning. In this paper, we
   describe new algorithms and a software framework for multiple volume of
   interest specified diffusion tensor imaging (DTI) fiber dynamic
   visualization. The displayed results have been integrated with a volume
   rendering pipeline for multimodality neurological data exploration. A
   depth texture indexing algorithm is used to detect DTI fiber tracts in
   graphics process units (GPUs), which makes fibers to be displayed and
   interactively manipulated with brain data acquired from functional
   magnetic resonance imaging, T 1- and T 2-weighted anatomic imaging, and
   angiographic imaging. The developed software platform is built on an
   object-oriented structure, which is transparent and extensible. It
   provides a comprehensive human-computer interface for data exploration
   and information extraction. The GPU-accelerated high-performance
   computing kernels have been implemented to enable our software to
   dynamically visualize neurological data. The developed techniques will
   be useful in computer-aided neurological disease diagnosis, brain
   structure exploration, and general cognitive neuroscience. </abstract><date>2016-Jan</date><author>Zhang, Qi
   Alexander, Murray
   Ryner, Lawrence</author></paper><paper><title>Foreword to the Special Section on the Spring Conference on Computer
   Graphics 2015 (SCCG'2015)</title><abstract></abstract><date>DEC 2015</date><author>Durikovic, Roman
   Santos, Luis Paulo</author></paper><paper><title>Which Images and Features in Graphic Cigarette Warnings Predict Their
   Perceived Effectiveness? Findings from an Online Survey of Residents in
   the UK</title><abstract>Background Many countries are implementing graphic warnings for
   cigarettes. Which graphic features influence their effectiveness remains
   unclear.Purpose To identify features of graphic warnings predicting
   their perceived effectiveness in discouraging smoking.Method Guided by
   the Common-Sense Model of responses to health threats, we
   content-analyzed 42 graphic warnings for attributes of illness risk
   representations and media features (e.g., photographs, metaphors). Using
   data from 15,536 survey participants, we conducted stratified logistic
   regressions testing which attributes predict participant selections of
   warnings as effective.Results Images of diseased body parts predicted
   greater perceived effectiveness; OR = 6.53-12.45 across smoking status
   (smoker, ex-smoker, young non-smoker) groups. Features increasing
   perceived effectiveness included images of dead or sick persons,
   children, and medical technology; focus on cancer; and photographs.
   Attributes decreasing perceived effectiveness included
   infertility/impotence, addictiveness, cigarette chemicals, cosmetic
   appearance, quitting self-efficacy, and metaphors.Conclusions These
   findings on representational and media attributes predicting perceived
   effectiveness can inform strategies for generating graphic warnings.</abstract><date>OCT 2015</date><author>Cameron, Linda D.
   Williams, Brian</author></paper><paper><title>Generalized additive models as an alternative approach to the modelling
   of the tree height-diameter relationship</title><abstract>Generalized additive models were tested using three types of smoothing
   functions as an alternative for modelling the height curve. The models
   were produced for 23 forest stands of Norway spruce (Picea abies [L.]
   Karst.) in the territory of the Training Forest Enterprise Masaryk
   Forest Kitiny. The results show that the best evaluated and recommended
   for practical use at the level of forest stand was the LOESS function
   (locally weighted scatterplot smooting) when using a greater width of
   the bandwidth. Due to the frequent overfitting and the associated
   unrealistic behaviour of the function, smoothing by spline functions
   cannot be recommended for modelling the height curve at the level of
   forest stand. It was validated that the resulting model must be assessed
   not only according to the calculated quality criteria, but also
   depending on the graphic pattern of the model which must ensure that the
   height curve pattern is realistic. The quality of the resulting models
   (with LOESS function) was assessed to be high, mainly due to the very
   precise determination of model heights.</abstract><date>JUN 2015</date><author>Adamec, Z.
   Drapela, K.</author></paper><paper><title>Photochemical Fate of Amicarbazone in Aqueous Media: Laboratory
   Measurement and Simulations</title><abstract>Amicarbazone (AMZ) is an extensively used, broad-spectrum triazolinone
   herbicide. The literature is scarce regarding experimental data on AMZ
   photodegradation, whose fate in natural waters has not yet been
   investigated in detail. By combining laboratory experiments using
   isolated natural organic matter, literature data, and mathematical
   simulations, we investigated the sunlight-driven direct and indirect
   degradation of AMZ. We show that the reaction with hydroxyl radicals
   ([GRAPHICS]) is the main pathway leading to AMZ degradation, with
   measured second-order reaction rate constant (k(AMZ, OH)) equal to
   2.05x10(10) L/mol center dot s. Simulations suggest that amicarbazone
   degradation is favored in shallow water bodies containing low dissolved
   organic carbon (DOC) and bicarbonate/carbonate concentrations, with
   herbicide half-life varying from about less than 1 day to more than 2
   months. These values of t(1/2) are upper limits since the reaction with
   (CDOM)-C-3* was not considered. Finally, the cross-effects of water
   depth/[DOC] are slightly influenced by nitrate/nitrite and
   bicarbonate/carbonate concentrations, depending on the pH range.</abstract><date>AUG 1 2015</date><author>Silva, Marcela Prado
   Mostafa, Simon
   McKay, Garrett
   Rosario-Ortiz, Fernando L.
   Silva Costa Teixeira, Antonio Carlos</author></paper><paper><title>PLASTIC LETTERS: ALPHABET MIXING AND IDEOLOGIES OF PRINT IN UKRAINIAN
   SHOP SIGNS</title><abstract>This article examines the complex intersection of language ideologies
   shaping alphabetic choices in Ukrainian outdoor advertising and shop
   signs, focusing on alphabet mixing through the insertion of Latin
   letters into Cyrillic texts and the juxtaposition of parallel or
   alternating texts using both of these writing systems. Drawing upon
   ethnographic data from work with graphic designers and consumers as well
   as analysis of language use in signs, I argue that while alphabet mixing
   is often characterized as "faddish" or "youth-oriented" these practices
   also reflect Soviet-era ideological stances towards both Latin
   typefaces, seen as "plastic" letters associated with Western capitalism,
   and Cyrillic typefaces, seen as "rigid" forms subject to strong central
   control by the Soviet state. The increasing availability of personal
   computers with word-processing and graphic design software in Ukraine
   has both increased access by individuals to print technology, and
   promoted a new typographic aesthetic through the dissemination of
   Cyrillic fonts based on Latin, not Soviet or pre-Soviet Cyrillic,
   models.</abstract><date>DEC 2015</date><author>Dickinson, Jennifer A.</author></paper><paper><title>Physically-Accurate Fur Reflectance: Modeling, Measurement and Rendering</title><abstract>Rendering photo-realistic animal fur is a long-standing problem in
   computer graphics. Considerable effort has been made on modeling the
   geometric complexity of fur, but the reflectance of fur fibers is not
   well understood. Fur has a distinct diffusive and saturated appearance,
   that is not captured by either the Marschner hair model or the
   Kajiya-Kay model. In this paper, we develop a physically-accurate
   reflectance model for fur fibers. Based on anatomical literature and
   measurements, we develop a double cylinder model for the reflectance of
   a single fur fiber, where an outer cylinder represents the biological
   observation of a cortex covered by multiple cuticle layers, and an inner
   cylinder represents the scattering interior structure known as the
   medulla. Our key contribution is to model medulla scattering
   accurately-in contrast, for human hair, the medulla has minimal width
   and thus negligible contributions to the reflectance. Medulla scattering
   introduces additional reflection and transmission paths, as well as
   diffusive reflectance lobes. We validate our physical model with
   measurements on real fur fibers, and introduce the first database in
   computer graphics of reflectance profiles for nine fur samples. We show
   that our model achieves significantly better fits to the measured data
   than the Marschner hair reflectance model. For efficient rendering, we
   develop a method to precompute 2D medulla scattering profiles and
   analytically approximate our reflectance model with factored lobes. The
   accuracy of the approach is validated by comparing our rendering model
   to full 3D light transport simulations. Our model provides an enriched
   set of controls, where the parameters we fit can be directly used to
   render realistic fur, or serve as a starting point from which artists
   can manually tune parameters for desired appearances.</abstract><date>NOV 2015</date><author>Yan, Ling-Qi
   Tseng, Chi-Wei
   Jensen, Henrik Wann
   Ramamoorthi, Ravi</author></paper><paper><title>Designing Postdisaster Temporary Housing Facilities: Case Study in
   Indonesia</title><abstract>This study reports a case study regarding designs of postdisaster
   temporary housing facilities in Indonesian communities. Frequent natural
   disasters have caused severe damage in Indonesia. Thus, during the
   period from emergency relief to permanent residence, accommodating
   communities in disaster areas is imperative. While investigating current
   temporary housing solutions regarding the design of housing facilities,
   three problems are identified: inappropriate information exploration,
   insufficient information representation, and inconvenient information
   integration. Indonesian communities ultimately are not satisfied with
   the temporary housing, because they experience difficulty in identifying
   resettlement sites, designing housing facilities, and understanding the
   interdependence between the sites and facilities. To resolve these
   problems, the author proposes an approach consisting of geographical,
   building, and graphics information-based mechanisms. After some tests,
   the results show that, compared to conventional methods (i. e.,
   paper-based maps and drawings) with human processes, this approach more
   effectively helps to identify resettlement sites, computerize building
   models, and integrate the sites and models. Overall, this study offers a
   useful reference for similar applications in postdisaster reconstruction
   and management. (C) 2014 American Society of Civil Engineers.</abstract><date>AUG 2015</date><author>Tsai, Ming-Kuan</author></paper><paper><title>Fast point-based method of a computer-generated hologram for a
   triangle-patch model by using a graphics processing unit</title><abstract>The point-based method and fast-Fourier-transform-based method are
   commonly used for calculation methods of computer-generation holograms.
   This paper proposes a novel fast calculation method for a patch model,
   which uses the point-based method. The method provides a calculation
   time that is proportional to the number of patches but not to that of
   the point light sources. This means that the method is suitable for
   calculating a wide area covered by patches quickly. Experiments using a
   graphics processing unit indicated that the proposed method is about 8
   times or more faster than the ordinary point-based method. (C) 2016
   Optical Society of America</abstract><date>JAN 20 2016</date><author>Sugawara, Takuya
   Ogihara, Yuki
   Sakamoto, Yuji</author></paper><paper><title>Flow-induced deformation of closed disclination lines near a spherical
   colloid immersed in a nematic host phase</title><abstract>We present nonequilibrium molecular dynamics simulations of a spherical
   colloidal particle with a chemically homogeneous surface immersed in a
   nematic liquid-crystal host phase. This setup is then placed between
   planar and atomically structured substrate surfaces that serve to fix
   the nematic far-field director . The substrates are separated by a
   sufficiently large distance such that they do not interfere directly
   with the environment of the colloid. Because of a mismatch between and
   the local homeotropic anchoring of molecules of the liquid crystal
   (i.e., mesogens) at the surface of the colloid circular defect (Saturn)
   rings arise if the host is in thermodynamic equilibrium (i.e., in the
   absence of flow). The size of these rings depends on the range of the
   mesogen-colloid interactions which we model via an attractive Yukawa
   potential. As Poiseuille flow is initiated, is deformed. The degree of
   deformation is analysed quantitatively in terms of characteristic
   geometric parameters fitted to suitable projections of . Our results
   suggest that smaller are shifted downstream while approximately
   maintaining their circular shape, whereas larger ones exhibit an elastic
   deformation in addition. We provide a simple geometric argument to
   predict the downstream shift of smaller, circular s in excellent
   agreement with the simulation data over the range of steady-state flows
   considered.[GRAPHICS]</abstract><date>JAN 17 2016</date><author>Stieger, Tillmann
   Pueschel-Schlotthauer, Sergej
   Schoen, Martin
   Mazza, Marco G.</author></paper><paper><title>HyperMix: An Open-Source Tool for Fast Spectral Unmixing on Graphics
   Processing Units</title><abstract>Spectral unmixing has been a popular technique for analyzing remotely
   sensed hyperspectral images. The goal of unmixing is to find a
   collection of pure spectral constituents (called endmembers) that can
   explain each (possibly mixed) pixel of the scene as a combination of
   endmembers, weighted by their coverage fractions in the pixel or
   abundances. Over the last years, many algorithms have been presented to
   address the three main parts of the spectral unmixing chain: 1)
   estimation of the number of endmembers; 2) identification of the
   endmember signatures; and 3) estimation of the per-pixel fractional
   abundances. However, to date, there is no standardized tool that
   integrates these algorithms in a unified framework. In this letter, we
   present HyperMix, an open-source tool for spectral unmixing that
   integrates different approaches for spectral unmixing and allows
   building unmixing chains in graphical fashion, so that the end-user can
   define one or several spectral unmixing chains in fully configurable
   mode. HyperMix provides efficient implementations of most of the
   algorithms used for spectral unmixing, so that the tool automatically
   recognizes if the computer has a graphics processing unit (GPU)
   available and optimizes the execution of these algorithms in the GPU.
   This allows for the execution of spectral unmixing chains on large
   hyperspectral scenes in computationally efficient fashion. The tool is
   available online from http://hypercomphypermix.blogspot.com.es and has
   been validated with real hyperspectral scenes, providing
   state-of-the-art unmixing results.</abstract><date>SEP 2015</date><author>Ignacio Jimenez, Luis
   Plaza, Antonio</author></paper><paper><title>An Improved Method for Predicting Linear B-cell Epitope Using Deep
   Maxout Networks</title><abstract>To establish a relation between an protein amino acid sequence and its
   tendencies to generate antibody response, and to investigate an improved
   in silico method for linear B-cell epitope (LBE) prediction. We present
   a sequence-based LBE predictor developed using deep maxout network (DMN)
   with dropout training techniques. A graphics processing unit (GPU) was
   used to reduce the training time of the model. A 10-fold
   cross-validation test on a large, non-redundant and experimentally
   verified dataset (Lbtope_Fixed_non_redundant) was performed to evaluate
   the performance. DMN-LBE achieved an accuracy of 68.33% and an area
   under the receiver operating characteristic curve (AUC) of 0.743,
   outperforming other prediction methods in the field. A web server,
   DMN-LBE, of the improved prediction model has been provided for public
   free use. We anticipate that DMN-LBE will be beneficial to vaccine
   development, antibody production, disease diagnosis, and therapy.</abstract><date>JUN 2015</date><author>Lian Yao
   Huang Ze Chi
   Ge Meng
   Pan Xian Ming</author></paper><paper><title>Hydrochemical characteristics of groundwater movement and evolution in
   the Xinli deposit of the Sanshandao gold mine using FCM and PCA methods</title><abstract>The objective of this study is to characterize the physicochemical
   properties of groundwater in the Sanshandao gold deposit, analyze the
   laws of motion and evolution of the groundwater system, and provide
   evidence for the design of undersea mining and safety warnings, using
   combined fuzzy c-means (FCM) and principal component analysis (PCA)
   methods. Thirteen physicochemical indicators and two isotopic indicators
   were surveyed from 100 fissure water samples and 13 typical water
   samples from Xinli, a deposit of the Sanshandao gold mine in China.
   First, PCA was used to extract four PCs (with explained variances 48,
   25, 10 and 7 %) and the characteristics of the hydrochemistry indicators
   of each PC were examined. The four PCs are identified as the processes
   of concentration and dilution, sericite rock orientation and
   carbonatization, potash feldspathization, and dissolution of carbon
   dioxide. Next, using the four PCs developed from PCA, 100 fissure water
   samples were grouped into four spatially continuous clusters using the
   FCM cluster method. With the aid of graphic technology, the four fuzzy
   clusters were determined as deep saline water, shallow saline water,
   bedrock brine and mixed wastewater, following careful analysis of the
   hydrochemical characteristics of each cluster combined with a related
   hydrogeological report. Finally, further analysis of the PCA and FCM
   results determined factors constituting the groundwater system,
   including sources of water, flow path, mixing zone, local fissure zone,
   and impact of mining on the groundwater system. This study shows that
   the combination of PCA and FCM is important for understanding
   hydrochemical characteristics of a groundwater system.</abstract><date>JUN 2015</date><author>Peng, Kang
   Li, Xibing
   Wang, Zewei</author></paper><paper><title>Equine Anatomedia: Development, Integration and Evaluation of an
   E-Learning Resource in Applied Veterinary Anatomy</title><abstract>In Egypt, there is a growing movement to encourage veterinary
   student-centered learning using the most up to date educational
   technologies. This paper focuses on a computer-facilitated learning
   program "Equine Anatomedia", which comprises now two modules (head and
   digit) fully integrated with the applied anatomy curriculum at
   Alexandria and Damanhour Universities. The educational design of this
   program allows students and clinicians to explore anatomical concepts,
   principles and procedure guidelines in a manner more suited to their
   individual learning needs than traditional methods. The program
   comprises over 300 high quality images and diagrams, audio and video
   clips as well as animated graphics with colored keys highlighting the
   anatomical features. Staff and student feedback indicates that Equine
   Anatomedia is an effective and engaging learning tool which helps
   students to develop their knowledge in anatomy and to appreciate its
   relevance in clinical situations. In addition, it encourages
   student-staff interaction and is a useful tool in overcoming the
   challenges of limited resources and increasing numbers of students.     
    </abstract><date>2015-12</date><author>El Sharaby, Ashraf A
   Alsafy, Mohamed A. M
   El-Gendy, Samir A.A</author></paper><paper><title>Participatory role of zinc in structural and functional characterization
   of bioremediase: a unique thermostable microbial silica leaching protein</title><abstract>A unique protein, bioremediase (UniProt Knowledgebase Accession No.:
   P86277), isolated from a hot spring bacterium BKH1 (GenBank Accession
   No.: FJ177512), has shown to exhibit silica leaching activity when
   incorporated to prepare bio-concrete material. Matrix-assisted laser
   desorption ionization mass spectrometry analysis suggests that
   bioremediase is 78 % homologous to bovine carbonic anhydrase II though
   it does not exhibit carbonic anhydrase-like activity. Bioinformatics
   study is performed for understanding the various physical and chemical
   parameters of the protein which predicts the involvement of zinc
   encircled by three histidine residues (His94, His96 and His119) at the
   active site of the protein. Isothermal titration calorimetric-based
   thermodynamic study on diethyl pyrocarbonate-modified protein recognizes
   the presence of Zn2+ in the enzyme moiety. Exothermic to endothermic
   transition as observed during titration of the protein with Zn2+
   discloses that there are at least two binding sites for zinc within the
   protein moiety. Addition of Zn2+ regains the activity of EDTA chelated
   bioremediase confirming the presence of extra binding site of Zn2+ in
   the protein moiety. Revival of folding pattern of completely unfolded
   urea-treated protein by Zn2+ explains the participatory role of zinc in
   structural stability of the protein. Restoration of the lambda (max) in
   intrinsic fluorescence emission study of the urea-treated protein by
   Zn2+ similarly confirms the involvement of Zn in the refolding of the
   protein. The utility of bioremediase for silica nanoparticles
   preparation is observed by field emission scanning electron
   microscopy.[GRAPHICS].</abstract><date>JUL 2015</date><author>Chowdhury, Trinath
   Sarkar, Manas
   Chaudhuri, Biswadeep
   Chattopadhyay, Brajadulal
   Halder, Umesh Chandra</author></paper><paper><title>An optimisation-based model for full-body upright reaching movements</title><abstract>An optimal simulation 3D model for full-body upright reaching movements
   was developed using graphic-based modelling tools (SimMechanics) to
   generate an inverse dynamics model of the skeleton and using
   parameterisation methods for a sensory motor controller. The adaptive
   weight coefficient of the cost function based on the final motor task
   error (i.e. distance between end-effector and target at the end of
   movement) was used to correct motor task error and physiological
   measurements (e.g. joint power, centre of mass displacement, etc.). The
   output of the simulation models using various cost functions were
   compared to experimental data from 15 healthy participants performing
   full-body upright reaching movements. The proposed method can reasonably
   predict full-body voluntary movements in terms of final posture, joint
   power, and movement of the centre of mass (COM) using simple algebraic
   calculations of inverse dynamics and forward kinematics instead of the
   complicated integrals of the forward dynamics. We found that the
   combination of several control strategies, i.e. minimising end-effector
   error, total joint power and body COM produced the best fit of the
   full-body reaching task.</abstract><date>JUN 11 2015</date><author>Sha, Daohang
   Thomas, James S.</author></paper><paper><title>Biomechanical comparison of osteosynthesis with poly-L-lactic acid and
   titanium screw in intracapsular condylar fracture fixation: An
   experimental study</title><abstract>Background and Aims: The aim of this study was to compare the
   biomechanical stability of poly-L-lactic acid and titanium screws in the
   fixation of intracapsular condylar fractures, in 10 polyurethane
   hemimandibles. Materials and Methods: Artificial intracapsular fractures
   were created with a steel disk and electronic micromotor. The first
   group was fixed with 15 mm long self-tapping 2.0 mm system titanium
   screws and the second group was fixed with 15 mm long 2.4 mm
   bioresorbable screws. Linear loads of 25, 50, 75, 100 N was applied in
   anteroposterior direction to the hemimandibles and the data were
   transmitted directly from the load cell to a computer that shows
   emergent results of material characteristics under same forces as a
   graphic containing force and displacement. Results: The results show
   that there were no significant differences between the two methods, with
   25 N of loading. (P &gt; 0,05) The difference became significant with a
   higher value of loading. Conclusion: The results suggest that treatment
   with a single resorbable screw is not functionally stable as a single
   titanium screw.</abstract><date>SEP-OCT 2015</date><author>Omezli, M. M.
   Torul, D.
   Polat, M. E.
   Dayi, E.</author></paper><paper><title>Estimation of Noise Level in Complex Textured Images and Monte
   Carlo-Rendered Images</title><abstract>The several noise level estimation algorithms that have been developed
   for use in image processing and computer graphics generally exhibit good
   performance. However, there are certain special types of noisy images
   that such algorithms are not suitable for. It is particularly still a
   challenge to use the algorithms to estimate the noise levels of complex
   textured photographic images because of the inhomogeneity of the
   original scenes. Similarly, it is difficult to apply most conventional
   noise level estimation algorithms to images rendered by the Monte Carlo
   (MC) method owing to the spatial variation of the noise in such images.
   This paper proposes a novel noise level estimation method based on
   histogram modification, and which can be used for more accurate
   estimation of the noise levels in both complex textured images and
   MC-rendered images. The proposed method has good performance, is simple
   to implement, and can be efficiently used in various image-based and
   graphic applications ranging from smartphone camera noise removal to
   game background rendition.</abstract><date>JAN 31 2016</date><author>Kim, I-Gil</author></paper><paper><title>Specific rehabilitation exercise for the treatment of patients with
   chronic low back pain</title><abstract>[Purpose] To evaluate the efficacy of our special rehabilitation method
   for patients with low back pain (LBP). [Subjects and Methods] All
   participants (n=33) received at least five individual 30-minute therapy
   sessions per week using the INFINITY method (R) and six group therapy
   sessions per week in a gymnasium and swimming pool, each lasting 30
   minutes and including the INFINITY method (R). The treatment lasted
   between four to seven weeks. Plantar function using a graphic method
   (computer plantography), graphical quantification of postural control
   during static standing (posturography), and pain were measured and
   evaluated before and after rehabilitation therapy. The INFINITY method
   (R) is a special rehabilitation method for patients with musculoskeletal
   problems. The method focuses on stabilization and strengthening of the
   trunk, dorsal and abdominal muscles, including the deep stabilization
   system which is closely linked with diaphragmatic breathing. It teaches
   the central nervous system to control muscles more precisely. [Results]
   Plantar functions, postural control in the upright stance and pain of
   LBP patients were significantly improved by 4-7 weeks of rehabilitation
   treatment with the INFINITY method (R). There were significant
   differences in all measured dependent variables of the patients between
   before and after treatment. [Conclusion] Rehabilitation therapy with the
   INFINITY method (R) positively influences body stabilization and pain in
   patients with problems of the lumbar spine. This method presents a new
   improved approach (with enhanced effect) to rehabilitation therapy for
   LBP patients.</abstract><date>AUG 2015</date><author>Tomanova, Michaela
   Lippert-Gruener, Marcela
   Lhotska, Lenka</author></paper><paper><title>Nonlinear Material Design Using Principal Stretches</title><abstract>The Finite Element Method is widely used for solid deformable object
   simulation in film, computer games, virtual reality and medicine.
   Previous applications of nonlinear solid elasticity employed materials
   from a few standard families such as linear corotational, nonlinear St.
   Venant-Kirchhoff, Neo-Hookean, Ogden or Mooney-Rivlin materials.
   However, the spaces of all nonlinear isotropic and anisotropic materials
   are infinite-dimensional and much broader than these standard materials.
   In this paper, we demonstrate how to intuitively explore the space of
   isotropic and anisotropic nonlinear materials, for design of animations
   in computer graphics and related fields. In order to do so, we first
   formulate the internal elastic forces and tangent stiffness matrices in
   the space of the principal stretches of the material. We then
   demonstrate how to design new isotropic materials by editing a single
   stress-strain curve, using a spline interface. Similarly, anisotropic
   (orthotropic) materials can be designed by editing three curves, one for
   each material direction. We demonstrate that modifying these curves
   using our proposed interface has an intuitive, visual, effect on the
   simulation. Our materials accelerate simulation design and enable visual
   effects that are difficult or impossible to achieve with standard
   nonlinear materials.</abstract><date>AUG 2015</date><author>Xu, Hongyi
   Sin, Funshing
   Zhu, Yufeng
   Barbic, Jernej</author></paper><paper><title>Precise Haptic Device Co-Location for Visuo-Haptic Augmented Reality</title><abstract>Visuo-haptic augmented reality systems enable users to see and touch
   digital information that is embedded in the real world. PHANToM haptic
   devices are often employed to provide haptic feedback. Precise
   co-location of computer-generated graphics and the haptic stylus is
   necessary to provide a realistic user experience. Previous work has
   focused on calibration procedures that compensate the non-linear
   position error caused by inaccuracies in the joint angle sensors. In
   this article we present a more complete procedure that additionally
   compensates for errors in the gimbal sensors and improves position
   calibration. The proposed procedure further includes software-based
   temporal alignment of sensor data and a method for the estimation of a
   reference for position calibration, resulting in increased robustness
   against haptic device initialization and external tracker noise. We
   designed our procedure to require minimal user input to maximize
   usability. We conducted an extensive evaluation with two different
   PHANToMs, two different optical trackers, and a mechanical tracker.
   Compared to state-of-the-art calibration procedures, our approach
   significantly improves the co-location of the haptic stylus. This
   results in higher fidelity visual and haptic augmentations, which are
   crucial for fine-motor tasks in areas such as medical training
   simulators, assembly planning tools, or rapid prototyping applications.</abstract><date>DEC 2015</date><author>Eck, Ulrich
   Pankratz, Frieder
   Sandor, Christian
   Klinker, Gudrun
   Laga, Hamid</author></paper><paper><title>Controlling the Simulation of Cumuliform Clouds Based on Fluid Dynamics</title><abstract>Controlling fluid simulation is one of the important research topics in
   computer graphics. In this paper, we focus on controlling the simulation
   of cumuliform cloud formation. Using a previously proposed method for
   controlling cloud simulation the convergence speed is very slow;
   therefore, it takes a long time before the clouds form the desired
   shapes. We improved the method and accelerated the convergence by
   introducing a new mechanism for controlling the amount of water vapor
   added. We demonstrate the effectiveness of the proposed method by
   several examples.</abstract><date>NOV 2015</date><author>Kawaguchi, Tatsuki
   Dobashi, Yoshinori
   Yamamoto, Tsuyoshi</author></paper><paper><title>GPU accelerated dynamic functional connectivity analysis for functional
   MRI data</title><abstract>Recent advances in multi-core processors and graphics card based
   computational technologies have paved the way for an improved and
   dynamic utilization of parallel computing techniques. Numerous
   applications have been implemented for the acceleration of
   computationally-intensive problems in various computational science
   fields including bioinformatics, in which big data problems are
   prevalent. In neuroimaging, dynamic functional connectivity (DFC)
   analysis is a computationally demanding method used to investigate
   dynamic functional interactions among different brain regions or
   networks identified with functional magnetic resonance imaging (fMRI)
   data. In this study, we implemented and analyzed a parallel DFC
   algorithm based on thread-based and block-based approaches. The
   thread-based approach was designed to parallelize DFC computations and
   was implemented in both Open Multi-Processing (OpenMP) and Compute
   Unified Device Architecture (CUDA) programming platforms. Another
   approach developed in this study to better utilize CUDA architecture is
   the block-based approach, where parallelization involves smaller parts
   of fMRI time-courses obtained by sliding-windows. Experimental results
   showed that the proposed parallel design solutions enabled by the GPUs
   significantly reduce the computation time for DFC analysis. Multicore
   implementation using OpenMP on 8-core processor provides up to 7.7x
   speed-up. GPU implementation using CUDA yielded substantial
   accelerations ranging from 18.5x to 157x speed-up once thread-based and
   block-based approaches were combined in the analysis. Proposed parallel
   programming solutions showed that multi-core processor and
   CUDA-supported GPU implementations accelerated the DFC analyses
   significantly. Developed algorithms make the DFC analyses more practical
   for multi-subject studies with more dynamic analyses. (C) 2015 Elsevier
   Ltd. All rights reserved.</abstract><date>JUL 2015</date><author>Akgun, Devrim
   Sakoglu, Uenal
   Esquivel, Johnny
   Adinoff, Bryon
   Mete, Mutlu</author></paper><paper><title>A digital tutor for learning fashion design</title><abstract>Computer-aided instruction (CAI) is an interactive instructional
   technique that effectively improves the students' understandings.
   However, CAI tool is currently lack of use in teaching and learning
   fashion design. The present research hence proposes a tool functions as
   a digital tutor poviding an intuitive visual aid for learning
   principles. The interfaces are designed based on two viewpoints: fashion
   style and design elements. Style Cognition Space and Style Cluster View
   are designed for visualizing fashion style knowledge, while Comparison
   View of Design Elements Effects and Elementary Attribute Space are
   designed for visualizing knowledge of design elements. Several skills of
   graphic user interface and information visualization are used, such as
   Details on demand, Portals, and Zoom. To obtain evaluations from users,
   three workshops of fashion design were held. Using questionnaires, all
   participants were asked to answer several questions about ease of use,
   effectiveness, and satisfaction. Whether professional lecturers or
   inexperienced novices at fashion, all users rate the system as highly
   satisfactory after use.</abstract><date>NOV 2015</date><author>Cheng, Ching-I
   Liu, Damon Shing-Min
   Lin, Charles Chia-Hsu</author></paper><paper><title>An analytic linear accelerator source model for GPU-based Monte Carlo
   dose calculations</title><abstract>Recently, there has been a lot of research interest in developing fast
   Monte Carlo (MC) dose calculation methods on graphics processing unit
   (GPU) platforms. A good linear accelerator (linac) source model is
   critical for both accuracy and efficiency considerations. In principle,
   an analytical source model should be more preferred for GPU-based MC
   dose engines than a phase-space file-based model, in that data loading
   and CPU-GPU data transfer can be avoided. In this paper, we presented an
   analytical field-independent source model specifically developed for
   GPU-based MC dose calculations, associated with a GPU-friendly sampling
   scheme. A key concept called phase-space-ring (PSR) was proposed. Each
   PSR contained a group of particles that were of the same type, close in
   energy and reside in a narrow ring on the phase-space plane located just
   above the upper jaws. The model parameterized the probability densities
   of particle location, direction and energy for each primary photon PSR,
   scattered photon PSR and electron PSR. Models of one 2D Gaussian
   distribution or multiple Gaussian components were employed to represent
   the particle direction distributions of these PSRs. A method was
   developed to analyze a reference phase-space file and derive
   corresponding model parameters. To efficiently use our model in MC dose
   calculations on GPU, we proposed a GPU-friendly sampling strategy, which
   ensured that the particles sampled and transported simultaneously are of
   the same type and close in energy to alleviate GPU thread divergences.
   To test the accuracy of our model, dose distributions of a set of open
   fields in a water phantom were calculated using our source model and
   compared to those calculated using the reference phase-space files. For
   the high dose gradient regions, the average distance-to-agreement (DTA)
   was within 1 mm and the maximum DTA within 2 mm. For relatively low dose
   gradient regions, the root-mean-square (RMS) dose difference was within
   1.1% and the maximum dose difference within 1.7%. The maximum relative
   difference of output factors was within 0.5%. Over 98.5% passing rate
   was achieved in 3D gamma-index tests with 2%/2 mm criteria in both an
   IMRT prostate patient case and a head-and-neck case. These results
   demonstrated the efficacy of our model in terms of accurately
   representing a reference phase-space file. We have also tested the
   efficiency gain of our source model over our previously developed
   phase-space-let file source model. The overall efficiency of dose
   calculation was found to be improved by similar to 1.3-2.2 times in
   water and patient cases using our analytical model.</abstract><date>OCT 21 2015</date><author>Tian, Zhen
   Li, Yongbao
   Folkerts, Michael
   Shi, Feng
   Jiang, Steve B.
   Jia, Xun</author></paper><paper><title>Patient Knowledge on Malaria Symptoms Is a Key to Promoting Universal
   Access of Patients to Effective Malaria Treatment in Palawan, the
   Philippines</title><abstract>IntroductionPalawan, where health care facilities are still limited, is
   one of the most malaria endemic provinces in the Philippines. Since
   1999, microscopists (community health workers) have been trained in
   malaria diagnosis and feasibility of early diagnosis and treatments have
   been enhanced throughout the province. To accelerate the universal
   access of malaria patients to diagnostic testing in Palawan, positive
   health seeking behavior should be encouraged when malaria infection is
   suspected.MethodsIn this cross-sectional study, structured interviews
   were carried out with residents (N = 218) of 20 remote malaria-endemic
   villages throughout Palawan with a history of suspected malaria from
   January to February in 2012. Structural equation modeling (SEM) was
   conducted to determine factors associated with appropriate treatment,
   which included: (1) socio-demo-graphic characteristics; (2) proximity to
   a health facility; (3) health seeking behavior; (4) knowledge on
   malaria; (5) participation in community awareness-raising
   activities.ResultsThree factors independently associated with
   appropriate treatment were identified by SEM (CMIN = 10.5, df = 11, CFI
   = 1.000, RMSEA = .000): "living near microscopist" (p &lt; 0.001), "not
   living near private pharmacy" (p &lt; 0.01), and "having severe symptoms"
   (p &lt; 0.01). "Severe symptoms" were positively correlated with more
   "knowledge on malaria symptoms" (p &lt; 0.001). This knowledge was
   significantly increased by attending "community awareness-raising
   activities by microscopists" (p &lt; 0.001).ConclusionsIn the
   resource-limited settings, microscopists played a significant role in
   providing appropriate treatment to all participants with severe malaria
   symptoms. However, it was considered that knowledge on malaria symptoms
   made participants more aware of their symptoms, and further progressed
   self-triage. Strengthening this recognition sensitivity and making
   residents aware of nearby microscopists may be the keys to accelerating
   universal access to effective malaria treatment in Palawan.</abstract><date>JUN 16 2015</date><author>Matsumoto-Takahashi, Emilie Louise Akiko
   Tongol-Rivera, Pilarita
   Villacorte, Elena A.
   Angluben, Ray U.
   Jimba, Masamine
   Kano, Shigeyuki</author></paper><paper><title>Position-Based Skinning for Soft Articulated Characters</title><abstract>In this paper, we introduce a two-layered approach addressing the
   problem of creating believable mesh-based skin deformation. For each
   frame, the skin is first deformed with a classic linear blend skinning
   approach, which usually leads to unsightly artefacts such as the
   well-known candy-wrapper effect and volume loss. Then we enforce some
   geometric constraints which displace the positions of the vertices to
   mimic the behaviour of the skin and achieve effects like volume
   preservation and jiggling. We allow the artist to control the amount of
   jiggling and the area of the skin affected by it. The geometric
   constraints are solved using a position-based dynamics (PBDs) schema. We
   employ a graph colouring algorithm for parallelizing the computation of
   the constraints. Being based on PBDs guarantees efficiency and real-time
   performances while enduring robustness and unconditional stability. We
   demonstrate the visual quality and the performance of our approach with
   a variety of skeleton-driven soft body characters.</abstract><date>SEP 2015</date><author>Abu Rumman, Nadine
   Fratarcangeli, Marco</author></paper><paper><title>G-SHOT: GPU accelerated 3D local descriptor for surface matching</title><abstract>Signature of histogram of orientations (SHOT) as a novel 3D object local
   descriptor can achieves a good balance between descriptiveness and
   robustness in surface matching. However, its computation workload is
   much higher than the other 3D local descriptors. This paper investigates
   the development of suitable massively parallel algorithms on the
   graphics processing unit (GPU) for computation of high density and large
   scale 3D object local descriptors through two alternative parallel
   algorithms; one exact, and one approximate. Both algorithms exhibit
   outstanding speedup performance. The exact parallel descriptor comes at
   no cost to the descriptiveness, with a speedup factor of up to 40.70,
   with respect to the serial SHOT on the central processing unit (CPU).
   The approximate version achieves a corresponding speedup factor of up to
   54 with minor degradation in descriptiveness. The proposed algorithms
   are integrated into point cloud library (PCL), a open source project for
   image and point cloud. (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>JUL 2015</date><author>Hu, Linjia
   Nooshabadi, Saeid</author></paper><paper><title>Unifying Color and Texture Transfer for Predictive Appearance
   Manipulation</title><abstract>Recent color transfer methods use local information to learn the
   transformation from a source to an exemplar image, and then transfer
   this appearance change to a target image. These solutions achieve very
   successful results for general mood changes, e.g., changing the
   appearance of an image from sunny to overcast. However, such methods
   have a hard time creating new image content, such as leaves on a bare
   tree. Texture transfer, on the other hand, can synthesize such content
   but tends to destroy image structure. We propose the first algorithm
   that unifies color and texture transfer, outperforming both by
   leveraging their respective strengths. A key novelty in our approach
   resides in teasing apart appearance changes that can be modeled simply
   as changes in color versus those that require new image content to be
   generated. Our method starts with an analysis phase which evaluates the
   success of color transfer by comparing the exemplar with the source.
   This analysis then drives a selective, iterative texture transfer
   algorithm that simultaneously predicts the success of color transfer on
   the target and synthesizes new content where needed. We demonstrate our
   unified algorithm by transferring large temporal changes between
   photographs, such as change of season - e.g., leaves on bare trees or
   piles of snow on a street - and flooding.</abstract><date>JUL 2015</date><author>Okura, Fumio
   Vanhoey, Kenneth
   Bousseau, Adrien
   Efros, Alexei A.
   Drettakis, George</author></paper><paper><title>Solving the initial value problem of discrete geodesics</title><abstract>Computing geodesic paths and distances is a common operation in computer
   graphics and computeraided geometric design. The existing discrete
   geodesic algorithms are mainly designed to solve the boundary value
   problem, i.e., to find the shortest path between two given points. In
   this paper, we focus on the initial value problem, i.e., finding a
   uniquely determined geodesic path from a given point in any direction.
   Since the shortest paths do not provide the unique solution on triangle
   meshes, we solve the initial value problem in an indirect manner: given
   a fixed point and an initial tangent direction on a triangle mesh M, we
   first compute a geodesic curve (gamma) over cap on a piecewise smooth
   surface (M) over cap, which well approximates the input mesh M and can
   be constructed at little cost. Then, we solve a first-order ODE of the
   tangent vector using the fourth-order Runge-Kutta method, and parallel
   transport it along (gamma) over cap. When the geodesic curve reaches the
   boundary of the current patch, its tangent can be directly transported
   to the neighboring patch, thanks to the G(1)-continuity along the common
   boundary of two adjacent patches. Finally, once the geodesic curve
   (gamma) over cap is available, we project it onto the underlying mesh M,
   producing the discrete geodesic path gamma, which is guaranteed to be
   unique on M. It is worth noting that our method is different from the
   conventional methods of directly solving the geodesic equation (i.e., a
   secondorder ODE of the position) on piecewise smooth surfaces, which are
   difficult to implement due to the complicated representation of the
   geodesic equation involving Christoffel symbols. The proposed method,
   based on the first-order ODE of the tangent vector, is intuitive and
   easy for implementation. Our method is particularly useful for computing
   geodesic paths on low-resolution meshes which may have large and/or
   skinny triangles, since the conventional straightest geodesic paths are
   usually far from the ground truth. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>JAN 2016</date><author>Cheng, Peng
   Miao, Chunyan
   Liu, Yong-Jin
   Tu, Changhe
   He, Ying</author></paper><paper><title>Show Me My Health Plans: a study protocol of a randomized trial testing
   a decision support tool for the federal health insurance marketplace in
   Missouri</title><abstract>Background: The implementation of the ACA has improved access to quality
   health insurance, a necessary first step to improving health outcomes.
   However, access must be supplemented by education to help individuals
   make informed choices for plans that meet their individual financial and
   health needs.Methods/Design: Drawing on a model of information
   processing and on prior research, we developed a health insurance
   decision support tool called Show Me My Health Plans. Developed with
   extensive stakeholder input, the current tool (1) simplifies information
   through plain language and graphics in an educational component; (2)
   assesses and reviews knowledge interactively to ensure comprehension of
   key material; (3) incorporates individual and/or family health status to
   personalize out-of-pocket cost estimates; (4) assesses preferences for
   plan features; and (5) helps individuals weigh information appropriate
   to their interests and needs through a summary page with "good fit"
   plans generated from a tailored algorithm. The current study will
   evaluate whether the online decision support tool improves health
   insurance decisions compared to a usual care condition (the healthcare.
   gov marketplace website). The trial will include 362 individuals (181 in
   each group) from rural, suburban, and urban settings within a 90 mile
   radius around St. Louis. Eligibility criteria includes English-speaking
   individuals 18-64 years old who are eligible for the ACA marketplace
   plans. They will be computer randomized to view the intervention or
   usual care condition.Discussion: Presenting individuals with options
   that they can understand tailored to their needs and preferences could
   help improve decision quality. By helping individuals narrow down the
   complexity of health insurance plan options, decision support tools such
   as this one could prepare individuals to better navigate enrollment in a
   plan that meets their individual needs.</abstract><date>FEB 16 2016</date><author>Politi, Mary C.
   Barker, Abigail R.
   Kaphingst, Kimberly A.
   McBride, Timothy
   Shacham, Enbal
   Kebodeaux, Carey S.</author></paper><paper><title>Common-mask guided image reconstruction (c-MGIR) for enhanced 4D
   cone-beam computed tomography</title><abstract>Compared to 3D cone beam computed tomography (3D CBCT), the image
   quality of commercially available four-dimensional (4D) CBCT is severely
   impaired due to the insufficient amount of projection data available for
   each phase. Since the traditional Feldkamp-Davis-Kress (FDK)-based
   algorithm is infeasible for reconstructing high quality 4D CBCT images
   with limited projections, investigators had developed several
   compress-sensing (CS) based algorithms to improve image quality. The aim
   of this study is to develop a novel algorithm which can provide better
   image quality than the FDK and other CS based algorithms with limited
   projections. We named this algorithm 'the common mask guided image
   reconstruction' (c-MGIR).In c-MGIR, the unknown CBCT volume is
   mathematically modeled as a combination of phase-specific motion vectors
   and phase-independent static vectors. The common-mask matrix, which is
   the key concept behind the c-MGIR algorithm, separates the common static
   part across all phase images from the possible moving part in each phase
   image. The moving part and the static part of the volumes were then
   alternatively updated by solving two subminimization problems
   iteratively. As the novel mathematical transformation allows the static
   volume and moving volumes to be updated (during each iteration) with
   global projections and 'well' solved static volume respectively, the
   algorithm was able to reduce the noise and under-sampling artifact (an
   issue faced by other algorithms) to the maximum extent. To evaluate the
   performance of our proposed c-MGIR, we utilized imaging data from both
   numerical phantoms and a lung cancer patient. The qualities of the
   images reconstructed with c-MGIR were compared with (1) standard FDK
   algorithm, (2) conventional total variation (CTV) based algorithm, (3)
   prior image constrained compressed sensing (PICCS) algorithm, and (4)
   motion-map constrained image reconstruction (MCIR) algorithm,
   respectively. To improve the efficiency of the algorithm, the code was
   implemented with a graphic processing unit for parallel processing
   purposes.Root mean square error (RMSE) between the ground truth and
   reconstructed volumes of the numerical phantom were in the descending
   order of FDK, CTV, PICCS, MCIR, and c-MGIR for all phases. Specifically,
   the means and the standard deviations of the RMSE of FDK, CTV, PICCS,
   MCIR and c-MGIR for all phases were 42.64 +/- 6.5%, 3.63 +/- 0.83%,
   1.31% +/- 0.09%, 0.86% +/- 0.11% and 0.52 % +/- 0.02%, respectively. The
   image quality of the patient case also indicated the superiority of
   c-MGIR compared to other algorithms.The results indicated that
   clinically viable 4D CBCT images can be reconstructed while requiring no
   more projection data than a typical clinical 3D CBCT scan. This makes
   c-MGIR a potential online reconstruction algorithm for 4D CBCT, which
   can provide much better image quality than other available algorithms,
   while requiring less dose and potentially less scanning time.</abstract><date>DEC 7 2015</date><author>Park, Justin C.
   Zhang, Hao
   Chen, Yunmei
   Fan, Qiyong
   Li, Jonathan G.
   Liu, Chihray
   Lu, Bo</author></paper><paper><title>Mixed Linear Model Approaches of Association Mapping for Complex Traits
   Based on Omics Variants</title><abstract>Precise prediction for genetic architecture of complex traits is impeded
   by the limited understanding on genetic effects of complex traits,
   especially on gene-by-gene (GxG) and gene-by-environment (GxE)
   interaction. In the past decades, an explosion of high throughput
   technologies enables omics studies at multiple levels (such as genomics,
   transcriptomics, proteomics, and metabolomics). The analyses of large
   omics data, especially two-loci interaction analysis, are very time
   intensive. Integrating the diverse omics data and environmental effects
   in the analyses also remain challenges. We proposed mixed linear model
   approaches using GPU (Graphic Processing Unit) computation to
   simultaneously dissect various genetic effects. Analyses can be
   performed for estimating genetic main effects, GxG epistasis effects,
   and GxE environment interaction effects on large-scale omics data for
   complex traits, and for estimating heritability of specific genetic
   effects. Both mouse data analyses and Monte Carlo simulations
   demonstrated that genetic effects and environment interaction effects
   could be unbiasedly estimated with high statistical power by using the
   proposed approaches.</abstract><date>JUL 30 2015</date><author>Zhang, Fu-Tao
   Zhu, Zhi-Hong
   Tong, Xiao-Ran
   Zhu, Zhi-Xiang
   Qi, Ting
   Zhu, Jun</author></paper><paper><title>Stochastic simulation and graphic visualization of mitotic processes
   (vol 51, pg 251, 2010)</title><abstract></abstract><date>JAN 1 2016</date><author>Gardner, Melissa K.
   Odde, David J.</author></paper><paper><title>PrinCCes: Continuity-based geometric decomposition and systematic
   visualization of the void repertoire of proteins</title><abstract>Grooves and pockets on the surface, channels through the protein, the
   chambers or cavities, and the tunnels connecting the internal points to
   each other or to the external fluid environment are fundamental
   determinants of a wide range of biological functions. PrinCCes (Protein
   internal Channel 82 Cavity estimation) is a computer program supporting
   the visualization of voids. It includes a novel algorithm for the
   decomposition of the entire void volume of the protein or protein
   complex to individual entities. The decomposition is based on
   continuity. An individual void is defined by uninterrupted extension in
   space: a spherical probe can freely move between any two internal
   locations of a continuous void. Continuous voids are detected
   irrespective of their topological complexity, they may contain any
   number of holes and bifurcations. The voids of a protein can be
   visualized one by one or in combinations as triangulated surfaces. The
   output is automatically exported to free VMD (Visual Molecular Dynamics)
   or Chimera software, allowing the 3D rotation of the surfaces and the
   production of publication quality images. PrinCCes with graphic user
   interface and command line versions are available for MS Windows and
   Linux. (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>NOV 2015</date><author>Czirjak, Gabor</author></paper><paper><title>Contour Gradient Tree for Automatic Extraction of Salient Object
   Surfaces from 3D Imaging Data</title><abstract>Isosurface extraction is one of the most popular techniques for
   visualizing scalar volume data. However, volume data contains infinitely
   many isosurfaces. Furthermore, a single isosurface might contain many
   connected components, or contours, with each representing a different
   object surface. Hence, it is often a tedious and time-consuming manual
   process to find and extract contours that are interesting to users. This
   paper describes a novel method for automatically extracting salient
   contours from volume data. For this purpose, we propose a contour
   gradient tree (CGT) that contains the information of salient contours
   and their saliency magnitude. We organize the CGT in a hierarchical way
   to generate a sequence of contours in saliency order. Our method was
   applied to various medical datasets. Experimental results show that our
   method can automatically extract salient contours that represent regions
   of interest in the data.</abstract><date>NOV 2015</date><author>Sohn, Bong-Soo</author></paper><paper><title>Improved Half Vector Space Light Transport</title><abstract>In this paper, we present improvements to half vector space light
   transport (HSLT) [KHD14], which make this approach more practical,
   robust for difficult input geometry, and faster. Our first contribution
   is the computation of half vector space ray differentials in a different
   domain than the original work. This enables a more uniform
   stratification over the image plane during Markov chain exploration.
   Furthermore, we introduce a new multi chain perturbation in half vector
   space, which, if combined appropriately with half vector perturbation,
   makes the mutation strategy both more robust to geometric configurations
   with fine displacements and faster due to reduced number of ray casts.
   We provide and analyze the results of improved HSLT and discuss possible
   applications of our new half vector ray differentials.</abstract><date>JUL 2015</date><author>Hanika, Johannes
   Kaplanyan, Anton
   Dachsbacher, Carsten</author></paper><paper><title>GPU/CPU parallel computation of material damage</title><abstract>In this paper compute unified device architecture programming and open
   multiprocessing are used for the graphics processing unit and central
   processing unit parallel computation of material damage. The material
   damage is evaluated by a multilevel finite element analysis within
   material domains reconstructed from a high-resolution micro-focus X-ray
   computed tomography system. An effective computational method is
   investigated for solving the linear equations of finite element
   analysis. Numerical results show an encouraging trend in reducing the
   computation cost for the digital diagnosis of material damage.</abstract><date>JUL 2015</date><author>Shen, Jie
   Vela, Diego
   Singh, Ankita
   Song, Kexing
   Zhang, Guoshang
   LaFreniere, Bradon
   Chen, Hao</author></paper><paper><title>Consistent Scene Editing by Progressive Difference Images</title><abstract>Even though much research was dedicated to the acceleration of
   consistent, progressive light transport simulations, the computation of
   fully converged images is still very time-consuming. This is
   problematic, as for the practical use in production pipelines, the rapid
   editing of lighting effects is important. While previous approaches
   restart the simulation with every scene manipulation, we make use of the
   coherence between frames before and after a modification in order to
   accelerate convergence of the context that remained similar. This is
   especially beneficial if a scene is edited that has already been
   converging for a long time, because much of the previous result can be
   reused, e.g., sharp caustics cast or received by the unedited scene
   parts. In its essence, our method performs the scene modification
   stochastically by predicting and accounting for the difference image. In
   addition, we employ two heuristics to handle cases in which stochastic
   removal is likely to lead to strong noise. Typical scene interactions
   can be broken down into object adding and removal, material
   substitution, camera movement and light editing, which we all examine in
   a number of test scenes both qualitatively and quantitatively. As we
   focus on caustics, we chose stochastic progressive photon mapping as the
   underlying light transport algorithm. Further, we show preliminary
   results of bidirectional path tracing and vertex connection and merging.</abstract><date>JUL 2015</date><author>Guenther, Tobias
   Grosch, Thorsten</author></paper><paper><title>Anisotropic hyperelastic behavior of soft biological tissues</title><abstract>Constitutive laws are fundamental to the studies of the mechanically
   dominated clinical interventions involving soft biological tissues which
   show a highly anisotropic hyperelastic mechanical properties. The
   purpose of this paper was to develop an improved constitutive law based
   on the Holzapfel-Gasser-Ogden's model: to replace the isotropic part
   with Gent constitutive law so as to model the noncollagenous matrix of
   the media due to its generality and capability to reproduce the
   Neo-Hookean model. This model is implemented into an in-house finite
   element program. A uniaxial tension test is considered to study the
   influence of material parameter[GRAPHICS]in Gent model
   and[GRAPHICS]which represents the angle between the collagen fibers and
   the circumferential direction. A simulation of an adventitial strip
   specimen under tension is performed to show the applicability of this
   constitutive law.</abstract><date>OCT 3 2015</date><author>Chen, Z. -W.
   Joli, P.
   Feng, Z. -Q.</author></paper><paper><title>Evaluating the Accuracy of Size Perception on Screen-Based Displays:
   Displayed Objects Appear Smaller Than Real Objects</title><abstract>Accurate perception of the size of objects in computer-generated imagery
   is important for a growing number of applications that rely on absolute
   scale, such as medical visualization and architecture. Addressing this
   problem requires both the development of effective evaluation methods
   and an understanding of what visual information might contribute to
   differences between virtual displays and the real world. In the current
   study, we use 2 affordance judgments-perceived graspability of an object
   or reaching through an aperture-to compare size perception in
   high-fidelity graphical models presented on a large screen display to
   the real world. Our goals were to establish the use of perceived
   affordances within spaces near to the observer for evaluating computer
   graphics and to assess whether the graphical displays were perceived
   similarly to the real world. We varied the nature of the affordance task
   and whether or not the display enabled stereo presentation. We found
   that judgments of grasping and reaching through can be made effectively
   with screen-based displays. The affordance judgments revealed that sizes
   were perceived as smaller than in the real world. However, this
   difference was reduced when stereo viewing was enabled or when the
   virtual display was viewed before the real world.</abstract><date>SEP 2015</date><author>Stefanucci, Jeanine K.
   Creem-Regehr, Sarah H.
   Thompson, William B.
   Lessard, David A.
   Geuss, Michael N.</author></paper><paper><title>Modeling Human Serum Albumin Tertiary Structure To Teach Upper-Division
   Chemistry Students Bioinformatics and Homology Modeling Basics</title><abstract>A homology modeling laboratory experiment has been developed for an
   introductory molecular modeling course for upper-division undergraduate
   chemistry students. With this experiment, students gain practical
   experience in homology model preparation and assessment as well as in
   protein visualization using the educational version of PyMOL
   state-of-the-art molecular graphics. Students create a human serum
   albumin homology model with relatively high resolution at 1.77 angstrom
   (heavy atom model) and with reasonable values for bond lengths, angles,
   and dihedrals. The suggested tasks integrate different fundamental
   aspects of structural biology and protein modeling. Ramachandran plots
   and side chain rotamers are discussed. The assignments are shown to be
   good not only to teach homology modeling basics, but also to introduce
   several other concepts of structural bioinformatics such as protein
   sequence data mining, basic local alignment search tool (BLAST), and
   multiple sequence alignment. Homology modeling is demonstrated to be a
   cornerstone in molecular docking and drug design projects if the crystal
   structure of the protein of interest is not yet determined.</abstract><date>JUL 2015</date><author>Petrovic, Dusan
   Zlatovic, Mario</author></paper><paper><title>Fast Wavefront Propagation (FWP) for Computing Exact Geodesic Distances
   on Meshes.</title><abstract>Computing geodesic distances on triangle meshes is a fundamental problem
   in computational geometry and computer graphics. To date, two notable
   classes of algorithms, the Mitchell-Mount-Papadimitriou (MMP) algorithm
   and the Chen-Han (CH) algorithm, have been proposed. Although these
   algorithms can compute exact geodesic distances if numerical computation
   is exact, they are computationally expensive, which diminishes their
   usefulness for large-scale models and/or time-critical applications. In
   this paper, we propose the fast wavefront propagation (FWP) framework
   for improving the performance of both the MMP and CH algorithms. Unlike
   the original algorithms that propagate only a single window (a data
   structure locally encodes geodesic information) at each iteration, our
   method organizes windows with a bucket data structure so that it can
   process a large number of windows simultaneously without compromising
   wavefront quality. Thanks to its macro nature, the FWP method is less
   sensitive to mesh triangulation than the MMP and CH algorithms. We
   evaluate our FWP-based MMP and CH algorithms on a wide range of
   large-scale real-world models. Computational results show that our
   method can improve the speed by a factor of 3-10. </abstract><date>2015-Jul</date><author>Chunxu Xu
   Wang, Tuanfeng Y
   Yong-Jin Liu
   Ligang Liu
   Ying He</author></paper><paper><title>Inflammation Markers and Major Depressive Disorder in Patients With
   Chronic Heart Failure: Results From the Sertraline Against Depression
   and Heart Disease in Chronic Heart Failure Study</title><abstract>BackgroundMajor depressive disorder (MDD) and chronic heart failure
   (CHF) have in common heightening states of inflammation, manifested by
   elevated inflammation markers such as C-reactive protein. This study
   compared inflammatory biomarker profiles in patients with CHF and MDD to
   those without MDD.MethodsThe study recruited patients admitted to
   inpatient care for acute heart failure exacerbations, after psychiatric
   diagnostic interview. Patients with Beck Depression Inventory (BDI)
   scores lower than 10 and with no history of depression served as the
   nondepressed reference group (n = 25). MDD severity was defined as
   follows: mild (BDI 10-15; n = 48), moderate (BDI 16-23; n = 51), and
   severe (BDI &gt;= 24; n = 33). A Bio-Plex assay measured 18 inflammation
   markers. Ordinal logistic models were used to examine the association of
   MDD severity and biomarker levels.ResultsAdjusting for age, sex, statin
   use, body mass index, left ventricular ejection fraction, tobacco use,
   and New York Heart Association class, the MDD overall group variable was
   significantly associated with elevated interleukin (IL)-2 (p = .019),
   IL-4 (p = .020), IL-6 (p = .026), interferon-gamma (p = .010), monocyte
   chemoattractant protein 1 (p = .002), macrophage inflammatory protein 1
   beta (p = .003), and tumor necrosis factor alpha (p = .004). MDD
   severity subgroups had a greater probability of elevated IL-6, IL-8,
   interferon-gamma, monocyte chemoattractant protein 1, macrophage
   inflammatory protein 1 beta, and tumor necrosis factor alpha compared
   with nondepressed group. The nondepressed group had greater probability
   of elevated IL-17 (p &lt; .001) and IL-1 beta (p &lt; .01).ConclusionsMDD in
   patients with CHF was associated with altered inflammation marker levels
   compared with patients with CHF who had no depression. Whether effective
   depression treatment will normalize the altered inflammation marker
   levels requires further study.Trial Registration:
   ClinicalTrials.gov[GRAPHICS].</abstract><date>SEP 2015</date><author>Xiong, Glen L.
   Prybol, Kevin
   Boyle, Stephen H.
   Hall, Russell
   Streilein, Robert D.
   Steffens, David C.
   Krishnan, Ranga
   Rogers, Joseph G.
   O'Connor, Christopher M.
   Jiang, Wei</author></paper><paper><title>UPDATION OF DISTRIBUTION AND NOTES ON THE BROWN THREE TOED SLOTH
   Bradypus variegatus castaneiceps (PILOSA: BRADIPODIDAE) IN HONDURAS</title><abstract>We present 15 new records for the brown three-toed sloth, Bradypus
   variegatus castaneiceps (Pilosa: Bradypodidae) in Honduras. These
   records confirm the existence of this species in the departments of El
   Paraiso and Atlantida, and expand its geographical distribution from the
   watershed of the Rio Segovia (border with Nicaraguan) to along the
   Caribbean coast of Honduras. Additionally, we propose a potential
   spatial distribution of the species based on the relationship between
   records and physio-graphic characteristics using the program MaxEnt. We
   document the presence of a juvenile in April 2013 and include a
   discussion of the common names of three-toed sloths in Honduras. We
   furthermore evaluate the position of this species on the List of Species
   of Special Concern and the qualification of its status as least concern
   (LC) in the IUCN Red List</abstract><date>JUN 2015</date><author>Marineros, Leonel
   Reyes, Hector Orlando Portillo</author></paper><paper><title>Hardware architecture for full analytical Fraunhofer computer-generated
   holograms</title><abstract>Hardware architecture of parallel computation is proposed for generating
   Fraunhofer computer-generated holograms (CGHs). A pipeline-based
   integrated circuit architecture is realized by employing the modified
   Fraunhofer analytical formulism, which is large scale and enables all
   components to be concurrently operated. The architecture of the CGH
   contains five modules to calculate initial parameters of amplitude,
   amplitude compensation, phases, and phase compensation, respectively.
   The precalculator of amplitude is fully adopted considering the
   "reusable design" concept. Each complex operation type (such as square
   arithmetic) is reused only once by means of a multichannel selector. The
   implemented hardware calculates an 800 x 600 pixels hologram in parallel
   using 39,319 logic elements, 21,074 registers, and 12,651 memory bits in
   an Altera field-programmable gate array environment with stable
   operation at 50 MHz. Experimental results demonstrate that the quality
   of the images reconstructed from the hardware-generated hologram can be
   comparable to that of a software implementation. Moreover, the
   calculation speed is approximately 100 times faster than that of a
   personal computer with an Intel i5-3230M 2.6 GHz CPU for a triangular
   object. (C) 2015 Society of Photo-Optical Instrumentation Engineers
   (SPIE)</abstract><date>SEP 2015</date><author>Pang, Zhi-Yong
   Xu, Zong-Xi
   Xiong, Yi
   Chen, Biao
   Dai, Hui-Min
   Jiang, Shao-Ji
   Dong, Jian-Wen</author></paper><paper><title>A Numerical Study on Stratified Shear Layers With Relevance to Oil-Boom
   Failure</title><abstract>Interface dynamics of two-phase flow, with relevance for leakage of oil
   retained by mechanical oil barriers, is studied by means of a
   two-dimensional (2D) lattice-Boltzmann method (LBM) combined with a
   phase-field model for interface capturing. A multirelaxation-time (MRT)
   model of the collision process is used to obtain a numerically stable
   model at high Reynolds number flow. In the phase-field model, the
   interface is given a finite but small thickness, where the fluid
   properties vary continuously across a thin interface layer. Surface
   tension is modeled as a volume force in the transition layer. The
   numerical model is implemented for simulations with the graphic
   processing unit (GPU) of a desktop personal computer. Verification tests
   of the model are presented. The model is then applied to simulate
   gravity currents (GCs) obtained from a lock-exchange configuration,
   using fluid parameters relevant for those of oil and water. Interface
   instability phenomena are observed, and obtained numerical results are
   in good agreement with theory. This work demonstrates that the numerical
   model presented can be used as a numerical tool for studies of
   stratified shear flows with relevance to oil-boom failure.</abstract><date>AUG 2015</date><author>Kristiansen, David
   Faltinsen, Odd M.</author></paper><paper><title>Lead optimization attrition analysis (LOAA): a novel and general
   methodology for medicinal chemistry</title><abstract>Herein, we report a novel and general method, lead optimization
   attrition analysis (LOAA), to benchmark two distinct small-molecule lead
   series using a relatively unbiased, simple technique and commercially
   available software. We illustrate this approach with data collected
   during lead optimization of two independent oncology programs as a case
   study. Easily generated graphics and attrition curves enabled us to
   calibrate progress and support go/no go decisions on each program. We
   believe that this data-driven technique could be used broadly by
   medicinal chemists and management to guide strategic decisions during
   drug discovery.</abstract><date>AUG 2015</date><author>Munson, Mark
   Lieberman, Harvey
   Tserlin, Elina
   Rocnik, Jennifer
   Ge, Jie
   Fitzgerald, Maria
   Patel, Vinod
   Garcia-Echeverria, Carlos</author></paper><paper><title>The effects of static avatars on impression formation across different
   contexts on social networking sites</title><abstract>When making judgments about others, people use whatever social
   information is available in online environments. Such is the case for
   forming impressions of others. One type of such social information is a
   user's avatar. This study examines different types of avatars
   (photographs, cartoon humans, and nonhumans) created for task, social or
   dating/romantic situations to study the effect of avatar type on
   judgments of uncertainty and task-specific attractiveness. Data suggest
   various patterns of uncertainty and attractiveness in these situations.
   Both the graphic from of an avatar and the context of impression
   formation have effects on subsequent impression formation. Judgments of
   uncertainty and attraction were affected by both the graphic from of
   avatar and by the consistency between the context of impression
   formation and the attractiveness cues of the avatar. These findings are
   discussed as are implications for future research. (C) 2015 Elsevier
   Ltd. All rights reserved.</abstract><date>DEC 2015</date><author>Westerman, David
   Tamborini, Ron
   Bowman, Nicholas David</author></paper><paper><title>Illumination-driven Mesh Reduction for Accelerating Light Transport
   Simulations</title><abstract>Progressive light transport simulations aspire a physically-based,
   consistent rendering to obtain visually appealing illumination effects,
   depth and realism. Thereby, the handling of large scenes is a difficult
   problem, as in typical scene subdivision approaches the parallel
   processing requires frequent synchronization due to the bouncing of
   light throughout the scene. In practice, however, only few object parts
   noticeably contribute to the radiance observable in the image, whereas
   large areas play only a minor role. In fact, a mesh simplification of
   the latter can go unnoticed by the human eye. This particular importance
   to the visible radiance in the image calls for an output-sensitive mesh
   reduction that allows to render originally out-of-core scenes on a
   single machine without swapping of memory. Thus, in this paper, we
   present a preprocessing step that reduces the scene size under the
   constraint of radiance preservation with focus on high-frequency effects
   such as caustics. For this, we perform a small number of preliminary
   light transport simulation iterations. Thereby, we identify mesh parts
   that contribute significantly to the visible radiance in the scene, and
   which we thus preserve during mesh reduction.</abstract><date>JUL 2015</date><author>Reich, Andreas
   Guenther, Tobias
   Grosch, Thorsten</author></paper><paper><title>The Role of Packaging Sites in Efficient and Specific Virus Assembly</title><abstract>During the life cycle of many single-stranded RNA viruses, including
   many human pathogens, a protein shell called the capsid spontaneously
   assembles around the viral genome. Understanding the mechanisms by which
   capsid proteins selectively assemble around the viral RNA amidst diverse
   host RNAs is a key question in virology. In one proposed mechanism,
   short sequences (packaging sites) within the genomic RNA promote rapid
   and efficient assembly through specific interactions with the capsid
   proteins. In this work, we develop a coarse-grained particle-based
   computational model for capsid proteins and RNA that represents protein
   RNA interactions arising both from nonspecific electrostatics and from
   specific packaging site interactions. Using Brownian dynamics
   simulations, we explore how the efficiency and specificity of assembly
   depend on solution conditions (which control protein protein and
   nonspecific protein RNA interactions) and the strength and number of
   packaging sites. We identify distinct regions in parameter space in
   which packaging sites lead to highly specific assembly via different
   mechanisms and others in which packaging sites lead to kinetic traps. We
   relate these computational predictions to in vitro assays for
   specificity in which cognate viral RNAs compete against non-cognate RNAs
   for assembly by capsid proteins. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>JUL 31 2015</date><author>Perlmutter, Jason D.
   Hagan, Michael F.</author></paper><paper><title>Fast computation of eikonal and transport equations on graphics
   processing units computer architectures</title><abstract>Eikonal models have been widely used for traveltime computations in the
   field of seismic imaging, but they are often criticized for having low
   accuracy and poor resolution of the output image. Including amplitude
   information can provide higher model resolution and accuracy of the
   images. We have developed a new approach for computing eikonal
   traveltimes and amplitudes, and we implemented it for multicore computer
   processing unit and graphics processing unit computer architectures.
   Traveltimes and amplitudes are computed simultaneously in iterations of
   the 3D velocity model. This is achieved by extending a fast sweeping
   method by computing amplitudes directly after the traveltimes with
   upwind numerical difference schemes. By performing the extra
   computations simultaneously with the traveltimes, the additional cost
   for the amplitude and raypaths is low. We tested our method on synthetic
   3D data sets to compute traveltimes, amplitudes, and raypaths, from
   which the Helmholtz Green's function was assembled. Using a grid of 1243
   nodes, the computations were performed in less than 1 s. The proposed
   method could work as a feasible alternative to full waveform modeling in
   seismic applications, which suffers from demanding computations because
   it requires several order of magnitudes shorter computing times.</abstract><date>SEP-OCT 2015</date><author>Noack, Marcus
   Gillberg, Tor</author></paper><paper><title>A VARIATIONAL APPROACH FOR DETECTING FEATURE LINES ON MESHES</title><abstract>Feature lines are fundamental shape descriptors and have been
   extensively applied to computer graphics, computer-aided design, image
   processing, and non-photorealistic rendering. This paper introduces a
   unified variational framework for detecting generic feature lines on
   polygonal meshes. The classic Mumford-Shah model is extended to
   surfaces. Using G-convergence method and discrete differential geometry,
   we discretize the proposed variational model to sequential coupled
   sparse linear systems. Through quadratic polynomials fitting, we develop
   a method for extracting valleys of functions defined on surfaces. Our
   approach provides flexible and intuitive control over the detecting
   procedure, and is easy to implement. Several measure functions are
   devised for different types of feature lines, and we apply our approach
   to various polygonal meshes ranging from synthetic to measured models.
   The experiments demonstrate both the effectiveness of our algorithms and
   the visual quality of results.</abstract><date>2016</date><author>Tong, Weihua
   Tai, Xuecheng</author></paper><paper><title>Five-dimensional ultrasound system for soft tissue visualization</title><abstract>A five-dimensional ultrasound (US) system is proposed as a real-time
   pipeline involving fusion of 3D B-mode data with the 3D ultrasound
   elastography (USE) data as well as visualization of these fused data and
   a real-time update capability over time for each consecutive scan. 3D
   B-mode data assist in visualizing the anatomy of the target organ, and
   3D elastography data adds strain information.We investigate the
   feasibility of such a system and show that an end-to-end real-time
   system, from acquisition to visualization, can be developed. We present
   a system that consists of (a) a real-time 3D elastography algorithm
   based on a normalized cross-correlation (NCC) computation on a GPU; (b)
   real-time 3D B-mode acquisition and network transfer; (c) scan
   conversion of 3D elastography and B-mode volumes (if acquired by 4D
   wobbler probe); and (d) visualization software that fuses, visualizes,
   and updates 3D B-mode and 3D elastography data in real time.We achieved
   a speed improvement of 4.45-fold for the threaded version of the
   NCC-based 3D USE versus the non-threaded version. The maximum speed was
   79 volumes/s for 3D scan conversion. In a phantom, we validated the
   dimensions of a 2.2-cm-diameter sphere scan-converted to B-mode volume.
   Also, we validated the 5D US system visualization transfer function and
   detected 1- and 2-cm spherical objects (phantom lesion). Finally, we
   applied the system to a phantom consisting of three lesions to delineate
   the lesions from the surrounding background regions of the phantom.A 5D
   US system is achievable with real-time performance. We can distinguish
   between hard and soft areas in a phantom using the transfer functions.</abstract><date>DEC 2015</date><author>Deshmukh, Nishikant P.
   Caban, Jesus J.
   Taylor, Russell H.
   Hager, Gregory D.
   Boctor, Emad M.</author></paper><paper><title>Research on Visualization of Multi-Dimensional Real-Time Traffic Data
   Stream Based on Cloud Computing</title><abstract>Based on efficient continuous parallel query series algorithm supporting
   multi-objective optimization, by using visual graphics technology for
   traffic data streams for efficient real-time graphical visualization, it
   improve human-computer interaction, to realize real-time and visual data
   analysis and to improve efficiency and accuracy of the analysis. This
   paper employs data mining processing and statistical analysis on
   real-time traffic data stream, based on the parameters standards of
   various data mining algorithms, and by using computer graphics and image
   processing technology, converts graphics or images and make them
   displayed on the screen according to the system requirements, in order
   to track, forecast and maintain the operating condition of all traffic
   service systems effectively. (C) 2016 The Authors. Published by Elsevier
   Ltd.</abstract><date>2016</date><author>Jia Chaolong
   Wang Hanning
   Wei Lili</author></paper><paper><title>Guidewire path determination for intravascular applications</title><abstract>Vascular diseases are among the major causes of death in developed
   countries and the treatment of those pathologies may require
   endovascular interventions, in which the physician utilizes guidewires
   and catheters through the vascular system to reach the injured vessel
   region. Several computational studies related to endovascular procedures
   are in constant development. Thus, predicting the guidewire path may be
   of great value for both physicians and researchers. However, attaining
   good accuracy and precision is still an important issue. We propose a
   method to simulate and predict the guidewire and catheter path inside a
   blood vessel based on equilibrium of a new set of forces, which leads,
   iteratively, to the minimum energy configuration. This technique was
   validated with phantoms using a empty set0.33mm stainless steel
   guidewire and compared to other relevant methods in the literature. This
   method presented RMS error 0.30mm and 0.97mm, which represents less than
   2% and 20% of the lumen diameter of the phantom, in 2D and 3D cases,
   respectively. The proposed technique presented better results than other
   methods from the literature, which were included in this work for
   comparison. Moreover, the algorithm presented low variation
   (&lt;inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink") due to the
   variation of the input parameters. Therefore, even for a wide range of
   different parameters configuration, similar results are presented for
   the proposed approach, which is an important feature and makes this
   technique easier to work with. Since this method is based on basic
   physics, it is simple, intuitive, easy to learn and easy to adapt.</abstract><date>APR 25 2016</date><author>Cardoso, Fernando M.
   Furuie, Sergio S.</author></paper><paper><title>Wavelets-based smoothness comparisons for volume data</title><abstract>In this study, the authors describe an objective smoothness assessment
   method for volume data. The metric can predict the extent of the
   difference in smoothness between a reference model, which may not be of
   perfect quality, and a distorted version. The proposed metric is based
   on the wavelet characterisation of Besov function spaces. The comparison
   of Besov norms between two models can resolve the global and local
   differences in smoothness between them. Experimental results from volume
   datasets with smoothing and sharpening operations demonstrate its
   effectiveness. By comparing direct volume rendered images, the
   experimental results show that the proposed smoothness index correlates
   well with human perceived vision. Finally, the metric can help the
   analyse compression distortions when they compare volume data with
   different smoothness.</abstract><date>DEC 2015</date><author>Lee, Mong-Shu
   Ueng, Shyh-Kuang
   Lin, Jhih-Jhong</author></paper><paper><title>ANALYSIS AND RESTITUTION VIA COMPUTER GRAPHICS OF THE ARCHITECTURAL
   SPACE OF THE MAIN STAIRCASE OF THE CASTLE OF VELEZ BLANCO (ALMERIA)</title><abstract>The aim of this project was to analyze and restore the architectural
   space of the main staircase of the castle of Velez using Rendering (a
   process of generating an image from a model by means of computer
   programs) in order to recreate the spatial, light and material qualities
   of the original room. Virtually the only intact structures that remain
   in the building are the walls. Therefore, the main focus of this
   research was the identification and creation of a virtual reconstruction
   of all the architectural elements that initially formed part of the main
   staircase space. The author has carried out a historical study and has
   made a three-dimensional survey of the current state of the remains of
   the staircase along with the pieces of marble from this room that have
   been preserved but disassembled in other areas of the castle.</abstract><date>JUL-SEP 2015</date><author>Motos Diaz, Ismael</author></paper><paper><title>Interoperability of product and manufacturing information using ontology</title><abstract>The communication among different computer-aided design/computer-aided
   manufacturing/analysis systems usually involves huge amount of
   information and heterogeneous data formats. Inefficient communication
   between computer-aided design and computer-aided design/computer-aided
   manufacturing/analysis systems leads to information loss, which will
   lead to design and fabrication errors. Therefore, a bridge is needed to
   be made between the systems so that models can be transported easily and
   efficiently from the computer-aided design system to another system. To
   carry out successful manufacturing operation, not only parametric
   information and machining information but also non-parametric
   information is needed to be transported. The non-parametric information
   are geometric dimensioning and tolerance, notes, and other annotation to
   three-dimensional models which, altogether, are regarded as product and
   manufacturing information. Exchange of product and manufacturing
   information is to be done not only syntactically but also semantically.
   It is necessary to identify ontology and semantics early in the process
   so that the system is able to view, understand, and define the concepts
   relating to parametric and non-parametric information to improve
   interoperability and overcome data sharing problems. Neutral formats
   such as STEP, Initial Graphics Exchange Specifications, or prominent
   interoperability-related research works offer integration of other
   systems with computer-aided design. However, exchange of non-parametric
   information or product and manufacturing information has not been
   considered in these research works. To achieve semantic integration of
   information between computer-aided design and computer-aided
   design/computer-aided manufacturing/analysis systems, this article
   proposes a data integration method for bothparametric information of a
   model and non-parametric information (product and manufacturing
   information)by constructing a neutral platform using OWL ontology, and
   also shows a pilot implementation that verifies interoperability between
   commercial computer-aided design and computer-aided
   design/computer-aided manufacturing/analysis systems.</abstract><date>SEP 2015</date><author>Ahmed, Fahim
   Han, Soonhung</author></paper><paper><title>MoleCollar and Tunnel Heat Map Visualizations for Conveying
   Spatio-Temporo-Chemical Properties Across and Along Protein Voids</title><abstract>Studying the characteristics of proteins and their inner void space,
   including their geometry, physico-chemical properties and dynamics are
   instrumental for evaluating the reactivity of the protein with other
   small molecules. The analysis of long simulations of molecular dynamics
   produces a large number of voids which have to be further explored and
   evaluated. In this paper we propose three new methods: two of them
   convey important properties along the long axis of a selected void
   during molecular dynamics and one provides a comprehensive picture
   across the void. The first two proposed methods use a specific heat map
   to present two types of information: an overview of all detected tunnels
   in the dynamics and their bottleneck width and stability over time, and
   an overview of a specific tunnel in the dynamics showing the bottleneck
   position and changes of the tunnel length over time. These methods help
   to select a small subset of tunnels, which are explored individually and
   in detail. For this stage we propose the third method, which shows in
   one static image the temporal evolvement of the shape of the most
   critical tunnel part, i.e., its bottleneck. This view is enriched with
   abstract depictions of different physicochemical properties of the amino
   acids surrounding the bottleneck. The usefulness of our newly proposed
   methods is demonstrated on a case study and the feedback from the domain
   experts is included. The biochemists confirmed that our novel methods
   help to convey the information about the appearance and properties of
   tunnels in a very intuitive and comprehensible manner.</abstract><date>JUN 2015</date><author>Byska, J.
   Jurcik, A.
   Groeller, M. E.
   Viola, I.
   Kozlikova, B.</author></paper><paper><title>Adaptation of quadtree meshes in the scaled boundary finite element
   method for crack propagation modelling</title><abstract>A crack propagation modelling technique combining the scaled boundary
   finite element method and quadtree meshes is developed. This technique
   automatically satisfies the compatibility requirement between adjacent
   quadtree cells irrespective of the presence of hanging nodes. The
   quadtree structure facilitates efficient data storage and rapid
   computations. Only a single cell is required to accurately model the
   stress field near crack tips. Crack growth is modelled by splitting the
   cells in the mesh into two. The resulting polygons are directly modelled
   by the scaled boundary formulation with minimal changes to the mesh.
   Four numerical examples demonstrate the salient features of the
   technique. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>AUG 2015</date><author>Ooi, E. T.
   Man, H.
   Natarajan, S.
   Song, C.</author></paper><paper><title>Shape Context for soft biometrics in person re-identification and
   database retrieval</title><abstract>We introduce a novel descriptor for the analysis of pedestrians and its
   applications to person re-identification and database retrieval. A Shape
   Context descriptor of the head-torso region of persons' silhouettes is
   shown to have a very good discrimination ability and application to
   re-identification. For database retrieval using human queries, we train
   a map from the Shape Context to interpretable soft biometric quantities
   that can be reasoned about by humans. We show that a good linear
   correlation exists between Shape Context descriptors and soft biometrics
   quantities in the upper human torso and illustrate its application to
   retrieval in databases from human queries. Shape Context to biometrics
   maps are learned from virtual avatars rendered by computer graphics
   engines, to circumvent the need for time-consuming manual labelling of
   data sets. We obtained promising results of Shape Context based person
   re-identification and database retrieval from human compliant
   description of biometric traits, in both synthetic data and real
   imagery. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>DEC 15 2015</date><author>Nambiar, Athira
   Bernardino, Alexandre
   Nascimento, Jacinto</author></paper><paper><title>Osteolytica: An automated image analysis software package that rapidly
   measures cancer-induced osteolytic lesions in in vivo models with
   greater reproducibility compared to other commonly used methods</title><abstract>Methods currently used to analyse osteolytic lesions caused by
   malignancies such as multiple myeloma and metastatic breast cancer vary
   from basic 2-D X-ray analysis to 2-D images of micro-CT datasets
   analysed with non-specialised image software such as ImageJ. However,
   these methods have significant limitations. They do not capture 3-D
   data, they are time-consuming and they often suffer from inter-user
   variability. We therefore sought to develop a rapid and reproducible
   method to analyse 3-D osteolytic lesions in mice with cancer induced
   bone disease. To this end, we have developed Osteolytica, an image
   analysis software method featuring an easy to use, step-by-step
   interface to measure lytic bone lesions. Osteolytica utilises novel
   graphics card acceleration (parallel computing) and 3-D rendering to
   provide rapid reconstruction and analysis of osteolytic lesions. To
   evaluate the use of Osteolytica we analysed tibial micro-CT datasets
   from murine models of cancer-induced bone disease and compared the
   results to those obtained using a standard ImageJ analysis method.
   Firstly, to assess inter-user variability we deployed four independent
   researchers to analyse tibial datasets from the U266-NSG murine model of
   myeloma. Using ImageJ, inter-user variability between the bones was
   substantial (+/- 19.6%), in contrast to using Osteolytica, which
   demonstrated minimal variability (+/- 0.5%). Secondly, tibial datasets
   from U266-bearing NSG mice or BALB/c mice injected with the metastatic
   breast cancer cell line 4T1 were compared to tibial datasets from aged
   and sex-matched non-tumour control mice. Analyses by both Osteolytica
   and ImageJ showed significant increases in bone lesion area in
   tumour-bearing mice compared to control mice. These results confirm that
   Osteolytica performs as well as the current 2-D ImageJ osteolytic lesion
   analysis method. However, Osteolytica is advantageous in that it
   analyses over the entirety of the bone volume (as opposed to selected
   2-D images), it is a more rapid method and it has less user variability.
   (c) 2015 The Authors. Published by Elsevier Inc.</abstract><date>FEB 2016</date><author>Evans, H. R.
   Karmakharm, T.
   Lawson, M. A.
   Walker, R. E.
   Harris, W.
   Fellows, C.
   Huggins, I. D.
   Richmond, P.
   Chantry, A. D.</author></paper><paper><title>Oxygenation measurement by multi-wavelength oxygen-dependent
   phosphorescence and delayed fluorescence: catchment depth and
   application in intact heart</title><abstract>Oxygen delivery and metabolism represent key factors for organ function
   in health and disease. We describe the optical key characteristics of a
   technique to comprehensively measure oxygen tension (PO2) in myocardium,
   using oxygen-dependent quenching of phosphorescence and delayed
   fluorescence of porphyrins, by means of Monte Carlo simulations and ex
   vivo experiments. Oxyphor G2 (microvascular PO2) was excited at 442 nm
   and 632 nm and protoporphyrin IX (mitochondrial PO2) at 510 nm. This
   resulted in catchment depths of 161 (86) mu m, 350 (307) mu m and 262
   (255) mu m respectively, as estimated by Monte Carlo simulations and ex
   vivo experiments (brackets). The feasibility to detect changes in
   oxygenation within separate anatomical compartments is demonstrated in
   rat heart in vivo.[GRAPHICS].</abstract><date>AUG 2015</date><author>Balestra, Gianmarco M.
   Aalders, Maurice C. G.
   Specht, Patricia A. C.
   Ince, Can
   Mik, Egbert G.</author></paper><paper><title>A Survey of Digital Earth</title><abstract>The creation of a digital representation of the Earth and its associated
   data is a complex and difficult task. The incredible size of geospatial
   data and differences between data sets pose challenges related to big
   data, data creation, and data integration. Advances in globe
   representation and visualization have made use of Discrete Global Grid
   Systems (DGGSs) that discretize the globe into a set of cells to which
   data are assigned. DGGSs are well studied and important in the GIS, OGC,
   and Digital Earth communities but have not been well-introduced to the
   computer graphics community. In this paper, we provide an overview of
   DGGSs and their use in digitally representing the Earth, describe
   several current Digital Earth systems and their methods of Earth
   representation, and list a number of applications of Digital Earths with
   related works. Moreover, we discuss the key research areas and related
   papers from computer graphics that are useful for a Digital Earth
   system, such as advanced techniques for geospatial data creation and
   representation. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>DEC 2015</date><author>Mahdavi-Amiri, Ali
   Alderson, Troy
   Samavati, Faramarz</author></paper><paper><title>iUbiq-Lys: prediction of lysine ubiquitination sites in proteins by
   extracting sequence evolution information via a gray system model</title><abstract>As one of the most important posttranslational modifications (PTMs),
   ubiquitination plays an important role in regulating varieties of
   biological processes, such as signal transduction, cell division,
   apoptosis, and immune response. Ubiquitination is also named "lysine
   ubiquitination" because it occurs when an ubiquitin is covalently
   attached to lysine (K) residues of targeting proteins. Given an
   uncharacterized protein sequence that contains many lysine residues,
   which one of them is the ubiquitination site, and which one is of
   non-ubiquitination site? With the avalanche of protein sequences
   generated in the postgenomic age, it is highly desired for both basic
   research and drug development to develop an automated method for rapidly
   and accurately annotating the ubiquitination sites in proteins. In view
   of this, a new predictor called "iUbiq-Lys" was developed based on the
   evolutionary information, gray system model, as well as the general form
   of pseudo-amino acid composition. It was demonstrated via the rigorous
   cross-validations that the new predictor remarkably outperformed all its
   counterparts. As a web-server, iUbiq-Lys is accessible to the public
   at[GRAPHICS]. For the convenience of most experimental scientists, we
   have further provided a protocol of step-by-step guide, by which users
   can easily get their desired results without the need to follow the
   complicated mathematics that were presented in this paper just for the
   integrity of its development process.</abstract><date>AUG 3 2015</date><author>Qiu, Wang-Ren
   Xiao, Xuan
   Lin, Wei-Zhong
   Chou, Kuo-Chen</author></paper><paper><title>A guide to genome-wide association analysis and post-analytic
   interrogation</title><abstract>This tutorial is a learning resource that outlines the basic process and
   provides specific software tools for implementing a complete genome-wide
   association analysis. Approaches to post-analytic visualization and
   interrogation of potentially novel findings are also presented.
   Applications are illustrated using the free and open-source R
   statistical computing and graphics software environment, Bioconductor
   software for bioinformatics and the UCSC Genome Browser. Complete
   genome-wide association data on 1401 individuals across 861,473 typed
   single nucleotide polymorphisms from the PennCATH study of coronary
   artery disease are used for illustration. All data and code, as well as
   additional instructional resources, are publicly available through the
   Open Resources in Statistical Genomics project: . (c) 2015 The Authors.
   Statistics in Medicine Published by John Wiley &amp; Sons Ltd.</abstract><date>DEC 10 2015</date><author>Reed, Eric
   Nunez, Sara
   Kulp, David
   Qian, Jing
   Reilly, Muredach P.
   Foulkes, Andrea S.</author></paper><paper><title>Extracting Microfacet-based BRDF Parameters from Arbitrary Materials
   with Power Iterations</title><abstract>We introduce a novel fitting procedure that takes as input an arbitrary
   material, possibly anisotropic, and automatically converts it to a
   microfacet BRDF. Our algorithm is based on the property that the
   distribution of microfacets may be retrieved by solving an eigenvector
   problem that is built solely from backscattering samples. We show that
   the eigenvector associated to the largest eigenvalue is always the only
   solution to this problem, and compute it using the power iteration
   method. This approach is straightforward to implement, much faster to
   compute, and considerably more robust than solutions based on nonlinear
   optimizations. In addition, we provide simple conversion procedures of
   our fits into both Beckmann and GGX roughness parameters, and discuss
   the advantages of microfacet slope space to make our fits editable. We
   apply our method to measured materials from two large databases that
   include anisotropic materials, and demonstrate the benefits of spatially
   varying roughness on texture mapped geometric models.</abstract><date>JUL 2015</date><author>Dupuy, Jonathan
   Heitz, Eric
   Iehl, Jean-Claude
   Poulin, Pierre
   Ostromoukhov, Victor</author></paper><paper><title>Solitary Langerhans cell histiocytosis of the occipital condyle: a case
   report and review of the literature</title><abstract>Despite the recent advent of various radiographic imaging techniques, it
   is still very difficult to correctly distinguish a pediatric osteolytic
   lesion in the occipital condyle, which makes it further complicated to
   decide on the necessity of and the adequate timing for radical resection
   and craniocervical fusions. To establish a legitimate therapeutic
   strategy for this deep-seated lesion, surgical biopsy is a reasonable
   choice for first-line intervention. The choice of surgical approach
   becomes very important because a sufficient amount of histological
   specimen must be obtained to confirm the diagnosis but, ideally, the
   residual bony structures and the muscular structures should be preserved
   so as not to increase craniocervical instability. In this report, we
   present our experience with a case of solitary Langerhans cell
   histiocytosis (LCH) involving the occipital condyle that was
   successfully treated with minimally invasive surgical biopsy with a far
   lateral condylar approach supported by preoperative 3D computer graphic
   simulation.An 8-year-old girl presented with neck pain. Magnetic
   resonance imaging and computed tomography (CT) revealed an osteolytic
   lesion of the left occipital condyle. At surgery, the patient was placed
   in the prone position. A 3-cm skin incision was made in the posterior
   auricular region, and the sternocleidomastoid and splenius capitis
   muscles were dissected in the middle of the muscle bundle along the
   direction of the muscle fiber. Under a navigation system, we approached
   the occipital condyle through the space between the longissimus capitis
   muscle and the posterior belly of the digastric muscle and lateral to
   the superior oblique muscle, verifying each muscle at each depth of the
   surgical field and, finally, obtained sufficient surgical specimen.
   After the biopsy, her craniocervical instability had not worsened, and
   chemotherapy was performed. Twelve weeks after chemotherapy, her neck
   pain had gradually disappeared along with her torticollis, and CT showed
   remission of the lesion and marked regeneration of the left occipital
   condyle. Within our knowledge, this is the first reported case of LCH
   involving the occipital condyle. Although very rare, our case indicated
   that LCH can be an alternative in the differential diagnosis of
   osteolytic lesions in the craniocervical junction, in which early bone
   regeneration with sufficient cervical stability is expected after
   chemotherapy.In cases of pediatric osteolytic lesions, when they
   initially presented with apparent cervical instability, craniocervical
   fusion may possibly become unnecessary after a series of treatments.
   Thus, the effort to maximally preserve the musculoskeletal structure
   should be made until its histological diagnosis is finally confirmed.</abstract><date>FEB 2016</date><author>Teranishi, Yu
   Shin, Masahiro
   Yoshino, Masanori
   Saito, Nobuhito</author></paper><paper><title>Assessing the performance of four different categories of histological
   criteria in brain tumours grading by means of a computer-aided diagnosis
   image analysis system</title><abstract>Brain tumours are considered one of the most lethal and difficult to
   treat forms of cancer, with unknown aetiology and lack of any realistic
   screening. In this study, we examine, whether the combination of
   descriptive criteria, used by expert histopathologists in assessing
   histologic tissue samples, and quantitative image analysis features may
   improve the diagnostic accuracy of brain tumour grading. Data comprised
   61 cases of brain cancers (astrocytomas, oligodendrogliomas,
   meningiomas) collected from the archives of the University Hospital of
   Patras, Greece. Incorporating physician's descriptive criteria and image
   analysis's quantitative features into a discriminant function, a
   computer-aided diagnosis system was designed for discriminating
   low-grade from high-grade brain tumours. Physician's descriptive
   features, when solely used in the system, proved of high discrimination
   accuracy (93.4%). When verbal descriptive features were combined with
   quantitative image analysis features in the system, discrimination
   accuracy improved to 98.4%. The generalization of the proposed system to
   unseen data converged to an overall prediction accuracy of 86.7% +/-
   5.4%. Considering that histological grading affects treatment selection
   and diagnostic errors may be notable in clinical practice, the
   utilization of the proposed system may safeguard against diagnostic
   misinterpretations in every day clinical practice.</abstract><date>OCT 2015</date><author>Kostopoulos, S.
   Konstandinou, C.
   Sidiropoulos, K.
   Ravazoula, P.
   Kalatzis, I.
   Asvestas, P.
   Cavouras, D.
   Glotsos, D.</author></paper><paper><title>Efficient localization and spectral estimation of an unknown number of
   ocean acoustic sources using a graphics processing unit</title><abstract>This paper develops a matched-field approach to localization and
   spectral estimation of an unknown number of ocean acoustic sources
   employing massively parallel implementation on a graphics processing
   unit (GPU) for real-time efficiency. A Bayesian formulation is developed
   in which the locations and complex spectra of multiple sources and noise
   variances are considered unknown random variables, and the Bayesian
   information criterion is minimized to estimate these parameters, as well
   as the number of sources present. Optimization is carried out using
   simulated annealing and includes steps that attempt to add/delete
   sources to/from the model. Closed-form maximum-likelihood (ML) solutions
   for source spectra and noise variances in terms of the source locations
   allow these parameters to be sampled implicitly, substantially reducing
   the dimensionality of the inversion. Source sampling, addition, and
   deletion are based on joint conditional probability distributions for
   source range and depth, which incorporate the ML spectral estimates.
   Computing these conditionals requires solving a very large number of
   systems of equations, which is carried out in parallel on a GPU,
   improving efficiency by 2 orders of magnitude. Simulated examples
   illustrate localizations and spectral estimation for a large number of
   sources (up to eight), and investigate mitigation of environmental
   mismatch via efficient multiple-frequency inversion. (C) 2015 Acoustical
   Society of America.</abstract><date>NOV 2015</date><author>Dosso, Stan E.
   Dettmer, Jan
   Wilmut, Michael J.</author></paper><paper><title>BilKristal 4.0: A tool for crystal parameters extraction and defect
   quantification</title><abstract>In this paper, we present a revised version of BilKristal 3.0 tool.
   Raycast screenshot functionality is added to provide improved visual
   analysis. We added atomic distance analysis functionality to assess
   crystalline defects. We improved visualization capabilities by adding
   high level cut function definitions. Discovered bugs are fixed and small
   performance optimizations are made.New version program summaryProgram
   title: BilKristal 4.0Catalogue identifier: ADYU_v4_0Program summary URL:
   http://cpc.cs.qub.ac.uk/summaries/ADYU_v4_0.htmlProgram obtainable from:
   CPC Program Library, Queen's University, Belfast, N. IrelandLicensing
   provisions: Standard CPC licence,
   http://cpc.cs.qub.ac.uk/licence/licence.htmlNo. of lines in distributed
   program, including test data, etc.: 370130No. of bytes in distributed
   program, including test data, etc.: 8350850Distribution format:
   tar.gzProgramming language: C, C++, Microsoft.NET Framework 2.0 and
   OpenGL Libraries.Computer: Personal Computers with Windows operating
   system.Operating system: Windows XP or higher.RAM: 20-60
   Megabytes.Classification: 8.External routines: Microsoft.NET Framework
   2.0. For the visualization tool, graphics card driver should also
   support OpenGL.Catalogue identifier of previous version:
   ADYU_v3_0Journal reference of previous version: Comput. Phys. Comm. 187
   (2015) 266Does the new version supersede the previous version?:
   YesNature of problem: Determining the crystal structure parameters of a
   material is a very important issue in crystallography. Knowing the
   crystal structure parameters helps the understanding of the physical
   behavior of material. For complex structures, particularly for materials
   which also contain local symmetry as well as global symmetry, obtaining
   crystal parameters can be very hard.Solution method: The tool extracts
   crystal parameters such as primitive vectors, basis vectors and
   identifies the space group from atomic coordinates of crystal
   structures.Reasons for new version: Additional features, Performance
   optimizations, Minor bug corrections.Summary of revisions:Raycast
   screenshot functionality is added to the visualization tool. A
   raycasting algorithm similar to the one described in [4, 6] is used. The
   algorithm is multi-core parallelized as described in [5].Atomic distance
   analysis functionality is added. Tool can analyze the crystal structure
   and give statistical information of atom to atom distances.In the
   visualization tool, high level cut function support is added. Users can
   define cut functions, not just as cut-planes but with more complex
   functions as well.Automatic primitive vector and basis vector selection
   options are added. This way the system selects the simplest primitive
   and basis vector alternatives and continues without interrupting the
   user.In the visualization tool, an unused log file was being created and
   a redundant configuration file was being used. These issues are
   corrected.Dead-codes are removed to improve clarity.Restrictions:
   Assumptions are explained in [1, 2, 3]. However, none of them can be
   considered as a restriction to the complexity of the problem.Running
   time: The tool was able to process input files with more than a million
   atoms in less than 20 seconds on a PC with an Athlon quad-core CPU at
   3.2 GHz using the default parameter values. (C) 2015 Elsevier B.V. All
   rights reserved.</abstract><date>OCT 2015</date><author>Okuyan, Erkan
   Okuyan, Ceyda</author></paper><paper><title>Quasi-Static Antenna Design Algorithm image licensed by graphic stock A
   scientific approach to designing electrically small antennas</title><abstract>The quasi-static antenna design algorithm is a scientific approach to
   designing electrically small antennas. The radiation resistance is
   computed from the electrostatic dipole potential. The capacitance is
   computed from the electrostatic potential on the enclosed sphere. The
   Q-factor is calculated from the radiation resistance and capacitance.
   The general thick-disk-cap monopole, enclosed by a sphere, is modeled
   with electrostatic multipole basis functions on the disk. A sequence of
   solutions is computed with one to five electrostatic multipole basis
   functions. The sequence of solutions converges in shape and Q-factor.
   Chu's [1] theoretical limit QChu for an antenna enclosed by a sphere is
   used to calculate the Q-factor ratio (Q/QChu). At low frequencies, the
   Q-factor ratio for the thick-disk-cap monopole is 1.825. This is almost
   the same as the spherical-cap monopole Q-factor ratio of 1.75. The
   thick-disk-cap and spherical-cap monopole were modeled with Computer
   Simulation Technology (CST) Microwave Studio. At resonance, the
   thick-disk-cap monopole's Q-factor is 22.5. This is much smaller than
   the spherical-cap monopole's Q-factor of 46.1. Above the resonant
   frequency, the thick-disk-cap monopole had a 37% lower Q-factor.
   Stuart's equivalent circuit is fit to the CST impedance near the
   resonant frequency. The simplest circuit is the dipole eigenmode (a
   series LC and a resistor parallel to the L). The dipole eigenmode
   circuit gives the omega(2) radiation resistance. The CST radiation
   resistance includes a omega 4 term. A capacitor approximation of the
   higher-order eigenmodes explains the omega(4) term in the CST radiation
   resistance. The capacitor is parallel to the dipole eigenmodeThe
   quasi-static antenna design algorithm is a scientific approach to
   designing electrically small antennas. The radiation resistance is
   computed from the electrostatic dipole potential. The capacitance is
   computed from the electrostatic potential on the enclosed sphere. The
   Q-factor is calculated from the radiation resistance and capacitance.
   The general thick-disk-cap monopole, enclosed by a sphere, is modeled
   with electrostatic multipole basis functions on the disk. A sequence of
   solutions is computed with one to five electrostatic multipole basis
   functions. The sequence of solutions converges in shape and Q-factor.
   Chu's [1] theoretical limit QChu for an antenna enclosed by a sphere is
   used to calculate the Q-circuit. Stuart's equivalent circuit is used to
   compute the dc capacitance, inductance, effective height, Q-factor, and
   Q-factor ratio at low frequencies. The circuit model and CST impedance
   are almost identical. The agreement between the CST impedance and the dc
   quasi-static values is reasonable. The circuit model impedance is
   expressed in an easy-to-understand format.The quasi-static antenna
   design algorithm considers only the dipole eigenmode; the
   current-sharing factor between the dipole and higher eigenmodes
   introduces a small error in the radiation resistance and Q-factor.</abstract><date>OCT 2015</date><author>Jones, Thomas O., III</author></paper><paper><title>PRROC: computing and visualizing precision-recall and receiver operating
   characteristic curves in R</title><abstract>Precision-recall (PR) and receiver operating characteristic (ROC) curves
   are valuable measures of classifier performance. Here, we present the
   R-package PRROC, which allows for computing and visualizing both PR and
   ROC curves. In contrast to available R-packages, PRROC allows for
   computing PR and ROC curves and areas under these curves for
   soft-labeled data using a continuous interpolation between the points of
   PR curves. In addition, PRROC provides a generic plot function for
   generating publication-quality graphics of PR and ROC curves.</abstract><date>AUG 1 2015</date><author>Grau, Jan
   Grosse, Ivo
   Keilwagen, Jens</author></paper><paper><title>GPU accelerated solver for nonlinear reaction-diffusion systems.
   Application to the electrophysiology problem</title><abstract>Solving the electric activity of the heart possess a big challenge, not
   only because of the structural complexities inherent to the heart
   tissue, but also because of the complex electric behaviour of the
   cardiac cells. The multi-scale nature of the electrophysiology problem
   makes difficult its numerical solution, requiring temporal and spatial
   resolutions of 0.1 ms and 0.2 mm respectively for accurate simulations,
   leading to models with millions degrees of freedom that need to be
   solved for thousand time steps. Solution of this problem requires the
   use of algorithms with higher level of parallelism in multi-core
   platforms. In this regard the newer programmable graphic processing
   units (GPU) has become a valid alternative due to their tremendous
   computational horsepower. This paper presents results obtained with a
   novel electrophysiology simulation software entirely developed in
   Compute Unified Device Architecture (CUDA). The software implements
   fully explicit and semi-implicit solvers for the monodomain model, using
   operator splitting. Performance is compared with classical multi-core
   MPI based solvers operating on dedicated high-performance computer
   clusters. Results obtained with the GPU based solver show enormous
   potential for this technology with accelerations over 50x for
   three-dimensional problems. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Mena, Andres
   Ferrero, Jose M.
   Rodriguez Matas, Jose F.</author></paper><paper><title>The value of Retrospective and Concurrent Think Aloud in formative
   usability testing of a physician data query tool</title><abstract>Objective: To compare the performance of the Concurrent (CFA) and
   Retrospective (RTA) Think Aloud method and to assess their value in a
   formative usability evaluation of an Intensive Care Registry-physician
   data query tool designed to support ICU quality improvement
   processes.Methods: Sixteen representative intensive care physicians
   participated in the usability evaluation study. Subjects were allocated
   to either the CTA or RTA method by a matched randomized design. Each
   subject performed six usability-testing tasks of varying complexity in
   the query tool in a real-working context. Methods were compared with
   regard to number and type of problems detected. Verbal protocols of CTA
   and RTA were analyzed in depth to assess differences in verbal output.
   Standardized measures were applied to assess thoroughness in usability
   problem detection weighted per problem severity level and method overall
   effectiveness in detecting usability problems with regard to the time
   subjects spent per method.Results: The usability evaluation of the data
   query tool revealed a total of 43 unique usability problems that the
   intensive care physicians encountered. CTA detected unique usability
   problems with regard to graphics/symbols, navigation issues, error
   messages, and the organization of information on the query tool's
   screens. RTA detected unique issues concerning system match with
   subjects' language and applied terminology. The in-depth verbal protocol
   analysis of CTA provided information on intensive care physicians' query
   design strategies. Overall, CTA performed significantly better than RTA
   in detecting usability problems. CTA usability problem detection
   effectiveness was 0.80 vs. 0.62 (p &lt; 0.05) respectively, with an average
   difference of 42% less time spent per subject compared to RTA. In
   addition, CFA was more thorough in detecting usability problems of a
   moderate (0.85 vs. 0.7) and severe nature (0.71 vs. 0.57).Conclusion: In
   this study, the CIA is more effective in usability-problem detection and
   provided clarification of intensive care physician query design
   strategies to inform redesign of the query tool. However, CTA does not
   outperform RTA. The RTA additionally elucidated unique usability
   problems and new user requirements. Based on the results of this study,
   we recommend the use of CFA in formative usability evaluation studies of
   health information technology. However, we recommend further research on
   the application of RTA in usability studies with regard to user
   expertise and experience when focusing on user profile customized
   (re)design. (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>JUN 2015</date><author>Peute, Linda W. P.
   de Keizer, Nicolette F.
   Jaspers, Monique W. M.</author></paper><paper><title>Elements of Style: Learning Perceptual Shape Style Similarity</title><abstract>The human perception of stylistic similarity transcends structure and
   function: for instance, a bed and a dresser may share a common style. An
   algorithmically computed style similarity measure that mimics human
   perception can benefit a range of computer graphics applications.
   Previous work in style analysis focused on shapes within the same class,
   and leveraged structural similarity between these shapes to facilitate
   analysis. In contrast, we introduce the first structure-transcending
   style similarity measure and validate it to be well aligned with human
   perception of stylistic similarity. Our measure is inspired by
   observations about style similarity in art history literature, which
   point to the presence of similarly shaped, salient, geometric elements
   as one of the key indicators of stylistic similarity. We translate these
   observations into an algorithmic measure by first quantifying the
   geometric properties that make humans perceive geometric elements as
   similarly shaped and salient in the context of style, then employing
   this quantification to detect pairs of matching style related elements
   on the analyzed models, and finally collating the element-level
   geometric similarity measurements into an object-level style measure
   consistent with human perception. To achieve this consistency we employ
   crowdsourcing to quantify the different components of our measure; we
   learn the relative perceptual importance of a range of elementary shape
   distances and other parameters used in our measurement from 50K
   responses to cross-structure style similarity queries provided by over
   2500 participants. We train and validate our method on this dataset,
   showing it to successfully predict relative style similarity with near
   90% accuracy based on 10-fold cross-validation.</abstract><date>AUG 2015</date><author>Lun, Zhaoliang
   Kalogerakis, Evangelos
   Sheffer, Alla</author></paper><paper><title>The Real Dynamics of Bieberbach's Example</title><abstract>Bieberbach constructed, in 1933, domains in C-2 which were biholomorphic
   to C-2 but not dense. The existence of such domains was unexpected. The
   special domains Bieberbach considered are basins of attraction of a
   cubic Henon map. This classical method of construction is one of the
   first applications of dynamical systems to complex analysis. In this
   paper, the boundaries of the real sections of Bieberbach's domains will
   be calculated explicitly as the stable manifolds of the saddle points.
   The real filled Julia sets and the real Julia sets of Bieberbach's map
   will also be calculated explicitly and illustrated with computer
   generated graphics. Basic differences between real and the complex
   dynamics will be shown.</abstract><date>OCT 2015</date><author>Hayes, Sandra
   Hundemer, Axel
   Milliken, Evan
   Moulinos, Tasos</author></paper><paper><title>Advanced free-form deformation and Kullback-Lieblier divergence measure
   for digital elevation model registration</title><abstract>Registration is the process of transforming various images of the same
   object, place, etc. to confer one coordinate system, be they multimodal,
   multi-temporal of the same place or images of different places having
   certain common characteristics. Many methods have been tried for image
   registration; however, only a handful of methods may be said to work for
   digital elevation model (DEM) registration. The motivation of the
   present work is to perform a robust and efficient algorithm to perform
   Elevation model image registration. In this paper, we present a new
   approach for DEM registration, particularly for multimodal or
   multi-temporal DEMs. For time efficient, robust and precise registration
   of DEMs, a novel idea has been proposed using rational DMS-spline
   volumes (Xu et al. in J Comput Sci Technol 23(5):862-873, 1996;
   McDonnell and Qin in J Gr Tools 12(3):24-41, 2007) (the term DMS is
   acronym for the three authors, namely Dahmen et al. in Math Comput
   59(199):97-115, 1997). This is based on the usage of arbitrary topology
   lattices (MacCracken and Joy in 23rd annual conference on computer
   graphics and interactive techniques proceedings. ACM-SIGGRAPH, pp
   181-188, 1996; Feng et al. in Vis Comput Int Comput Gr 22(1):28-42,
   2005). Using free-form deformation has lead to the usage of global and
   local transformations for registration of candidate image domain to
   reference image domain. Also, a similarity measure based on
   Kullback-Leibler divergence (KLD) has been used for measuring robustness
   of the method so proposed. The task of registration is achieved by
   minimizing the cost function using rational DMS-spline functions for
   local registration. After experimentations, the results show that the
   registration process, of registration of candidate DEM to reference DEM,
   could be completed successfully. Comparison of similarity measurement
   methods such as mutual information, correlation coefficient and peak
   signal-to-noise ratio with that of KLD-based has been performed.
   Comparative study with existing works suggests that the presented scheme
   is better, when compared with respect to above mentioned parameters.</abstract><date>OCT 2015</date><author>Dawn, Suma
   Saxena, Vikas
   Sharma, Bhu Dev</author></paper><paper><title>Two-level parallelization of a fluid mechanics algorithm exploiting
   hardware heterogeneity</title><abstract>The prospect of wildly heterogeneous computer systems has led to a
   renewed discussion of programming approaches in high-performance
   computing, of which computational fluid dynamics is a major field. The
   challenge consists in harvesting the performance of all available
   hardware components while retaining good programmability. In particular
   the use of graphic cards is an important trend. This is addressed in the
   present paper by devising a hybrid programming model to create a
   heterogeneous data-parallel computation with a single source code. The
   concept is demonstrated for a one-dimensional spectral-element
   discretization of a fluid dynamics problem. To exploit the additional
   hardware available when coupling GPGPU-accelerated processes with excess
   CPU cores, a straight-forward load balancing model for such
   heterogeneous environments is developed. The paper presents a large
   number of run time measurements and demonstrates that the achieved
   performance gains are close to optimal. This provides valuable
   information for the implementation of fluid dynamics codes on modern
   heterogeneous hardware. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>AUG 31 2015</date><author>Huismann, Immo
   Stiller, Joerg
   Froehlich, Jochen</author></paper><paper><title>Use of graphics processing units for automatic synthesis of programs</title><abstract>Genetic programming (GP) is an evolutionary method that allows computers
   to solve problems automatically. However, the computational power
   required for the evaluation of billions of programs imposes a serious
   limitation on the problem size. This work focuses on accelerating GP to
   support the synthesis of large problems. This is done by completely
   exploiting the highly parallel environment of graphics processing units
   (GPUs). Here, we propose a new quantum-inspired linear GP approach that
   implements all the GP steps in the GPU and provides the following: (1)
   significant performance improvements in the GP steps, (2) elimination of
   the overhead of copying the fitness results from the GPU to the CPU, and
   (3) incorporation of a new selection mechanism-to recognize the programs
   with the best evaluations. The proposed approach outperforms the
   previous approach for large-scale synthetic and real-world problems.
   Further, it provides a remarkable speedup over the CPU execution. (C)
   2015 Elsevier Ltd. All rights reserved.</abstract><date>AUG 2015</date><author>da Silva, Cleomar Pereira
   Dias, Douglas Mota
   Bentes, Cristiana
   Cavalcanti Pacheco, Marco Aurelio</author></paper><paper><title>Image Stitching System Based on ORB Feature-Based Technique and
   Compensation Blending</title><abstract>The construction of a high-resolution panoramic image from a sequence of
   input overlapping images of the same scene is called image
   stitching/mosaicing. It is considered as an important, challenging topic
   in computer vision, multimedia, and computer graphics. The quality of
   the mosaic image and the time cost are the two primary parameters for
   measuring the stitching performance. Therefore, the main objective of
   this paper is to introduce a high-quality image stitching system with
   least computation time. First, we compare many different features
   detectors. We test Harris corner detector, SIFT, SURF, FAST,
   GoodFeaturesToTrack, MSER, and ORB techniques to measure the detection
   rate of the corrected keypoints and processing time. Second, we
   manipulate the implementation of different common categories of image
   blending methods to increase the quality of the stitching process. From
   experimental results, we conclude that ORB algorithm is the fastest,
   more accurate, and with higher performance. In addition, Exposure
   Compensation is the highest stitching quality blending method. Finally,
   we have generated an image stitching system based on ORB using Exposure
   Compensation blending method.</abstract><date>SEP 2015</date><author>Adel, Ebtsam
   Elmogy, Mohammed
   Elbakry, Hazem</author></paper><paper><title>Segmentation-based semi-regular remeshing of 3D models using
   curvature-adapted subdivision surface fitting</title><abstract>This paper proposes a novel method of semi-regular remeshing for
   triangulated surfaces to achieve superior triangles lead to advanced
   visualization of 3D model. It is based on mesh segmentation and
   subdivision surface fitting which uses curvature-adapted polygon
   patches. Our contribution lies in building a sophisticated system with
   three stages, i.e., curvature-aware mesh segmentation, submesh surface
   fitting to generate a high-quality semi-regular mesh and finally,
   stitching the segments using an efficient algorithm. Our method uses
   centroidal Voronoi tessellation and Lloyd's relaxation to generate
   curvature-adapted site centers. Geodesic distances from site centers are
   used for labeling segments and indexing corner vertices for each segment
   boundary. Using information of site centers and corner vertices,
   feature-adapted polygonal patches are generated for each segment. These
   patches are then subdivided and optimized using squared distance metric
   to adjust position of the subdivision sampling with segment details and
   prevent oversampling. At last, an efficient stitching algorithm is
   introduced to connect regular submeshes together and build the final
   semi-regular mesh. We have demonstrated the results of our semi-regular
   remeshing algorithm on meshes with different topology and complexity and
   compared them with known methods. Superior triangle quality with higher
   aspect ratio together with acceptable distortion error is achieved
   according to the experimental results.</abstract><date>FEB 2016</date><author>Mansouri, Saeid
   Ebrahimnezhad, Hossein</author></paper><paper><title>Diagnostic ability for glaucomatous optic neuropathy of Humphrey
   perimetry, Octopus perimetry and Cirrus OCT</title><abstract></abstract><date>JUN 2015</date><author>Monsalve, Blanca
   Ferreras, Antonio
   Calvo, Pilar
   Ruiz, Gema
   Monsalve, Juan</author></paper><paper><title>ADMIRE: analysis and visualization of differential methylation in
   genomic regions using the Infinium HumanMethylation450 Assay</title><abstract>Background: DNA methylation at cytosine nucleotides constitutes
   epigenetic gene regulation impacting cellular development and a wide
   range of diseases. Cytosine bases of the DNA are converted to
   5-methylcytosine by the methyltransferase enzyme, acting as a reversible
   regulator of gene expression. Due to its outstanding importance in the
   epigenetic field, a number of lab techniques were developed to
   interrogate DNA methylation on a global range. Besides whole-genome
   bisulfite sequencing, the Infinium HumanMethylation450 Assay represents
   a versatile and cost-effective tool to investigate genome-wide changes
   of methylation patterns.Results: Analysis of DNA Methylation In genomic
   REgions (ADMIRE) is an open source, semi-automatic analysis pipeline and
   visualization tool for Infinium HumanMethylation450 Assays with a
   special focus on ease of use. It features flexible experimental
   settings, quality control, automatic filtering, normalization, multiple
   testing, and differential analyses on arbitrary genomic regions.
   Publication-ready graphics, genome browser tracks, and table outputs
   include summary data and statistics, permitting instant comparison of
   methylation profiles between sample groups and the exploration of
   methylation patterns along the whole genome. ADMIREs statistical
   approach permits simultaneous large-scale analyses of hundreds of assays
   with little impact on algorithm runtimes.Conclusions: The web-based
   version of ADMIRE provides a simple interface to researchers with
   limited programming skills, whereas the offline version is suitable for
   integration into custom pipelines. ADMIRE may be used via our freely
   available web service at https://bioinformatics.mpi-bn.mpg.de without
   any limitations concerning the size of a project. An offline version for
   local execution is available from our website or GitHub
   (https://github.molgen.mpg.de/loosolab/admire).</abstract><date>DEC 1 2015</date><author>Preussner, Jens
   Bayer, Julia
   Kuenne, Carsten
   Looso, Mario</author></paper><paper><title>Evaluation of Low-Contrast Detectability of Iterative Reconstruction
   across Multiple Institutions, CT Scanner Manufacturers, and Radiation
   Exposure Levels</title><abstract>Purpose: To compare image resolution from iterative reconstruction with
   resolution from filtered back projection for lowcontrast objects on
   phantom computed tomographic (CT) images across vendors and exposure
   levels.Materials and Methods: Randomized repeat scans of an American
   College of Radiology CT accreditation phantom (module 2, low contrast)
   were performed for multiple radiation exposures, vendors, and vendor
   iterative reconstruction algorithms. Eleven volunteers were presented
   with 900 images by using a custom-designed graphical user interface to
   perform a task created specifically for this reader study. Results were
   analyzed by using statistical graphics and analysis of variance.Results:
   Across three vendors (blinded as A, B, and C) and across three exposure
   levels, the mean correct classification rate was higher for iterative
   reconstruction than filtered back projection (P &lt; .01): 87.4% iterative
   reconstruction and 81.3% filtered back projection at 20 mGy, 70.3%
   iterative reconstruction and 63.9% filtered back projection at 12 mGy,
   and 61.0% iterative reconstruction and 56.4% filtered back projection at
   7.2 mGy. There was a significant difference in mean correct
   classification rate between vendor B and the other two vendors. Across
   all exposure levels, images obtained by using vendor B's scanner
   outperformed the other vendors, with a mean correct classification rate
   of 74.4%, while the mean correct classification rate for vendors A and C
   was 68.1% and 68.3%, respectively. Across all readers, the mean correct
   classification rate for iterative reconstruction (73.0%) was higher
   compared with the mean correct classification rate for filtered back
   projection (67.0%).Conclusion: The potential exists to reduce radiation
   dose without compromising low-contrast detectability by using iterative
   reconstruction instead of filtered back projection. There is substantial
   variability across vendor reconstruction algorithms. (C) RSNA, 2015</abstract><date>OCT 2015</date><author>Saiprasad, Ganesh
   Filliben, James
   Peskin, Adele
   Siegel, Eliot
   Chen, Joseph
   Trimble, Christopher
   Yang, Zhitong
   Christianson, Olav
   Samei, Ehsan
   Krupinski, Elizabeth
   Dima, Alden</author></paper><paper><title>An integrated environment model for a constructed wetland Hydrodynamics
   and transport processes</title><abstract>Constructed wetlands (CW) have become a popular technology for treating
   urban and agricultural stormwater runoff. In South Florida, Stormwater
   Treatment Areas (STAs) have been built to reduce phosphorus (P)
   concentrations in runoff from agriculture and other sources, including
   Lake Okeechobee discharges, prior to delivery to the Everglades
   Protection Area. The scale of this constructed wetland project is
   unprecedented in terms of size, cost, and scientific challenges.
   Models/tools are needed to provide detailed spatial and temporal
   information to optimize the P removal efficiency and to predict the
   dynamic response of STAs under a variety of management conditions. The
   Lake Okeechobee Environment Model (LOEM) developed for Lake Okeechobee
   has been enhanced to simulate hydrodynamics and transport processes in
   the wetland environment. The flow resistance caused by Submerged Aquatic
   Vegetation (SAV) and Emergent Aquatic Vegetation (EAV) is included in
   the LOEM-CW. The LOEM-CW is calibrated and validated with 6 years of
   measured data (2008-2013) at different locations in STA-314 Cells 3A and
   3B. Through graphic and statistical comparisons, it is shown that the
   model simulated stage, flow velocity, water temperature, and total
   suspended solid (TSS) in the study area reasonably well. The LOEM-CW is
   poised to serve as a powerful tool in wetland management and STA
   operation. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Jin, Kang-Ren
   Ji, Zhen-Gang</author></paper><paper><title>Shape-constrained level set segmentation for hybrid CPU-GPU computers</title><abstract>Due to its intrinsic advantages such as the ability to handle complex
   shapes, the level set method (LSM) has been widely applied to image
   segmentation. Nevertheless, the LSM is computationally expensive. In
   order to improve the performance of the traditional LSM both in terms of
   efficiency and effectiveness, we propose a novel algorithm based on the
   lattice Boltzmann method (LBM). Using local region statistics and prior
   shape, we design an effective and local speed function for the LSM, from
   which we deduce a shape prior based body force for LBM solver. An NVIDIA
   graphics processing units (GPU) is used to accelerate the method. Our
   introduced algorithm has several advantages. First, it is accurate even
   if there are some geometric transformations (rotation angle, scaling
   factor and translation vector) between the object to be segmented and
   the prior shape. Second, it is local and therefore suitable for
   massively parallel architectures. Third, the use of local region
   information allows it to deal with intensity inhomogeneities. Fourth,
   including shape prior allows the method to handle occlusion and noise.
   Fourth, the model is fast. Finally the algorithm can be used without
   shape prior by means of minor modification. Intensive experiments
   demonstrate, objectively and subjectively, the performance of the
   introduced framework. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>FEB 12 2016</date><author>Balla-Arabe, Souleymane
   Gao, Xinbo
   Ginhac, Dominique
   Yang, Fan</author></paper><paper><title>Flow Aligned Surfacing of Curve Networks</title><abstract>We propose a new approach for automatic surfacing of 3D curve networks,
   a long standing computer graphics problem which has garnered new
   attention with the emergence of sketch based modeling systems capable of
   producing such networks. Our approach is motivated by recent studies
   suggesting that artist-designed curve networks consist of descriptive
   curves that convey intrinsic shape properties, and are dominated by
   representative flow lines designed to convey the principal curvature
   lines on the surface. Studies indicate that viewers complete the
   intended surface shape by envisioning a surface whose curvature lines
   smoothly blend these flow-line curves. Following these observations we
   design a surfacing framework that automatically aligns the curvature
   lines of the constructed surface with the representative flow lines and
   smoothly interpolates these representative flow, or curvature directions
   while minimizing undesired curvature variation. Starting with an initial
   triangle mesh of the network, we dynamically adapt the mesh to maximize
   the agreement between the principal curvature direction field on the
   surface and a smooth flow field suggested by the representative
   flow-line curves. Our main technical contribution is a framework for
   curvature-based surface modeling, that facilitates the creation of
   surfaces with prescribed curvature characteristics. We validate our
   method via visual inspection, via comparison to artist created and
   ground truth surfaces, as well as comparison to prior art, and confirm
   that our results are well aligned with the computed flow fields and with
   viewer perception of the input networks.</abstract><date>AUG 2015</date><author>Pan, Hao
   Liu, Yang
   Sheffer, Alla
   Vining, Nicholas
   Li, Chang-Jian
   Wang, Wenping</author></paper><paper><title>Optimizing Deltoid Efficiency with Reverse Shoulder Arthroplasty Using a
   Novel Inset Center of Rotation Glenosphere Design.</title><abstract>INTRODUCTION: Paul Grammont's hemispherical gleno - sphere concept
   medializes the center of rotation (CoR) to the glenoid face to increase
   deltoid abductor moment arms and improve muscle efficiency. Reducing
   glenosphere thickness to less than half its spherical radius further
   medializes the CoR and offers the potential for even greater
   improvements in efficiency. To that end, this study quantifies deltoid
   abduc - tor moment arms for six different rTSA prostheses during
   scapular abduction from 0 degrees to 140 degrees.METHODS: A 3D computer model was
   developed in Uni - graphics to quantify deltoid moment arms during
   scapular abduction for the normal anatomic shoulder, the 36 mm Grammont
   Delta III (Depuy, Inc.), 36 mm BIO-RSA  (Tornier, Inc.), the 32 mm RSP 
   (DJO, Inc.), and the Equi - noxe  rTSA (Exactech, Inc.) with three
   different glenosphere geometries: 38 mm x 21 mm, 46 mm x 25 mm, and the
   novel 46 mm x 21 mm. Each muscle was simulated as three lines from
   origin to insertion as the arm was elevated; positional data was
   exported to Matlab where the abductor moment arms were calculated for
   the anterior, middle, and posterior deltoid from 0 degrees to 140 degrees humeral
   abduction in the scapular plane using a 1.8:1 scapular rhythm.RESULTS:
   The 46 mm x 21 mm glenosphere had the larg - est average abductor moment
   arms and also the largest efficiency for all three heads of the deltoid,
   having a 4.8% to 40.7% increase in the average deltoid efficiency
   relative to all other designs tested. The glenosphere design with the
   next most efficient deltoid was the 36 mm Delta III, which had the next
   most medialized CoR. The two least efficient designs were the BIO-RSA 
   and the DJO RSP  , which had the most lateral CoR.DISCUSSION: These
   results provide new biomechanical insights on the impact of glenosphere
   geometry on deltoid abductor moment arms and demonstrate that subtle
   changes in rTSA prosthesis design can result in dramatic improve -
   ments. Increasing glenosphere diameter while also decreas - ing
   thickness to be less than half its spherical radius may minimize the
   muscle forces required to perform activities of daily living. Clinical
   follow-up is necessary to demonstrate a reduction in complications
   related to joint over-loading and also demonstrate greater increases in
   range of motion for patients with weak musculature.</abstract><date>2015-Dec</date><author>Roche, Christopher P
   Hamilton, Matthew A
   Diep, Phong
   Wright, Thomas W
   Flurin, Pierre-Henr
   Zuckerman, Joseph D
   Routman, Howard D</author></paper><paper><title>A new optimization approach for mass-spring models parameterization</title><abstract>Mass-spring models (MSM) are frequently used to model deformable objects
   for computer graphics applications due to their simplicity and
   computational efficiency. However, the model parameters are not related
   to the constitutive laws of elastic material in an obvious way. The MSM
   parameters computation from a model based on continuum mechanics is a
   possibility to address this problem. Therefore, in this paper we propose
   a new method to derive MSM parameters using a data-driven strategy with
   a new objective function based on the model acceleration so that the MSM
   and the reference model behave similarly. The proposed methodology does
   not depend on reference model, mesh topology or static equilibrium
   configuration. We validate the methodology for deriving MSM systems
   using finite element method (FEM) and MSM itself as reference models.
   The obtained results are compared with related works. We also discuss
   its robustness against different discretizations and material
   properties. (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>SEP 2015</date><author>da Silva, Josildo Pereira
   Giraldi, Gilson A.
   Apolinario, Antonio L., Jr.</author></paper><paper><title>Spins Dynamics in a Dissipative Environment: Hierarchal Equations of
   Motion Approach Using a Graphics Processing Unit (GPU)</title><abstract>A system with many energy states coupled to a harmonic oscillator bath
   is considered. To study quantum non-Markovian system-bath dynamics
   numerically rigorously and nonperturbatively, we developed a computer
   code for the reduced hierarchy equations of motion (HEOM) for a graphics
   processor unit (GPU) that can treat the system as large as 4096 energy
   states. The code employs a Pade spectrum decomposition (PSD) for a
   construction of HEOM and the exponential integrators. Dynamics of a
   quantum spin glass system are studied by calculating the free induction
   decay signal for the cases of 3 x 2 to 3 x 4 triangular lattices with
   antiferromagnetic interactions. We found that spins relax faster at
   lower temperature due to transitions through a quantum coherent state,
   as represented by the offdiagonal elements of the reduced density
   matrix, while it has been known that the spins relax slower due to
   suppression of thermal activation in a classical case. The decay of the
   spins are qualitatively similar regardless of the lattice sizes. The
   pathway of spin relaxation is analyzed under a sudden temperature drop
   condition. The Compute Unified Device Architecture (CUDA) based source
   code used in the present calculations is provided as Supporting
   Information.</abstract><date>AUG 2015</date><author>Tsuchimoto, Masashi
   Tanimura, Yoshitaka</author></paper><paper><title>Symptoms and Symptom Clusters Identified by Adolescents and Young Adults
   With Cancer Using a Symptom Heuristics App</title><abstract>Adolescents and young adults (AYAs) with cancer experience multiple
   distressing symptoms during treatment. Because the typical approach to
   symptom assessment does not easily reflect the symptom experience of
   individuals, alternative approaches to enhancing communication between
   the patient and provider are needed. We developed an iPad-based
   application that uses a heuristic approach to explore AYAs' cancer
   symptom experiences. In this mixed-methods descriptive study, 72 AYAs
   (13-29 years old) with cancer receiving myelosuppressive chemotherapy
   used the Computerized Symptom Capture Tool (C-SCAT) to create images of
   the symptoms and symptom clusters they experienced from a list of 30
   symptoms. They answered open-ended questions within the C-SCAT about the
   causes of their symptoms and symptom clusters. The images generated
   through the C-SCAT and accompanying free-text data were analyzed using
   descriptive, content, and visual analyses. Most participants (n=70)
   reported multiple symptoms (M=8.14). The most frequently reported
   symptoms were nausea (65.3%), feeling drowsy (55.6%), lack of appetite
   (55.6%), and lack of energy (55.6%). Forty-six grouped their symptoms
   into one or more clusters. The most common symptom cluster was
   nausea/eating problems/appetite problems. Nausea was most frequently
   named as the priority symptom in a cluster and as a cause of other
   symptoms. Although common threads were present in the symptoms
   experienced by AYAs, the graphic images revealed unique perspectives and
   a range of complexity of symptom relationships, clusters, and causes.
   Results highlight the need for a tailored approach to symptom management
   based on how the AYA with cancer perceives his or her symptom
   experience. (c) 2015 Wiley Periodicals, Inc.</abstract><date>DEC 2015</date><author>Ameringer, Suzanne
   Erickson, Jeanne M.
   Macpherson, Catherine Fiona
   Stegenga, Kristin
   Linder, Lauri A.</author></paper><paper><title>Homogeneous nucleation and microstructure evolution in million-atom
   molecular dynamics simulation</title><abstract>Homogeneous nucleation from an undercooled iron melt is investigated by
   the statistical sampling of million-atom molecular dynamics (MD)
   simulations performed on a graphics processing unit (GPU). Fifty
   independent instances of isothermal MD calculations with one million
   atoms in a quasi-two-dimensional cell over a nanosecond reveal that the
   nucleation rate and the incubation time of nucleation as functions of
   temperature have characteristic shapes with a nose at the critical
   temperature. This indicates that thermally activated homogeneous
   nucleation occurs spontaneously in MD simulations without any inducing
   factor, whereas most previous studies have employed factors such as
   pressure, surface effect, and continuous cooling to induce nucleation.
   Moreover, further calculations over ten nanoseconds capture the
   microstructure evolution on the order of tens of nanometers from the
   atomistic viewpoint and the grain growth exponent is directly estimated.
   Our novel approach based on the concept of "melting pots in a
   supercomputer" is opening a new phase in computational metallurgy with
   the aid of rapid advances in computational environments.</abstract><date>AUG 27 2015</date><author>Shibuta, Yasushi
   Oguchi, Kanae
   Takaki, Tomohiro
   Ohno, Munekazu</author></paper><paper><title>u-CARE: user-friendly Comprehensive Antibiotic resistance Repository of
   Escherichia coli</title><abstract>Background and aims Despite medical advancements, Escherichia
   coli-associated infections remain a major public health concern and
   although an abundant information about E. coli and its antibiotic
   resistance mechanisms is available, no effective tool exists that
   integrates gene and genomic data in context to drug resistance, thus
   raising a need to develop a repository that facilitates integration and
   assimilation of factors governing drug resistance in E.
   coli.Descriptions User-friendly Comprehensive Antibiotic resistance
   Repository of Escherichia coli (u-CARE) is a manually curated catalogue
   of 52 antibiotics with reported resistance, 107 genes, transcription
   factors and single nucleotide polymorphism (SNPs) involved in multiple
   drug resistance of this pathogen. Each gene page provides detailed
   information about its resistance mechanisms, while antibiotic page
   consists of summary, chemical description and structural descriptors
   with links to external public databases like GO, CDD, DEG, Ecocyc, KEGG,
   Drug Bank, PubChem and UniProt. Moreover, the database integrates this
   reductive information to holistic data such as strain-specific and
   segment-specific pathogenic islands and operons. In addition, the
   database offers rich user interface for the visualisation and retrieval
   of information using various search criteria such as sequence, keyword,
   image and class search.Conclusions u-CARE is aimed to cater to the needs
   of researchers working in the field of antimicrobial drug resistance
   with minimal knowledge of bioinformatics. This database is also intended
   as a guide book to medical practitioners to avoid use of antibiotics
   against which resistance has already been reported in E. coli. The
   database is available from: http://www.e-bioinformatics.net/ucare</abstract><date>AUG 2015</date><author>Saha, Saurav B.
   Uttam, Vishwas
   Verma, Vivek</author></paper><paper><title>Anti-aliasing in image based shadow generation techniques: a
   comprehensive survey</title><abstract>This research provides an overview of popular and widely used techniques
   to overcome aliasing in image based shadow generation techniques.
   Aliasing is a critical drawback of image based techniques in shadow
   generation. Many techniques are proposed to enhance the anti-aliasing.
   We have classified and systemized these techniques. The main goal of
   this paper is to provide researchers with background on a variety of
   techniques to reduce the aliasing so as make it easier for them to
   choose the method best suited to their aims. During categorizing the
   anti-aliasing techniques, well-known techniques to enhance aliasing is
   described detail, along with a discussion of the advantages and
   drawbacks of each. The algorithms are also comprehensively summarized
   and analysed in depth. It is also hoped that our analysis helps
   researchers find solutions to the shortcomings of each technique.</abstract><date>SEP 2015</date><author>Kolivand, Hoshang
   Sunar, Mohd Shahrizal</author></paper><paper><title>From Sensory Signals to Modality-Independent Conceptual Representations:
   A Probabilistic Language of Thought Approach</title><abstract>People learn modality-independent, conceptual representations from
   modality-specific sensory signals. Here, we hypothesize that any system
   that accomplishes this feat will include three components: a
   representational language for characterizing modality-independent
   representations, a set of sensory-specific forward models for mapping
   from modality-independent representations to sensory signals, and an
   inference algorithm for inverting forward models-that is, an algorithm
   for using sensory signals to infer modality-independent representations.
   To evaluate this hypothesis, we instantiate it in the form of a
   computational model that learns object shape representations from visual
   and/or haptic signals. The model uses a probabilistic grammar to
   characterize modality-independent representations of object shape, uses
   a computer graphics toolkit and a human hand simulator to map from
   object representations to visual and haptic features, respectively, and
   uses a Bayesian inference algorithm to infer modality-independent object
   representations from visual and/or haptic signals. Simulation results
   show that the model infers identical object representations when an
   object is viewed, grasped, or both. That is, the model's percepts are
   modality invariant. We also report the results of an experiment in which
   different subjects rated the similarity of pairs of objects in different
   sensory conditions, and show that the model provides a very accurate
   account of subjects' ratings. Conceptually, this research significantly
   contributes to our understanding of modality invariance, an important
   type of perceptual constancy, by demonstrating how modality-independent
   representations can be acquired and used. Methodologically, it provides
   an important contribution to cognitive modeling, particularly an
   emerging probabilistic language-of-thought approach, by showing how
   symbolic and statistical approaches can be combined in order to
   understand aspects of human perception.</abstract><date>NOV 2015</date><author>Erdogan, Goker
   Yildirim, Ilker
   Jacobs, Robert A.</author></paper><paper><title>Harnessing the power of disgust: a randomized trial to reduce
   high-calorie food appeal through implicit priming</title><abstract>Background: In our increasingly obesogenic environment, in which
   high-calorie convenience foods are readily available, food choices can
   drastically affect weight and overall health: Learned food preferences,
   which are developed through repeated pairings with positively and
   negatively valenced stimuli, can contribute to obesity susceptibility if
   positive attitudes toward high-calorie foods are developed. Thus, the
   modification of automatic associations with food may be a viable
   strategy to promote healthier eating behaviors.Objective: In this study,
   we investigated the ability of an implicit priming (IP) intervention to
   alter responses to visual food cues by using an evaluative conditioning
   approach. The main objective was to implicitly (i.e., below conscious
   perception) associate disgust with high-calorie foods with the aim of
   reducing liking of these foods.Design: Participants were randomly
   assigned to active or control IP. In active IP (n = 22), high-calorie
   food images were implicitly primed with negatively valenced images, and
   low-calorie food images were implicitly primed with positively valenced
   images. In control IP (n = 20), all food images were primed with neutral
   images of fixation crosses. Food images were rated on the desire to eat
   immediately before and after EP.Results: A significant main effect of
   calorie (high compared with low; P &lt; 0.001) and a significant
   calorie-by-group (active compared with control) interaction (P = 0.025)
   were observed. Post hoc tests identified a significantly greater
   high-calorie rating decline after active IP than after control IP (P =
   0.036). Furthermore, there was significantly greater change in
   high-calorie ratings than in low-calorie ratings in the active group (P
   = 0.001). Active IP effects extended to high-calorie foods not
   specifically included in the intervention, which suggested an effect
   generalization. Moreover, a greater change in high-calorie ratings than
   in low-calorie ratings persisted 3-5 d after active IP (P &lt; 0.007),
   which suggested lasting effects.Conclusion: This study provides initial
   evidence that IP can be used to alter high-calorie food preferences,
   which could promote healthier eating habits. This trial was registered
   at clinicaltrials.gov as NCT02347527.</abstract><date>AUG 2015</date><author>Legget, Kristina T.
   Cornier, Marc-Andre
   Rojas, Donald C.
   Lawful, Benjamin
   Tregellas, Jason R.</author></paper><paper><title>The Affine Particle-In-Cell Method</title><abstract>Hybrid Lagrangian/Eulerian simulation is commonplace in computer
   graphics for fluids and other materials undergoing large deformation. In
   these methods, particles are used to resolve transport and topological
   change, while a background Eulerian grid is used for computing
   mechanical forces and collision responses. Particlein-Cell (PIC)
   techniques, particularly the Fluid Implicit Particle (FLIP) variants
   have become the norm in computer graphics calculations. While these
   approaches have proven very powerful, they do suffer from some well
   known limitations. The original PIC is stable, but highly dissipative,
   while FLIP, designed to remove this dissipation, is more noisy and at
   times, unstable. We present a novel technique designed to retain the
   stability of the original PIC, without suffering from the noise and
   instability of FLIP. Our primary observation is that the dissipation in
   the original PIC results from a loss of information when transferring
   between grid and particle representations. We prevent this loss of
   information by augmenting each particle with a locally affine, rather
   than locally constant, description of the velocity. We show that this
   not only stably removes the dissipation of PIC, but that it also allows
   for exact conservation of angular momentum across the transfers between
   particles and grid.</abstract><date>AUG 2015</date><author>Jiang, Chenfanfu
   Schroeder, Craig
   Selle, Andrew
   Teran, Joseph
   Stomakhin, Alexey</author></paper><paper><title>DisseminACTION: disseminating science in the information age
   (www.action-euproject.eu: a website for researchers and parents)</title><abstract>www.action-euproject.eu is a website designed at the University of
   Cagliari, by the Department of Surgery, Faculty of Medicine, within the
   project "ACTION-Aggression in Children: unravelling gene-environment
   interplay to inform Treatment and InterventiON strategies", a
   collaborative project which includes twelve international partners,
   funded under the 7th Framework Programme for Research, technological
   Development and Demonstration.Its aim is to properly disseminate
   official news, events, medical discoveries carried out within the
   project, with an intent to connect European researchers and citizens
   with the official source of ACTION's scientific research. One of the
   main problems of the so called "web 2.0" is represented by the growth of
   viral misinformation, which contributes to create rumours and hoaxes
   around scientific threads. In order to avoid this kind of problems,
   www.action-euproject.eu is also designed to directly reach its audience
   even with social networks integration and with newsletters.Informatics
   is the discipline that studies the information processing through
   automated elaborations. The term appears for the first time in 1957, and
   since that time Computer Science has grown, reaching an unthinkable
   evolution, so that the common devices we use in our everyday lives
   (personal computers,notebooks, tablets, smartphones) are more powerful
   than the NASA calculators at the time of moon's landing. This evolution
   leads to privacy and security matters: our devices process everyday an
   important number of sensitive data, and are everyday exposed to the
   risks of computer security.This website has been designed following
   usability guidelines, with a logical sitemap, an easy system of options,
   a clear graphic style, a responsive graphic template and a robust
   Content Management System, in order to ensure the website security and a
   rigid privacy policy.</abstract><date>OCT 2015</date><author>Mauri, Matteo</author></paper><paper><title>Combined use of diffusion tensor tractography and multifused
   contrast-enhanced FIESTA for predicting facial and cochlear nerve
   positions in relation to vestibular schwannoma</title><abstract>OBJECT The authors assessed whether the combined use of diffusion tensor
   tractography (DTT) and contrast-enhanced (CE) fast imaging employing
   steady-state acquisition (FIESTA) could improve the accuracy of
   predicting the courses of the facial and cochlear nerves before
   surgery.METHODS The population was composed of 22 patients with
   vestibular schwannoma in whom both the facial and cochlear nerves could
   be identified during surgery. According to DTT, depicted fibers running
   from the internal auditory canal to the brainstem were judged to
   represent the facial or vestibulocochlear nerve. With regard to imaging,
   the authors investigated multifused CE-FIESTA scans, in which all 3D
   vessel models were shown simultaneously, from various angles. The
   low-intensity areas running along the tumor from brainstem to the
   internal auditory canal were judged to represent the facial or
   vestibulocochlear nerve.RESULTS For all 22 patients, the rate of fibers
   depicted by DTT coinciding with the facial nerve was 13.6% (3/22), and
   that of fibers depicted by DTT coinciding with the cochlear nerve was
   63.6% (14/22). The rate of candidates for nerves predicted by multifused
   CE-FIESTA coinciding with the facial nerve was 59.1% (13/22), and that
   of candidates for nerves predicted by multifused CE-FIESTA coinciding
   with the cochlear nerve was 4.5% (1/22). The rate of candidates for
   nerves predicted by combined DTT and multifused CE-FIESTA coinciding
   with the facial nerve was 63.6% (14/22), and that of candidates for
   nerves predicted by combined DTT and multifused CE-FIESTA coinciding
   with the cochlear nerve was 63.6% (14/22). The rate of candidates
   predicted by DTT coinciding with both facial and cochlear nerves was
   0.0% (0/22), that of candidates predicted by multifused CE-FIESTA
   coinciding with both facial and cochlear nerves was 4.5% (1/22), and
   that of candidates predicted by combined DTT and multifused CE-FIESTA
   coinciding with both the facial and cochlear nerves was 45.5%
   (10/22).CONCLUSIONS By using a combination of DTT and multifused
   CE-FIESTA, the-authors were able to increase the number of vestibular
   schwannoma patients for whom predicted results corresponded with the
   courses of both the facial and cochlear nerves, a result-that has been
   considered difficult to achieve by use of a single modality only.
   Although the 3D image including these prediction results helped with
   comprehension of the 3D operative anatomy, the reliability of prediction
   remains to be established.</abstract><date>DEC 2015</date><author>Yoshino, Masanori
   Kin, Taichi
   Ito, Akihiro
   Saito, Toki
   Nakagawa, Daichi
   Ino, Kenji
   Kamada, Kyousuke
   Mori, Harushi
   Kunimatsu, Akira
   Nakatomi, Hirofumi
   Oyama, Hiroshi
   Saito, Nobuhito</author></paper><paper><title>Origin of the balanomorph barnacles (Crustacea, Cirripedia, Thoracica):
   new evidence from the Late Cretaceous (Campanian) of Sweden</title><abstract>New material of thoracican cirripedes, traditionally assigned to
   Brachylepadomorpha and basal Balanomorpha, is described from abundant
   isolated plates collected from sediment deposited between boulders on a
   rocky coastline of Late Campanian age (c. 80 Ma) at Ivo Klack in Scania,
   southern Sweden. Two new genera, Epibrachylepas Gale gen. nov. and
   Parabrachylepas Gale gen. nov. (type species P. ifoensis Withers, 1935)
   are described, as is a new species, Epibrachylepas newmani Gale sp. nov.
   Pachydiadema cretacea Withers, 1935 and Brachylepas guascoi (Bosquet,
   1857) are redescribed on the basis of extensive new material. It is
   concluded that the long-held homologies between lateral plates of
   pedunculate cirripedes and balanomorphs are incorrect, and a new
   nomenclature is proposed for the latter group. Cladistic analysis based
   on 40 morphological characters of 12 species yields a consensus tree
   showing successive Brachylepas species and Pachydiadema as sister taxa
   to the crown group balanomorphs, which are here called Neobalanomorpha
   Gale suborder nov. Both 'Brachylepadomorpha' and 'Brachylepadidae' are
   paraphyletic, and together with P. cretacea form a morphocline leading
   from pedunculate ancestors (Pycnolepas articulata), through to basal
   sessile forms (B. naissanti, B. guascoi) and on to taxa identified as
   basal balanomorphs (Parabrachylepas, Epibrachylepas, Pachydiadema). The
   functional significance of the progressive changes is discussed with
   reference to living taxa. It is suggested that the radiation of
   Neobalanomorpha, dominant shallow water thoracicans in the Cenozoic,
   postdated the K-Pg near-extinction of more basal sessile barnacle
   groups.[GRAPHICS]</abstract><date>SEP 2 2015</date><author>Gale, Andrew Scott
   Sorensen, Anne Mehlin</author></paper><paper><title>Visualization of multi-property landscapes for compound selection and
   optimization</title><abstract>Compound optimization generally requires considering multiple properties
   in concert and reaching a balance between them. Computationally, this
   process can be supported by multi-objective optimization methods that
   produce numerical solutions to an optimization task. Since a variety of
   comparable multi-property solutions are usually obtained further
   prioritization is required. However, the underlying multi-dimensional
   property spaces are typically complex and difficult to rationalize.
   Herein, an approach is introduced to visualize multi-property landscapes
   by adapting the concepts of star and parallel coordinates from computer
   graphics. The visualization method is designed to complement
   multi-objective compound optimization. We show that visualization makes
   it possible to further distinguish between numerically equivalent
   optimization solutions and helps to select drug-like compounds from
   multi-dimensional property spaces. The methodology is intuitive,
   applicable to a wide range of chemical optimization problems, and made
   freely available to the scientific community.</abstract><date>AUG 2015</date><author>de Leon, Antonio de la Vega
   Kayastha, Shilva
   Dimova, Dilyana
   Schultz, Thomas
   Bajorath, Juergen</author></paper><paper><title>Real-Time Three-Dimensional Cell Segmentation in Large-Scale Microscopy
   Data of Developing Embryos</title><abstract>d We present the Real-time Accurate Cell-shape Extractor (RACE), a
   high-throughput image analysis framework for automated three-dimensional
   cell segmentation in large-scale images. RACE is 55330 times faster and
   2-5 times more accurate than state-of-the-art methods. We demonstrate
   the generality of RACE by extracting cell-shape information from entire
   Drosophila, zebrafish, and mouse embryos imaged with confocal and
   light-sheet microscopes. Using RACE, we automatically reconstructed
   cellular-resolution tissue anisotropy maps across developing Drosophila
   embryos and quantified differences in cell-shape dynamics in wild-type
   and mutant embryos. We furthermore integrated RACE with our framework
   for automated cell lineaging and performed joint segmentation and cell
   tracking in entire Drosophila embryos. RACE processed these
   terabyte-sized datasets on a single computer within 1.4 days. RACE is
   easy to use, as it requires adjustment of only three parameters, takes
   full advantage of state-of-the-art multi-core processors and graphics
   cards, and is available as open-source software for Windows, Linux, and
   Mac OS.</abstract><date>JAN 25 2016</date><author>Stegmaier, Johannes
   Amat, Fernando
   Lemon, William C.
   McDole, Katie
   Wan, Yinan
   Teodoro, George
   Mikut, Ralf
   Keller, Philipp J.</author></paper><paper><title>Euler-Rodrigues formula variations, quaternion conjugation and intrinsic
   connections</title><abstract>This paper reviews the Euler-Rodrigues formula in the axis-angle
   representation of rotations, studies its variations and derivations in
   different mathematical forms as vectors, quaternions and Lie groups and
   investigates their intrinsic connections. The Euler-Rodrigues formula in
   the Taylor series expansion is presented and its use as an exponential
   map of Lie algebras is discussed particularly with a non-normalized
   vector. The connection between Euler-Rodrigues parameters and the
   Euler-Rodrigues formula is then demonstrated through quaternion
   conjugation and the equivalence between quaternion conjugation and an
   adjoint action of the Lie group is subsequently presented. The paper
   provides a rich reference for the Euler-Rodrigues formula, the
   variations and their connections and for their use in rigid body
   kinematics, dynamics and computer graphics. (C) 2015 The Author.
   Published by Elsevier Ltd.</abstract><date>OCT 2015</date><author>Dai, Jian S.</author></paper><paper><title>VisualCNA: a GUI for interactive constraint network analysis and protein
   engineering for improving thermostability</title><abstract>Constraint network analysis (CNA) is a graph theory-based rigidity
   analysis approach for linking a biomolecule's structure, flexibility,
   (thermo) stability and function. Results from CNA are highly
   information-rich and require intuitive, synchronized and interactive
   visualization for a comprehensive analysis. We developed VisualCNA, an
   easy-to-use PyMOL plug-in that allows setup of CNA runs and analysis of
   CNA results linking plots with molecular graphics representations. From
   a practical viewpoint, the most striking feature of VisualCNA is that it
   facilitates interactive protein engineering aimed at improving
   thermostability.</abstract><date>JUL 15 2015</date><author>Rathi, Prakash Chandra
   Mulnaes, Daniel
   Gohlke, Holger</author></paper><paper><title>Clinician-Graded Electronic Facial Paralysis Assessment: The eFACE</title><abstract>Background: The subjective nature of facial aesthetics and the
   difficulties associated with quantifying facial function have made
   outcomes analysis in facial paralysis challenging. Clinicians rely on
   photographs, subjective descriptions, and scales, limiting assessment,
   communication among providers, and communication between providers and
   patients. The authors describe the development and validation of a
   comprehensive, electronic, clinician-graded facial function scale
   (eFACE), which generates an overall disfigurement score and offers
   simple graphic output for clinician communication, assessment of various
   interventions, and patient understanding. The eFACE application may be
   used in a variety of electronic devices, including smartphones, tablets,
   and computers.Methods: An instrument consisting of 16 items in a visual
   analogue scale format was developed to assess facial function and
   symmetry (the eFACE). Video recordings of subjects performing facial
   expressions were viewed, and the eFACE instrument was applied, along
   with an overall facial disfigurement score. A multiple regression
   analysis was performed to determine the best linear relationship between
   overall expert-determined disfigurement and the eFACE items. The
   resulting equation was tested by three independent facial nerve
   clinicians, using an additional series of patients, to determine both
   interrater and intrarater reliability of the instrument.Results:
   Multiple regression analysis produced good fit of eFACE parameters to
   overall expert-rated global facial disfigurement when dynamic parameters
   were weighted twice as heavily as static and synkinesis parameters.
   eFACE scores demonstrated very high interrater and intrarater
   reliability.Conclusion: The eFACE is a reliable, reproducible, and
   straightforward digital clinical measure with which to assess facial
   function and disfigurement in patients with facial paralysis.</abstract><date>AUG 2015</date><author>Banks, Caroline A.
   Bhama, Prabhat K.
   Park, Jong
   Hadlock, Charles R.
   Hadlock, Tessa A.</author></paper><paper><title>Intuitive modeling of vaporish objects</title><abstract>Attempts to model gases in computer graphics started in the late 1970s.
   Since that time, there have been many approaches developed. In this
   paper we present a non-physical method allowing to create vaporish
   objects like clouds or smoky characters. The idea is to create a few
   sketches describing the rough shape of the final vaporish object. These
   sketches will be used as condensation sets of Iterated Function Systems,
   providing intuitive control over the object. The advantages of the new
   method are: simplicity, good control of resulting shapes and ease of
   eventual object animation. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>DEC 2015</date><author>Sokolov, Dmitry
   Gentil, Christian</author></paper><paper><title>Psoralen derivatives as inhibitors of NF- interaction: the critical role
   of the furan ring</title><abstract>Simplified analogues of previously reported NF- interaction inhibitors,
   lacking the furan moiety, were synthesized and evaluated by performing
   experiments based on electrophoretic mobility shift assay (EMSA). The
   synthetic modifications led to simpler coumarin derivatives with lower
   activity allowing to better understand the minimal structural
   requirement for the binding to NF-kappa B.[GRAPHICS]</abstract><date>AUG 2015</date><author>Marzaro, Giovanni
   Lampronti, Ilaria
   Borgatti, Monica
   Manzini, Paolo
   Gambari, Roberto
   Chilin, Adriana</author></paper><paper><title>Nonlinear electronic excitations in crystalline solids using
   meta-generalized gradient approximation and hybrid functional in
   time-dependent density functional theory</title><abstract>We develop methods to calculate electron dynamics in crystalline solids
   in real-time time-dependent density functional theory employing
   exchange-correlation potentials which reproduce band gap energies of
   dielectrics; a meta-generalized gradient approximation was proposed by
   Tran and Blaha [Phys. Rev. Lett. 102, 226401 (2009)] (TBm-BJ) and a
   hybrid functional was proposed by Heyd, Scuseria, and Ernzerhof [J.
   Chem. Phys. 118, 8207 (2003)] (HSE). In time evolution calculations
   employing the TB-mBJ potential, we have found it necessary to adopt the
   predictor-corrector step for a stable time evolution. We have developed
   a method to evaluate electronic excitation energy without referring to
   the energy functional which is unknown for the TB-mBJ potential. For the
   HSE functional, we have developed a method for the operation of the
   Fock-like term in Fourier space to facilitate efficient use of massive
   parallel computers equipped with graphic processing units. We compare
   electronic excitations in silicon and germanium induced by femtosecond
   laser pulses using the TB-mBJ, HSE, and a simple local density
   approximation (LDA). At low laser intensities, electronic excitations
   are found to be sensitive to the band gap energy: they are close to each
   other using TB-mBJ and HSE and are much smaller in LDA. At high laser
   intensities close to the damage threshold, electronic excitation
   energies do not differ much among the three cases. (C) 2015 AIP
   Publishing LLC.</abstract><date>DEC 14 2015</date><author>Sato, Shunsuke A.
   Taniguchi, Yasutaka
   Shinohara, Yasushi
   Yabana, Kazuhiro</author></paper><paper><title>Computer graphics "Made in Germany" Darmstadt, the leading "Computer
   Graphics and Visual Computing Hub" in Europe: The way from 1975 to 2014</title><abstract>The paper reports on the 40 years of development of Computer Graphics
   and, more recently, Visual computing (VC) at the Technische Universitat
   Darmstadt in Germany, from its beginning in 1975 to the leading
   "Computer Graphics and Visual Computing Hub" in Europe as of 2014. This
   development is described along three axes. First, the institutional
   development and its rational to establish Computer Graphics as a
   discipline of Computer Science and as an enabling technology for
   developing our Knowledge Society, are described. Second, the scientific
   and technological impact based on the teaching activities and the large
   number of theses submitted in Darmstadt for the area during these 40
   years are addressed. Finally, the research road maps of the Computer
   Graphics and Visual Computing Hub in Darmstadt are presented relatively
   to the different stages of CG and VC research, relatively to a
   scientific view to the large number of projects implemented over these
   40 years and, finally, also relatively to the project results as seen
   from the media. In order to manage the quantity as well as the
   complexity of the information available, the description of these road
   maps is divided in four time periods: 1975-1984, 1985-1994,1995-2004 and
   2004-2015. The paper also gives the view of the authors on how they see
   the future of Computer Graphics and Visual Computing. At the end, the
   paper includes an extensive list of references for the reported content.
   (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>DEC 2015</date><author>Encarnacao, Jose L.
   Fellner, Dieter W.</author></paper><paper><title>Evaluating the Quality of Face Alignment without Ground Truth</title><abstract>The study of face alignment has been an area of intense research in
   computer vision, with its achievements widely used in computer graphics
   applications. The performance of various face alignment methods is often
   image-dependent or somewhat random because of their own strategy. This
   study aims to develop a method that can select an input image with good
   face alignment results from many results produced by a single method or
   multiple ones. The task is challenging because different face alignment
   results need to be evaluated without any ground truth. This study
   addresses this problem by designing a feasible feature extraction scheme
   to measure the quality of face alignment results. The feature is then
   used in various machine learning algorithms to rank different face
   alignment results. Our experiments show that our method is promising for
   ranking face alignment results and is able to pick good face alignment
   results, which can enhance the overall performance of a face alignment
   method with a random strategy. We demonstrate the usefulness of our
   ranking-enhanced face alignment algorithm in two practical applications:
   face cartoon stylization and digital face makeup.</abstract><date>OCT 2015</date><author>Sheng, Kekai
   Dong, Weiming
   Kong, Yan
   Mei, Xing
   Li, Jilin
   Wang, Chengjie
   Huang, Feiyue
   Hu, Bao-Gang</author></paper><paper><title>Camera array calibration for light field acquisition</title><abstract>Light field cameras are becoming popular in computer vision and
   graphics, with many research and commercial applications already having
   been proposed. Various types of cameras have been developed with the
   camera array being one of the ways of acquiring a 4D light field image
   using multiple cameras. Camera calibration is essential, since each
   application requires the correct projection and ray geometry of the
   light field. The calibrated parameters are used in the light field image
   rectified from the images captured by multiple cameras. Various camera
   calibration approaches have been proposed for a single camera, multiple
   cameras, and a moving camera. However, although these approaches can be
   applied to calibrating camera arrays, they are not effective in terms of
   accuracy and computational cost. Moreover, less attention has been paid
   to camera calibration of a light field camera. In this paper, we propose
   a calibration method for a camera array and a rectification method for
   generating a light field image from the captured images. We propose a
   two-step algorithm consisting of closed form initialization and
   nonlinear refinement, which extends Zhang's well-known method to the
   camera array. More importantly, we introduce a rigid camera constraint
   whereby the array of cameras is rigidly aligned in the camera array and
   utilize this constraint in our calibration. Using this constraint, we
   obtained much faster and more accurate calibration results in the
   experiments.</abstract><date>OCT 2015</date><author>Xu, Yichao
   Maeno, Kazuki
   Nagahara, Hajime
   Taniguchi, Rin-ichiro</author></paper><paper><title>Speeding up the log-polar transform with inexpensive parallel hardware:
   graphics units and multi-core architectures</title><abstract>Log-polar imaging is a kind of foveal, biologically inspired visual
   representation with advantageous properties in practical applications in
   computer vision, robotics, and other fields. While the cheapest, most
   flexible, and most common approach to get log-polar images is to use
   software-based mappers, this solution entails a cost which prevents
   certain experiments or applications from being feasible. This may be the
   case in some real-time (robotic) applications and, in general, when the
   conversion cost is not affordable for the task at hand. To overcome this
   drawback and make log-polar imaging more generally available, parallel
   solutions with affordable modern multi-core architectures have been
   devised, implemented, and tested in this work. Experimental results
   reveal that speed-up factors as high as or higher than 10 or 20,
   depending on the configuration, are possible to get log-polar images
   from large gray-level or color cartesian images using commodity graphics
   processors. Remarkable speedups are also reported for current multi-core
   processors. This noteworthy performance allows visual tasks that would
   otherwise be unthinkable with sequential implementations to become
   feasible. Additionally, since three different approaches have been
   explored and compared in terms of several criteria, different
   cost-effective choices are advisable depending on different visual task
   requirements or hardware availability.</abstract><date>SEP 2015</date><author>Antonelli, Marco
   Igual, Francisco D.
   Ramos, Francisco
   Traver, V. Javier</author></paper><paper><title>Label-free in vivo imaging of peripheral nerve by multispectral
   photoacoustic tomography</title><abstract>Unintentional surgical damage to nerves is mainly due to poor
   visualization of nerve tissue relative to adjacent structures.
   Multispectral photoacoustic tomography can provide chemical information
   with specificity and ultrasonic spatial resolution with centimeter
   imaging depth, making it a potential tool for noninvasive neural
   imaging. To implement this label-free imaging approach, a multispectral
   photoacoustic tomography platform was built. Imaging depth and spatial
   resolution were characterized. In vivo imaging of the femoral nerve that
   is 2 mm deep in a nude mouse was performed. Through multivariate curve
   resolution analysis, the femoral nerve was discriminated from the
   femoral artery and chemical maps of their spatial distributions were
   generated.[GRAPHICS]The femoral nerve was discriminated from the femoral
   artery by multivariate curve resolution analysis.</abstract><date>JAN 2016</date><author>Li, Rui
   Phillips, Evan
   Wang, Pu
   Goergen, Craig J.
   Cheng, Ji-Xin</author></paper><paper><title>Diffusion accessibility as a method for visualizing macromolecular
   surface geometry</title><abstract>Important three-dimensional spatial features such as depth and surface
   concavity can be difficult to convey clearly in the context of
   two-dimensional images. In the area of macromolecular visualization, the
   computer graphics technique of ray-tracing can be helpful, but further
   techniques for emphasizing surface concavity can give clearer
   perceptions of depth. The notion of diffusion accessibility is
   well-suited for emphasizing such features of macromolecular surfaces,
   but a method for calculating diffusion accessibility has not been made
   widely available. Here we make available a web-based platform that
   performs the necessary calculation by solving the Laplace equation for
   steady state diffusion, and produces scripts for visualization that
   emphasize surface depth by coloring according to diffusion
   accessibility. The URL is http://services.mbi.ucla.edu/DiffAcc/.</abstract><date>OCT 2015</date><author>Tsai, Yingssu
   Holton, Thomas
   Yeates, Todd O.</author></paper><paper><title>Single calcium channel domain gating of synaptic vesicle fusion at fast
   synapses; analysis by graphic modeling</title><abstract>At fast-transmitting presynaptic terminals Ca2+ enter through voltage
   gated calcium channels (CaVs) and bind to a synaptic vesicle (SV)
   -associated calcium sensor (SV-sensor) to gate fusion and discharge. An
   open CaV generates a high-concentration plume, or nanodomain of Ca2+
   that dissipates precipitously with distance from the pore. At most fast
   synapses, such as the frog neuromuscular junction (NMJ), the SV sensors
   are located sufficiently close to individual CaVs to be gated by single
   nanodomains. However, at others, such as the mature rodent calyx of Held
   (calyx of Held), the physiology is more complex with evidence that CaVs
   that are both close and distant from the SV sensor and it is argued that
   release is gated primarily by the overlapping Ca2+ nanodomains from many
   CaVs. We devised a 'graphic modeling' method to sum Ca2+ from individual
   CaVs located at varying distances from the SV-sensor to determine the SV
   release probability and also the fraction of that probability that can
   be attributed to single domain gating. This method was applied first to
   simplified, low and high CaV density model release sites and then to
   published data on the contrasting frog NMJ and the rodent calyx of Held
   native synapses. We report 3 main predictions: the SV-sensor is
   positioned very close to the point at which the SV fuses with the
   membrane; single domain-release gating predominates even at synapses
   where the SV abuts a large cluster of CaVs, and even relatively remote
   CaVs can contribute significantly to single domain-based gating.</abstract><date>SEP 3 2015</date><author>Stanley, Elise F.</author></paper><paper><title>SpirPro: A Spirulina proteome database and web-based tools for the
   analysis of protein-protein interactions at the metabolic level in
   Spirulina (Arthrospira) platensis C1</title><abstract>Background: Spirulina (Arthrospira) platensis is the only cyanobacterium
   that in addition to being studied at the molecular level and subjected
   to gene manipulation, can also be mass cultivated in outdoor ponds for
   commercial use as a food supplement. Thus, encountering environmental
   changes, including temperature stresses, is common during the mass
   production of Spirulina. The use of cyanobacteria as an experimental
   platform, especially for photosynthetic gene manipulation in plants and
   bacteria, is becoming increasingly important. Understanding the
   mechanisms and protein-protein interaction networks that underlie
   low-and high-temperature responses is relevant to Spirulina mass
   production. To accomplish this goal, high-throughput techniques such as
   OMICs analyses are used. Thus, large datasets must be collected, managed
   and subjected to information extraction. Therefore, databases including
   (i) proteomic analysis and protein-protein interaction (PPI) data and
   (ii) domain/motif visualization tools are required for potential use in
   temperature response models for plant chloroplasts and photosynthetic
   bacteria.Descriptions: A web-based repository was developed including an
   embedded database, SpirPro, and tools for network visualization.
   Proteome data were analyzed integrated with protein-protein interactions
   and/or metabolic pathways from KEGG. The repository provides various
   information, ranging from raw data (2D-gel images) to associated
   results, such as data from interaction and/or pathway analyses. This
   integration allows in silico analyses of protein-protein interactions
   affected at the metabolic level and, particularly, analyses of
   interactions between and within the affected metabolic pathways under
   temperature stresses for comparative proteomic analysis. The developed
   tool, which is coded in HTML with CSS/JavaScript and depicted in
   Scalable Vector Graphics (SVG), is designed for interactive analysis and
   exploration of the constructed network. SpirPro is publicly available on
   the web at http://spirpro.sbi.kmutt.ac.th.Conclusions: SpirPro is an
   analysis platform containing an integrated proteome and PPI database
   that provides the most comprehensive data on this cyanobacterium at the
   systematic level. As an integrated database, SpirPro can be applied in
   various analyses, such as temperature stress response networking
   analysis in cyanobacterial models and interacting domain-domain analysis
   between proteins of interest.</abstract><date>JUL 29 2015</date><author>Senachak, Jittisak
   Cheevadhanarak, Supapon
   Hongsthong, Apiradee</author></paper><paper><title>A Survey on Implicit Surface Polygonization</title><abstract>Implicit surfaces (IS) are commonly used in image creation, modeling
   environments, modeling objects, and scientific data visualization. In
   this article, we present a survey of different techniques for fast
   visualization of IS. The main classes of visualization algorithms are
   identified along with the advantages of each in the context of the
   different types of IS commonly used in computer graphics. We focus
   closely on polygonization methods, as they are the most suited to fast
   visualization. Classification and comparison of existing approaches are
   presented using criteria extracted from current research. This enables
   the identification of the best strategies according to the number of
   specific requirements, such as speed, accuracy, quality, or stylization.</abstract><date>JUL 2015</date><author>de Araujo, B. R.
   Lopes, Daniel S.
   Jepp, Pauline
   Jorge, Joaquim A.
   Wyvill, Brian</author></paper><paper><title>Numerical simulation of radiative heat transfer in indoor environments
   on programmable graphics hardware</title><abstract>The efficient use of energy for heating and cooling of indoor
   environments requires an accurate prediction and analysis of radiative
   heat transfer. Therefore it is necessary to use modern computer methods
   as otherwise the computational costs may become higher than for
   convective heat transfer, which is known to be a huge computational
   problem. A key problem in calculations of radiative heat transfer is the
   problem of mutual visibility arising in the determination of view
   factors. This is the same challenge that global illumination has to cope
   with which is one of the fundamental topics in computational graphics.
   The visibility problem is efficiently solved by modern graphics
   hardware. Therefore an OpenGL-based algorithm is developed to quickly
   and accurately calculate view factors for arbitrary, complex geometries.
   Theoretical and implementation details of the applied methods are given.
   We demonstrate the advantages of the developed computational method by a
   virtual test room for a tubular radiator, a heating of a warehouse by
   ceramic infrared heaters and the heat transfer in a car cabin. (C) 2015
   Elsevier Masson SAS. All rights reserved.</abstract><date>OCT 2015</date><author>Kramer, Stephan
   Gritzki, Ralf
   Perschk, Alf
   Roesler, Markus
   Felsmann, Clemens</author></paper><paper><title>Assessment of freeware programs for the reconstruction of tomography
   datasets obtained with a monochromatic synchrotron-based X-ray source</title><abstract>Synchrotron-based in-line phase-contrast computed tomography (PC-CT)
   allows soft tissue to be imaged with sub-gross resolution and has
   potential to be used as a diagnostic tool. The reconstruction and
   processing of in-line PC-CT datasets is a computationally demanding
   task; thus, an efficient and user-friendly software program is
   desirable. Four freeware programs (NRecon, PITRE, H-PITRE and Athabasca
   Recon) were compared for the availability of features such as dark- and
   flat-field calibration, beam power normalization, ring artifact removal,
   and alignment tools for optimizing image quality. An in-line PC-CT
   projection dataset (3751 projections, 180 degrees rotation, 10.13 mm x
   0.54 mm) was collected from a formalin-fixed canine prostate at the
   Biomedical Imaging and Therapy Bending Magnet (BMIT-BM) beamline of the
   Canadian Light Source. This dataset was processed with each of the four
   software programs and usability of the program was evaluated. Efficiency
   was assessed by how each program maximized computer processing power
   during computation. Athabasca Recon had the least-efficient memory
   usage, least user-friendly interface, and lacked a ring artifact removal
   feature. NRecon, PITRE and H-PITRE produced similar quality images, but
   the Athabasca Recon reconstruction suffered from the lack of a native
   ring remover algorithm. The 64-bit version of NRecon uses GPU (graphics
   processor unit) memory for accelerated processing and is userfriendly,
   but does not provide necessary parameters for in-line PC-CT data, such
   as dark- field and flat-field correction and beam power normalization.
   PITRE has many helpful features and tools, but lacks a comprehensive
   user manual and help section. H-PITRE is a condensed version of PITRE
   and maximizes computer memory for efficiency. To conclude, NRecon has
   fewer imaging processing tools than PITRE and H-PITRE, but is ideal for
   less experienced users due to a simple user interface. Based on the
   quality of reconstructed images, efficient use of computer memory and
   parameter availability, H-PITRE was the preferred of the four programs
   compared.</abstract><date>JUL 2015</date><author>Wolkowski, Bailey
   Snead, Elisabeth
   Wesolowski, Michal
   Singh, Jaswant
   Pettitt, Murray
   Chibbar, Rajni
   Melli, Seyedali
   Montgomery, James</author></paper><paper><title>Multi-stage interactive genetic algorithm for collaborative product
   customization</title><abstract>Products are becoming increasingly more complex and intelligent, which
   requires users to participate in the design process in order to meet
   customer demands and enhance market competition. Interactive genetic
   algorithm (IGA) can effectively solve the optimization problem. However,
   the challenge still remains for IGA to ameliorate user fatigue and
   reduce the noise in the process of evolution. To address the issue, a
   multi-stage interactive genetic algorithm (MS-IGA) is proposed, which
   divides the large population of the traditional interactive genetic
   algorithm (TIGA) into several stages according to different functional
   requirements. The proposed MS-IGA is then applied to the car console
   conceptual design system, to better capture the knowledge of users'
   personalized requirements and accomplish the product design. This is
   especially important in the field of complex product configuration
   design, such as in cars, personal computers, smart phones and the like.
   Through the users' graphic interface, customers separately evaluate
   product design at every different stage of its evolution, which makes
   the proposed algorithm more directional than the TIGA. We also introduce
   genetic sense units, which represent different functional modules, in
   order to realize the customers' collaborative design. The extensive
   experimental results are provided to demonstrate that our proposed
   algorithm is correct and efficient according to the efficiency test,
   convergence analysis and fatigue test for application of the product
   design system, including car interior and other modular product. (C)
   2015 Elsevier B.V. All rights reserved.</abstract><date>JAN 15 2016</date><author>Dou, Runliang
   Zong, Chao
   Nan, Guofang</author></paper><paper><title>Virtual Movement of the Ankle and Subtalar Joints Using Cadaver Surface
   Models</title><abstract>Medical students in the dissection room do not fully understand the
   ankle joint for dorsiflexion and plantar flexion as well as the subtalar
   joint for inversion and eversion. Thus, a three-dimensional simulation
   of the movements would be beneficial as a complementary pedagogic tool.
   The bones and five muscles (tibialis anterior, tibialis posterior,
   fibularis longus, fibularis brevis, and fibularis tertius) of the left
   ankle and foot were outlined in serially sectioned cadaver images from
   the Visible Korean project. The outlines were verified and revised; and
   were stacked to build surface models using Mimics software. Dorsiflexion
   and plantar flexion were simulated using the models on Maya to determine
   the mediolateral axis. Then, inversion and eversion were done to
   determine the anteroposterior axis. The topographic relationship of the
   two axes with the five affecting muscles was examined to demonstrate
   correctness. The models were placed in a PDF file, with which users were
   capable of mixed display of structures. The stereoscopic image data,
   developed in this investigation, clearly explain ankle movement. These
   graphic contents, accompanied by the sectioned images, are expected to
   facilitate the development of simulation for the medical students'
   learning and the orthopedic surgeons' clinical trial.</abstract><date>SEP 2015</date><author>Shin, Dong Sun
   Chung, Min Suk</author></paper><paper><title>Automatic High-Level Data-Flow Synthesis and Optimization of Polynomial
   Datapaths Using Functional Decomposition</title><abstract>This paper concentrates on high-level data-flow optimization and
   synthesis techniques for datapath intensive designs such as those in
   Digital Signal Processing (DSP), computer graphics and embedded systems
   applications, which are modeled as polynomial computations over Z(2n1) x
   Z(2n2) x ... x Z(2nd) to Z(2m). Our main contribution in this paper is
   proposing an optimization method based on functional decomposition of
   multivariate polynomial in the form of f(x) = g(x) o h(x) + f(0) =
   g(h(x)) + f(0) to obtain good building blocks, and vanishing polynomials
   over Z(2m) to add/delete redundancy to/from given polynomial functions
   to extract further common sub-expressions. Experimental results for
   combinational implementation of the designs have shown an average saving
   of 38.85 and 18.85 percent in the number of gates and critical path
   delay, respectively, compared with the state-of-the-art techniques.
   Regarding the comparison with our previous works, the area and delay are
   improved by 10.87 and 11.22 percent, respectively. Furthermore,
   experimental results of sequential implementations have shown an average
   saving of 39.26 and 34.70 percent in the area and the latency,
   respectively, compared with the state-of-the-art techniques.</abstract><date>JUN 2015</date><author>Ghandali, Samaneh
   Alizadeh, Bijan
   Fujita, Masahiro
   Navabi, Zainalabedin</author></paper><paper><title>GeNN: a code generation framework for accelerated brain simulations</title><abstract>Large-scale numerical simulations of detailed brain circuit models are
   important for identifying hypotheses on brain functions and testing
   their consistency and plausibility. An ongoing challenge for simulating
   realistic models is, however, computational speed. In this paper, we
   present the GeNN (GPU-enhanced Neuronal Networks) framework, which aims
   to facilitate the use of graphics accelerators for computational models
   of large-scale neuronal networks to address this challenge. GeNN is an
   open source library that generates code to accelerate the execution of
   network simulations on NVIDIA GPUs, through a flexible and extensible
   interface, which does not require in-depth technical knowledge from the
   users. We present performance benchmarks showing that 200-fold speedup
   compared to a single core of a CPU can be achieved for a network of one
   million conductance based Hodgkin-Huxley neurons but that for other
   models the speedup can differ. GeNN is available for Linux, Mac OS X and
   Windows platforms. The source code, user manual, tutorials, Wiki,
   in-depth example projects and all other related information can be found
   on the project website http://genn-team.github.io/genn/.</abstract><date>JAN 7 2016</date><author>Yavuz, Esin
   Turner, James
   Nowotny, Thomas</author></paper><paper><title>NFFinder: an online bioinformatics tool for searching similar
   transcriptomics experiments in the context of drug repositioning</title><abstract>Drug repositioning, using known drugs for treating conditions different
   from those the drug was originally designed to treat, is an important
   drug discovery tool that allows for a faster and cheaper development
   process by using drugs that are already approved or in an advanced trial
   stage for another purpose. This is especially relevant for orphan
   diseases because they affect too few people to make drug research de
   novo economically viable. In this paper we present NFFinder, a
   bioinformatics tool for identifying potential useful drugs in the
   context of orphan diseases. NFFinder uses transcriptomic data to find
   relationships between drugs, diseases and a phenotype of interest, as
   well as identifying experts having published on that domain. The
   application shows in a dashboard a series of graphics and tables
   designed to help researchers formulate repositioning hypotheses and
   identify potential biological relationships between drugs and diseases.
   NFFinder is freely available at http://nffinder.cnb.csic.es.</abstract><date>JUL 1 2015</date><author>Setoain, Javier
   Franch, Monica
   Martinez, Marta
   Tabas-Madrid, Daniel
   Sorzano, Carlos O. S.
   Bakker, Annette
   Gonzalez-Couto, Eduardo
   Elvira, Juan
   Pascual-Montano, Alberto</author></paper><paper><title>Electroencephalographic Data Analysis With Visibility Graph Technique
   for Quantitative Assessment of Brain Dysfunction</title><abstract>Usual techniques for electroencephalographic (EEG) data analysis lack
   some of the important properties essential for quantitative assessment
   of the progress of the dysfunction of the human brain. EEG data are
   essentially nonlinear and this nonlinear time series has been identified
   as multi-fractal in nature. We need rigorous techniques for such
   analysis. In this article, we present the visibility graph as the
   latest, rigorous technique that can assess the degree of multifractality
   accurately and reliably. Moreover, it has also been found that this
   technique can give reliable results with test data of comparatively
   short length. In this work, the visibility graph algorithm has been used
   for mapping a time series-EEG signals-to a graph to study complexity and
   fractality of the time series through investigation of its complexity.
   The power of scale-freeness of visibility graph has been used as an
   effective method for measuring fractality in the EEG signal. The
   scale-freeness of the visibility graph has also been observed after
   averaging the statistically independent samples of the signal.
   Scale-freeness of the visibility graph has been calculated for 5 sets of
   EEG data patterns varying from normal eye closed to epileptic. The
   change in the values is analyzed further, and it has been observed that
   it reduces uniformly from normal eye closed to epileptic.</abstract><date>JUL 2015</date><author>Bhaduri, Susmita
   Ghosh, Dipak</author></paper><paper><title>A Modular Framework for EEG Web Based Binary Brain Computer Interfaces
   to Recover Communication Abilities in Impaired People</title><abstract>A Brain Computer Interface (BCI) allows communication for impaired
   people unable to express their intention with common channels.
   Electroencephalography (EEG) represents an effective tool to allow the
   implementation of a BCI. The present paper describes a modular framework
   for the implementation of the graphic interface for binary BCIs based on
   the selection of symbols in a table. The proposed system is also
   designed to reduce the time required for writing text. This is made by
   including a motivational tool, necessary to improve the quality of the
   collected signals, and by containing a predictive module based on the
   frequency of occurrence of letters in a language, and of words in a
   dictionary. The proposed framework is described in a top-down approach
   through its modules: signal acquisition, analysis, classification,
   communication, visualization, and predictive engine. The framework,
   being modular, can be easily modified to personalize the graphic
   interface to the needs of the subject who has to use the BCI and it can
   be integrated with different classification strategies, communication
   paradigms, and dictionaries/languages. The implementation of a scenario
   and some experimental results on healthy subjects are also reported and
   discussed: the modules of the proposed scenario can be used as a
   starting point for further developments, and application on severely
   disabled people under the guide of specialized personnel.</abstract><date>JAN 2016</date><author>Placidi, Giuseppe
   Petracca, Andrea
   Spezialetti, Matteo
   Iacoviello, Daniela</author></paper><paper><title>Trichloroethylene degradation by persulphate with magnetite as a
   heterogeneous activator in aqueous solution</title><abstract>Iron oxide-magnetite (Fe3O4) as a heterogeneous activator to activate
   persulphate anions[GRAPHICS]for trichloroethylene (TCE) degradation was
   investigated in this study. The experimental results showed that TCE
   could be completely oxidized within 5 h by using 5 g L-1 magnetite and
   63 mM[GRAPHICS]indicating the effectiveness of the process for TCE
   removal. Various factors of the process, including.[GRAPHICS]and
   magnetite dosages, and initial solution pH, were evaluated, and TCE
   degradation fitted well to the pseudo-first-order kinetic model. The
   calculated kinetic rate constant was increased with
   increasing[GRAPHICS]and magnetite dosages, but it was independent of
   solution pH. In addition, the changes of magnetite morphology examined
   by scanning electron microscopy and X-ray powder diffraction,
   respectively, confirmed the slight corrosion with alpha-Fe2O3 coated on
   the magnetite surface. The probe compounds tests clearly identified the
   generation of the reactive oxygen species in the system. While the free
   radical quenching studies further demonstrated that[GRAPHICS]and center
   dot OH were the major radicals responsible for TCE degradation,
   whereas[GRAPHICS]contributed less in the system, and therefore the roles
   of reactive oxygen species on TCE degradation mechanisms were proposed
   accordingly. To our best knowledge, this is the first time the
   performance and mechanism of magnetite-activated persulphate oxidation
   for TCE degradation are reported. The findings of this study provided a
   new insight into the heterogeneous catalysis mechanism and showed a
   great potential for the practical application of this technique in in
   situ TCE-contaminated groundwater remediation.</abstract><date>JUN 3 2015</date><author>Ruan, Xiaoxin
   Gu, Xiaogang
   Lu, Shuguang
   Qiu, Zhaofu
   Sui, Qian</author></paper><paper><title>2D to 3D geologic mapping transformation using virtual globes and flight
   simulators and their applications in the analysis of geodiversity in
   natural areas</title><abstract>This work describes the transformation process from 2D cartography to
   3D, simply by overlapping images in common formats (jpeg, bmp, tiff,
   png, etc.) on Google Earth's virtual globe. Arribes del Duero Natural
   Park, located west of the province of Salamanca, Spain, was the object
   of this part of the study. Other natural areas are also discussed and
   were used to establish a procedure for mapping geodiversity and for
   identifying areas of geological uniqueness and naturalness within the
   natural areas. To do this, different parametric indices were used to
   empirically generate different degrees of geological diversity in the
   Quilamas Natural Area, located south of Salamanca, Spain. Intermediate
   parametric maps were processed using two types of GIS technical:
   graphical (neighbourhood operations) and alphanumeric (calculated from
   the fields in the attribute table). Intermediate parametric maps were
   processed using two types of technical GIS: neighbourhood operations
   (graphics) and alphanumeric (calculated from the fields in the attribute
   table). These maps were used to establish areas with the greatest
   concentration of geological diversity elements and to define areas with
   a greater need for protection when planning the management of human
   activities in natural areas. Finally, the flight simulator tool, which
   was implemented in the free virtual globe and controlled using a
   keyboard or joystick, allows you to "fly" through the projected
   geological mapping of Arribes Del Duero Natural Park or view the
   parametric mapping and geodiversity in the newly created Quilamas
   Natural Area. Interoperability with the Google Maps application allows
   you to identify and observe the outcrops of the various geological
   materials in natural or anthropic terrain cuts.</abstract><date>JUN 2015</date><author>Martinez-Grana, A. M.
   Goy, J. L.
   Cimarra, C.</author></paper><paper><title>3D Image Reconstructions and the Nyquist-Shannon Theorem</title><abstract>Fracture surfaces are occasionally modelled by Fourier's two-dimensional
   series that can be converted into digital 3D reliefs mapping the
   morphology of solid surfaces. Such digital replicas may suffer from
   various artefacts when processed inconveniently. Spatial aliasing is one
   of those artefacts that may devalue Fourier's replicas. According to the
   Nyquist-Shannon sampling theorem the spatial aliasing occurs when
   Fourier's frequencies exceed the Nyquist critical frequency. In the
   present paper it is shown that the Nyquist frequency is not the only
   critical limit determining aliasing artefacts but there are some other
   frequencies that intensify aliasing phenomena and form an infinite set
   of points at which numerical results abruptly and dramatically change
   their values. This unusual type of spatial aliasing is explored and some
   consequences for 3D computer reconstructions are presented.Graphical
   Abstract Fourier's replicas of scanned surfaces correctly reproduce all
   morphological features only if the number N of harmonic terms in the
   Fourier sum does not exceed a critical value derived from the Nyquist
   frequency of the scanned original. Incorporating harmonic terms of
   higher frequencies in the Fourier sum, an abrupt increase of aliasing
   effects occurs whenever the frequencies in the Fourier sum reach even
   multiples of the Nyquist frequency. Illustration N = 50 represents a
   correct reproduction whereas illustrations N = 228 and N = 440 show
   critical abrupt increases of aliasing artifacts.[GRAPHICS].</abstract><date>SEP 2015</date><author>Ficker, T.
   Martisek, D.</author></paper><paper><title>Efficient visualization of high-throughput targeted proteomics
   experiments: TAPIR</title><abstract>Motivation: Targeted mass spectrometry comprises a set of powerful
   methods to obtain accurate and consistent protein quantification in
   complex samples. To fully exploit these techniques, a cross-platform and
   open-source software stack based on standardized data exchange formats
   is required.Results: We present TAPIR, a fast and efficient Python
   visualization software for chromatograms and peaks identified in
   targeted proteomics experiments. The input formats are open,
   community-driven standardized data formats (mzML for raw data storage
   and TraML encoding the hierarchical relationships between transitions,
   peptides and proteins). TAPIR is scalable to proteome-wide targeted
   proteomics studies (as enabled by SWATH-MS), allowing researchers to
   visualize high-throughput datasets. The framework integrates well with
   existing automated analysis pipelines and can be extended beyond
   targeted proteomics to other types of analyses.</abstract><date>JUL 15 2015</date><author>Roest, Hannes L.
   Rosenberger, George
   Aebersold, Ruedi
   Malmstroem, Lars</author></paper><paper><title>Hierarchical genetic clusters for phenotypic analysis</title><abstract>Methods to obtain phenotypic information were evaluated to help breeders
   choosing the best methodology for analysis of genetic diversity in
   backcross populations. Phenotypes were simulated for 13 characteristics
   generated in 10 populations with 100 individuals each. Genotypic
   information was generated from 100 loci of which 20 were taken at random
   to determine the characteristics expressing two alleles. Dissimilarity
   measures were calculated, and genetic diversity was analyzed through
   hierarchical clustering and graphic projection of the distances. A
   backcross was performed from the two most divergent populations. A set
   of characteristics with variable heritability was taken into account.
   The environmental effect was simulated assuming x similar to N (0,
   sigma(2)). For hierarchical clusters, the following methods were used:
   Gower Method, average linkage within the cluster, average linkage among
   clusters, the furthest neighbor method, the nearest neighbor method,
   Ward's method, and the median method. The environmental effect and
   heritability of the analyzed variables had an influence on the pattern
   of hierarchical clustering populations according to the backcrossed
   generations. The nearest neighbor method was the most efficient in
   reconstructing the system of backcrossing, and it presented the highest
   cophenetic correlation. The efficiency of the nearest neighbor method
   was the highest when the analysis involved characteristics of high
   heritability.</abstract><date>OCT-DEC 2015</date><author>da Matta, Luiza Barbosa
   Oliveira Tome, Livia Gracielle
   Salgado, Caio Cesio
   Cruz, Cosme Damiao
   Silva, Leticia de Faria</author></paper><paper><title>A study of graphics hardware accelerated particle swarm optimization
   with digital pheromones</title><abstract>Programmable Graphics Processing Units (GPUs) have lately become a
   promising means to perform scientific computations. Modern GPUs have
   proven to outperform the number of floating point operations when
   compared to traditional Central Processing Units (CPUs) through inherent
   data parallel architecture and higher bandwidth capabilities. They allow
   scientific computations to be performed without noticeable degradation
   in accuracy in a fraction of the time compared to traditional CPUs at
   substantially reduced costs, making them viable alternatives to
   expensive computer clusters or workstations. GPU programmability
   however, has fostered the development of a variety of programming
   languages making it challenging to select a computing language and use
   it consistently without the pitfall of being obsolete. Some GPU
   languages are hardware specific and are designed to rake in performance
   boosts when used with their host GPUs (e.g., Nvidia Cuda). Others are
   operating system specific (e.g., Microsoft HLSL). A few are platform
   agnostic lending themselves to be used on a workstation with any CPU and
   a GPU (e.g., GLSL, OpenCL).Of a number of companies and organizations
   that implement formal optimization into their processes, only a few
   utilize GPUs. It is either because the others are either vested much
   into CPU based computing or they are not fully aware of the benefits of
   implementing population based optimization routines in GPUs. Literature
   shows a large number of research publications specifically in the field
   of optimization utilizing GPUs. However, most of them are limited to a
   specific GPU hardware or addressed specific problems. The diversity in
   current GPU hardware and software APIs present overwhelming number of
   choices making it challenging to decide where and how to begin
   transitioning to GPU based computing, impeding promising computing
   avenues that relatively is very cost effective. In this paper, the
   authors precisely intend to address some of these issues by broadly
   classifying GPU APIs into three categories: 1) Hardware vendor dependent
   GPU APIs, 2) Graphical in context APIs, and 3) Platform agnostic APIs.
   Prior work by the authors demonstrated the capability of digital
   pheromones within Particle Swarm Optimization (PSO) for searching
   n-dimensional design spaces with improved accuracy, efficiency and
   reliability in serial and parallel CPU computing environments. To study
   the impact of GPUs, the authors have taken this digital pheromone
   variant of PSO and implemented it on three GPU APIs, each representing a
   category listed above, in a simplistic sense - delegate unconstrained
   explicit objective function evaluations to GPUs. While this approach
   itself cannot be considered novel, the takeaways from implementing it on
   different GPU APIs provided a wealth of information that the authors
   believe can help optimization companies and organizations make informed
   decisions in implementing GPUs in their processes.</abstract><date>JUN 2015</date><author>Kalivarapu, Vijay
   Winer, Eliot</author></paper><paper><title>Point cloud modeling using the homogeneous transformation for
   non-cooperative pose estimation</title><abstract>A modeling process to simulate point cloud range data that a lidar
   (light detection and ranging) sensor produces is presented in this paper
   in order to support the development of non-cooperative pose (relative
   attitude and position) estimation approaches which will help improve
   proximity operation capabilities between two adjacent vehicles. The
   algorithms in the modeling process were based on the homogeneous
   transformation, which has been employed extensively in robotics and
   computer graphics, as well as in recently developed pose estimation
   algorithms. Using a flash lidar in a laboratory testing environment,
   point cloud data of a test article was simulated and compared against
   the "measured point cloud data. The simulated and measured data sets
   match closely, validating the modeling process. The modeling capability
   enables close examination of the characteristics of point cloud images
   of an object as it undergoes various translational and rotational
   motions. Relevant characteristics that will be crucial in
   non-cooperative pose estimation were identified such as shift,
   shadowing, perspective projection, jagged edges, and differential point
   cloud density. These characteristics will have to be considered in
   developing effective non-cooperative pose estimation algorithms. The
   modeling capability will allow extensive non-cooperative pose estimation
   performance simulations prior to field testing, saving development cost
   and providing performance metrics of the pose estimation concepts and
   algorithms under evaluation. The modeling process also provides "truth"
   pose of the test objects with respect to the sensor frame so that the
   pose estimation error can be quantified. Published by Elsevier Ltd. on
   behalf of IAA.</abstract><date>JUN-JUL 2015</date><author>Lim, Tae W.</author></paper><paper><title>Accelerating the Pace of Protein Functional Annotation With Intel Xeon
   Phi Coprocessors</title><abstract>Intel Xeon Phi is a new addition to the family of powerful parallel
   accelerators. The range of its potential applications in computationally
   driven research is broad; however, at present, the repository of
   scientific codes is still relatively limited. In this study, we describe
   the development and benchmarking of a parallel version of eFindSite, a
   structural bioinformatics algorithm for the prediction of ligand-binding
   sites in proteins. Implemented for the Intel Xeon Phi platform, the
   parallelization of the structure alignment portion of eFindSite using
   pragma-based OpenMP brings about the desired performance improvements,
   which scale well with the number of computing cores. Compared to a
   serial version, the parallel code runs 11.8 and 10.1 times faster on the
   CPU and the coprocessor, respectively; when both resources are utilized
   simultaneously, the speedup is 17.6. For example, ligand-binding
   predictions for 501 benchmarking proteins are completed in 2.1 hours on
   a single Stampede node equipped with the Intel Xeon Phi card compared to
   3.1 hours without the accelerator and 36.8 hours required by a serial
   version. In addition to the satisfactory parallel performance, porting
   existing scientific codes to the Intel Xeon Phi architecture is
   relatively straightforward with a short development time due to the
   support of common parallel programming models by the coprocessor. The
   parallel version of eFindSite is freely available to the academic
   community at www.brylinski.org/efindsite.</abstract><date>JUN 2015</date><author>Feinstein, Wei P.
   Moreno, Juana
   Jarrell, Mark
   Brylinski, Michal</author></paper><paper><title>A patient-centered symptom monitoring and reporting system for children
   and young adults with cancer (SyMon-SAYS)</title><abstract>BackgroundThis study evaluated the feasibility of implementing a
   patient-centered, technology-based symptom monitoring and reporting
   system (SyMon-SAYS) in pediatric oncology clinics using fatigue as a
   prototypic symptom. Timely identification of symptoms related to
   multi-modal therapy for children with cancer is fundamental to the
   overall success of cancer treatment. SyMon-SAYS was developed to address
   this need.ProcedurePatients with a cancer diagnosis, ages 7-21 years,
   currently on treatment, or off treatment within 6 months, were eligible.
   Patients/parents completed weekly fatigue assessments over 8 weeks via
   the internet or interactive voice response (IVR) by phone. Alert emails
   were generated when pre-defined fatigue score thresholds were met, and
   fatigue reports were forwarded to clinicians accordingly. Clinicians and
   parents/patients received cumulative graphic reports of fatigue scores
   prior to clinic visits at 4 and 8 weeks post-baseline to facilitate
   discussion. Parents/patients completed an exit survey at their last
   visit.ResultsFifty-seven patients/parents completed the study. The
   majority of patients (93%) and parents (78%) felt it was very/extremely
   easy to complete SyMon-SAYS; 95% of parents were satisfied with the
   system; 60% reported it helped deal with their child's fatigue; 70%
   reported that clinicians didn't discuss fatigue with them; 81% would be
   willing to use SyMon-SAYS to manage fatigue and other symptoms.
   Clinicians reported insufficient time to review reports, yet 71% were
   willing to receive the report on a monthly basis.ConclusionSyMon-SAYS is
   feasible and acceptable to patients and parents. Future efforts should
   focus on better integrating the system into the clinical workflow to
   improve clinicians' acceptance. Pediatr Blood Cancer 2015;62:1813-1818.
   (c) 2015 Wiley Periodicals, Inc.</abstract><date>OCT 2015</date><author>Lai, Jin-Shei
   Yount, Susan
   Beaumont, Jennifer L.
   Cella, David
   Toia, Jacquie
   Goldman, Stewart</author></paper><paper><title>Evaluating an Online Cognitive Training Platform for Older Adults: User
   Experience and Implementation Requirements</title><abstract></abstract><date>AUG 2015</date><author>Haesner, Marten
   Steinert, Anika
   Weichenberger, Markus</author></paper><paper><title>CUDA-quicksort: an improved GPU-based implementation of quicksort</title><abstract>Sorting is a very important task in computer science and becomes a
   critical operation for programs making heavy use of sorting algorithms.
   General-purpose computing has been successfully used on Graphics
   Processing Units (GPUs) to parallelize some sorting algorithms. Two
   GPU-based implementations of the quicksort were presented in literature:
   the GPU-quicksort, a compute-unified device architecture (CUDA)
   iterative implementation, and the CUDA dynamic parallel (CDP) quicksort,
   a recursive implementation provided by NVIDIA Corporation. We propose
   CUDA-quicksort an iterative GPU-based implementation of the sorting
   algorithm. CUDA-quicksort has been designed starting from GPU-quicksort.
   Unlike GPU-quicksort, it uses atomic primitives to perform inter-block
   communications while ensuring an optimized access to the GPU memory.
   Experiments performed on six sorting benchmark distributions show that
   CUDA-quicksort is up to four times faster than GPU-quicksort and up to
   three times faster than CDP-quicksort. An in-depth analysis of the
   performance between CUDA-quicksort and GPU-quicksort shows that the main
   improvement is related to the optimized GPU memory access rather than to
   the use of atomic primitives. Moreover, in order to assess the
   advantages of using the CUDA dynamic parallelism, we implemented a
   recursive version of the CUDA-quicksort. Experimental results show that
   CUDA-quicksort is faster than the CDP-quicksort provided by NVIDIA, with
   better performance achieved using the iterative implementation.
   Copyright (c) 2015 John Wiley &amp; Sons, Ltd.</abstract><date>JAN 2016</date><author>Manca, Emanuele
   Manconi, Andrea
   Orro, Alessandro
   Armano, Giuliano
   Milanesi, Luciano</author></paper><paper><title>CBIR Based Testing Oracles: An Experimental Evaluation of Similarity
   Functions</title><abstract>Content-Based Image Retrieval (CBIR) systems constitute an innovative
   approach to store, to compare and to query images in a database. Visual
   aspects such as color, texture or shape are used to perform such
   operations. Recently, CBIR concepts were applied to build testing
   oracles for image processing programs, where test verdicts
   (approval/disapproval) are based on similarity measures between images
   produced by the program and reference images. However, the results of a
   CBIR system may vary depending on the components employed in the system
   (feature extractors and similarity functions), and few studies assessing
   this influence have been found in the literature. Our aim is to present
   an empirical analysis of ten similarity functions in CBIR systems within
   the context of software testing with graphic outputs. A case study with
   images obtained from a computer-aided diagnosis system in mammography
   indicated some variability among image test verdicts
   (approval/disapproval) according to the similarity function choice. The
   case study also indicates the existence of some clusters of similarity
   functions with high correlation coefficients.</abstract><date>OCT 2015</date><author>Nunes, Fatima L. S.
   Delamaro, Marcio Eduardo
   Goncalves, Vagner Mendocna
   Lauretto, Marcelo De Souza</author></paper><paper><title>Auto-tuned Krylov methods on cluster of graphics processing unit</title><abstract>Exascale computers are expected to have highly hierarchical
   architectures with nodes composed by multiple core processors (CPU;
   central processing unit) and accelerators (GPU; graphics processing
   unit). The different programming levels generate new difficult algorithm
   issues. In particular when solving extremely large linear systems, new
   programming paradigms of Krylov methods should be defined and evaluated
   with respect to modern state of the art of scientific methods. Iterative
   Krylov methods involve linear algebra operations such as dot product,
   norm, addition of vectors and sparse matrix-vector multiplication. These
   operations are computationally expensive for large size matrices. In
   this paper, we aim to focus on the best way to perform effectively these
   operations, in double precision, on GPU in order to make iterative
   Krylov methods more robust and therefore reduce the computing time. The
   performance of our algorithms is evaluated on several matrices arising
   from engineering problems. Numerical experiments illustrate the
   robustness and accuracy of our implementation compared to the existing
   libraries. We deal with different preconditioned Krylov methods:
   Conjugate Gradient for symmetric positive-definite matrices, and
   Generalized Conjugate Residual, Bi-Conjugate Gradient Conjugate
   Residual, transpose-free Quasi Minimal Residual, Stabilized BiConjugate
   Gradient and Stabilized BiConjugate Gradient (L) for the solution of
   sparse linear systems with non symmetric matrices. We consider and
   compare several sparse compressed formats, and propose a way to
   implement effectively Krylov methods on GPU and on multicore CPU.
   Finally, we give strategies to faster algorithms by auto-tuning the
   threading design, upon the problem characteristics and the hardware
   changes. As a conclusion, we propose and analyse hybrid sub-structuring
   methods that should pave the way to exascale hybrid methods.</abstract><date>JUN 3 2015</date><author>Magoules, Frederic
   Ahamed, Abal-Kassim Cheik
   Putanowicz, Roman</author></paper><paper><title>Topological subtleties for molecular movies</title><abstract>Synchronous movies permit visual analysis of shape perturbation during
   molecular simulations. The molecule is conceptualized as a knot and
   modeled as a spline curve. As the molecule writhes, the graphics
   approximation in each frame should display an ambient isotopic image of
   the perturbing spline. These graphics approximations raise subtleties
   for correctly rendering the embedding. A cautionary example was
   discovered through visualization experiments and the relevant
   characteristics are formally proved. (C) 2015 Elsevier B.V. All rights
   reserved.</abstract><date>JUN 15 2015</date><author>Li, J.
   Peters, T. J.
   Marinelli, K.
   Kovalev, E.
   Jordan, K. E.</author></paper><paper><title>MetaMapR: pathway independent metabolomic network analysis incorporating
   unknowns</title><abstract>Metabolic network mapping is a widely used approach for integration of
   metabolomic experimental results with biological domain knowledge.
   However, current approaches can be limited by biochemical domain or
   pathway knowledge which results in sparse disconnected graphs for real
   world metabolomic experiments. MetaMapR integrates enzymatic
   transformations with metabolite structural similarity, mass spectral
   similarity and empirical associations to generate richly connected
   metabolic networks. This open source, web-based or desktop software,
   written in the R programming language, leverages KEGG and PubChem
   databases to derive associations between metabolites even in cases where
   biochemical domain or molecular annotations are unknown. Network
   calculation is enhanced through an interface to the Chemical Translation
   System, which allows metabolite identifier translation between &gt;200
   common biochemical databases. Analysis results are presented as
   interactive visualizations or can be exported as high-quality graphics
   and numerical tables which can be imported into common network analysis
   and visualization tools.</abstract><date>AUG 15 2015</date><author>Grapov, Dmitry
   Wanichthanarak, Kwanjeera
   Fiehn, Oliver</author></paper><paper><title>Modeling Luminance Perception at Absolute Threshold</title><abstract>When human luminance perception operates close to its absolute
   threshold, i. e., the lowest perceivable absolute values, appearance
   changes substantially compared to common photopic or scotopic vision. In
   particular, most observers report perceiving temporally-varying noise.
   Two reasons are physiologically plausible; quantum noise (due to the low
   absolute number of photons) and spontaneous photochemical reactions.
   Previously, static noise with a normal distribution and no account for
   absolute values was combined with blue hue shift and blur to simulate
   scotopic appearance on a photopic display for movies and interactive
   applications (e.g., games). We present a computational model to
   reproduce the specific distribution and dynamics of scotopic noise for
   specific absolute values. It automatically introduces a
   perceptually-calibrated amount of noise for a specific luminance level
   and supports animated imagery. Our simulation runs in milliseconds at HD
   resolution using graphics hardware and favorably compares to simpler
   alternatives in a perceptual experiment.</abstract><date>JUL 2015</date><author>Kellnhofer, Petr
   Ritschel, Tobias
   Myszkowski, Karol
   Eisemann, Elmar
   Seidel, Hans-Peter</author></paper><paper><title>DigitalVHI--a freeware open-source software application to capture the
   Voice Handicap Index and other questionnaire data in various languages.</title><abstract>In this short report we introduce DigitalVHI, a free open-source
   software application for obtaining Voice Handicap Index (VHI) and other
   questionnaire data, which can be put on a computer in clinics and used
   in clinical practice. The software can simplify performing clinical
   studies since it makes the VHI scores directly available for analysis in
   a digital form. It can be downloaded from
   http://www.christian-herbst.org/DigitalVHI/. </abstract><date>2015-Jul</date><author>Herbst, Christian T
   Oh, Jinook
   Vydrova, Jitka
   Svec, Jan G</author></paper><paper><title>Image Segmentation With Cage Active Contours</title><abstract>In this paper, we present a framework for image segmentation based on
   parametrized active contours. The evolving contour is parametrized
   according to a reduced set of control points that form a closed polygon
   and have a clear visual interpretation. The parametrization, called mean
   value coordinates, stems from the techniques used in computer graphics
   to animate virtual models. Our framework allows to easily formulate
   region-based energies to segment an image. In particular, we present
   three different local region-based energy terms: 1) the mean model; 2)
   the Gaussian model; 3) and the histogram model. We show the behavior of
   our method on synthetic and real images and compare the performance with
   state-of-the-art level set methods.</abstract><date>DEC 2015</date><author>Garrido, Lluis
   Guerrieri, Marite
   Igual, Laura</author></paper><paper><title>AQUAgpusph, a new free 3D SPH solver accelerated with OpenCL</title><abstract>In this paper, AQUAgpusph, a new free Smoothed Particle Hydrodynamics
   (SPH) software accelerated with OpenCL, is described. The main
   differences and progress with respect to other existing alternatives are
   considered. These are the use of the Open Computing Language (OpenCL)
   framework instead of the Compute Unified Device Architecture (CUDA), the
   implementation of the most popular boundary conditions, the easy
   customization of the code to different problems, the extensibility with
   regard to Python scripts, and the runtime output which allows the
   tracking of simulations in real time, or a higher frequency in saving
   some results without a significant performance lost. These modifications
   are shown to improve the solver speed, the results quality, and allow
   for a wider areas of application. AQUAgpusph has been designed trying to
   provide researchers and engineers with a valuable tool to test and apply
   the SPH method. Three practical applications are discussed in detail.
   The evolution of a dam break is used to quantify and compare the
   computational performance and modeling accuracy with the most popular
   SPH Graphics Processing Unit (CPU) accelerated alternatives. The
   dynamics of a coupled system, a Tuned Liquid Damper (TLD), is discussed
   in order to show the integration capabilities of the solver with
   external dynamics. Finally, the sloshing flow inside a nuclear reactor
   is simulated in order to show the capabilities of the solver to treat
   3-D problems with complex geometries and of industrial interest.Program
   summaryProgram title: AQUAgpusph 1.5Catalogue identifier:
   AEVG_v1_0Program summary URL:
   http://cpc.cs.qub.ac.uk/summaries/AEVG_v1_0.htmlProgram obtainable from:
   CPC Program Library, Queen's University, Belfast, N. IrelandLicensing
   provisions: GNU General Public License, version 3No. of lines in
   distributed program, including test data, etc.: 1702666No. of bytes in
   distributed program, including test data, etc.: 75117178Distribution
   format: tar.gzProgramming language: C++, OpenCL, Python.Computer: Linux
   based computers with OpenCL support.Operating system: Linux.Has the code
   been vectorized or parallelized?: Code is parallelized with
   OpenCL.Classification: 1.5.Nature of problem: Complex geometry or
   heavily fragmented free surface fluid dynamics problems where mesh based
   method cannot be successfully applied.Solution method: SPH is a meshless
   method where the fluid domain is discretized as a set of fluid
   particles. The fields in the fluid domain are smoothed using a kernel
   function, that allows to develop differential operators from the flow
   field values in scattered sets of particles.Running time: Using an AMD
   HD-7970 graphic device 2 x 10(5) time steps of a 2-D simulation, with
   10(5) particles and 8 x 10(2) neighs per particle, is requiring around 9
   h of computation. A more detailed performance analysis will be carried
   out in the practical application section herein. (C) 2015 Elsevier B.V.
   All rights reserved.</abstract><date>JUL 2015</date><author>Cercos-Pita, J. L.</author></paper><paper><title>Standardization of a Graphic Symbol System as an Alternative
   Communication Tool for Turkish</title><abstract>Graphic symbols are commonly used across countries in order to support
   individuals with communicative deficiency. The literature review
   revealed the absence of such a system for Turkish socio-cultural
   context. In this study, the aim was to develop a symbol system
   appropriate for the Turkish socio-cultural context. The process began
   with studies designed to delineate the scope of the proposed system.
   Firstly, a dictionary was formed, founded on the literature. In the
   following stage, a visual design form was developed with a view to
   identifying visual representations of these words. Using this form, 106
   participants were asked for their opinion. The data collected were
   examined, and common traits of the graphic(s) indicated for each word
   were identified so that alternative graphic(s) could be prepared
   accordingly. From one to five visual representations were identified for
   each word and the corresponding graphic symbols were drawn in electronic
   media. An e-measure was developed in order to find out whether these
   graphics were sufficient to represent the corresponding objects,
   concepts, or situations. The scale was sent to participant groups across
   Turkey to obtain the opinions of individuals from divergent age,
   culture, and educational backgrounds. A total of 1,099 participants were
   asked for their opinion. This resulted in a new system consisting of
   standard graphic symbol(s) for 843 words and seven forms of affix
   structures appropriate for writing Turkish using graphic symbols.</abstract><date>JAN 2016</date><author>Karal, Yasemin
   Karal, Hasan
   Silbir, Lokman
   Altun, Taner</author></paper><paper><title>Near Video-Rate Optical Coherence Elastography by Acceleration With a
   Graphics Processing Unit</title><abstract>We present a graphics processing unit (GPU)-accelerated optical
   coherence elastography (OCE) system capable of generating strain images
   (elastograms) of soft tissue at near video-rates. The system implements
   phase-sensitive compression OCE using a pipeline of GPU kernel functions
   to enable a highly parallel implementation of OCE processing using the
   OpenCL framework. Developed on a commercial-grade GPU and desktop
   computer, the system achieves a processing rate of 21 elastograms per
   second at an image size of 960 x 400 pixels, enabling high-rate
   visualization during acquisition. The system is demonstrated on both
   tissue-simulating phantoms and fresh ex vivo mouse muscle. To the best
   of our knowledge, this is the first implementation of near video-rate
   OCE and the fastest reported OCE processing rate, enabling, for the
   first time, a system capable of computing and displaying OCE elastograms
   interactively during acquisition. This advance provides new
   opportunities for medical imaging of soft tissue stiffness using optical
   methods.</abstract><date>AUG 15 2015</date><author>Kirk, Rodney W.
   Kennedy, Brendan F.
   Sampson, David D.
   McLaughlin, Robert A.</author></paper><paper><title>Design, synthesis and molecular docking analysis of some novel
   7-[(quinolin-6-yl)methyl] purines as potential c-Met inhibitors</title><abstract>HGF/c-Met signaling pathway has come into the spotlight as a promising
   therapeutic target for inhibiting tumor growth and has become one of the
   leading molecular targets in cancer. Various strategies are currently in
   development to disrupt the HGF-Met signal transduction pathway, in which
   small molecular inhibitors have been a particularly active field. On the
   basis of the structures of two c-Met inhibitors, PF-04217903 and
   JNJ-38877605, some novel 7-[(quinolin-6-yl)methyl] purines (4a-4e) were
   rationally designed on the principle of bioisosterism strategy. These
   compounds were synthesized and evaluated as novel c-Met inhibitors.
   Molecular docking experiments analyzed the results and explained the
   molecular mechanism of eminent activities to c-Met. The results showed
   that all the title compounds were active against c-Met enzyme to some
   extent. Though these compounds did not demonstrate inhibition as we
   expected, this study provided important information for building
   diversification of chemical library and molecular docking experiment
   supplied the basis for further research works.Some novel
   7-[(quinolin-6-yl)methyl] purines as potential c-Met inhibitors have
   been designed and prepared. Their synthesis and spectral
   characterization are reported here.[GRAPHICS].</abstract><date>AUG 2015</date><author>Ye, Lianbao
   Wu, Jie
   Yang, Jiebo
   Chen, Weiqiang
   Luo, Yan
   Zhang, Yanmei</author></paper><paper><title>GPU Accelerated Digital Volume Correlation</title><abstract>A sub-voxel digital volume correlation (DVC) method combining the 3D
   inverse compositional Gauss-Newton (ICGN) algorithm with the 3D fast
   Fourier transform-based cross correlation (FFT-CC) algorithm is proposed
   to eliminate path-dependence in current iterative DVC methods caused by
   the initial guess transfer scheme. The proposed path-independent DVC
   method is implemented on NVIDIA compute unified device architecture
   (CUDA) for GPU devices. Powered by parallel computing technology, the
   proposed DVC method achieves a significant improvement in computation
   speed on a common desktop computer equipped with a low-end graphics card
   containing 1536 CUDA cores, i.e., up to 23.3 times faster than the
   sequential implementation and 3.7 times faster than the multithreaded
   implementation of the same DVC method running on a 6-core CPU. This
   speedup, which has no compromise with resolution, accuracy and
   precision, benefits from the coarse-grained parallelism that the points
   of interest (POIs) are processed simultaneously and also from the
   fine-grained parallelism that the calculation at each POI is performed
   with multiple threads in GPU. The experimental study demonstrates the
   superiority of the GPU-based parallel computing for acceleration of DVC
   over the multi-core CPU-based one, in particular on a PC level computer.</abstract><date>FEB 2016</date><author>Wang, T.
   Jiang, Z.
   Kemao, Q.
   Lin, F.
   Soon, S. H.</author></paper><paper><title>Cell lineage visualisation</title><abstract>Cell lineages describe the developmental history of Cell populations and
   are produced by combining time-lapse imaging and image processing.
   Biomedical researchers study Cell lineages to understand fundamental
   processes such as Cell differentiation and the pharmacodynamic action of
   anticancer agents. Yet, the interpretation of Cell lineages is hindered
   by their complexity and insufficient capacity for visual analysis. We
   present a novel approach for interactive visualisation of Cell lineages.
   Based on an understanding of Cellular biology and live-Cell imaging
   methodology, we identify three requirements: multimodality (Cell
   lineages combine spatial, temporal, and other properties), symmetry
   (related to lineage branching structure), and synchrony (related to
   temporal alignment of Cellular events). We address these by combining
   visual summaries of the spatiotemporal behaviour of an arbitrary number
   of lineages, including variation from average behaviour, with node-link
   representations that emphasise the presence or absence of symmetry and
   synchrony. We illustrate the merit of our approach by presenting a
   real-world case study where the cytotoxic action of the anticancer drug
   topotecan was determined.</abstract><date>JUN 2015</date><author>Pretorius, A. J.
   Khan, I. A.
   Errington, R. J.</author></paper><paper><title>A parametric reflectance approximation for rendering Japanese
   lacquerware and Maki-e</title><abstract>This paper proposes a reflectance distribution model that is able to
   express complex reflections from materials such as Japanese lacquerware
   and Maki-e to produce highly realistic computer graphics. Our method
   improves the Ward model for materials with anisotropic reflection, and
   uses the approximate coefficients calculated from measured reflectance
   distribution data to reconstruct the actual reflectance. Our research
   derives a Gaussian distribution summation method and a modified Fresnel
   reflectance approximation function from measured reflectance data. Our
   Fresnel reflectance approximation function adds parameters to Schlick's
   approximation function to control the whole curve and extinction
   component, and is thus closer to the actual Fresnel reflectance. Based
   on these functions, a decision process is developed for the
   approximation coefficients, and this makes it possible to easily
   reconstruct the actual reflectance. The reconstructed reflectance
   distributions obtained using these coefficients are compared with
   measured reflectance data using the L2 norm of the difference. In
   addition, reconstructed Fresnel reflection effects are compared with
   those from actual sample materials. Using these coefficient values, this
   paper shows a sample of simulated Maki-e images under two different
   lighting environments.</abstract><date>NOV 2015</date><author>Yamaguchi, Satoshi</author></paper><paper><title>BMRF-Net: a software tool for identification of protein interaction
   subnetworks by a bagging Markov random field-based method</title><abstract>Identification of protein interaction subnetworks is an important step
   to help us understand complex molecular mechanisms in cancer. In this
   paper, we develop a BMRF-Net package, implemented in Java and C++, to
   identify protein interaction subnetworks based on a bagging Markov
   random field (BMRF) framework. By integrating gene expression data and
   protein-protein interaction data, this software tool can be used to
   identify biologically meaningful subnetworks. A user friendly graphic
   user interface is developed as a Cytoscape plugin for the BMRF-Net
   software to deal with the input/output interface. The detailed structure
   of the identified networks can be visualized in Cytoscape conveniently.
   The BMRF-Net package has been applied to breast cancer data to identify
   significant subnetworks related to breast cancer recurrence.</abstract><date>JUL 15 2015</date><author>Shi, Xu
   Barnes, Robert O.
   Chen, Li
   Shajahan-Haq, Ayesha N.
   Hilakivi-Clarke, Leena
   Clarke, Robert
   Wang, Yue
   Xuan, Jianhua</author></paper><paper><title>A high performance crashworthiness simulation system based on GPU</title><abstract>Crashworthiness simulation system is one of the key computer-aided
   engineering (CAE) tools for the automobile industry and implies two
   potential conflicting requirements: accuracy and efficiency. A parallel
   crashworthiness simulation system based on graphics processing unit
   (GPU) architecture and the explicit finite element (FE) method is
   developed in this work. Implementation details with compute unified
   device architecture (CUDA) are considered. The entire parallel
   simulation system involves a parallel hierarchy-territory
   contact-searching algorithm (HITA) and a parallel penalty contact force
   calculation algorithm. Three basic GPU-based parallel strategies are
   suggested to meet the natural parallelism of the explicit FE algorithm.
   Two free GPU-based numerical calculation libraries, cuBLAS and Thrust,
   are introduced to decrease the difficulty of programming. Furthermore, a
   mixed array and a thread map to element strategy are proposed to improve
   the performance of the test pairs searching. The outer loop of the
   nested loop through the mixed array is unrolled to realize parallel
   searching. An efficient storage strategy based on data sorting is
   presented to realize data transfer between different hierarchies with
   coalesced access during the contact pairs searching. A thread map to
   element pattern is implemented to calculate the penetrations and the
   penetration forces; a double float atomic operation is used to scatter
   contact forces. The simulation results of the three different models
   based on the Intel Core i7-930 and the NVIDIA GeForce GTX 580
   demonstrate the precision and efficiency of this developed parallel
   crashworthiness simulation system. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>AUG 2015</date><author>Cai, Yong
   Wang, Guoping
   Li, Guangyao
   Wang, Hu</author></paper><paper><title>A Six-Year Longitudinal Evaluation of the DICOM GSDF Conformance
   Stability of LCD Monitors</title><abstract></abstract><date>JUN 2015</date><author>McKenney, S.
   Bevins, N.
   Olariu, E.
   Flynn, M.</author></paper><paper><title>Using Presentation Software To Flip an Undergraduate Analytical
   Chemistry Course</title><abstract>An undergraduate analytical chemistry course has been adapted to a
   flipped course format. Course content was provided by video clips, text,
   graphics, audio, and simple animations organized as concept maps using
   the cloud-based presentation platform, Prezi. The advantages of using
   Prezi to present course content in a flipped course format are
   discussed. Results of an American Chemical Society analytical chemistry
   examination were encouraging. Results of pre- and postsurveys of student
   perception of their learning are summarized.</abstract><date>SEP 2015</date><author>Fitzgerald, Neil
   Li, Luisa</author></paper><paper><title>HapFACS 3.0: FACS-Based Facial Expression Generator for 3D Speaking
   Virtual Characters</title><abstract>With the growing number of researchers interested in modeling the inner
   workings of affective social intelligence, the need for tools to easily
   model its associated expressions has emerged. The goal of this article
   is two-fold: 1) we describe HapFACS, a free software and API that we
   developed to provide the affective computing community with a resource
   that produces static and dynamic facial expressions for
   three-dimensional speaking characters; and 2) we discuss results of
   multiple experiments that we conducted in order to scientifically
   validate our facial expressions and head animations in terms of the
   widely accepted Facial Action Coding System (FACS) standard, and its
   Action Units (AU). The result is that users, without any 3D-modeling nor
   computer graphics expertise, can animate speaking virtual characters
   with FACS-based realistic facial expression animations, and embed these
   expressive characters in their own application(s). The HapFACS software
   and API can also be used for generating repertoires of realistic
   FACS-validated facial expressions, useful for testing emotion expression
   generation theories.</abstract><date>OCT-DEC 2015</date><author>Amini, Reza
   Lisetti, Christine
   Ruiz, Guido</author></paper><paper><title>Intuitive and Efficient Camera Control with the Toric Space</title><abstract>A large range of computer graphics applications such as data
   visualization or virtual movie production require users to position and
   move viewpoints in 3D scenes to effectively convey visual information or
   tell stories. The desired viewpoints and camera paths are required to
   satisfy a number of visual properties (e.g. size, vantage angle,
   visibility, and on-screen position of targets). Yet, existing camera
   manipulation tools only provide limited interaction methods and
   automated techniques remain computationally expensive.In this work, we
   introduce the Toric space, a novel and compact representation for
   intuitive and efficient virtual camera control. We first show how visual
   properties are expressed in this Toric space and propose an efficient
   interval-based search technique for automated viewpoint computation. We
   then derive a novel screen-space manipulation technique that provides
   intuitive and real-time control of visual properties. Finally, we
   propose an effective viewpoint interpolation technique which ensures the
   continuity of visual properties along the generated paths. The proposed
   approach (i) performs better than existing automated viewpoint
   computation techniques in terms of speed and precision, (ii) provides a
   screen-space manipulation tool that is more efficient than classical
   manipulators and easier to use for beginners, and (iii) enables the
   creation of complex camera motions such as long takes in a very short
   time and in a controllable way. As a result, the approach should quickly
   find its place in a number of applications that require interactive or
   automated camera control such as 3D modelers, navigation tools or 3D
   games.</abstract><date>AUG 2015</date><author>Lino, Christophe
   Christie, Marc</author></paper><paper><title>Accurate calculation of computer-generated holograms using
   angular-spectrum layer-oriented method</title><abstract>Fast calculation and correct depth cue are crucial issues in the
   calculation of computer-generated hologram (CGH) for high quality
   three-dimensional (3-D) display. An angular-spectrum based algorithm for
   layer-oriented CGH is proposed. Angular spectra from each layer are
   synthesized as a layer-corresponded sub-hologram based on the fast
   Fourier transform without paraxial approximation. The proposed method
   can avoid the huge computational cost of the point-oriented method and
   yield accurate predictions of the whole diffracted field compared with
   other layer-oriented methods. CGHs of versatile formats of 3-D digital
   scenes, including computed tomography and 3-D digital models, are
   demonstrated with precise depth performance and advanced image quality.
   (C) 2015 Optical Society of America</abstract><date>OCT 5 2015</date><author>Zhao, Yan
   Cao, Liangcai
   Zhang, Hao
   Kong, Dezhao
   Jin, Guofan</author></paper><paper><title>A Microscopic Optically Tracking Navigation System That Uses
   High-resolution 3D Computer Graphics</title><abstract>Three-dimensional (3D) computer graphics (CG) are useful for
   preoperative planning of neurosurgical operations. However, application
   of 3D CG to intraoperative navigation is not widespread because existing
   commercial operative navigation systems do not show 3D CG in sufficient
   detail. We have developed a microscopic optically tracking navigation
   system that uses high-resolution 3D CG. This article presents the
   technical details of our microscopic optically tracking navigation
   system. Our navigation system consists of three components: the
   operative microscope, registration, and the image display system. An
   optical tracker was attached to the microscope to monitor the position
   and attitude of the microscope in real time; point-pair registration was
   used to register the operation room coordinate system, and the image
   coordinate system; and the image display system showed the 3D CG image
   in the field-of-view of the microscope. Ten neurosurgeons (seven males,
   two females; mean age 32.9 years) participated in an experiment to
   assess the accuracy of this system using a phantom model. Accuracy of
   our system was compared with the commercial system. The 3D CG provided
   by the navigation system coincided well with the operative scene under
   the microscope. Target registration error for our system was 2.9 +/- 1.9
   mm. Our navigation system provides a clear image of the operation
   position and the surrounding structures. Systems like this may reduce
   intraoperative complications.</abstract><date>AUG 2015</date><author>Yoshino, Masanori
   Saito, Toki
   Kin, Taichi
   Nakagawa, Daichi
   Nakatomi, Hirofumi
   Oyama, Hiroshi
   Saito, Nobuhito</author></paper><paper><title>Multi-scale Visualization of Molecular Architecture Using Real-Time
   Ambient Occlusion in Sculptor</title><abstract>The modeling of large biomolecular assemblies relies on an efficient
   rendering of their hierarchical architecture across a wide range of
   spatial level of detail. We describe a paradigm shift currently under
   way in computer graphics towards the use of more realistic global
   illumination models, and we apply the so-called ambient occlusion
   approach to our opensource multi-scale modeling program, Sculptor. While
   there are many other higher quality global illumination approaches going
   all the way up to full GPU-accelerated ray tracing, they do not provide
   size-specificity of the features they shade. Ambient occlusion is an
   aspect of global lighting that offers great visual benefits and powerful
   user customization. By estimating how other molecular shape features
   affect the reception of light at some surface point, it effectively
   simulates indirect shadowing. This effect occurs between molecular
   surfaces that are close to each other, or in pockets such as protein or
   ligand binding sites. By adding ambient occlusion, large macromolecular
   systems look much more natural, and the perception of characteristic
   surface features is strongly enhanced. In this work, we present a
   realtime implementation of screen space ambient occlusion that delivers
   realistic cues about tunable spatial scale characteristics of
   macromolecular architecture. Heretofore, the visualization of large
   biomolecular systems, comprising e.g. hundreds of thousands of atoms or
   Mega-Dalton size electron microscopy maps, did not take into account the
   length scales of interest or the spatial resolution of the data. Our
   approach has been uniquely customized with shading that is tuned for
   pockets and cavities of a user-defined size, making it useful for
   visualizing molecular features at multiple scales of interest. This is a
   feature that none of the conventional ambient occlusion approaches
   provide. Actual Sculptor screen shots illustrate how our implementation
   supports the size-dependent rendering of molecular surface features.</abstract><date>OCT 2015</date><author>Wahle, Manuel
   Wriggers, Willy</author></paper><paper><title>Digitalized accurate modeling of SPCB with multi-spiral surface based on
   CPC algorithm</title><abstract>The main methods of the existing multi-spiral surface geometry modeling
   include spatial analytic geometry algorithms, graphical method,
   interpolation and approximation algorithms. However, there are some
   shortcomings in these modeling methods, such as large amount of
   calculation, complex process, visible errors, and so on. The above
   methods have, to some extent, restricted the design and manufacture of
   the premium and high-precision products with spiral surface
   considerably. This paper introduces the concepts of the spatially
   parallel coupling with multi-spiral surface and spatially parallel
   coupling body. The typical geometry and topological features of each
   spiral surface forming the multi-spiral surface body are determined, by
   using the extraction principle of datum point cluster, the algorithm of
   coupling point cluster by removing singular point, and the "spatially
   parallel coupling" principle based on the non-uniform B-spline for each
   spiral surface. The orientation and quantitative relationships of datum
   point cluster and coupling point cluster in Euclidean space are
   determined accurately and in digital description and expression,
   coupling coalescence of the surfaces with multi-coupling point clusters
   under the Pro/E environment. The digitally accurate modeling of
   spatially parallel coupling body with multi-spiral surface is realized.
   The smooth and fairing processing is done to the three-blade end-milling
   cutter's end section area by applying the principle of spatially
   parallel coupling with multi-spiral surface, and the alternative entity
   model is processed in the four axis machining center after the end mill
   is disposed. And the algorithm is verified and then applied effectively
   to the transition area among the multi-spiral surface. The proposed
   model and algorithms may be used in design and manufacture of the
   multi-spiral surface body products, as well as in solving essentially
   the problems of considerable modeling errors in computer graphics and
   engineering in multi-spiral surface's connection available with
   approximate methods or graphical methods.</abstract><date>SEP 2015</date><author>Huang Yanhua
   Gu Lizhi</author></paper><paper><title>Nonlinear Dynamic Analysis Efficiency by Using a GPU Parallelization</title><abstract>A graphics processing unit (GPU) parallelization approach was
   implemented to improve the efficiency of nonlinear dynamic analysis. The
   GPU parallelization approach speeded up the computation of implicit time
   integration and reduced total calculation time. In addition, a parallel
   equations solver is introduced to solve the equation system. Numerical
   examples of reinforced concrete (RC) frames were used to investigate the
   parallel computing speedup of the GPU parallelization approach. An
   implementation of these RC frame models for fiber beam-column elements
   was presented. The parallel finite element program is developed to
   provide parallel execution on personal computer (PC) with different
   CUDA-capable GPUs. The different number of degrees of freedom from low
   to high was adopted in the numerical examples. Detailed tests on
   accuracy, runtime, and speedup are conducted on different GPUs. The
   nonlinear dynamic response using the GPU parallelization program was in
   good agreement with that obtained by ABAQUS. Numerical studies indicate
   that compared with original sequential approach, the GPU parallelization
   program achieves a 22 times speedups of the solving equation system and
   improves the overall efficiency of time integration by up to 94%.</abstract><date>NOV 17 2015</date><author>Li, Hong-yu
   Teng, Jun
   Li, Zuo-hua
   Zhang, Lu</author></paper><paper><title>The VideoMob Interactive Art Installation Connecting Strangers through
   Inclusive Digital Crowds</title><abstract>VideoMob is an interactive video platform and an artwork that enables
   strangers visiting different installation locations to interact across
   time and space through a computer interface that detects their presence,
   video-records their actions while automatically removing the video
   background through computer vision, and co-situates visitors as part of
   the same digital environment. Through the combination of individual user
   videos to form a digital crowd, strangers are connected through the
   graphic display. Our work is inspired by the way distant people can
   interact with each other through technology and influenced by artists
   working in the realm of interactive art. We deployed VideoMob in a
   variety of settings, locations, and contexts to observe hundreds of
   visitors' reactions. By analyzing behavioral data collected through
   depth cameras from our 1,068 recordings across eight venues, we studied
   how participants behave when given the opportunity to record their own
   video portrait into the artwork. We report the specific activity
   performed in front of the camera and the influences that existing crowds
   impose on new participants. Our analysis informs the integration of a
   series of possible novel interaction paradigms based on real-time
   analysis of the visitors' behavior through specific computer vision and
   machine learning techniques that have the potential to increase the
   engagement of the artwork's visitors and to impact user experience.</abstract><date>JUL 2015</date><author>Grenader, Emily
   Rodrigues, Danilo Gasques
   Nos, Fernando
   Weibel, Nadir</author></paper><paper><title>Drawings of the hand and numerical skills in children of preschool age</title><abstract>This study aims at studying the relationship between manual and digital
   representations and numerical skills through the drawing of a hand.
   Sixty-two children (ages from 4 years old to 5 years and 6 months old)
   were asked to draw a hand and to carry out 2 numerical tasks: first to
   produce 2 equivalent collections and then to use the order-irrelevance
   principle. The statistical implicative analysis shows that more
   elaborated graphic strategies more strongly imply success producing 2
   equivalent collections.</abstract><date>JUL 2015</date><author>Bonneton-Botte, Nathalie
   Hili, Helene
   De La Haye, Fanny
   Noel, Yvonnick</author></paper><paper><title>Assessment of the Renal Function in Potential Donors of Living Kidney
   Transplants: Expanded Study</title><abstract>Introduction. It is very important to determine as accurately as
   possible the renal function in potential living renal transplant donors,
   especially those with limited renal function (CrCl &lt;90 mL/m/1.73 m(2)),
   age older than 50 years, and cardiovascular risk factors that might
   favor the development of long-term kidney diseases.Objective. The
   objective of this study was to compare the direct measurement of
   glomerular filtration rate (GFR) using EDTA-Cr51 and the estimations
   based on creatinine (eGFR): Cr clearance (CCr) with 24-hour urine and
   estimated using Cockroft-Gault (adjusted by using body surface
   area-Mosteller formula-SC), MDRD-4, MDRD-6, and CKD-EPI to determine the
   usefulness of different methods from EDTA-Cr51 to evaluate the kidney
   function.Patients and Methods. The kidney function evaluation has been
   made to 105 potential kidney donors using the EDTA-Cr51 method. The GFR
   obtained through the EDTA-Cr51 is compared with the CCr values in
   24-hour urine and eGFR based on creatinine (Cockcroft-Gault, MDRD4,
   MDRD6, and CKD-EPI).Results. Using the Bland Altman graphic we have
   observed that the most dispersed results are obtained with the eGFR
   using CCr in 24-hour urine and CKD-EPI. By means of Pasing &amp; Bablock, we
   realized that MDRD-4 and MDRD-6 show the highest approximation to the
   reference method proposed to be substituted, whereas CCr shows a high
   dispersion.Conclusions. eGFR using MDRD-4 and MDRD-6 formulas reveal the
   best adjustment to the measure by EDTA-Cr51. This might represent the
   best option if a direct eGFR measure is not available.</abstract><date>NOV 2015</date><author>Macias, L. B.
   Poblet, M. S.
   Perez, N. N.
   Jerez, R. I.
   Gonzalez Roncero, F. M.
   Blanco, G. B.
   Valdivia, M. A. P.
   Benjumea, A. S.
   Gentil Govantes, M. A.</author></paper><paper><title>Automatic analysis and recognition of graphical content in SVG-based
   engineering documents</title><abstract>Graphical engineering documents embody a fundamental piece of
   information used by operators and process experts in every-day plant
   activities. Such documents have been created with computers already for
   decades. In most industrial facilities, however, engineering
   documentation is maintained on paper or in elementary digital formats,
   which prevents their effective management and automatic exploitation by
   control and enterprise systems. In an effort to cope with such
   limitations, this paper presents a novel method for the automatic
   analysis and computer-interpretable description of plant engineering
   documents which are available as Scalable Vector Graphics (SVG).</abstract><date>FEB 2016</date><author>Hoang, Xuan Luu
   Arroyo, Esteban
   Fay, Alexander</author></paper><paper><title>No Sexual Dimorphism Detected in Digit Ratios of the Fire Salamander
   (Salamandra salamandra)</title><abstract>It has been proposed that digit ratio may be used as a biomarker of
   early developmental effects. Specifically, the second-to-fourth digit
   ratio (2D:4D) has been linked to the effects of sex hormones and their
   receptor genes, but other digit ratios have also been investigated.
   Across taxa, patterns of sexual dimorphism in digit ratios are ambiguous
   and a scarcity of studies in basal tetrapods makes it difficult to
   understand how ratios have evolved. Here, we focus on examining sex
   differences in digit ratios (2D:3D, 2D:4D, and 3D:4D) in a common
   amphibian, the fire salamander (Salamandra salamandra). We used graphic
   software to measure soft tissue digit length and digit bone length from
   X-rays. We found a nonsignificant tendency in males to have a lower
   2D:3D than females; however, no sexual differences were detected in the
   other ratios. We discuss our results in the context of other studies of
   digit ratios, and how sex determination systems, as well as other
   factors, might impact patterns of sexual dimorphism, particularly in
   reptiles and in amphibians. Our findings suggest that caution is needed
   when using digit ratios as a potential indicator of prenatal hormonal
   effects in amphibians and highlight the need for more comparative
   studies to elucidate the evolutionary and genetic mechanisms implicated
   in sexually dimorphic patterns across taxonomic groups. Anat Rec,
   298:1786-1795, 2015. (c) 2015 Wiley Periodicals, Inc.</abstract><date>OCT 2015</date><author>Balogova, Monika
   Nelson, Emma
   Uhrin, Marcel
   Figurova, Maria
   Ledecky, Valent
   Zysk, Bartlomiej</author></paper><paper><title>A new possibility in thoracoscopic virtual reality simulation training:
   development and testing of a novel virtual reality simulator for
   video-assisted thoracoscopic surgery lobectomy</title><abstract>OBJECTIVES: The aims of this study were to develop virtual reality
   simulation software for video-assisted thoracic surgery (VATS)
   lobectomy, to explore the opinions of thoracic surgeons concerning the
   VATS lobectomy simulator and to test the validity of the simulator
   metrics.METHODS: Experienced VATS surgeons worked with computer
   specialists to develop a VATS lobectomy software for a virtual reality
   simulator. Thoracic surgeons with different degrees of experience in
   VATS were enrolled at the 22nd meeting of the European Society of
   Thoracic Surgeons (ESTS) held in Copenhagen in June 2014. The surgeons
   were divided according to the number of performed VATS lobectomies:
   novices (0 VATS lobectomies), intermediates (1-49 VATS lobectomies) and
   experienced (&gt;50 VATS lobectomies). The participants all performed a
   lobectomy of a right upper lobe on the simulator and answered a
   questionnaire regarding content validity. Metrics were compared between
   the three groups.RESULTS: We succeeded in developing the first version
   of a virtual reality VATS lobectomy simulator. A total of 103 thoracic
   surgeons completed the simulated lobectomy and were distributed as
   follows: novices n = 32, intermediates n = 45 and experienced n = 26.
   All groups rated the overall user realism of the VATS lobectomy scenario
   to a median of 5 on a scale 1-7, with 7 being the best score. The
   experienced surgeons found the graphics and movements realistic and
   rated the scenario high in terms of usefulness as a training tool for
   novice and intermediate experienced thoracic surgeons, but not very
   useful as a training tool for experienced surgeons. The metric scores
   were not statistically significant between groups.CONCLUSIONS: This is
   the first study to describe a commercially available virtual reality
   simulator for a VATS lobectomy. More than 100 thoracic surgeons found
   the simulator realistic, and hence it showed good content validity.
   However, none of the built-in simulator metrics could significantly
   distinguish between novice, intermediate experienced and experienced
   surgeons, and further development of the simulator software is necessary
   to develop valid metrics.</abstract><date>OCT 2015</date><author>Jensen, Katrine
   Bjerrum, Flemming
   Hansen, Henrik Jessen
   Petersen, Rene Horsleben
   Pedersen, Jesper Holst
   Konge, Lars</author></paper><paper><title>Stochastic Soft Shadow Mapping</title><abstract>In this paper, we extend the concept of pre-filtered shadow mapping to
   stochastic rasterization, enabling real-time rendering of soft shadows
   from planar area lights. Most existing soft shadow mapping methods lose
   important visibility information by relying on pinhole renderings from
   an area light source, providing plausible results only for small light
   sources. Since we sample the entire 4D shadow light field
   stochastically, we are able to closely approximate shadows of large area
   lights as well. In order to efficiently reconstruct smooth shadows from
   this sparse data, we exploit the analogy of soft shadow computation to
   rendering defocus blur, and introduce a multiplane pre-filtering
   algorithm. We demonstrate how existing pre-filterable approximations of
   the visibility function, such as variance shadow mapping, can be
   extended to four dimensions within our framework.</abstract><date>JUL 2015</date><author>Liktor, G.
   Spassov, S.
   Mueckl, G.
   Dachsbacher, C.</author></paper><paper><title>Immersive virtual reality to vindicate the application of value stream
   mapping in an US-based SME</title><abstract>Value stream mapping (VSM) assists in identifying opportunities for
   improvement by revealing the inefficiencies in the current state.
   However, several difficulties appear while evaluating such "as-is" state
   for leaner future state. Trial and error method is often employed for
   continuous improvement to accomplish the desired level of future state.
   This causes numerous iterations and improper usage of resources which
   makes lean application costly and inefficient. In order to tackle this,
   an immersive virtual reality (IVR) approach to visualize and interact
   with the image of real models in a computer graphics environment is
   presented in this article. This allows conducting a quick
   experimentation in a virtual world to reach optimal future state without
   exhausting resources or incurring additional cost. In order to reinforce
   applicability and usefulness of the proposed framework, a case study of
   an US-based SME is also discussed. This paper first illustrates the
   implementation procedure of VSM in the manufacturing processes to
   develop current and future states. Data is collected for a year to
   analyse the current state and then IVR is used to validate results for
   future state. A reduction of more than 40 % in lead-time, 41 % in floor
   space and 47 % in manpower is achieved after a period of 3 months of
   implementing the recommendations.</abstract><date>NOV 2015</date><author>Tyagi, Satish
   Vadrevu, Sarat</author></paper><paper><title>Comparison of three artificial models of the magnetohydrodynamic effect
   on the electrocardiogram</title><abstract>The electrocardiogram (ECG) is often acquired during magnetic resonance
   imaging (MRI), but its analysis is restricted by the presence of a
   strong artefact, called magnetohydrodynamic (MHD) effect. MHD effect is
   induced by the flow of electrically charged particles in the blood
   perpendicular to the static magnetic field, which creates a potential of
   the order of magnitude of the ECG and temporally coincident with the
   repolarisation period. In this study, a new MHD model is proposed by
   using MRI-based 4D blood flow measurements made across the aortic arch.
   The model is extended to several cardiac cycles to allow the simulation
   of a realistic ECG acquisition during MRI examination and the quality
   assessment of MHD suppression techniques. A comparison of two existing
   models, based, respectively, on an analytical solution and on a
   numerical method-based solution of the fluids dynamics problem, is made
   with the proposed model and with an estimate of the MHD voltage observed
   during a real MRI scan. Results indicate a moderate agreement between
   the proposed model and the estimated MHD model for most leads, with an
   average correlation factor of 0.47. However, the results demonstrate
   that the proposed model provides a closer approximation to the observed
   MHD effects and a better depiction of the complexity of the MHD effect
   compared with the previously published models, with an improved
   correlation ([GRAPHICS]), coefficient of determination ([GRAPHICS]) and
   fraction of energy ([GRAPHICS]) compared with the best previous model.
   The source code will be made freely available under an open source
   licence to facilitate collaboration and allow more rapid development of
   more accurate models of the MHD effect.</abstract><date>OCT 3 2015</date><author>Oster, Julien
   Llinares, Raul
   Payne, Stephen
   Tse, Zion Tsz Ho
   Schmidt, Ehud Jeruham
   Clifford, Gari D.</author></paper><paper><title>ChIPseeker: an R/Bioconductor package for ChIP peak annotation,
   comparison and visualization</title><abstract>ChIPseeker is an R package for annotating ChIP-seq data analysis. It
   supports annotating ChIP peaks and provides functions to visualize ChIP
   peaks coverage over chromosomes and profiles of peaks binding to TSS
   regions. Comparison of ChIP peak profiles and annotation are also
   supported. Moreover, it supports evaluating significant overlap among
   ChIP-seq datasets. Currently, ChIPseeker contains 15 000 bed file
   information from GEO database. These datasets can be downloaded and
   compare with user's own data to explore significant overlap datasets for
   inferring co-regulation or transcription factor complex for further
   investigation.</abstract><date>JUL 15 2015</date><author>Yu, Guangchuang
   Wang, Li-Gen
   He, Qing-Yu</author></paper><paper><title>Effects of multispectral excitation on the sensitivity of molecular
   optoacoustic imaging</title><abstract>Molecular optoacoustic (photoacoustic) imaging typically relies on the
   spectral identification of absorption signatures from molecules of
   interest. To achieve this, two or more excitation wavelengths are
   employed to sequentially illuminate tissue. Due to depth-related
   spectral dependencies and detection related effects, the multispectral
   optoacoustic tomography (MSOT) spectral unmixing problem presents a
   complex non-linear inversion operation. So far, different studies have
   showcased the spectral capacity of optoacoustic imaging, without however
   relating the performance achieved to the number of wavelengths employed.
   Overall, the dependence of the sensitivity and accuracy of optoacoustic
   imaging as a function of the number of illumination wavelengths has not
   been so far comprehensively studied. In this paper we study the impact
   of the number of excitation wavelengths employed on the sensitivity and
   accuracy achieved by molecular optoacoustic tomography. We present a
   quantitative analysis, based on synthetic MSOT datasets and observe a
   trend of sensitivity increase for up to 20 wavelengths. Importantly we
   quantify this relation and demonstrate an up to an order of magnitude
   sensitivity increase of multi-wavelength illumination vs. single or dual
   wavelength optoacoustic imaging. Examples from experimental animal
   studies are finally utilized to support the findings.[GRAPHICS]In vivo
   MSOT imaging of a mouse brain bearing a tumor that is expressing a
   near-infrared fluorescent protein. (a) Monochromatic optoacoustic
   imaging at the peak excitation wavelength of the fluorescent protein.
   (b) Overlay of the detected bio-distribution of the protein (red
   pseudocolor) on the monochromatic optoacoustic image. (c) Ex vivo
   validation by means of cryoslicing fluorescence imaging.</abstract><date>AUG 2015</date><author>Tzoumas, Stratis
   Nunes, Antonio
   Deliolanis, Nikolaos C.
   Ntziachristos, Vasilis</author></paper><paper><title>PhyloGene server for identification and visualization of co-evolving
   proteins using normalized phylogenetic profiles</title><abstract>Proteins that function in the same pathways, protein complexes or the
   same environmental conditions can show similar patterns of sequence
   conservation across phylogenetic clades. In species that no longer
   require a specific protein complex or pathway, these proteins, as a
   group, tend to be lost or diverge. Analysis of the similarity in
   patterns of sequence conservation across a large set of eukaryotes can
   predict functional associations between different proteins, identify new
   pathway members and reveal the function of previously uncharacterized
   proteins. We used normalized phylogenetic profiling to predict protein
   function and identify new pathway members and disease genes. The
   phylogenetic profiles of tens of thousands conserved proteins in the
   human, mouse, Caenorhabditis elegans and Drosophila genomes can be
   queried on our new web server, PhyloGene. PhyloGene provides intuitive
   and user-friendly platform to query the patterns of conservation across
   86 animal, fungal, plant and protist genomes. A protein query can be
   submitted either by selecting the name from whole-genome protein sets of
   the intensively studied species or by entering a protein sequence. The
   graphic output shows the profile of sequence conservation for the query
   and the most similar phylogenetic profiles for the proteins in the
   genome of choice. The user can also download this output in numerical
   form.</abstract><date>JUL 1 2015</date><author>Sadreyev, Ilyas R.
   Ji, Fei
   Cohen, Emiliano
   Ruvkun, Gary
   Tabach, Yuval</author></paper><paper><title>Scalability of 3D deterministic particle transport on the Intel MIC
   architecture</title><abstract>The key to large-scale parallel solutions of deterministic particle
   transport problem is single-node computation performance. Hence,
   single-node computation is often parallelized on multi-core or many-core
   computer architectures. However, the number of on-chip cores grows
   quickly with the scale-down of feature size in semiconductor technology.
   In this paper, we present a scalability investigation of one energy
   group time-independent deterministic discrete ordinates neutron
   transport in 3D Cartesian geometry (Sweep3D) on Intel's Many Integrated
   Core (MIC) architecture, which can provide up to 62 cores with four
   hardware threads per core now and will own up to 72 in the future. The
   parallel programming model, OpenMP, and vector intrinsic functions are
   used to exploit thread parallelism and vector parallelism for the
   discrete ordinates method, respectively. The results on a 57-core MIC
   coprocessor show that the implementation of Sweep3D on MIC has good
   scalability in performance. In addition, the application of the Roofline
   model to assess the implementation and performance comparison between
   MIC and Tesla K20C Graphics Processing Unit (GPU) are also reported.</abstract><date>OCT 2015</date><author>Wang Qing-Lin
   Liu Jie
   Gong Chun-Ye
   Xing Zuo-Cheng</author></paper><paper><title>Computer implementations of iterative and non-iterative crystal
   plasticity solvers on high performance graphics hardware</title><abstract>We present parallel implementations of Newton-Raphson iterative and
   spectral based non-iterative solvers for single-crystal visco-plasticity
   models on a specialized computer hardware integrating a
   graphics-processing unit (GPU). We explore two implementations for the
   iterative solver on GPU multiprocessors: one based on a thread per
   crystal parallelization on local memory and another based on multiple
   threads per crystal on shared memory. The non-iterative solver
   implementation on the GPU hardware is based on a divide-conquer approach
   for matrix operations. The reduction of computational time for the
   iterative scheme was found to approach one order of magnitude. From
   detailed performance comparisons of the developed GPU iterative and
   non-iterative implementations, we conclude that the spectral
   non-iterative solver programed on a GPU platform is superior over the
   iterative implementation in terms of runtime as well as ease of
   implementation. It provides remarkable speedup factors exceeding three
   orders of magnitude over the iterative scalar version of the solver.</abstract><date>OCT 2015</date><author>Savage, Daniel J.
   Knezevic, Marko</author></paper><paper><title>Wanderer, an interactive viewer to explore DNA methylation and gene
   expression data in human cancer</title><abstract>Background: The Cancer Genome Atlas (TCGA) offers a multilayered view of
   genomics and epigenomics data of many human cancer types. However, the
   retrieval of expression and methylation data from TCGA is a cumbersome
   and time-consuming task.Results: Wanderer is an intuitive Web tool
   allowing real time access and visualization of gene expression and DNA
   methylation profiles from TCGA. Given a gene query and selection of a
   TCGA dataset (e.g., colon adenocarcinomas), the Web resource provides
   the expression profile, at the single exon level, and the DNA
   methylation levels of HumanMethylation450 BeadChip loci inside or in the
   vicinity of the queried gene. Graphic and table outputs include
   individual and summary data as well as statistical tests, allowing the
   comparison of tumor and normal profiles and the exploration along the
   genomic locus and across tumor collections.Conclusions: Wanderer offers
   a simple interface to straightforward access to TCGA data, amenable to
   experimentalists and clinicians without bioinformatics expertise.
   Wanderer may be accessed at http://maplab.cat/wanderer.</abstract><date>JUN 23 2015</date><author>Diez-Villanueva, Anna
   Mallona, Izaskun
   Peinado, Miguel A.</author></paper><paper><title>An instance selection method for large datasets based on Markov
   Geometric Diffusion</title><abstract>Given the growing amount of data produced from within different areas of
   knowledge, data mining methods currently have to face challenging
   datasets with greater numbers of instances and attributes. However, the
   processing capacity of data mining algorithms is struggling under this
   growth. One alternative for tackling the problem is to perform instance
   selection on the data in order to reduce its size, as a preprocessing
   step for data mining algorithms.This study presents e-MGD, a method for
   instance selection as an extension of the Markov Geometric Diffusion
   method, which is a linear complexity method used in computer graphics
   for the simplification of triangular meshes. The original method was
   extended so that it was capable of reducing datasets commonly found in
   the field of data mining. For this purpose, two essential points of
   adjustment were required. Firstly, it was necessary to build a geometric
   structure from the data and secondly, to adjust the method so that it
   could deal with types of attributes encountered within these datasets.
   These adjustments however, did not influence the complexity of the final
   e-MGD, since it remained linear, which enabled it to be applied to
   datasets with a greater number of instances and features. One distinct
   characteristic of the proposed extension is that it focuses on
   preserving dataset information rather than improving classification
   accuracy, as in the case of most instance selection methods.In order to
   assess the performance of the method, we compared it with a number of
   classical and contemporary instance selection methods using medium to
   large datasets, plus a further set of very large datasets. The results
   demonstrated a good performance in terms of classification accuracy when
   compared to results from other methods, indicating that the e-MGD is a
   good alternative for instance selection. (C) 2015 Elsevier B.V. All
   rights reserved.</abstract><date>JAN 2016</date><author>Silva, Duilio A. N. S.
   Souza, Leandro C.
   Motta, Gustavo H. M. B.</author></paper><paper><title>Generating Grinding Profile between Screw Rotor and Forming Tool by
   Digital Graphic Scanning (DGS) Method</title><abstract>This work presents a digital graphic scanning (DGS) method, based on
   computer scanning graphics, to generate a grinding profile avoiding the
   difficulties appeared from the complex equations of the contact line.
   First the enveloping surface between the forming tool (rotor) profile
   and its corresponding cutting locus was developed, then based on
   Bresenham algorithm, the best possible pixels of the enveloping surface
   in the pixel matrix of screen were demonstrated using a specified color
   Finally, the grinding profile data of the rotor (forming tool) were
   collected by scanning the pixel matrix of screen, capturing the
   coordinates of the indicated color of the best possible pixels.
   Comparing the analytical gearing envelope method and the DGS method, the
   feasibility of the DGS method was indicated The DGS method was shown as
   a precise, rapid, efficient and stable computing tool to generate a
   grinding profile. In addition, such an approach can be applied in
   designing other similarly conjugated products such as gears, perpetual
   screws and milling cutters.</abstract><date>JAN 2016</date><author>Shen, Zhihuang
   Yao, Bin
   Teng, Weibin
   Feng, Wei
   Sun, Weifang</author></paper><paper><title>Adaptive GPU-accelerated force calculation for interactive rigid
   molecular docking using haptics</title><abstract>Molecular docking systems model and simulate in silico the interactions
   of intermolecular binding. Haptics-assisted docking enables the user to
   interact with the simulation via their sense of touch but a stringent
   time constraint on the computation of forces is imposed due to the
   sensitivity of the human haptic system. To simulate high fidelity smooth
   and stable feedback the haptic feedback loop should run at rates of 500
   Hz to 1 kHz. We present an adaptive force calculation approach that can
   be executed in parallel on a wide range of Graphics Processing Units
   (GPUs) for interactive haptics-assisted docking with wider applicability
   to molecular simulations. Prior to the interactive session either a
   regular grid or an octree is selected according to the available GPU
   memory to determine the set of interatomic interactions within a cutoff
   distance. The total force is then calculated from this set. The approach
   can achieve force updates in less than 2 ms for molecular structures
   comprising hundreds of thousands of atoms each, with performance
   improvements of up to 90 times the speed of current CPU-based force
   calculation approaches used in interactive docking. Furthermore, it
   overcomes several computational limitations of previous approaches such
   as pre-computed force grids, and could potentially be used to model
   receptor flexibility at haptic refresh rates. (C) 2015 Elsevier Inc. All
   rights reserved.</abstract><date>SEP 2015</date><author>Iakovou, Georgios
   Hayward, Steven
   Laycock, Stephen D.</author></paper><paper><title>Ocular Defect Rehabilitation Using Photography and Digital Imaging: A
   Clinical Report</title><abstract>Ocular disorders occasionally necessitate surgical intervention that may
   lead to eye defects. The primary objective in restoring and
   rehabilitating such defects with an ocular prosthesis is to enable
   patients to cope better with associated psychological stress and to
   return to their accustomed lifestyle. A series of detailed steps for
   custom-made ocular prosthesis fabrication using the advantages of
   digital photography to replace the conventional oil paint and monopoly
   iris painting technique are presented in this article. In the present
   case, a digital photograph of the patient's iris was captured using a
   digital camera and manipulated on a computer using graphic software to
   produce a replica of the natural iris. The described technique reduces
   treatment time, increases simplicity, and permits the patient's natural
   iris to be replicated without the need for iris painting and special
   artistic skills.</abstract><date>AUG 2015</date><author>Buzayan, Muaiyed M.
   Ariffin, Yusnidar T.
   Yunus, Norsiah
   Mahmood, Wan Adida Azina Binti</author></paper><paper><title>Fast and Exact (Poisson) Solvers on Symmetric Geometries</title><abstract>In computer graphics, numerous geometry processing applications reduce
   to the solution of a Poisson equation. When considering geometries with
   symmetry, a natural question to consider is whether and how the symmetry
   can be leveraged to derive an efficient solver for the underlying system
   of linear equations. In this work we provide a simple
   representation-theoretic analysis that demonstrates how symmetries of
   the geometry translate into block diagonalization of the linear
   operators and we show how this results in efficient linear solvers for
   surfaces of revolution with and without angular boundaries.</abstract><date>AUG 2015</date><author>Kazhdan, M.</author></paper><paper><title>Colloquium: Large scale simulations on GPU clusters</title><abstract>Graphics processing units (GPU) are currently used as a cost-effective
   platform for computer simulations and big-data processing. Large scale
   applications require that multiple GPUs work together but the efficiency
   obtained with cluster of GPUs is, at times, sub-optimal because the GPU
   features are not exploited at their best. We describe how it is possible
   to achieve an excellent efficiency for applications in statistical
   mechanics, particle dynamics and networks analysis by using suitable
   memory access patterns and mechanisms like CUDA streams, profiling
   tools, etc. Similar concepts and techniques may be applied also to other
   problems like the solution of Partial Differential Equations.</abstract><date>JUN 17 2015</date><author>Bernaschi, Massimo
   Bisson, Mauro
   Fatica, Massimiliano</author></paper><paper><title>From Geocentrism to Allocentrism: Teaching the Phases of the Moon in a
   Digital Full-Dome Planetarium</title><abstract>An increasing number of planetariums worldwide are turning digital,
   using ultra-fast computers, powerful graphic cards, and high-resolution
   video projectors to create highly realistic astronomical imagery in real
   time. This modern technology makes it so that the audience can observe
   astronomical phenomena from a geocentric as well as an allocentric
   perspective (the view from space). While the dome creates a sense of
   immersion, the digital planetarium introduces a new way to teach
   astronomy, especially for topics that are inherently three-dimensional
   and where seeing the phenomenon from different points of view is
   essential. Like a virtual-reality environment, an immersive digital
   planetarium helps learners create a more scientifically accurate
   visualization of astronomical phenomena. In this study, a digital
   planetarium was used to teach the phases of the Moon to children aged 12
   to 14. To fully grasp the lunar phases, one must imagine the spherical
   Moon (as perceived from space), revolving around the Earth while being
   illuminated by the Sun, and then reconcile this view with the geocentric
   perspective. Digital planetariums allow learners to have both an
   allocentric and a geocentric perspective on the lunar phases. Using a
   Design experiment approach, we tested an educational scenario in which
   the lunar phases were taught in an allocentric digital planetarium.
   Based on qualitative data collected before, during, and after the
   planetarium intervention, we were able to demonstrate that five out of
   six participants had a better understanding of the lunar phases after
   the planetarium session.</abstract><date>FEB 2016</date><author>Chastenay, Pierre</author></paper><paper><title>A GPU OpenCL based cross-platform Monte Carlo dose calculation engine
   (goMC)</title><abstract>Monte Carlo (MC) simulation has been recognized as the most accurate
   dose calculation method for radiotherapy. However, the extremely long
   computation time impedes its clinical application. Recently, a lot of
   effort has been made to realize fast MC dose calculation on graphic
   processing units (GPUs). However, most of the GPU-based MC dose engines
   have been developed under NVidia's CUDA environment. This limits the
   code portability to other platforms, hindering the introduction of
   GPU-based MC simulations to clinical practice. The objective of this
   paper is to develop a GPU OpenCL based cross-platform MC dose engine
   named goMC with coupled photon-electron simulation for external photon
   and electron radiotherapy in the MeV energy range. Compared to our
   previously developed GPU-based MC code named gDPM (Jia et al 2012 Phys.
   Med. Biol. 57 7783-97), goMC has two major differences. First, it was
   developed under the OpenCL environment for high code portability and
   hence could be run not only on different GPU cards but also on CPU
   platforms. Second, we adopted the electron transport model used in
   EGSnrc MC package and PENELOPE's random hinge method in our new dose
   engine, instead of the dose planning method employed in gDPM. Dose
   distributions were calculated for a 15 MeV electron beam and a 6 MV
   photon beam in a homogenous water phantom, a water-bone-lung-water slab
   phantom and a half-slab phantom. Satisfactory agreement between the two
   MC dose engines goMC and gDPM was observed in all cases. The average
   dose differences in the regions that received a dose higher than 10% of
   the maximum dose were 0.48-0.53% for the electron beam cases and
   0.15-0.17% for the photon beam cases. In terms of efficiency, goMC was
   similar to 4-16% slower than gDPM when running on the same NVidia TITAN
   card for all the cases we tested, due to both the different electron
   transport models and the different development environments. The code
   portability of our new dose engine goMC was validated by successfully
   running it on a variety of different computing devices including an
   NVidia GPU card, two AMD GPU cards and an Intel CPU processor.
   Computational efficiency among these platforms was compared.</abstract><date>OCT 7 2015</date><author>Tian, Zhen
   Shi, Feng
   Folkerts, Michael
   Qin, Nan
   Jiang, Steve B.
   Jia, Xun</author></paper><paper><title>FAST: framework for heterogeneous medical image computing and
   visualization</title><abstract>Computer systems are becoming increasingly heterogeneous in the sense
   that they consist of different processors, such as multi-core CPUs and
   graphic processing units. As the amount of medical image data increases,
   it is crucial to exploit the computational power of these processors.
   However, this is currently difficult due to several factors, such as
   driver errors, processor differences, and the need for low-level memory
   handling. This paper presents a novel FrAmework for heterogeneouS
   medical image compuTing and visualization (FAST). The framework aims to
   make it easier to simultaneously process and visualize medical images
   efficiently on heterogeneous systems.FAST uses common image processing
   programming paradigms and hides the details of memory handling from the
   user, while enabling the use of all processors and cores on a system.
   The framework is open-source, cross-platform and available online.Code
   examples and performance measurements are presented to show the
   simplicity and efficiency of FAST. The results are compared to the
   insight toolkit (ITK) and the visualization toolkit (VTK) and show that
   the presented framework is faster with up to 20 times speedup on several
   common medical imaging algorithms.FAST enables efficient medical image
   computing and visualization on heterogeneous systems. Code examples and
   performance evaluations have demonstrated that the toolkit is both easy
   to use and performs better than existing frameworks, such as ITK and
   VTK.</abstract><date>NOV 2015</date><author>Smistad, Erik
   Bozorgi, Mohammadmehdi
   Lindseth, Frank</author></paper><paper><title>gPGA: GPU Accelerated Population Genetics Analyses</title><abstract>BackgroundThe isolation with migration (IM) model is important for
   studies in population genetics and phylogeography. IM program applies
   the IM model to genetic data drawn from a pair of closely related
   populations or species based on Markov chain Monte Carlo (MCMC)
   simulations of gene genealogies. But computational burden of IM program
   has placed limits on its application.MethodologyWith strong
   computational power, Graphics Processing Unit (GPU) has been widely used
   in many fields. In this article, we present an effective implementation
   of IM program on one GPU based on Compute Unified Device Architecture
   (CUDA), which we call gPGA.ConclusionsCompared with IM program, gPGA can
   achieve up to 52.30X speedup on one GPU. The evaluation results
   demonstrate that it allows datasets to be analyzed effectively and
   rapidly for research on divergence population genetics.</abstract><date>AUG 6 2015</date><author>Zhou, Chunbao
   Lang, Xianyu
   Wang, Yangang
   Zhu, Chaodong</author></paper><paper><title>Bayesian State-Space Modelling on High-Performance Hardware Using LibBi</title><abstract>LibBi is a software package for state space modelling and Bayesian
   inference on modern computer hardware, including multi-core central
   processing units, many-core graphics processing units, and
   distributed-memory clusters of such devices. The software parses a
   domain-specific language for model specification, then optimizes,
   generates, compiles and runs code for the given model, inference method
   and hardware platform. In presenting the software, this work serves as
   an introduction to state space models and the specialized methods
   developed for Bayesian inference with them. The focus is on sequential
   Monte Carlo (SMC) methods such as the particle filter for state
   estimation, and the particle Markov chain Monte Carlo and SMC2 methods
   for parameter estimation. All are well-suited to current computer
   hardware. Two examples are given and developed throughout, one a linear
   three-element windkessel model of the human arterial system, the other a
   nonlinear Lorenz '96 model. These are specified in the prescribed
   modelling language, and LibBi demonstrated by performing inference with
   them. Empirical results are presented, including a performance
   comparison of the software with different hardware configurations.</abstract><date>OCT 2015</date><author>Murray, Lawrence M.</author></paper><paper><title>Comparison of Acceleration Data Structures for Electromagnetic
   Ray-Tracing Purposes on GPUs</title><abstract>We analyze and compare the performances of two acceleration data
   structures for electromagnetic ray-tracing purposes on graphical
   processing units (GPUs) using the CUDA programming language, namely the
   K-Dimensional (KD)-tree and the Split Bounding Volume Hierarchy (SBVH).
   Our implementations have been based on the approach made available by
   Nvidia, which takes into account the programming optimizations made
   possible by the latest version of CUDA and the most recent Nvidia GPU
   architectures. We have tested the two approaches on standard computer
   graphics scenes (conference and bunny) and on a scene of electromagnetic
   interest ("ship"). In all the cases considered, the SBVH has shown to
   perform better in terms of both speed and memory-saving properties.</abstract><date>OCT 2015</date><author>Breglia, Alfonso
   Capozzoli, Amedeo
   Curcio, Claudio
   Liseno, Angelo</author></paper><paper><title>DOCK 6: Impact of New Features and Current Docking Performance</title><abstract>This manuscript presents the latest algorithmic and methodological
   developments to the structure-based design program DOCK 6.7 focused on
   an updated internal energy function, new anchor selection control,
   enhanced minimization options, a footprint similarity scoring function,
   a symmetry-corrected root-mean-square deviation algorithm, a database
   filter, and docking forensic tools. An important strategy during
   development involved use of three orthogonal metrics for assessment and
   validation: pose reproduction over a large database of 1043
   protein-ligand complexes (SB2012 test set), cross-docking to 24
   drug-target protein families, and database enrichment using large active
   and decoy datasets (Directory of Useful Decoys [DUD]-E test set) for
   five important proteins including HIV protease and IGF-1R. Relative to
   earlier versions, a key outcome of the work is a significant increase in
   pose reproduction success in going from DOCK 4.0.2 (51.4%) 5.4 (65.2%)
   6.7 (73.3%) as a result of significant decreases in failure arising from
   both sampling 24.1% 13.6% 9.1% and scoring 24.4% 21.1% 17.5%. Companion
   cross-docking and enrichment studies with the new version highlight
   other strengths and remaining areas for improvement, especially for
   systems containing metal ions. The source code for DOCK 6.7 is available
   for download and free for academic users at . (c) 2015 Wiley
   Periodicals, Inc.</abstract><date>JUN 5 2015</date><author>Allen, William J.
   Balius, Trent E.
   Mukherjee, Sudipto
   Brozell, Scott R.
   Moustakas, Demetri T.
   Lang, P. Therese
   Case, David A.
   Kuntz, Irwin D.
   Rizzo, Robert C.</author></paper><paper><title>The DynDom3D Webserver for the Analysis of Domain Movements in
   Multimeric Proteins</title><abstract>DynDom3D is a program for the analysis of domain movements in multimeric
   proteins. Its inputs are two structure files that indicate a possible
   domain movement, but the onus has been on the user to process the files
   so that there is the necessary one-to-one equivalence between atoms in
   the two atom lists. This is often a prohibitive task to carry out
   manually, which has limited the application of DynDom3D. Here we report
   on a webserver with a preprocessor that automatically creates an
   equivalence between atoms using sequence alignment methods. The
   processed structure files are passed to DynDom3D and the results are
   presented on a webpage that includes molecular graphics for easy
   visualization.</abstract><date>JAN 1 2016</date><author>Girdlestone, Christopher
   Hayward, Steven</author></paper><paper><title>Organization of a geophysical information space by using an
   event-bush-based collaborative tool</title><abstract>Development of knowledge engineering makes it possible to bring an
   information space relating to an entire domain of knowledge within the
   field of geoscience into a strict form, which is both computer-tractable
   and convenient for collaborative research work. Nevertheless, there are
   issues that seriously hamper this process - the problem of defining key
   terms, which is often not shared by the colleagueship, and interrelation
   of concepts developed by different schools within the colleagueship
   focused on different aspects of this domain. Another issue is the export
   of results to a wider community unfamiliar with the specificity of local
   studies. All these issues can be successfully addressed by a novel
   technique of knowledge engineering, the event bush, brought into the
   COLLA environment for geoscientific collaborative studies. This paper
   demonstrates how the said issues can be resolved by the example of one
   of the most important information domains in the field of seismology,
   the site effects. Text, graphics, tabular data and a physical model
   coming from different sources and different contexts are united in one
   context keeping all the specificity of original understanding and
   allowing the researchers keep on following their own context and
   terminology.</abstract><date>SEP 2015</date><author>Diviacco, Paolo
   Pshenichny, Cyril
   Carniel, Roberto
   Khrabrykh, Zinaida
   Shterkhun, Victoria
   Mouromtsev, Dmitry
   Guzman, Silvina
   Pascolo, Paolo</author></paper><paper><title>iTagPlot: an accurate computation and interactive drawing tool for tag
   density plot</title><abstract>Motivation: Tag density plots are very important to intuitively reveal
   biological phenomena from capture-based sequencing data by visualizing
   the normalized read depth in a region.Results: We have developed
   iTagPlot to compute tag density across functional features in parallel
   using multicores and a grid engine and to interactively explore it in a
   graphical user interface. It allows us to stratify features by defining
   groups based on biological function and measurement, summary statistics
   and unsupervised clustering.</abstract><date>JUL 15 2015</date><author>Kim, Sung-Hwan
   Ezenwoye, Onyeka
   Cho, Hwan-Gue
   Robertson, Keith D.
   Choi, Jeong-Hyeon</author></paper><paper><title>Layered holographic stereogram based on inverse Fresnel diffraction</title><abstract>We propose an efficient algorithm using layered holographic stereogram
   for three-dimensional (3D) computer-generated holograms. The hologram is
   spatially partitioned into multiple holographic elements (hogels) to
   provide the occlusion effect and motion parallax by use of multiple
   viewpoint rendering. Each hogel is calculated with inverse Fresnel
   diffraction by slicing the viewing frustum according to the depth image.
   The sliced layers can provide accurate depth cues for reconstruction
   since the geometric information of the 3D scene is faithfully matched.
   The algorithm is compatible with computer graphics rendering techniques
   and robust for holograms with different parameters. When the hogel size
   equals 1 mm, the signal-to-noise ratio of the diffraction calculation is
   above 39 dB with a propagation distance longer than 10 mm. Numerical
   simulations and optical experiments have demonstrated that the proposed
   method can reconstruct quality 3D images with reduced computational
   load. (C) 2016 Optical Society of America</abstract><date>JAN 20 2016</date><author>Zhang, Hao
   Zhao, Yan
   Cao, Liangcai
   Jin, Guofan</author></paper><paper><title>The Light Field Stereoscope Immersive Computer Graphics via Factored
   Near-Eye Light Field Displays with Focus Cues</title><abstract>Over the last few years, virtual reality (VR) has re-emerged as a
   technology that is now feasible at low cost via inexpensive cell-phone
   components. In particular, advances of high-resolution micro displays,
   low-latency orientation trackers, and modern GPUs facilitate immersive
   experiences at low cost. One of the remaining challenges to further
   improve visual comfort in VR experiences is the vergence-accommodation
   conflict inherent to all stereoscopic displays. Accurate reproduction of
   all depth cues is crucial for visual comfort. By combining well-known
   stereoscopic display principles with emerging factored light field
   technology, we present the first wearable VR display supporting high
   image resolution as well as focus cues. A light field is presented to
   each eye, which provides more natural viewing experiences than
   conventional near-eye displays. Since the eye box is just slightly
   larger than the pupil size, rank-1 light field factorizations are
   sufficient to produce correct or nearly-correct focus cues; no
   time-multiplexed image display or gaze tracking is required. We analyze
   lens distortions in 4D light field space and correct them using the
   afforded high-dimensional image formation. We also demonstrate
   significant improvements in resolution and retinal blur quality over
   related near-eye displays. Finally, we analyze diffraction limits of
   these types of displays.</abstract><date>AUG 2015</date><author>Huang, Fu-Chung
   Chen, Kevin
   Wetzstein, Gordon</author></paper><paper><title>A non-rigid point matching method with local topology preservation for
   accurate bladder dose summation in high dose rate cervical brachytherapy</title><abstract>GEC-ESTRO guidelines for high dose rate cervical brachytherapy advocate
   the reporting of the D2cc (the minimum dose received by the maximally
   exposed 2cc volume) to organs at risk. Due to large interfractional
   organ motion, reporting of accurate cumulative D2cc over a
   multifractional course is a non-trivial task requiring deformable image
   registration and deformable dose summation. To efficiently and
   accurately describe the point-to-point correspondence of the bladder
   wall over all treatment fractions while preserving local topologies, we
   propose a novel graphic processing unit (GPU)-based non-rigid point
   matching algorithm. This is achieved by introducing local anatomic
   information into the iterative update of correspondence matrix
   computation in the 'thin plate splines-robust point matching' (TPS-RPM)
   scheme. The performance of the GPU-based TPS-RPM with local topology
   preservation algorithm (TPS-RPM-LTP) was evaluated using four
   numerically simulated synthetic bladders having known deformations, a
   custom-made porcine bladder phantom embedded with twenty one fiducial
   markers, and 29 fractional computed tomography (CT) images from seven
   cervical cancer patients. Results show that TPS-RPM-LTP achieved
   excellent geometric accuracy with landmark residual distance error (RDE)
   of 0.7 +/- 0.3 mm for the numerical synthetic data with different scales
   of bladder deformation and structure complexity, and 3.7 +/- 1.8 mm and
   1.6 +/- 0.8 mm for the porcine bladder phantom with large and small
   deformation, respectively. The RDE accuracy of the urethral orifice
   landmarks in patient bladders was 3.7 +/- 2.1 mm. When compared to the
   original TPS-RPM, the TPS-RPMLTP improved landmark matching by reducing
   landmark RDE by 50 +/- 19%, 37 +/- 11% and 28 +/- 11% for the synthetic,
   porcine phantom and the patient bladders, respectively. This was
   achieved with a computational time of less than 15s in all cases with
   GPU acceleration. The efficiency and accuracy shown with the TPS-RPM-LTP
   indicate that it is a practical and promising tool for bladder dose
   summation in adaptive cervical cancer brachytherapy.</abstract><date>FEB 7 2016</date><author>Chen, Haibin
   Zhong, Zichun
   Liao, Yuliang
   Pompos, Arnold
   Hrycushko, Brian
   Albuquerque, Kevin
   Zhen, Xin
   Zhou, Linghong
   Gu, Xuejun</author></paper><paper><title>Real-time fMRI processing with physiological noise correction -
   Comparison with off-line analysis</title><abstract>Background: While applications of real-time functional magnetic
   resonance imaging (rtfMRI) are growing rapidly, there are still
   limitations in real-time data processing compared to off-line
   analysis.New methods: We developed a proof-of-concept real-time fMRI
   processing (rtfMRIp) system utilizing a personal computer (PC) with a
   dedicated graphic processing unit (GPU) to demonstrate that it is now
   possible to perform intensive whole-brain fMRI data processing in
   real-time. The rtfMRIp performs slice-timing correction, motion
   correction, spatial smoothing, signal scaling, and general linear model
   (GLM) analysis with multiple noise regressors including physiological
   noise modeled with cardiac (RETROICOR) and respiration volume per time
   (RVT).Results: The whole-brain data analysis with more than 100,000
   voxels and more than 250 volumes is completed in less than 300 ms, much
   faster than the time required to acquire the fMRI volume. Real-time
   processing implementation cannot be identical to off-line analysis when
   time-course information is used, such as in slice-timing correction,
   signal scaling, and GLM. We verified that reduced slice-timing
   correction for real-time analysis had comparable output with off-line
   analysis. The real-time GLM analysis, however, showed over-fitting when
   the number of sampled volumes was small.Comparison with existing
   methods: Our system implemented real-time RETROICOR and RVT
   physiological noise corrections for the first time and it is capable of
   processing these steps on all available data at a given time, without
   need for recursive algorithms.Conclusions: Comprehensive data processing
   in rtfMRI is possible with a PC, while the number of samples should be
   considered in real-time GLM. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>DEC 30 2015</date><author>Misaki, Masaya
   Barzigar, Nafise
   Zotev, Vadim
   Phillips, Raquel
   Cheng, Samuel
   Bodurka, Jerzy</author></paper><paper><title>SPS: A Simulation Tool for Calculating Power of Set-Based Genetic
   Association Tests</title><abstract>Set-based association tests, combining a set of single-nucleotide
   polymorphisms into a unified test, have become important approaches to
   identify weak-effect or low-frequency risk loci of complex diseases.
   However, there is no comprehensive and user-friendly tool to estimate
   power of set-based tests for study design. We developed a simulation
   tool to estimate statistical power of multiple representative set-based
   tests (SPS). SPS has a graphic interface to facilitate parameter
   settings and result visualization. Advanced functions include loading
   real genotypes to define genetic architecture, set-based meta-analysis
   for risk loci with or without heterogeneity, and parallel simulations.
   In proof-of-principle examples, SPS took no more than 3 sec on average
   to estimate the power in a conventional setting. The SPS has been
   integrated into a user-friendly software tool (KGG) as an independent
   functional module and it is freely available at
   http://statgenpro.psychiatry.hku.hk/limx/kgg/. (C) 2015 Wiley
   Periodicals, Inc.</abstract><date>JUL 2015</date><author>Li, Jiang
   Sham, Pak Chung
   Song, Youqiang
   Li, Miaoxin</author></paper><paper><title>Integrative Simulation Environment for Conceptual Structural Analysis</title><abstract>This paper introduces the integrative simulation environment (ISE)
   methodology and a proof-of-concept implementation, designed as modular
   and extensible approach for interactive structural simulation. The ISE
   methodology allows users to sketch, model, simulate, visualize, and
   augment data based on an immediate-mode paradigm where sketches are
   automatically turned into models, which are immediately passed to a
   simulation engine to be analyzed in order to have the results passed
   back to a visualizer, operating within the same display context that the
   initial sketch and model were created in. In other words, data can be
   acquired, defined, delivered, transformed, computed, visualized, and
   augmented throughout the entire integrative simulation process in a
   real-time loop, providing the user with an intuitive and interactive
   modeling interface. ISE as such provides a mechanism to conceptually
   study cause and effect relationships of design decisions, while allowing
   for real-time, context specific, digital content overlay onto as-built
   structures via an augmented reality technique. (C) 2014 American Society
   of Civil Engineers.</abstract><date>JUL 2015</date><author>Ge, Li
   Kuester, Falko</author></paper><paper><title>Anomaly detection with low magnetic flux: A fluxgate sensor network
   application</title><abstract>Recent studies on remote detection methods were mostly for improving
   variables like sensing distance, sensitivity and power consumption.
   Especially using anisotropic magneto-resistive sensors with low power
   consumption and high sensitivity for detecting subsurface magnetic
   materials became very popular in last decades. In our study, for
   detecting subsurface materials, we have used fluxgate sensor network for
   having even higher sensitivity and also minimizing the power consumption
   by detecting the changing rates of horizontal component of earth's
   magnetic flux which is assumed to be very low. We have constituted a
   magnetic measurement system which comprises a detector system, which has
   a mechanism enables sensors to move in 3-D space, a data acquisition
   module for processing and sending all sensor information, and a computer
   for running the magnetic flux data evaluation and recording software.
   Using this system, tests are carried out to detect anomalies on
   horizontal component of earth's magnetic flux which is created by
   different subsurface materials with known magnetic, chemical and
   geometric properties. The harmonics of horizontal component of earth's
   magnetic flux in scanned area are analyzed by the help of DSP Lock-In
   amplifier and the amplitudes of high variation harmonics are shown as
   computer graphics. Using the graphic information, the upside surface
   geometry of subsurface material is defined. For identifying the magnetic
   anomalies, we have used the scale-invariant feature transform
   (SIFT)-binary robust invariant scalable keypoints (BRISKs) as keypoint
   and descriptor. We used an algorithm for matching the newly scanned
   image to the closest image in database which is constituted of mines and
   possible other metal objects like cans, etc. Results show that, if the
   proposed detection system is used instead of metal detectors which
   cannot distinguish mines from other metal materials and alert for every
   type of metal with different geometries, it can be said that miss alarm
   count, work force and time can be decreased dramatically. In this paper,
   mostly the setup of the system is described and in Appendix A some
   experimental outputs of the system for different geometries of metal
   samples are given. And also for comparing the results of the proposed
   system, additional experiments are carried out with a different type of
   sensor chip, namely KMZ51, and also given in Appendix A. (C) 2015
   Elsevier Ltd. All rights reserved.</abstract><date>MAR 2016</date><author>Ege, Yavuz
   Coramik, Mustafa
   Kabadayi, Murat
   Citak, Hakan
   Kalender, Osman
   Yuruklu, Emrah
   Kurt, Unal
   Nazlibilek, Sedat</author></paper><paper><title>Fast Methods for Spherical Linear Interpolation in Minkowski Space</title><abstract>Spherical linear interpolation in Minkowski space has got a number of
   important applications in computer graphics, physics and kinematics.
   Spherical linear interpolation in Minkowski space involves the
   computation of trigonometric functions, which are computationally
   expensive. The computation will be fast since the implementation does
   not need to evaluate any trigonometric functions in the inner loop.
   Furthermore, no renormalization is necessary and therefore it is a true
   spherical interpolation in Minkowski space. We propose that incremental
   Slerp in Minkowski space. In this paper we demonstrate four different
   methods for incremental Slerp in Minkowski space.</abstract><date>DEC 2015</date><author>Ghadami, Raheleh
   Rahebi, Javad
   Yayly, Yusuf</author></paper><paper><title>Description, molecular phylogeny, and natural history of a new
   kleptoparasitic species of gelechiid moth (Lepidoptera) associated with
   Melastomataceae galls in Brazil</title><abstract>The male, female, pupa and larva of a new species of Gelechiidae
   (Lepidoptera), Locharcha opportuna Moreira and Becker, are described and
   illustrated with the aid of optical and scanning electron microscopy. A
   preliminary analysis of mitochondrial DNA sequences including members of
   related lineages is also provided. The immature stages are associated
   with galls induced by a species of Palaeomystella Fletcher (Lepidoptera:
   Momphidae) on Tibouchina sellowiana (Cham.) Cogn. (Melastomataceae),
   endemic to the Atlantic Rainforest. Larvae are kleptoparasitic, usurping
   the gall internal space and thereafter feeding on the internal tissues.
   By determining the variation in population density of both species and
   following gall development individually throughout ontogeny under field
   conditions, we demonstrated that the kleptoparasite completes its life
   cycle inside galls induced by Palaeomystella, where pupation occurs. The
   variation in seasonal abundance of the kleptoparasite is tied to that of
   the cecidogenous species, with their corresponding peaks in density
   occurring sequentially.[GRAPHICS]</abstract><date>AUG 26 2015</date><author>Luz, Fernando A.
   Goncalves, Gislene L.
   Moreira, Gilson R. P.
   Becker, Vitor O.</author></paper><paper><title>Reliability of phantom pain relief in neurorehabilitation using a
   multimodal virtual reality system.</title><abstract>The objective of this study is to demonstrate the reliability of relief
   from phantom limb pain in neurore-habilitation using a multimodal
   virtual reality system. We have developed a virtual reality
   rehabilitation system with multimodal sensory feedback and applied it to
   six patients with brachial plexus avulsion or arm amputation. In an
   experiment, patients executed a reaching task using a virtual phantom
   limb displayed in a three-dimensional computer graphic environment
   manipulated by their real intact limb. The intensity of the phantom limb
   pain was evaluated through a short-form McGill pain questionnaire. The
   experiments were conducted twice on different days at more than
   four-week intervals for each patient. The reliability of our task's
   ability to relieve pain was demonstrated by the test-retest method,
   which checks the degree of the relative similarity between the pain
   reduction rates in two experiments using Fisher's intraclass correlation
   coefficient (ICC). The ICC was 0.737, indicating sufficient
   reproducibility of our task. The average of the reduction rates across
   participants was 50.2%, and it was significantly different from 0 (p &lt;;
   0:001). Overall, our findings indicate that neurorehabilitation using
   our multimodal virtual reality system reduces the phantom limb pain with
   sufficient reliability. </abstract><date>2015-Aug</date><author>Sano, Yuko
   Ichinose, Akimichi
   Wake, Naoki
   Osumi, Michihiro
   Sumitani, Masahiko
   Kumagaya, Shin-Ichiro
   Kuniyoshi, Yasuo</author></paper><paper><title>Parrondo Games with Spatial Dependence, III</title><abstract>We study Toral's Parrondo games with N players and one-dimensional
   spatial dependence as modified by Xie et al. Specifically, we use
   computer graphics to sketch the Parrondo and anti-Parrondo regions for 3
   &lt;= N &lt;= 9. Our work was motivated by a recent paper of Li et al., who
   applied a state space reduction method to this model, reducing the
   number of states from 2(N) to N + 1. We show that their reduced Markov
   chains are inconsistent with the model of Xie et al.</abstract><date>DEC 2015</date><author>Ethier, S. N.
   Lee, Jiyeon</author></paper><paper><title>Regularization Based Iterative Point Match Weighting for Accurate Rigid
   Transformation Estimation</title><abstract>Feature extraction and matching (FEM) for 3D shapes finds numerous
   applications in computer graphics and vision for object modeling,
   retrieval, morphing, and recognition. However, unavoidable incorrect
   matches lead to inaccurate estimation of the transformation relating
   different datasets. Inspired by AdaBoost, this paper proposes a novel
   iterative re-weighting method to tackle the challenging problem of
   evaluating point matches established by typical FEM methods. Weights are
   used to indicate the degree of belief that each point match is correct.
   Our method has three key steps: (i) estimation of the underlying
   transformation using weighted least squares, (ii) penalty parameter
   estimation via minimization of the weighted variance of the matching
   errors, and (iii) weight re-estimation taking into account both matching
   errors and information learnt in previous iterations. A comparative
   study, based on real shapes captured by two laser scanners, shows that
   the proposed method outperforms four other state-of-the-art methods in
   terms of evaluating point matches between overlapping shapes established
   by two typical FEM methods, resulting in more accurate estimates of the
   underlying transformation. This improved transformation can be used to
   better initialize the iterative closest point algorithm and its
   variants, making 3D shape registration more likely to succeed.</abstract><date>SEP 2015</date><author>Liu, Yonghuai
   De Dominicis, Luigi
   Wei, Baogang
   Chen, Liang
   Martin, Ralph R.</author></paper><paper><title>The Ettention software package</title><abstract>We present a novel software package for the problem "reconstruction from
   projections" in electron microscopy. The Ettention framework consists of
   a set of modular building-blocks for tomographic reconstruction
   algorithms. The well-known block iterative reconstruction method based
   on Kaczmarz algorithm is implemented using these building-blocks,
   including adaptations specific to electron tomography. Ettention
   simultaneously features (1) a modular, object-oriented software design,
   (2) optimized access to high-performance computing (HPC) platforms such
   as graphic processing units (GPU) or many-core architectures like Xeon
   Phi, and (3) accessibility to microscopy end-users via integration in
   the IMOD package and eTomo user interface. We also provide developers
   with a clean and well-structured application programming interface (API)
   that allows for extending the software easily and thus makes it an ideal
   platform for algorithmic research while hiding most of the technical
   details of high-performance computing. (c) 2015 Elsevier B.V. All rights
   reserved.</abstract><date>FEB 2016</date><author>Dahmen, Tim
   Marsalek, Lukas
   Marniok, Nico
   Turonova, Beata
   Bogachev, Sviatoslav
   Trampert, Patrick
   Nickels, Stefan
   Slusallek, Philipp</author></paper><paper><title>A Fully GPU-Based Ray-Driven Backprojector via a Ray-Culling Scheme with
   Voxel-Level Parallelization for Cone-Beam CT Reconstruction</title><abstract>A ray-driven backprojector is based on ray-tracing, which computes the
   length of the intersection between the ray paths and each voxel to be
   reconstructed. To reduce the computational burden caused by these
   exhaustive intersection tests, we propose a fully graphics processing
   unit (GPU)-based ray-driven backprojector in conjunction with a
   ray-culling scheme that enables straightforward parallelization without
   compromising the high computing performance of a GPU. The purpose of the
   ray-culling scheme is to reduce the number of ray-voxel intersection
   tests by excluding rays irrelevant to a specific voxel computation. This
   rejection step is based on an axis-aligned bounding box (AABB) enclosing
   a region of voxel projection, where eight vertices of each voxel are
   projected onto the detector plane. The range of the rectangular-shaped
   AABB is determined by min/max operations on the coordinates in the
   region. Using the indices of pixels inside the AABB, the rays passing
   through the voxel can be identified and the voxel is weighted as the
   length of intersection between the voxel and the ray. This procedure
   makes it possible to reflect voxel-level parallelization, allowing an
   independent calculation at each voxel, which is feasible for a GPU
   implementation. To eliminate redundant calculations during ray-culling,
   a shared-memory optimization is applied to exploit the GPU memory
   hierarchy. In experimental results using real measurement data with
   phantoms, the proposed GPU-based ray-culling scheme reconstructed a
   volume of resolution 280x280x176 in 77 seconds from 680 projections of
   resolution 1024x768 , which is 26 times and 7.5 times faster than
   standard CPU-based and GPU-based ray-driven backprojectors,
   respectively. Qualitative and quantitative analyses showed that the
   ray-driven backprojector provides high-quality reconstruction images
   when compared with those generated by the Feldkamp-Davis-Kress algorithm
   using a pixel-driven backprojector, with an average of 2.5 times higher
   contrast-to-noise ratio, 1.04 times higher universal quality index, and
   1.39 times higher normalized mutual information.</abstract><date>DEC 2015</date><author>Park, Hyeong-Gyu
   Shin, Yeong-Gil
   Lee, Ho</author></paper><paper><title>In vitro and in silico studies of urea-induced denaturation of yeast
   iso-1-cytochrome c and its deletants at pH 6.0 and 25 degrees C</title><abstract>Yeast iso-1-cytochrome c (y-cyt-c) has five extra residues at N-terminus
   in comparison to the horse cytochrome c. These residues are numbered as
   -5 to -1. Here, these extra residues are sequentially removed from
   y-cyt-c to establish their role in folding and stability of the protein.
   We performed urea-induced denaturation of wild-type (WT) y-cyt-c and its
   deletants. Denaturation was followed by observing change in Delta
   epsilon(405) (probe for measuring change in the heme environment within
   the protein), [theta](405) (probe for measuring the change in Phe82 and
   Met80 axial bonding), [theta](222) (probe for measuring change in
   secondary structure) and [theta](416) (probe for measuring change in the
   heme-methionine environment). The urea-induced reversible denaturation
   curves were used to estimate Delta[GRAPHICS], the value of Gibbs free
   energy change (Delta G(D)) in the absence of urea; C-m, the midpoint of
   the denaturation curve, i.e. molar urea concentration ([urea]) at which
   Delta G(D)=0; and m, the slope (= partial differential Delta G(D)/
   partial differential [urea]). Our in vitro results clearly show that
   except Delta(-5/-4) all deletants are less stable than WT protein.
   Coincidence of normalized transition curves of all physical properties
   suggests that unfolding/refolding of WT protein and its deletants is a
   two-state process. To confirm our in vitro observations, we performed
   40ns MD simulation of both WT y-cyt-c and its deletants. MD simulation
   results clearly show that extra N-terminal residues play a role in
   stability but not in folding of the protein.</abstract><date>JUL 3 2015</date><author>Haque, Md Anzarul
   Zaidi, Sobia
   Ubaid-ullah, Shah
   Prakash, Amresh
   Hassan, Md Imtaiyaz
   Islam, Asimul
   Batra, Janendra K.
   Ahmad, Faizan</author></paper><paper><title>Box, cable and smartphone: a simple laparoscopic trainer.</title><abstract>BACKGROUND: Laparoscopic surgery requires different abilities to open
   surgery, and is challenging to learn within the confines of the
   operating theatre. With the development of laparoscopic surgery in
   modern surgery, the importance in improving these skills is becoming an
   increasing focus of surgical training programmes.CONTEXT: The assembly
   of the laparoscopic trainer and exercises was performed at the
   University of Sydney Clinical School located at Hornsby Hospital in
   Sydney, Australia. The objective was to design and construct a new
   concept smartphone box laparoscopic trainer that is affordable and
   replicable, and to demonstrate its usefulness in practising laparoscopic
   techniques to improve skills outside of the operating
   theatre.INNOVATION: The trainer was constructed using a personal
   smartphone, cardboard box, video graphics array (VGA) adaptor, VGA cable
   and a computer screen. Laparoscopic instruments and materials used for
   simulated task exercises were obtained from the operating theatre.
   Simulated demonstrations of simple laparoscopic tasks included suture
   handling, instrument knot-tying and anastomotic suture
   techniques.IMPLICATIONS: The smartphone box trainer is inexpensive
   (approximately $60) and took less than 20minutes to build. The cost was
   almost entirely for the VGA adaptor. The box trainer was light, portable
   and easily transported to any setting that provided a computer screen.
   It is an inexpensive, easy-to-assemble, replicable model that benefits
   from the advanced technology of personal smartphones, and can be easily
   accessed as a useful tool in learning and improving laparoscopic
   techniques. Laparoscopic surgery requires different abilities to open
   surgery.</abstract><date>2015-Dec</date><author>Lee, Migie
   Savage, Jason
   Dias, Maxwell
   Bergersen, Philip
   Winter, Matthew</author></paper><paper><title>Introduction of a nomogram for predicting adverse pregnancy outcomes
   based on maternal serum markers in the quad screen test</title><abstract>The aim of this study was to develop a nomogram that can calculate a
   total score, derived from each serum marker in the quad screen test, for
   systematically predicting adverse pregnancy outcomes (APOs).We
   retrospectively reviewed 3684 singleton pregnant women who underwent a
   quad screen test and gave birth at a single medical centre from January
   2005 to December 2010. The serum marker data from the quad screen test
   and pregnancy outcomes were used to construct logistic regression models
   for predicting the risks of APOs. APO was defined as the presence of at
   least one of the following: preeclampsia, preterm delivery before 34
   weeks of gestation, small for gestational age, foetal loss, and foetal
   demise. A graphic nomogram was generated to represent the scoring model
   using the regression coefficient of each serum marker.A nomogram for the
   prediction of APOs using each serum marker in the quad test was
   developed based on the logistic regression analysis. The positive
   predictive values for the subsequent development of an APO were ascended
   stepwise as the calculated score increases. The area under the receiver
   operating characteristic curve of this score for the prediction of APO
   was 0.596 (95 % confidence interval 0.569-0.623).We here introduced a
   nomogram for stratifying the risk of APOs in patients with abnormal
   serum markers in the quad screen test. Although the validity of the
   nomogram is too weak to be used in clinical routine, but it may provide
   additional information for practitioners counselling pregnant women and
   for predicting APOs.</abstract><date>SEP 2015</date><author>An, Jung-Joo
   Ji, Hyun-Young
   You, Ji Yeon
   Woo, Sook-Young
   Choi, Suk-Joo
   Oh, Soo-young
   Roh, Cheong-Rae
   Kim, Jong-Hwa</author></paper><paper><title>Visual Analytics for the Exploration of Tumor Tissue Characterization</title><abstract>Tumors are heterogeneous tissues consisting of multiple regions with
   distinct characteristics. Characterization of these intra-tumor regions
   can improve patient diagnosis and enable a better targeted treatment.
   Ideally, tissue characterization could be performed non-invasively,
   using medical imaging data, to derive per voxel a number of features,
   indicative of tissue properties. However, the high dimensionality and
   complexity of this imaging-derived feature space is prohibiting for easy
   exploration and analysis - especially when clinical researchers require
   to associate observations from the feature space to other reference
   data, e.g., features derived from histopathological data. Currently, the
   exploratory approach used in clinical research consists of juxtaposing
   these data, visually comparing them and mentally reconstructing their
   relationships. This is a time consuming and tedious process, from which
   it is difficult to obtain the required insight. We propose a visual tool
   for: (1) easy exploration and visual analysis of the feature space of
   imaging-derived tissue characteristics and (2) knowledge discovery and
   hypothesis generation and confirmation, with respect to reference data
   used in clinical research. We employ, as central view, a 2D embedding of
   the imaging-derived features. Multiple linked interactive views provide
   functionality for the exploration and analysis of the local structure of
   the feature space, enabling linking to patient anatomy and clinical
   reference data. We performed an initial evaluation with ten clinical
   researchers. All participants agreed that, unlike current practice, the
   proposed visual tool enables them to identify, explore and analyze
   heterogeneous intra-tumor regions and particularly, to generate and
   confirm hypotheses, with respect to clinical reference data.</abstract><date>JUN 2015</date><author>Raidou, R. G.
   van der Heide, U. A.
   Dinh, C. V.
   Ghobadi, G.
   Kallehauge, J. F.
   Breeuwer, M.
   Vilanova, A.</author></paper><paper><title>L-2 and pointwise a posteriori error estimates for FEM for elliptic PDEs
   on surfaces</title><abstract>Surface finite element methods (SFEMs) are widely used to solve surface
   partial differential equations arising in applications including crystal
   growth, fluid mechanics and computer graphics. A posteriori error
   estimators are computable measures of the error and are used to
   implement adaptive mesh refinement. Previous studies of a posteriori
   error estimation in SFEM have mainly focused on bounding energy norm
   errors. In this work, we derive a posteriori L-2 and pointwise error
   estimates for piecewise linear SFEM for the Laplace-Beltrami equation on
   implicitly defined surfaces. There are two main error sources in SFEM, a
   `Galerkin error' arising in the usual way for finite element methods,
   and a 'geometric error' arising from replacing the continuous surface by
   a discrete approximation when writing the finite element equations. Our
   work includes numerical estimation of the dependence of the error bounds
   on the geometric properties of the surface. We provide also numerical
   experiments where the estimators have been used to implement an adaptive
   FEM over surfaces with different curvatures.</abstract><date>JUL 2015</date><author>Camacho, Fernando
   Demlow, Alan</author></paper><paper><title>EBSDinterp 1.0: A MATLAB((R)) Program to Perform Microstructurally
   Constrained Interpolation of EBSD Data</title><abstract>EBSDinterp is a graphic user interface (GUI)-based MATLAB (R) program to
   perform microstructurally constrained interpolation of nonindexed
   electron backscatter diffraction data points. The area available for
   interpolation is restricted using variations in pattern quality or band
   contrast (BC). Areas of low BC are not available for interpolation, and
   therefore cannot be erroneously filled by adjacent grains growing into
   them. Points with the most indexed neighbors are interpolated first and
   the required number of neighbors is reduced with each successive round
   until a minimum number of neighbors is reached. Further iterations allow
   more data points to be filled by reducing the BC threshold. This method
   ensures that the best quality points (those with high BC and most
   neighbors) are interpolated first, and that the interpolation is
   restricted to grain interiors before adjacent grains are grown together
   to produce a complete microstructure. The algorithm is implemented
   through a GUI, taking advantage of MATLAB (R)'s parallel processing
   toolbox to perform the interpolations rapidly so that a variety of
   parameters can be tested to ensure that the final microstructures are
   robust and artifact-free. The software is freely available through the
   CSIRO Data Access Portal (doi:10.4225/08/5510090C6E620) as both a
   compiled Windows executable and as source code.</abstract><date>AUG 2015</date><author>Pearce, Mark A.</author></paper><paper><title>Trends in Continuity and Interpolation for Computer Graphics</title><abstract></abstract><date>NOV-DEC 2015</date><author>Gonzalez Garcia, Francisco</author></paper><paper><title>Enhancing the quality of reconstructed 3D objects by using point
   clusters</title><abstract>A novel algorithm for constructing computer-generated holograms using
   point clusters is presented. This method exploits the precalculated
   triangular meshes used in previous research and can reconstruct less
   noisy 3D objects. In addition, the high-speed property of a
   ferroelectric liquid crystal spatial light modulator is utilized to
   enhance the reconstruction quality. All 3D holograms generated in this
   paper are based on Fresnel propagation; thus, the Fresnel plane is
   treated as a vital element in producing the hologram. A GeForce GTX 770
   graphics card with 2 GB memory was used to achieve parallel high-speed
   hologram generation. (C) 2015 Optical Society of America</abstract><date>JUN 20 2015</date><author>Yang, Fan
   Kaczorowski, Andrzej
   Wilkinson, Tim D.</author></paper><paper><title>A web-based application to inform consumers about the products based on
   corn and soybeans sold in Romania</title><abstract></abstract><date>AUG 20 2015</date><author>Dorottya, Domokos Alice</author></paper><paper><title>The informed sampler: A discriminative approach to Bayesian inference in
   generative computer vision models</title><abstract>Computer vision is hard because of a large variability in lighting,
   shape, and texture; in addition the image signal is non-additive due to
   occlusion. Generative models promised to account for this variability by
   accurately modelling the image formation process as a function of latent
   variables with prior beliefs. Bayesian posterior inference could then,
   in principle, explain the observation. While intuitively appealing,
   generative models for computer vision have largely failed to deliver on
   that promise due to the difficulty of posterior inference. As a result
   the community has favoured efficient discriminative approaches. We still
   believe in the usefulness of generative models in computer vision, but
   argue that we need to leverage existing discriminative or even heuristic
   computer vision methods. We implement this idea in a principled way with
   an informed sampler and in careful experiments demonstrate it on
   challenging generative models which contain renderer programs as their
   components. We concentrate on the problem of inverting an existing
   graphics rendering engine, an approach that can be understood as
   "Inverse Graphics". The informed sampler, using simple discriminative
   proposals based on existing computer vision technology, achieves
   significant improvements of inference. (C) 2015 Elsevier Inc. All rights
   reserved.</abstract><date>JUL 2015</date><author>Jampani, Varun
   Nowozin, Sebastian
   Loper, Matthew
   Gehler, Peter V.</author></paper><paper><title>Bounded Distortion Harmonic Mappings in the Plane</title><abstract>We present a framework for the computation of harmonic and conformal
   mappings in the plane with mathematical guarantees that the computed
   mappings are C-infinity, locally injective and satisfy strict bounds on
   the conformal and isometric distortion. Such mappings are very desirable
   in many computer graphics and geometry processing applications.We
   establish the sufficient and necessary conditions for a harmonic planar
   mapping to have bounded distortion. Our key observation is that these
   conditions relate solely to the boundary behavior of the mapping. This
   leads to an efficient and accurate algorithm that supports handle-based
   interactive shape-and-image deformation and is demonstrated to
   outperform other state-of-the-art methods.</abstract><date>AUG 2015</date><author>Chen, Renjie
   Weber, Ofir</author></paper><paper><title>Solving Parker's transport equation with stochastic differential
   equations on GPUs</title><abstract>The numerical solution of transport equations for energetic charged
   particles in space is generally very costly in terms of time. Besides
   the use of multi-core CPUs and computer clusters in order to decrease
   the computation times, high performance calculations on graphics
   processing units (CPUs) have become available during the last years. In
   this work we introduce and describe a CPU-accelerated implementation of
   Parker's equation using Stochastic Differential Equations (SDEs) for the
   simulation of the transport of energetic charged particles with the CUDA
   toolkit, which is the focus of this work. We briefly discuss the set of
   SDEs arising from Parker's transport equation and their application to
   boundary value problems such as that of the Jovian magnetosphere. We
   compare the runtimes of the GPU code with a CPU version of the same
   algorithm. Compared to the CPU implementation (using OpenMP and eight
   threads) we find a performance increase of about a factor of 10-60,
   depending on the assumed set of parameters. Furthermore, we benchmark
   our simulation using the results of an existing SDE implementation of
   Parker's transport equation. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>JUL 2015</date><author>Dunzlaff, P.
   Strauss, R. D.
   Potgieter, M. S.</author></paper><paper><title>A Scalable Formal Debugging Approach with Auto-Correction Capability
   Based on Static Slicing and Dynamic Ranking for RTL Datapath Designs</title><abstract>By increasing the complexity of digital systems, verification and
   debugging of such systems have become a major problem and economic
   issue. Although many computer aided design (CAD) solutions have been
   suggested to enhance efficiency of existing debugging approaches, they
   are still suffering from lack of providing a small set of potential
   error locations and also automatic correction mechanisms. On the other
   hand, the ever-growing usage of digital signal processing (DSP),
   computer graphics and embedded systems applications that can be modeled
   as polynomial computations in their datapath designs, necessitate an
   effective method to deal with their verification, debugging and
   correction. In this paper, we introduce a formal debugging approach
   based on static slicing and dynamic ranking methods to derive a reduced
   ordered set of potential error locations. In addition, to speed up
   finding true errors in the presence of multiple design errors, error
   candidates are sorted in decreasing order of their probability of being
   an error. After that, a mutation-based technique is employed to
   automatically correct bugs even in the case of multiple bugs. In order
   to evaluate the effectiveness of our approach, we have applied it to
   several industrial designs. The experimental results show that the
   proposed technique enables us to locate and correct even multiple bugs
   with high confidence in a short run time even for complex designs of up
   to several thousand lines of RTL code.</abstract><date>JUN 2015</date><author>Alizadeh, Bijan
   Behnam, Payman
   Sadeghi-Kohan, Somayeh</author></paper><paper><title>A localized meshless method for diffusion on folded surfaces</title><abstract>Partial differential equations (PDEs) on surfaces arise in a variety of
   application areas including biological systems, medical imaging, fluid
   dynamics, mathematical physics, image processing and computer graphics.
   In this paper, we propose a radial basis function (RBF) discretization
   of the closest point method. The corresponding localized meshless method
   may be used to approximate diffusion on smooth or folded surfaces. Our
   method has the benefit of having an a priori error bound in terms of
   percentage of the norm of the solution. A stable solver is used to avoid
   the ill-conditioning that arises when the radial basis functions (RBFs)
   become flat. (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>SEP 15 2015</date><author>Cheung, Ka Chun
   Ling, Leevan
   Ruuth, Steven J.</author></paper><paper><title>Comparative Analyses of Universal Extraction Buffers for Assay of Stress
   Related Biochemical and Physiological Parameters</title><abstract>Comparative efficiency of three extraction solutions, including the
   universal sodium phosphate buffer (USPB), the Tris-HCl buffer (UTHB),
   and the specific buffers, were compared for assays of soluble protein,
   free proline, superoxide radical ([GRAPHICS]), hydrogen peroxide (H2O2),
   and the antioxidant enzymes such as superoxide dismutase (SOD), catalase
   (CAT), guaiacol peroxidase (POD), ascorbate peroxidase (APX),
   glutathione peroxidase (GPX), and glutathione reductase (GR) in Populus
   deltoide. Significant differences for protein extraction were detected
   via sodium dodecyl sulfate polyacrylamide gel electrophoresis (SDS-PAGE)
   and two-dimensional electrophoresis (2-DE). Between the two universal
   extraction buffers, the USPB showed higher efficiency for extraction of
   soluble protein, CAT, GR,[GRAPHICS], GPX, SOD, and free proline, while
   the UTHB had higher efficiency for extraction of APX, POD, and H2O2.
   When compared with the specific buffers, the USPB showed higher
   extraction efficiency for measurement of soluble protein, CAT, GR,
   and[GRAPHICS], parallel extraction efficiency for GPX, SOD, free
   proline, and H2O2, and lower extraction efficiency for APX and POD,
   whereas the UTHB had higher extraction efficiency for measurement of POD
   and H2O2. Further comparisons proved that 100mM USPB buffer showed the
   highest extraction efficiencies. These results indicated that USPB would
   be suitable and efficient for extraction of soluble protein, CAT, GR,
   GPX, SOD, H2O2,[GRAPHICS], and free proline.</abstract><date>OCT 3 2015</date><author>Han, Chunyu
   Chan, Zhulong
   Yang, Fan</author></paper><paper><title>Complete description of the skull and mandible of the giant mustelid
   Eomellivora piveteaui Ozansoy, 1965 (Mammalia, Carnivora, Mustelidae),
   from Batallones (MN10), late Miocene (Madrid, Spain)</title><abstract>We describe cranial, mandibular, and dental remains of five individuals
   of the giant mustelid Eomellivora piveteaui Ozansoy, 1965, from the late
   Miocene (MN10) site of Cerro de los Batallones (Madrid, Spain)-the first
   complete cranial remains recorded for this species and the most complete
   remains of the genus. This new sample enables a review of the systematic
   status of Eomellivora, leading us to accept as valid the species E.
   piveteaui Ozansoy, 1965, E. wimani Zdansky, 1924, E. ursogulo (Orlov,
   1948), and E. hungarica Kretzoi, 1942. Our phylogenetic hypothesis
   indicates that Eomellivora is the sister taxon of the extant Mellivora
   capensis and E. piveteaui had a common ancestor within the crown group
   E. wimani-E. ursogulo. Eomellivora piveteaui was specialized for a more
   hypercarnivorous diet than the largest extant terrestrial mustelids,
   although it also had some derived bone-crushing adaptations. Eomellivora
   piveteaui had an active predatory role in the late Miocene carnivore
   faunas, exploiting both small and relatively large prey.SUPPLEMENTAL
   DATA-Supplemental materials are available for this article for free
   at[GRAPHICS]</abstract><date>JUL 4 2015</date><author>Valenciano, Alberto
   Abella, Juan
   Sanisidro, Oscar
   Hartstone-Rose, Adam
   Angeles Alvarez-Sierra, Maria
   Morales, Jorge</author></paper><paper><title>Aligning the unalignable: bacteriophage whole genome alignments</title><abstract>Background: In recent years, many studies focused on the description and
   comparison of large sets of related bacteriophage genomes. Due to the
   peculiar mosaic structure of these genomes, few informative approaches
   for comparing whole genomes exist: dot plots diagrams give a mostly
   qualitative assessment of the similarity/dissimilarity between two or
   more genomes, and clustering techniques are used to classify genomes.
   Multiple alignments are conspicuously absent from this scene. Indeed,
   whole genome aligners interpret lack of similarity between sequences as
   an indication of rearrangements, insertions, or losses. This behavior
   makes them ill-prepared to align bacteriophage genomes, where even
   closely related strains can accomplish the same biological function with
   highly dissimilar sequences.Results: In this paper, we propose a
   multiple alignment strategy that exploits functional collinearity shared
   by related strains of bacteriophages, and uses partial orders to capture
   mosaicism of sets of genomes. As classical alignments do, the computed
   alignments can be used to predict that genes have the same biological
   function, even in the absence of detectable similarity. The Alpha
   aligner implements these ideas in visual interactive displays, and is
   used to compute several examples of alignments of Staphylococcus aureus
   and Mycobacterium bacteriophages, involving up to 29 genomes. Using
   these datasets, we prove that Alpha alignments are at least as good as
   those computed by standard aligners. Comparison with the progressive
   Mauve aligner - which implements a partial order strategy, but whose
   alignments are linearized - shows a greatly improved interactive graphic
   display, while avoiding misalignments.Conclusions: Multiple alignments
   of whole bacteriophage genomes work, and will become an important
   conceptual and visual tool in comparative genomics of sets of related
   strains. A python implementation of Alpha, along with installation
   instructions for Ubuntu and OSX, is available on bitbucket
   (https://bitbucket.org/thekswenson/alpha).</abstract><date>JAN 13 2016</date><author>Berard, Severine
   Chateau, Annie
   Pompidor, Nicolas
   Guertin, Paul
   Bergeron, Anne
   Swenson, Krister M.</author></paper><paper><title>Speeding up the high-accuracy surface modelling method with GPU</title><abstract>In order to find a solution for accurate, topographic data-demanding
   applications, such as catchment hydrologic modeling and assessments of
   anthropic activities impact on environmental systems, high-accuracy
   surface modeling (HASM) method is developed. Although it can produce a
   digital elevation model (DEM) surface of higher accuracy than classical
   methods, e.g. inverse distance weighted, spline and kriging, HASM
   requires numerous iterations to solve large linear systems, which impede
   its applications in high-resolution and large-scale surface
   interpolation. This paper aims to demonstrate the utilization of
   graphics' processing units (GPUs) device to accelerate HASM in
   constructing large-scale and high-resolution DEM surfaces. We
   parallelized the linear system algorithm for solving HASM with Compute
   Unified Device Architecture, a parallel programming model developed by
   NVIDIA. We designed a memory-saving strategy to enable the HASM
   algorithm to run on GPUs. The speedup ratio of GPU-based algorithm was
   tested and compared with CPU-based algorithm through simulations of both
   ideal Gaussian synthetic surface and real topographic surface in the
   loess plateau of Gansu province. The GPU-parallelized algorithm can
   attain an over 10x speedup ratio with the CPU-based algorithm as a
   reference. The speedup ratio increases with the scale and resolution of
   the dataset. The memory management strategy efficiently reduces the
   memory usage by more than eight times the grid cell number. Implementing
   HASM in the GPUs device enables modeling large-scale and high-resolution
   surfaces in a reasonable time period and implies the potential benefits
   from the use of GPUs as massive, parallel co-processors for
   arithmetic-intensive data-processing applications.</abstract><date>OCT 2015</date><author>Yan, Changqing
   Zhao, Gang
   Yue, Tianxiang
   Chen, Chuanfa
   Liu, Jimin
   Li, Han
   Su, Na</author></paper><paper><title>IBiSS, a versatile and interactive tool for integrated sequence and 3D
   structure analysis of large macromolecular complexes</title><abstract>Motivation: In the past few years, an increasing number of crystal and
   cryo electron microscopy (cryo-EM) structures of large macromolecular
   complexes, such as the ribosome or the RNA polymerase, have become
   available from various species. These multi-subunit complexes can be
   difficult to analyze at the level of amino acid sequence in combination
   with the 3D structural organization of the complex. Therefore, novel
   tools for simultaneous analysis of structure and sequence information of
   complex assemblies are required to better understand the basis of
   molecular mechanisms and their functional implications.Results: Here, we
   present a web-based tool, Integrative Biology of Sequences and
   Structures (IBiSS), which is designed for interactively displaying 3D
   structures and selected sequences of subunits from large macromolecular
   complexes thus allowing simultaneous structure-sequence analysis such as
   conserved residues involved in catalysis or protein-protein interfaces.
   This tool comprises a Graphic User Interface and uses a rapid-access
   internal database, containing the relevant pre-aligned multiple
   sequences across all species available and 3D structural information.
   These annotations are automatically retrieved and updated from UniProt
   and crystallographic and cryo-EM data available in the Protein Data Bank
   (PDB) and Electron Microscopy Data Bank (EMDB).</abstract><date>OCT 15 2015</date><author>Beinsteiner, Brice
   Michalon, Jonathan
   Klaholz, Bruno P.</author></paper><paper><title>Combining sigma-lognormal modeling and classical features for analyzing
   graphomotor performances in kindergarten children</title><abstract>This paper investigates the advantage of using the kinematic theory of
   rapid human movements as a complementary approach to those based on
   classical dynamical features to characterize and analyze kindergarten
   children's ability to engage in graphomotor activities as a preparation
   for handwriting learning. This study analyzes nine different movements
   taken from 48 children evenly distributed among three different school
   grades corresponding to pupils aged 3, 4, and 5 years. On the one hand,
   our results show that the ability to perform graphomotor activities
   depends on kindergarten grades. More importantly, this study shows which
   performance criteria, from sophisticated neuromotor modeling as well as
   more classical kinematic parameters, can differentiate children of
   different school grades. These criteria provide a valuable tool for
   studying children's graphomotor control learning strategies. On the
   other hand, from a practical point of view, it is observed that school
   grades do not clearly reflect pupils' graphomotor performances. This
   calls for a large-scale investigation, using a more efficient
   experimental design based on the various observations made throughout
   this study regarding the choice of the graphic shapes, the number of
   repetitions and the features to analyze. (C) 2015 Elsevier B.V. All
   rights reserved.</abstract><date>OCT 2015</date><author>Duval, Theresa
   Remi, Celine
   Plamondon, Rejean
   Valliant, Jean
   O'Reilly, Christian</author></paper><paper><title>Chlorophyll content retrieval from hyperspectral remote sensing imagery.</title><abstract>Chlorophyll content is the essential parameter in the photosynthetic
   process determining leaf spectral variation in visible bands. Therefore,
   the accurate estimation of the forest canopy chlorophyll content is a
   significant foundation in assessing forest growth and stress affected by
   diseases. Hyperspectral remote sensing with high spatial resolution can
   be used for estimating chlorophyll content. In this study, the
   chlorophyll content was retrieved step by step using Hyperion imagery.
   Firstly, the spectral curve of the leaf was analyzed, 25 spectral
   characteristic parameters were identified through the correlation
   coefficient matrix, and a leaf chlorophyll content inversion model was
   established using a stepwise regression method. Secondly, the pixel
   reflectance was converted into leaf reflectance by a geometrical-optical
   model (4-scale). The three most important parameters of reflectance
   conversion, including the multiple scattering factor (M 0 ), and the
   probability of viewing the sunlit tree crown (P T ) and the background
   (P G ), were estimated by leaf area index (LAI), respectively. The
   results indicated that M 0 , P T , and P G could be described as a
   logarithmic function of LAI, with all R (2) values above 0.9. Finally,
   leaf chlorophyll content was retrieved with RMSE=7.3574mug/cm(2), and
   canopy chlorophyll content per unit ground surface area was estimated
   based on leaf chlorophyll content and LAI. Chlorophyll content mapping
   can be useful for the assessment of forest growth stage and diseases. </abstract><date>2015-Jul</date><author>Yang, Xiguang
   Yu, Ying
   Fan, Wenyi</author></paper><paper><title>Multipurpose prevention technologies for sexual and reproductive health:
   mapping global needs for introduction of new preventive products</title><abstract>Objectives: Worldwide, Women face sexual and reproductive health (SRH)
   risks including unintended pregnancy and sexually transmitted infections
   (STIs) including HIV. Multipurpose prevention technologies (MPTs)
   combine protection against two or more SRH risks into one product. Male
   and female condoms are the only currently available MPT products, but
   several other forms of MPTs are in development We examined the global
   distribution of selected SRH issues to determine where various risks
   have the greatest geographical overlap.Study design: We examined four
   indicators relevant to MPTs in development: HIV prevalence, herpes
   simplex virus type 2 prevalence (HSV-2), human papillomavirus prevalence
   (HPV) and the proportion of women with unmet need for modern
   contraception. Using ArcGIS Desktop, we mapped these indicators
   individually and in combination on choropleth and graduated symbol maps.
   We conducted a principal components analysis to reduce data and enable
   visual mapping of all four indicators on one graphic to identify
   overlap.Results: Our findings document the greatest overlapping risks in
   Sub-Saharan Africa, and we specify countries in greatest need by
   specific MPT indication.Conclusions: These results can inform strategic
   planning for MPT introduction, market segmentation and demand
   generation; data limitations also highlight the need for improved
   (non-HIV) STI surveillance globally.Implications: MPTs are products in
   development with the potential to empower women to prevent two or more
   SRH risks. Geographic analysis of overlapping SRH risks demonstrates
   particularly high need in Sub-Saharan Africa. This study can help to
   inform strategic planning for MPT introduction, market segmentation and
   demand generation. (C) 2016 The Authors. Published by Elsevier Inc.</abstract><date>JAN 2016</date><author>Schelar, Erin
   Polis, Chelsea B.
   Essam, Timothy
   Looker, Katharine J.
   Bruni, Laia
   Chrisman, Cara J.
   Manning, Judy</author></paper><paper><title>3D Printing of Protein Models in an Undergraduate Laboratory: Leucine
   Zippers</title><abstract>An upper-division undergraduate laboratory experiment is described that
   explores the structure/function relationship of protein domains, namely
   leucine zippers, through a molecular graphics computer program and
   physical models fabricated by 3D printing. By generating solvent
   accessible surfaces and color-coding hydrophobic, basic, and acidic
   amino acid residues, students are able to visualize noncovalent
   interactions that are important in protein folding and protein protein
   interactions.</abstract><date>DEC 2015</date><author>Meyer, Scott C.</author></paper><paper><title>Physically-based smoke simulation for computer graphics: a survey</title><abstract>We present an up-to-date survey on physically-based smoke simulation.
   Physically-based method becomes predominant in smoke simulation in
   computer graphics community. It prevails over traditional methods for
   its plausible visual effect. Significant results have been carried out
   over past two decades. We give a latest overview of state-of-the-art of
   smoke simulation and also compare various techniques according to their
   characteristics. We discuss several issues in terms of computational
   efficiency, numerical stability, numerical dissipation, and runtime
   performance. A number of open challenging problems are also addressed
   for further exploration.</abstract><date>SEP 2015</date><author>Huang, Zhanpeng
   Gong, Guanghong
   Han, Liang</author></paper><paper><title>Tablet and phone applications--A reflection on the experience of
   development.</title><abstract>Tablet devices are now ubiquitous. Medical illustrators have the skills
   to produce a wide range of media content. These devices offer the
   potential of using their creative abilities in new and exciting ways.
   There is much to explore. The primary difficulty lies in understanding
   the necessary computer technical skills to realise a vision. </abstract><date>2015-Jun</date><author>Edwards, Simon
   Winckles, Derek
   Leonard, Mark</author></paper><paper><title>Designing Planar Deployable Objects via Scissor Structures</title><abstract>Scissor structure is used to generate deployable objects for
   space-saving in a variety of applications, from architecture to
   aerospace science. While deployment from a small, regular shape to a
   larger one is easy to design, we focus on a more challenging task:
   designing a planar scissor structure that deploys from a given source
   shape into a specific target shape. We propose a two-step constructive
   method to generate a scissor structure from a high-dimensional parameter
   space. Topology construction of the scissor structure is first performed
   to approximate the two given shapes, as well as to guarantee the
   deployment. Then the geometry of the scissor structure is optimized in
   order to minimize the connection deflections and maximize the shape
   approximation. With the optimized parameters, the deployment can be
   simulated by controlling an anchor scissor unit. Physical deployable
   objects are fabricated according to the designed scissor structures by
   using 3D printing or manual assembly. We show a number of results for
   different shapes to demonstrate that even with fabrication errors, our
   designed structures can deform fluently between the source and target
   shapes.</abstract><date>FEB 2016</date><author>Zhang, Ran
   Wang, Shiwei
   Chen, Xuejin
   Ding, Chao
   Jiang, Luo
   Zhou, Jie
   Liu, Ligang</author></paper><paper><title>SNSMIL, a real-time single molecule identification and localization
   algorithm for super-resolution fluorescence microscopy</title><abstract>Single molecule localization based super-resolution fluorescence
   microscopy offers significantly higher spatial resolution than predicted
   by Abbe's resolution limit for far field optical microscopy. Such
   super-resolution images are reconstructed from wide-field or total
   internal reflection single molecule fluorescence recordings.
   Discrimination between emission of single fluorescent molecules and
   background noise fluctuations remains a great challenge in current data
   analysis. Here we present a real-time, and robust single molecule
   identification and localization algorithm, SNSMIL (Shot Noise based
   Single Molecule Identification and Localization). This algorithm is
   based on the intrinsic nature of noise, i.e., its Poisson or shot noise
   characteristics and a new identification criterion, Q(SNSMIL), is
   defined. SNSMIL improves the identification accuracy of single
   fluorescent molecules in experimental or simulated datasets with high
   and inhomogeneous background. The implementation of SNSMIL relies on a
   graphics processing unit (GPU), making real-time analysis feasible as
   shown for real experimental and simulated datasets.</abstract><date>JUN 22 2015</date><author>Tang, Yunqing
   Dai, Luru
   Zhang, Xiaoming
   Li, Junbai
   Hendriks, Johnny
   Fan, Xiaoming
   Gruteser, Nadine
   Meisenberg, Annika
   Baumann, Arnd
   Katranidis, Alexandros
   Gensch, Thomas</author></paper><paper><title>Whiteboard: a framework for the programmatic visualization of complex
   biological analyses</title><abstract>A Summary: Whiteboard is a class library implemented in C++ that enables
   visualization to be tightly coupled with computation when analyzing
   large and complex datasets.</abstract><date>JUN 15 2015</date><author>Sundstrom, Gorel
   Zamani, Neda
   Grabherr, Manfred G.
   Mauceli, Evan</author></paper><paper><title>Multi-layer Lattice Model for Real-Time Dynamic Character Deformation</title><abstract>Due to the recent advancement of computer graphics hardware and software
   algorithms, deformable characters have become more and more popular in
   real-time applications such as computer games. While there are mature
   techniques to generate primary deformation from skeletal movement,
   simulating realistic and stable secondary deformation such as jiggling
   of fats remains challenging. On one hand, traditional volumetric
   approaches such as the finite element method require higher
   computational cost and are infeasible for limited hardware such as game
   consoles. On the other hand, while shape matching based simulations can
   produce plausible deformation in real-time, they suffer from a stiffness
   problem in which particles either show unrealistic deformation due to
   high gains, or cannot catch up with the body movement. In this paper, we
   propose a unified multi-layer lattice model to simulate the primary and
   secondary deformation of skeleton-driven characters. The core idea is to
   voxelize the input character mesh into multiple anatomical layers
   including the bone, muscle, fat and skin. Primary deformation is applied
   on the bone voxels with lattice-based skinning. The movement of these
   voxels is propagated to other voxel layers using lattice shape matching
   simulation, creating a natural secondary deformation. Our multi-layer
   lattice framework can produce simulation quality comparable to those
   from other volumetric approaches with a significantly smaller
   computational cost. It is best to be applied in real-time applications
   such as console games or interactive animation creation.</abstract><date>OCT 2015</date><author>Iwamoto, Naoya
   Shum, Hubert P. H.
   Yang, Longzhi
   Morishima, Shigeo</author></paper><paper><title>Manifold Next Event Estimation</title><abstract>We present manifold next event estimation (MNEE), a specialised
   technique for Monte Carlo light transport simulation to render
   refractive caustics by connecting surfaces to light sources (next event
   estimation) across transmissive interfaces. We employ correlated
   sampling by means of a perturbation strategy to explore all half vectors
   in the case of rough transmission while remaining outside of the context
   of Markov chain Monte Carlo, improving temporal stability. MNEE builds
   on differential geometry and manifold walks. It is very lightweight in
   its memory requirements, as it does not use light caching methods such
   as photon maps or importance sampling records. The method integrates
   seamlessly with existing Monte Carlo estimators via multiple importance
   sampling.</abstract><date>JUL 2015</date><author>Hanika, Johannes
   Droske, Marc
   Fascione, Luca</author></paper><paper><title>Optimized parallel implementation of face detection based on GPU
   component</title><abstract>Face detection is an important aspect for various domains such as:
   biometrics, video surveillance and human computer interaction. Generally
   a generic face processing system includes a face detection, or
   recognition step, as well as tracking and rendering phase. In this
   paper, we develop a real-time and robust face detection implementation
   based on GPU component. Face detection is performed by adapting the
   Viola and Jones algorithm. We have developed and designed optimized
   several parallel implementations of these algorithms based on graphics
   processors GPU using CUDA (Compute Unified Device Architecture)
   description.First, we implemented the Viola and Jones algorithm in the
   basic CPU version. The basic application is widened to GPU version using
   CUDA technology, and freeing CPU to perform other tasks. Then, the face
   detection algorithm has been optimized for the GPU using a grid topology
   and shared memory. These programs are compared and the results are
   presented. Finally, to improve the quality of face detection a second
   proposition was performed by the implementation of WaldBoost algorithm.
   (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>AUG 2015</date><author>Chouchene, Marwa
   Sayadi, Fatma Ezahra
   Bahri, Haythem
   Dubois, Julien
   Miteran, Johel
   Atri, Mohamed</author></paper><paper><title>The Potential of a Text-Based Interface as a Design Medium: An
   Experiment in a Computer Animation Environment</title><abstract>Since the birth of the concept of direct manipulation, the graphical
   user interface has been the dominant means of controlling digital
   objects. In this research, we hypothesize that the benefits of a
   text-based interface involve multiple tradeoffs, and we explore the
   potential of text as a medium of design from three perspectives: (i) the
   perceived level of control of the designed object, (ii) a tool for
   realizing creative ideas and (iii) an effective form for a highly
   learnable user interface. Our experiment in a computer animation
   environment shows that (i) participants did feel a high level of control
   of characters, (ii) creativity was both restricted and facilitated
   depending on the task and (iii) natural language expedited the learning
   of a new interface language. Our research provides experimental proof of
   the effect of a text-based interface and offers guidelines for the
   design of future computer-aided design applications.</abstract><date>JAN 2016</date><author>Lee, Sangwon
   Yan, Jin</author></paper><paper><title>SAT Charts New Territory</title><abstract></abstract><date>SEP 2015</date><author>Nuwer, Rachel</author></paper><paper><title>The Human Face as a Dynamic Tool for Social Communication</title><abstract>As a highly social species, humans frequently exchange social
   information to support almost all facets of life. One of the richest and
   most powerful tools in social communication is the face, from which
   observers can quickly and easily make a number of inferences about
   identity, gender, sex, age, race, ethnicity, sexual orientation,
   physical health, attractiveness, emotional state, personality traits,
   pain or physical pleasure, deception, and even social status. With the
   advent of the digital economy, increasing globalization and cultural
   integration, understanding precisely which face information supports
   social communication and which produces misunderstanding is central to
   the evolving needs of modern society (for example, in the design of
   socially interactive digital avatars and companion robots). Doing so is
   challenging, however, because the face can be thought of as comprising a
   high-dimensional, dynamic information space, and this impacts cognitive
   science and neuroimaging, and their broader applications in the digital
   economy. New opportunities to address this challenge are arising from
   the development of new methods and technologies, coupled with the
   emergence of a modern scientific culture that embraces
   cross-disciplinary approaches. Here, we briefly review one such approach
   that combines state-of-the-art computer graphics, psychophysics and
   vision science, cultural psychology and social cognition, and highlight
   the main knowledge advances it has generated. In the light of current
   developments, we provide a vision of the future directions in the field
   of human facial communication within and across cultures.</abstract><date>JUL 20 2015</date><author>Jack, Rachael E.
   Schyns, Philippe G.</author></paper><paper><title>THE ROLE OF COMPUTER ANIMATION IN TEACHING TECHNICAL SUBJECTS</title><abstract>Computer animation has a positive effect on memorizing knowledge by
   students. Used in the process of teaching of technical subjects, it is
   conductive to the development of mind. Animation allows to familiarize
   the students with the schemes of solving technical problems and shows
   the mode of operation of machinery and equipment. In the technique,
   animations are used, inter alia, in the processes of designing,
   engineering calculations, visualisation and monitoring technological
   processes and visualisation of assembly processes. The article discusses
   the role of computer animation in the teaching process and the examples
   of applications using computer animation and supporting the teaching
   process of technical subjects. Selected examples of technical processes
   in both computer-aided design and manufacturing programs as well as in
   graphics and animation programs are presented.</abstract><date>DEC 2015</date><author>Dziedzic, Krzysztof
   Barszcz, Marcin
   Pasnikowska-Lukaszuk, Magdalena
   Jankowska, Agnieszka</author></paper><paper><title>VDJtools: Unifying Post-analysis of T Cell Receptor Repertoires</title><abstract>Despite the growing number of immune repertoire sequencing studies, the
   field still lacks software for analysis and comprehension of this
   high-dimensional data. Here we report VDJtools, a complementary software
   suite that solves a wide range of T cell receptor (TCR) repertoires
   post-analysis tasks, provides a detailed tabular output and
   publication-ready graphics, and is built on top of a flexible API. Using
   TCR datasets for a large cohort of unrelated healthy donors, twins, and
   multiple sclerosis patients we demonstrate that VDJtools greatly
   facilitates the analysis and leads to sound biological conclusions.
   VDJtools software and documentation are available at
   https://github.com/mikessh/vdjtools.</abstract><date>NOV 2015</date><author>Shugay, Mikhail
   Bagaev, Dmitriy V.
   Turchaninova, Maria A.
   Bolotin, Dmitriy A.
   Britanova, Olga V.
   Putintseva, Ekaterina V.
   Pogorelyy, Mikhail V.
   Nazarov, Vadim I.
   Zvyagin, Ivan V.
   Kirgizova, Vitalina I.
   Kirgizov, Kirill I.
   Skorobogatova, Elena V.
   Chudakov, Dmitriy M.</author></paper><paper><title>From action icon to knowledge icon: Objective-oriented icon taxonomy in
   computer science</title><abstract>Icon plays a critical role in computer interface design. Studies on icon
   taxonomy explain the way in which various types of icon represent the
   objects and provide designers creation rules by which icons are more in
   line with users' cognitive psychology. However, along with larger and
   larger use of icons, the previous classification criterion causes the
   boundary between categories blur. What's more, Single classification
   standard is not able to well illustrate the icons applied in today's
   computer applications. The purpose of this paper is to present an
   objective-oriented icon taxonomy which proposes to categorize icons into
   action icon and knowledge icon. To assess this proposition, we analyzed
   a sample of icons that applied in computer interface and suggest precise
   application domains to both action icon and knowledge icon categories.
   The results of this practice manifested that action icon and knowledge
   icon implied a high relation with applied environment and explicated the
   development trace of computer icons. This work is one of the first to
   point out the notion of knowledge icon and to highlight the importance
   of objective of icon application. Findings in this paper could enrich
   icon use in computer interface design, especially provides possible way
   to improve online knowledge sharing by visual tool like icon. (C) 2015
   Elsevier B.V. All rights reserved.</abstract><date>OCT 2015</date><author>Ma, Xiaoyue
   Matta, Nada
   Cahier, Jean-Pierre
   Qin, Chunxiu
   Cheng, Yanjie</author></paper><paper><title>Rational cubic clipping with linear complexity for computing roots of
   polynomials</title><abstract>Many problems in computer aided geometric design and computer graphics
   can be turned into a root-finding problem of polynomial equations. Among
   various clipping methods, the ones based on the Bernstein-Bezier form
   have good numerical stability. One of such clipping methods is the
   k-clipping method, where k = 2, 3 and often called a cubic clipping
   method when k = 3. It utilizes 0(n(2)) time to find two polynomials of
   degree k bounding the given polynomial f(t) of degree n, and achieves a
   convergence rate of k + 1 for a single root. The roots of the bounding
   polynomials of degree k are then used for bounding the roots of f(t).
   This paper presents a rational cubic clipping method for finding two
   bounding cubics within 0(n) time, which can achieve a higher convergence
   rate 5 than that of 4 of the previous cubic clipping method. When the
   bounding cubics are obtained, the remaining operations are the same as
   those of previous cubic clipping method. Numerical examples show the
   efficiency and the convergence rate of the new method. (C) 2015 Elsevier
   Inc. All rights reserved.</abstract><date>JAN 15 2016</date><author>Chen, Xiao-diao
   Ma, Weiyin</author></paper><paper><title>A low-latency, big database system and browser for storage, querying and
   visualization of 3D genomic data</title><abstract>Recent releases of genome three-dimensional (3D) structures have the
   potential to transform our understanding of genomes. Nonetheless, the
   storage technology and visualization tools need to evolve to offer to
   the scientific community fast and convenient access to these data. We
   introduce simultaneously a database system to store and query 3D genomic
   data (3DBG), and a 3D genome browser to visualize and explore 3D genome
   structures (3DGB). We benchmark 3DBG against state-of-the-art systems
   and demonstrate that it is faster than previous solutions, and
   importantly gracefully scales with the size of data. We also illustrate
   the usefulness of our 3D genome Web browser to explore human genome
   structures. The 3D genome browser is available at
   http://3dgb.cs.mcgill.ca/.</abstract><date>SEP 18 2015</date><author>Butyaev, Alexander
   Mavlyutov, Ruslan
   Blanchette, Mathieu
   Cudre-Mauroux, Philippe
   Waldispuehl, Jerome</author></paper><paper><title>Effects of season on ecological processes in extensive earthen tilapia
   ponds in Southeastern Brazil</title><abstract>In Southeastern Brazil tilapia culture is conducted in extensive and
   semi-intensive flow-through earthen ponds, being water availability and
   flow management different in the rainy and dry seasons. In this region
   lettuce wastes are a potential cheap input for tilapia culture. This
   study examined the ecological processes developing during the rainy and
   dry seasons in three extensive flow-through earthen tilapia ponds
   fertilized with lettuce wastes. Water quality, plankton and sediment
   parameters were sampled monthly during a year. Factor analysis was used
   to identify the ecological processes occurring within the ponds and to
   construct a conceptual graphic model of the pond ecosystem functioning
   during the rainy and dry seasons. Processes related to nitrogen cycling
   presented differences between both seasons while processes related to
   phosphorus cycling did not. Ecological differences among ponds were due
   to effects of wind protection by surrounding vegetation, organic loading
   entering, tilapia density and its grazing pressure on zooplankton.
   Differences in tilapia growth among ponds were related to stocking
   density and ecological process affecting tilapia food availability and
   intraspecific competition. Lettuce wastes addition into the ponds did
   not produce negative effects, thus this practice may be considered a
   disposal option and a low-cost input source for tilapia, at least at the
   amounts applied in this study.</abstract><date>NOV 2015</date><author>Favaro, E. G. P.
   Sipauba-Tavares, L. H.
   Milstein, A.</author></paper><paper><title>Process Tracing Analysis of Hurricane Information Displays</title><abstract>To study people's processing of hurricane forecast advisories, we
   conducted a computer-based experiment that examined 11 research
   questions about the information seeking patterns of students assuming
   the role of a county emergency manager in a sequence of six hurricane
   forecast advisories for each of four different hurricanes. The results
   show that participants considered a variety of different sources of
   information-textual, graphic, and numeric-when tracking hurricanes.
   Click counts and click durations generally gave the same results but
   there were some significant differences. Moreover, participants'
   information search strategies became more efficient over forecast
   advisories and with increased experience tracking the four hurricanes.
   These changes in the search patterns from the first to the fourth
   hurricane suggest that the presentation of abstract principles in a
   training manual was not sufficient for them to learn how to track
   hurricanes efficiently but they were able to significantly improve their
   search efficiency with a modest amount (roughly an hour) of practice.
   Overall, these data indicate that information search patterns are
   complex and deserve greater attention in studies of dynamic decision
   tasks.</abstract><date>DEC 2015</date><author>Wu, Hao-Che
   Lindell, Michael K.
   Prater, Carla S.</author></paper><paper><title>SHEsisPCA: A GPU-Based Software to Correct for Population Stratification
   that Efficiently Accelerates the Process for Handling Genome-Wide
   Datasets</title><abstract>Population stratification is a problem in genetic association studies
   because it is likely to highlight loci that underlie the population
   structure rather than disease-related loci. At present, principal
   component analysis (PCA) has been proven to be an effective way to
   correct for population stratification. However, the conventional PCA
   algorithm is time-consuming when dealing with large datasets. We
   developed a Graphic processing unit (GPU)-based PCA software named
   SHEsisPCA (http://analysis.bio-x.cn/SHEsisMain.htm) that is highly
   parallel with a highest speedup greater than 100 compared with its CPU
   version. A cluster algorithm based on X-means was also implemented as a
   way to detect population subgroups and to obtain matched cases and
   controls in order to reduce the genomic inflation and increase the
   power. A study of both simulated and real datasets showed that SHEsisPCA
   ran at an extremely high speed while the accuracy was hardly reduced.
   Therefore, SHEsisPCA can help correct for population stratification much
   more efficiently than the conventional CPU-based algorithms.</abstract><date>AUG 20 2015</date><author>Shen, Jiawei
   Li, Zhiqiang
   Shi, Yongyong</author></paper><paper><title>A recommended workflow methodology in the creation of an educational and
   training application incorporating a digital reconstruction of the
   cerebral ventricular system and cerebrospinal fluid circulation to aid
   anatomical understanding</title><abstract>Background: The use of computer-aided learning in education can be
   advantageous, especially when interactive three-dimensional (3D) models
   are used to aid learning of complex 3D structures. The anatomy of the
   ventricular system of the brain is difficult to fully understand as it
   is seldom seen in 3D, as is the flow of cerebrospinal fluid (CSF). This
   article outlines a workflow for the creation of an interactive training
   tool for the cerebral ventricular system, an educationally challenging
   area of anatomy. This outline is based on the use of widely available
   computer software packages.Methods: Using MR images of the cerebral
   ventricular system and several widely available commercial and free
   software packages, the techniques of 3D modelling, texturing, sculpting,
   image editing and animations were combined to create a workflow in the
   creation of an interactive educational and training tool. This was
   focussed on cerebral ventricular system anatomy, and the flow of
   cerebrospinal fluid.Results: We have successfully created a robust
   methodology by using key software packages in the creation of an
   interactive education and training tool. This has resulted in an
   application being developed which details the anatomy of the ventricular
   system, and flow of cerebrospinal fluid using an anatomically accurate
   3D model. In addition to this, our established workflow pattern
   presented here also shows how tutorials, animations and self-assessment
   tools can also be embedded into the training application.Conclusions:
   Through our creation of an established workflow in the generation of
   educational and training material for demonstrating cerebral ventricular
   anatomy and flow of cerebrospinal fluid, it has enormous potential to be
   adopted into student training in this field. With the digital age
   advancing rapidly, this has the potential to be used as an innovative
   tool alongside other methodologies for the training of future healthcare
   practitioners and scientists. This workflow could be used in the
   creation of other tools, which could be developed for use not only on
   desktop and laptop computers but also smartphones, tablets and fully
   immersive stereoscopic environments. It also could form the basis on
   which to build surgical simulations enhanced with haptic interaction.</abstract><date>OCT 19 2015</date><author>Manson, Amy
   Poyade, Matthieu
   Rea, Paul</author></paper><paper><title>Evaluation of the Risk of Grade 3 Oral and Pharyngeal Dysphagia Using
   Atlas-Based Method and Multivariate Analyses of Individual Patient Dose
   Distributions</title><abstract>Purpose: The study aimed to apply the atlas of complication incidence
   (ACI) method to patients receiving radical treatment for head and neck
   squamous cell carcinomas (HNSCC), to generate constraints based on
   dose-volume histograms (DVHs), and to identify clinical and dosimetric
   parameters that predict the risk of grade 3 oral mucositis (g3OM) and
   pharyngeal dysphagia (g3PD).Methods and Materials: Oral and pharyngeal
   mucosal DVHs were generated for 253 patients who received radiation (RT)
   or chemoradiation (CRT). They were used to produce ACI for g3OM and
   g3PD. Multivariate analysis (MVA) of the effect of dosimetry, clinical,
   and patient-related variables was performed using logistic regression
   and bootstrapping. Receiver operating curve (ROC) analysis was also
   performed, and the Youden index was used to find volume constraints that
   discriminated between volumes that predicted for toxicity.Results: We
   derived statistically significant dose-volume constraints for g3OM over
   the range v28 to v70. Only 3 statistically significant constraints were
   derived for g3PD v67, v68, and v69. On MVA, mean dose to the oral mucosa
   predicted for g3OM and concomitant chemotherapy and mean dose to the
   inferior constrictor (IC) predicted for g3PD.Conclusions: We have used
   the ACI method to evaluate incidences of g3OM and g3PD and ROC analysis
   to generate constraints to predict g3OM and g3PD derived from entire
   individual patient DVHs. On MVA, the strongest predictors were radiation
   dose (for g3OM) and concomitant chemotherapy (for g3PD). Crown Copyright
   (C) 2015 Published by Elsevier Inc. All rights reserved.</abstract><date>NOV 1 2015</date><author>Otter, Sophie
   Schick, Ulrike
   Gulliford, Sarah
   Lal, Punita
   Franceschini, Davide
   Newbold, Katie
   Nutting, Christopher
   Harrington, Kevin
   Bhide, Shreerang</author></paper><paper><title>Reducing Dose Uncertainty for Spot-Scanning Proton Beam Therapy of
   Moving Tumors by Optimizing the Spot Delivery Sequence</title><abstract>Purpose: To develop and validate a novel delivery strategy for reducing
   the respiratory motion-induced dose uncertainty of spot-scanning proton
   therapy.Methods and Materials: The spot delivery sequence was optimized
   to reduce dose uncertainty. The effectiveness of the delivery sequence
   optimization was evaluated using measurements and patient simulation.
   One hundred ninety-one 2-dimensional measurements using different
   delivery sequences of a single-layer uniform pattern were obtained with
   a detector array on a 1-dimensional moving platform. Intensity modulated
   proton therapy plans were generated for 10 lung cancer patients, and
   dose uncertainties for different delivery sequences were evaluated by
   simulation.Results: Without delivery sequence optimization, the maximum
   absolute dose error can be up to 97.2% in a single measurement, whereas
   the optimized delivery sequence results in a maximum absolute dose error
   of &lt;= 11.8%. In patient simulation, the optimized delivery sequence
   reduces the mean of fractional maximum absolute dose error compared with
   the regular delivery sequence by 3.3% to 10.6% (32.5-68.0% relative
   reduction) for different patients.Conclusions: Optimizing the delivery
   sequence can reduce dose uncertainty due to respiratory motion in
   spot-scanning proton therapy, assuming the 4-dimensional CT is a true
   representation of the patients' breathing patterns. (C) 2015 Elsevier
   Inc. All rights reserved.</abstract><date>NOV 1 2015</date><author>Li, Heng
   Zhu, X. Ronald
   Zhang, Xiaodong</author></paper><paper><title>Fast Detection of Transformed Data Leaks</title><abstract>The leak of sensitive data on computer systems poses a serious threat to
   organizational security. Statistics show that the lack of proper
   encryption on files and communications due to human errors is one of the
   leading causes of data loss. Organizations need tools to identify the
   exposure of sensitive data by screening the content in storage and
   transmission, i.e., to detect sensitive information being stored or
   transmitted in the clear. However, detecting the exposure of sensitive
   information is challenging due to data transformation in the content.
   Transformations (such as insertion and deletion) result in highly
   unpredictable leak patterns. In this paper, we utilize sequence
   alignment techniques for detecting complex data-leak patterns. Our
   algorithm is designed for detecting long and inexact sensitive data
   patterns. This detection is paired with a comparable sampling algorithm,
   which allows one to compare the similarity of two separately sampled
   sequences. Our system achieves good detection accuracy in recognizing
   transformed leaks. We implement a parallelized version of our algorithms
   in graphics processing unit that achieves high analysis throughput. We
   demonstrate the high multithreading scalability of our data leak
   detection method required by a sizable organization.</abstract><date>MAR 2016</date><author>Shu, Xiaokui
   Zhang, Jing
   Yao, Danfeng (Daphne)
   Feng, Wu-Chun</author></paper><paper><title>SequenceCEROSENE: a computational method and web server to visualize
   spatial residue neighborhoods at the sequence level</title><abstract>Background: To understand the molecular function of biopolymers,
   studying their structural characteristics is of central importance.
   Graphics programs are often utilized to conceive these properties, but
   with the increasing number of available structures in databases or
   structure models produced by automated modeling frameworks this process
   requires assistance from tools that allow automated structure
   visualization. In this paper a web server and its underlying method for
   generating graphical sequence representations of molecular structures is
   presented.Results: The method, called SequenceCEROSENE (color encoding
   of residues obtained by spatial neighborhood embedding), retrieves the
   sequence of each amino acid or nucleotide chain in a given structure and
   produces a color coding for each residue based on three-dimensional
   structure information. From this, color-highlighted sequences are
   obtained, where residue coloring represent three-dimensional residue
   locations in the structure. This color encoding thus provides a
   one-dimensional representation, from which spatial interactions,
   proximity and relations between residues or entire chains can be deduced
   quickly and solely from color similarity. Furthermore, additional
   heteroatoms and chemical compounds bound to the structure, like ligands
   or coenzymes, are processed and reported as well.To provide free access
   to SequenceCEROSENE, a web server has been implemented that allows
   generating color codings for structures deposited in the Protein Data
   Bank or structure models uploaded by the user. Besides retrieving
   visualizations in popular graphic formats, underlying raw data can be
   downloaded as well. In addition, the server provides user interactivity
   with generated visualizations and the three-dimensional structure in
   question.Conclusions: Color encoded sequences generated by
   SequenceCEROSENE can aid to quickly perceive the general characteristics
   of a structure of interest (or entire sets of complexes), thus
   supporting the researcher in the initial phase of structure-based
   studies. In this respect, the web server can be a valuable tool, as
   users are allowed to process multiple structures, quickly switch between
   results, and interact with generated visualizations in an intuitive
   manner.The SequenceCEROSENE web server is available at
   https://biosciences.hs-mittweida.de/seqcerosene.</abstract><date>JAN 27 2016</date><author>Heinke, Florian
   Bittrich, Sebastian
   Kaiser, Florian
   Labudde, Dirk</author></paper><paper><title>DensToolKit: A comprehensive open-source package for analyzing the
   electron density and its derivative scalar and vector fields</title><abstract>DensToolKit is a suite of cross-platform, optionally parallelized,
   programs for analyzing the molecular electron density (p) and several
   fields derived from it. Scalar and vector fields, such as the gradient
   of the electron density (del rho), electron localization function (ELF)
   and its gradient, localized orbital locator (LOL), region of slow
   electrons (RoSE), reduced density gradient, localized electrons detector
   (LED), information entropy, molecular electrostatic potential, kinetic
   energy densities K and G, among others, can be evaluated on zero, one,
   two, and three dimensional grids. The suite includes a program for
   searching critical points and bond paths of the electron density, under
   the framework of Quantum Theory of Atoms in Molecules. DensToolKit also
   evaluates the momentum space electron density on spatial grids, and the
   reduced density matrix of order one along lines joining two arbitrary
   atoms of a molecule. The source code is distributed under the GNU-GPLv3
   license, and we release the code with the intent of establishing an
   open-source collaborative project. The style of DensToolKit's code
   follows some of the guidelines of an object-oriented program. This
   allows us to supply the user with a simple manner for easily implement
   new scalar or vector fields, provided they are derived from any of the
   fields already implemented in the code. In this paper, we present some
   of the most salient features of the programs contained in the suite,
   some examples of how to run them, and the mathematical definitions of
   the implemented fields along with hints of how we optimized their
   evaluation. We benchmarked our suite against both a freely-available
   program and a commercial package. Speed-ups of similar to 2x, and up to
   12x were obtained using a non-parallel compilation of DensToolKit for
   the evaluation of fields. DensToolKit takes similar times for finding
   critical points, compared to a commercial package. Finally, we present
   some perspectives for the future development and growth of the
   suite.Program summaryProgram title: DensToolKitCatalogue identifier:
   AEXI_v1_0 Program summary
   RL:http://cpc.cs.qub.ac.uk/summaries/AEXI_v1_0.htmlProgram obtainable
   from: CPC Program Library, Queen's University, Belfast, N.
   IrelandLicensing provisions: GNU, General Public License, version 3No.
   of lines in distributed program, including test data, etc.: 142037No. of
   bytes in distributed program, including test data, etc.:
   5517409Distribution format: tar.gzProgramming language: C++,
   bash.Computer: Any.Operating system: Linux, MacOSX, Windows
   (cygwin).RAM: The memory requirements grow quadratically with the number
   of primitives describing the wavefunction. A wavefunction with 1,500
   primitives uses similar to 17MB, and 2GB RAM are enough to process
   wavefunctions of around 10,000 primitives. A few more MB may be needed
   by some of the most demanding programs of the package if the number of
   primitives is large.Classification: 6.5, 7.3, 16, 16.1.External
   routines: (optional) gnuplot, povray, epstool, Graphics-Magick,
   epstopdfNature of problem: The study of the electron density of
   molecules, some reactivity indices, and the topology of the electron
   density can be used to analyze the chemical nature, stability and
   reactivity of those molecules. Furthermore, the study of the electron
   density and functionals of it may help us in gaining a better
   understanding of the chemical bond. Reactivity indices and the molecular
   topological properties may also aid in molecular design.Solution method:
   The suite provides several programs in order to compute scalar and
   vector fields derivatives of the electron density. Those fields are
   obtained from a wavefunction file, which is in turn obtained from
   programs such as Nwchem, MolPro, etc. The functions, whereby the fields
   are computed, are implemented following mathematically standard but
   computationally optimized and parallelized algorithms built upon the
   Density Matrix. The suite provides several small but efficient programs,
   easily scriptable, for evaluation of the fields upon spatial grids.
   Regarding the topology analysis, the suite uses the algorithm proposed
   by Popelier, which uses the eigen-values of the Hessian of the electron
   density for locating the critical points. Bond paths are traced using a
   fifth-order Runge-Kutta-Dormand-Prince algorithm. Optional visualization
   of the produced data can be carried out by scripts generated by the
   suite, which can be parsed later to gnuplot, or povray. In addition,
   DensToolKit provides an open door for the user to program new scalar or
   vector fields, with almost complete functionality for evaluating such
   fields upon the same spatial grids as those implemented for the fields
   already provided in the suite.Restrictions: Wavefunctions with more than
   99 nuclei must be input in wfx format. In the current version,
   wavefunctions that use pseudopotentials must have only one Additional
   Electron Density Function (EDF) entry in the wfx file, which in this
   case is the only accepted input format, and pseudopotential support is
   provided only in non-parallel compilation.Additional comments: A simple
   method for implementing new indices (derived from any of the implemented
   fields) is provided. In this manner, the final user may easily program
   his/her own scalar or vector field with a few code lines.Running time:
   Strongly dependent on the number of primitives used for approximating
   the wavefunction (similar to N-p(2)). It also depends on the evaluated
   number of points and type of field. Wavefunctions comprised of 1,500
   primitives may take several hours to complete, while small molecules
   described by two or three hundred primitives take a few seconds. Typical
   running times are at least as fast as the times taken by some commercial
   or freely available codes. In many cases, the programs perform the
   computations with a speed-up of 2x (with respect to other available
   programs), and in some cases 10 x speed-ups or more can be attained. (C)
   2015 Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Solano-Altamirano, J. M.
   Hernandez-Perez, Julio M.</author></paper><paper><title>Root growth of maize as studied with minirhizotrons and monolith methods</title><abstract>The root minirhizotron technique (MT) has been used to monitor
   nondestructively the root growth of field crops. The objective of this
   study was to compare the ability of MT to measure the root length
   density (RLD) of maize (Zea mays L.) as compared to the quadrate
   monolith method (QMM). The experiment was conducted at the Gucheng
   Ecological-Meteorological Experiment Station in China during the summer
   of 2007. RLD was estimated using MT and QMM. Results showed that the
   vertical distribution of RLD decreased top-down gradually, starting at
   the top of the root zone. The growth rate of RLD decreased as soil depth
   increased based on both methods. RLD was underestimated by MT at depths
   between 0 and 40cm at the milk and maturity stages and overestimated by
   MT at depths between 0 and 20cm at the jointing and tasseling stages.
   There was a significant correlation (r(2)=0.715,[GRAPHICS]= 0.05)
   between RLD estimates based on QMM and MT. The results of this study
   indicate that properly calibrated MT is a reliable method to screen
   nondestructively the root growth of maize.</abstract><date>OCT 3 2015</date><author>Liao, Rongwei
   Bai, Yueming
   Liang, Hong
   An, Shunqing
   Ren, Sanxue
   Cao, Yujing
   Le, Zhangyan
   Lu, Jianli
   Liu, Jingmiao</author></paper><paper><title>soilphysics: An R package for calculating soil water availability to
   plants by different soil physical indices</title><abstract>Soil available water is an important factor for plant growth. It has
   been estimated by different soil physical indices, such as the least
   limiting water range (LLWR), integral water capacity (IWC) and integral
   energy (E-I). Moreover, salinity is an important limitation for soil
   water availability to plants. Despite the advances in the quantification
   of LLWR, IWC and E-I, a comprehensive description of the computational
   methods, including data management, curve fitting procedures and
   graphing techniques, is still lacking. The salinity effect on these
   quantities has still not been implemented in a computer package. In this
   paper, we present an R package soilphysics and its implementations to
   determine LLWR, IWC and E-I. We described the theory behind each
   implementation, illustrated the functionalities and validated the
   outcomes of soilphysics with other software packages for LLWR, IWC and
   E-I calculations (an Excels (R) algorithm and SAWCal). The salinity
   effect on soil available water was also employed in the package. The
   outcomes are basically the same as other software available, with small
   differences (&lt;4%). The package soilphysics takes advantage of all the
   power of R for dealing with extensive algorithms and for building
   high-quality graphics. It is currently available from the CRAN website
   (http://cran.r-project.org/web/packages/soilphysics/index.html). (c)
   2015 Elsevier B.V. All rights reserved.</abstract><date>JAN 2016</date><author>de Lima, R. P.
   da Silva, A. R.
   da Silva, A. P.
   Leao, T. P.
   Mosaddeghi, M. R.</author></paper><paper><title>A shape deformation algorithm for constrained multidimensional scaling</title><abstract>We present a new Euclidean embedding technique based on volumetric shape
   registration. Extrinsic representation of the intrinsic geometry of a
   shape is preferable in various computer graphics applications as it
   poses only a small degrees of freedom to deal with during processing. A
   popular Euclidean embedding approach to achieve such a representation is
   multidimensional scaling (MDS), which, however, distorts the original
   geometric details drastically. Our method introduces a constraint on the
   original MDS formulation in order, to preserve the initial geometric
   details while the input shape is pulled towards its MDS pose using the
   perfectly accurate bijection in between. The regularizer of this
   registration framework is chosen in such a way that the system supports
   large deformations yet remains fast. Consequently, we produce a
   detail-preserving MDS pose in 90 s for a 53 K-vertex high-resolution
   mesh on a modest computer. We can also add pairwise point constraints on
   the deforming shape without any additional cost. Detail-preserving MDS
   is superior for non-rigid shape retrieval and useful for shape
   segmentation, as demonstrated. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>DEC 2015</date><author>Sahillioglu, Yusuf</author></paper><paper><title>Graphical Aids to the Estimation and Discrimination of Uncertain
   Numerical Data</title><abstract>This research investigates the performance of graphical dot arrays
   designed to make discrimination of relative numerosity as effortless as
   possible at the same time as making absolute (quantitative) numerosity
   estimation as effortful as possible. Comparing regular, random, and
   hybrid (randomized regular) configurations of dots, the results indicate
   that both random and hybrid configurations reduce absolute numerosity
   estimation precision, when compared with regular dots arrays. However,
   discrimination of relative numerosity is significantly more accurate for
   hybrid dot arrays than for random dot arrays. Similarly, human subjects
   report significantly lower levels of subjective confidence in judgments
   when using hybrid dot configurations as compared with regular
   configurations; and significantly higher levels of subjective confidence
   as compared with random configurations. These results indicate that data
   graphics based on the hybrid, randomized-regular configurations of dots
   are well-suited to applications that require decisions to be based on
   numerical data in which the absolute quantities are less certain than
   the relative values. Examples of such applications include
   decision-making based on the outputs of empirically-based mathematical
   models, such as health-related policy decisions using data from
   predictive epidemiological models.</abstract><date>OCT 27 2015</date><author>Jeong, Myeong-Hun
   Duckham, Matt
   Bleisch, Susanne</author></paper></paperCache>