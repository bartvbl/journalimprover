<paperCache><paper><title>Characterizing the performance of an image-based recognizer for planar
   mechanical linkages in textbook graphics and hand-drawn sketches</title><abstract>In this work, we present a computational framework for automatically
   generating kinematic models of planar mechanical linkages from raw
   images. The hallmark of our approach is a novel combination of
   supervised learning methods for detecting mechanical parts (e.g. joints,
   rigid bodies) with the optimizing power of a multiobjective evolutionary
   algorithm, which concurrently maximizes image consistency and mechanical
   feasibility. A rigorous set of experiments was conducted to
   systematically evaluate the performance of each phase in our framework,
   comparing various combinations of joint and body detection schemes and
   feasibility constraints. Precision-recall curves are used to assess
   object detection performance. For the optimization, in addition to
   standard accuracy measures such as top-N accuracy, we introduce a new
   performance metric called user effort ratio that quantifies the amount
   of user interaction required to correct an inaccurate optimization
   solution. Current state-of-the-art performance is achieved with (i) one
   (or a cascade of) support vector machines for joint detection, (ii)
   foreground extraction to reduce false positives, (iii) supervised body
   detection using normalized geodesic time, distance, and detected joint
   confidence, and (iv) feasibility constraints derived from graph theory.
   The proposed framework generalizes moderately well from textbook
   graphics to hand-drawn sketches, and user effort ratio results
   demonstrate the potential power of an interactive system in which simple
   user interactions complement computer recognition for fast kinematic
   modeling. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>NOV 2015</date><author>Eicholtz, Matthew
   Kara, Levent Burak</author></paper><paper><title>ConnectViz: Accelerated Approach for Brain Structural Connectivity Using
   Delaunay Triangulation.</title><abstract>Stroke is a cardiovascular disease with high mortality and long-term
   disability in the world. Normal functioning of the brain is dependent on
   the adequate supply of oxygen and nutrients to the brain complex network
   through the blood vessels. Stroke, occasionally a hemorrhagic stroke,
   ischemia or other blood vessel dysfunctions can affect patients during a
   cerebrovascular incident. Structurally, the left and the right carotid
   arteries, and the right and the left vertebral arteries are responsible
   for supplying blood to the brain, scalp and the face. However, a number
   of impairment in the function of the frontal lobes may occur as a result
   of any decrease in the flow of the blood through one of the internal
   carotid arteries. Such impairment commonly results in numbness, weakness
   or paralysis. Recently, the concepts of brain's wiring representation,
   the connectome, was introduced. However, construction and visualization
   of such brain network requires tremendous computation. Consequently,
   previously proposed approaches have been identified with common problems
   of high memory consumption and slow execution. Furthermore,
   interactivity in the previously proposed frameworks for brain network is
   also an outstanding issue. This study proposes an accelerated approach
   for brain connectomic visualization based on graph theory paradigm using
   compute unified device architecture, extending the previously proposed
   SurLens Visualization and computer aided hepatocellular carcinoma
   frameworks. The accelerated brain structural connectivity framework was
   evaluated with stripped brain datasets from the Department of Surgery,
   University of North Carolina, Chapel Hill, USA. Significantly, our
   proposed framework is able to generate and extract points and edges of
   datasets, displays nodes and edges in the datasets in form of a network
   and clearly maps data volume to the corresponding brain surface.
   Moreover, with the framework, surfaces of the dataset were
   simultaneously displayed with the nodes and the edges. The framework is
   very efficient in providing greater interactivity as a way of
   representing the nodes and the edges intuitively, all achieved at a
   considerably interactive speed for instantaneous mapping of the
   datasets' features. Uniquely, the connectomic algorithm performed
   remarkably fast with normal hardware requirement specifications. </abstract><date>2016-Mar</date><author>Adeshina, A M
   Hashim, R</author></paper><paper><title>Interactive Disassembly Planning for Complex Objects</title><abstract>We present an approach for the automatic generation, interactive
   exploration and real-time modification of disassembly procedures for
   complex, multipartite CAD data sets. In order to lift the performance
   barriers prohibiting interactive disassembly planning, we run a detailed
   analysis on the input model to identify recurring part constellations
   and efficiently determine blocked part motions in parallel on the GPU.
   Building on the extracted information, we present an interface for
   computing and editing extensive disassembly sequences in real-time while
   considering user-defined constraints and avoiding unstable
   configurations. To evaluate the performance of our C++/CUDA
   implementation, we use a variety of openly available CAD data sets,
   ranging from simple to highly complex. In contrast to previous
   approaches, our work enables interactive disassembly planning for
   objects which consist of several thousand parts and require cascaded
   translations during part removal.</abstract><date>MAY 2015</date><author>Kerbl, Bernhard
   Kalkofen, Denis
   Steinberger, Markus
   Schmalstieg, Dieter</author></paper><paper><title>A Study on Value of Artistic Presentation on Motion Graphics -Focused on
   F5 and Semi-Permanent-</title><abstract>This paper presents the historical development process and expressive
   style of motion graphics which are intended to artistic expression, and
   discusses the value of artistic expressions in motion graphics through
   the works and activities of F5 and Semi-Permanent which are the
   representative organization of that. Commercial use of digital motion
   graphics has been rapidly widespread and used in most of the picture
   contents by changes in the digital video production environment with the
   development of the computer. Motion graphics begins with a graphic
   design applied to the motion graphic content and is transferred from the
   abstract representation of the animation at the same time. Since then
   motion graphics have been developed as an independent genre on the basis
   of graphic design and animation, and take the important position for the
   understanding and development of the contemporary design and content
   industry. Moreover, the motion graphics which present the experiment of
   various techniques and the aspect of mixed genres contribute to the
   development and diversification of the visual image. This paper is
   covered on not only F5 and Semi-Permanent that have published
   experimental artistic motion graphics but also the analysis of the
   developed ways and expressive patters to produce experimental motion
   graphics of their works. Furthermore, it discusses the experimental
   artistic value of these works.</abstract><date>2016</date><author>????</author></paper><paper><title>Real-time multi-view deconvolution</title><abstract>In light-sheet microscopy, overall image content and resolution are
   improved by acquiring and fusing multiple views of the sample from
   different directions. State-of-the-art multi-view (MV) deconvolution
   simultaneously fuses and deconvolves the images in 3D, but processing
   takes a multiple of the acquisition time and constitutes the bottleneck
   in the imaging pipeline. Here, we show that MV deconvolution in 3D can
   finally be achieved in real-time by processing cross-sectional planes
   individually on the massively parallel architecture of a graphics
   processing unit (GPU). Our approximation is valid in the typical case
   where the rotation axis lies in the imaging plane.</abstract><date>OCT 15 2015</date><author>Schmid, Benjamin
   Huisken, Jan</author></paper><paper><title>Towards in vivo estimation of reaction kinetics using high-throughput
   metabolomics data: a maximum likelihood approach</title><abstract>Background: High-throughput assays such as mass spectrometry have opened
   up the possibility for large-scale in vivo measurements of the
   metabolome. This data could potentially be used to estimate kinetic
   parameters for many metabolic reactions. However, high-throughput in
   vivo measurements have special properties that are not taken into
   account in existing methods for estimating kinetic parameters, including
   significant relative errors in measurements of metabolite concentrations
   and reaction rates, and reactions with multiple substrates and products,
   which are sometimes reversible. A new method is needed to estimate
   kinetic parameters taking into account these factors.Results: A new
   method, InVEst (In Vivo Estimation), is described for estimating
   reaction kinetic parameters, which addresses the specific challenges of
   in vivo data. InVEst uses maximum likelihood estimation based on a model
   where all measurements have relative errors. Simulations show that
   InVEst produces accurate estimates for a reversible enzymatic reaction
   with multiple reactants and products, that estimated parameters can be
   used to predict the effects of genetic variants, and that InVEst is more
   accurate than general least squares and graphic methods on data with
   relative errors. InVEst uses the bootstrap method to evaluate the
   accuracy of its estimates.Conclusions: InVEst addresses several
   challenges of in vivo data, which are not taken into account by existing
   methods. When data have relative errors, InVEst produces more accurate
   and robust estimates. InVEst also provides useful information about
   estimation accuracy using bootstrapping. It has potential applications
   of quantifying the effects of genetic variants, inference of the target
   of a mutation or drug treatment and improving flux estimation.</abstract><date>OCT 5 2015</date><author>Zhang, Weiruo
   Kolte, Ritesh
   Dill, David L.</author></paper><paper><title>ARCHER, a new Monte Carlo software tool for emerging heterogeneous
   computing environments</title><abstract>The Monte Carlo radiation transport community faces a number of
   challenges associated with peta- and exa-scale computing systems that
   rely increasingly on heterogeneous architectures involving hardware
   accelerators such as GPUs and Xeon Phi coprocessors. Existing Monte
   Carlo codes and methods must be strategically upgraded to meet emerging
   hardware and software needs. In this paper, we describe the development
   of a software, called ARCHER (Accelerated Radiation-transport
   Computations in Heterogeneous EnviRonments), which is designed as a
   versatile testbed for future Monte Carlo codes. Preliminary results from
   five projects in nuclear engineering and medical physics are presented.
   (C) 2014 Elsevier Ltd. All rights reserved.</abstract><date>AUG 2015</date><author>Xu, X. George
   Liu, Tianyu
   Su, Lin
   Du, Xining
   Riblett, Matthew
   Ji, Wei
   Gu, Deyang
   Carothers, Christopher D.
   Shephard, Mark S.
   Brown, Forrest B.
   Kalra, Mannudeep K.
   Liu, Bob</author></paper><paper><title>Optical tomography with discretized path integral.</title><abstract>We present a framework for optical tomography based on a path integral.
   Instead of directly solving the radiative transport equations, which
   have been widely used in optical tomography, we use a path integral that
   has been developed for rendering participating media based on the volume
   rendering equation in computer graphics. For a discretized
   two-dimensional layered grid, we develop an algorithm to estimate the
   extinction coefficients of each voxel with an interior point method.
   Numerical simulation results are shown to demonstrate that the proposed
   method works well. </abstract><date>2015-Jul</date><author>Yuan, Bingzhi
   Tamaki, Toru
   Kushida, Takahiro
   Mukaigawa, Yasuhiro
   Kubo, Hiroyuki
   Raytchev, Bisser
   Kaneda, Kazufumi</author></paper><paper><title>Redefining the Autonomic Nerve Distribution of the Bladder Using
   3-Dimensional Image Reconstruction</title><abstract>Purpose: We sought to create a 3-dimensional reconstruction of the
   autonomic nervous tissue innervating the bladder using male and female
   cadaver histopathology.Materials and Methods: We obtained bladder tissue
   from a male and a female cadaver. Axial cross sections of the bladder
   were generated at 3 to 5 mm intervals and stained with S100 protein. We
   recorded the distance between autonomic nerves and bladder mucosa. We
   manually demarcated nerve tracings using ImageScope software (Aperio,
   Vista, California), which we imported into BlenderTM graphics software
   to generate 3-dimensional reconstructions of autonomic nerve
   anatomy.Results: Mean nerve density ranged from 0.099 to 0.602 and 0.012
   to 0.383 nerves per mm(2) in female and male slides, respectively. The
   highest concentrations of autonomic innervation were located in the
   posterior aspect of the bladder neck in the female specimen and in the
   posterior region of the prostatic urethra in the male specimen. Nerve
   density at all levels of the proximal urethra and bladder neck was
   significantly higher in posterior vs anterior regions in female
   specimens (0.957 vs 0.169 nerves per mm(2), p &lt;0.001) and male specimens
   (0.509 vs 0.206 nerves per mm(2), p = 0.04).Conclusions: Novel
   3-dimensional reconstruction of the bladder is feasible and may help
   redefine our understanding of human bladder innervation. Autonomic
   innervation of the bladder is highly focused in the posterior aspect of
   the proximal urethra and bladder neck in male and female bladders.</abstract><date>DEC 2015</date><author>Spradling, Kyle
   Khoyilar, Cyrus
   Abedi, Garen
   Okhunov, Zhamshid
   Wikenheiser, Jamie
   Yoon, Renai
   Huang, Jiaoti
   Youssef, Ramy F.
   Ghoniem, Gamal
   Landman, Jaime</author></paper><paper><title>3D model reconstruction using neural gas accelerated on GPU</title><abstract>In this work, we propose the use of the neural gas (NG), a neural
   network that uses an unsupervised Competitive Hebbian Learning (CHL)
   rule, to develop a reverse engineering process. This is a simple and
   accurate method to reconstruct objects from point clouds obtained from
   multiple overlapping views using low-cost sensors. In contrast to other
   methods that may need several stages that include downsampling, noise
   filtering and many other tasks, the NG automatically obtains the 3D
   model of the scanned objects. To demonstrate the validity of our
   proposal we tested our method with several models and performed a study
   of the neural network parameterization computing the quality of
   representation and also comparing results with other neural methods like
   growing neural gas and Kohonen maps or classical methods like Voxel
   Grid. We also reconstructed models acquired by low cost sensors that can
   be used in virtual and augmented reality environments for redesign or
   manipulation purposes. Since the NG algorithm has a strong computational
   cost we propose its acceleration. We have redesigned and implemented the
   NG learning algorithm to fit it onto Graphics Processing Units using
   CUDA. A speed-up of 180x faster is obtained compared to the sequential
   CPU version. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>JUL 2015</date><author>Orts-Escolano, Sergio
   Garcia-Rodriguez, Jose
   Antonio Serra-Perez, Jose
   Jimeno-Morenilla, Antonio
   Garcia-Garcia, Alberto
   Morell, Vicente
   Cazorla, Miguel</author></paper><paper><title>HaShRECA: Hadoop Based Short Read Error Correction Algorithm for Genome
   Assembly</title><abstract>Next-generation high-throughput sequencing technologies have opened up
   new and challenging research opportunities. In particular,
   Next-generation sequencers produce a massive amount of short-reads data
   in a single run. However, the large amount of short-reads data produced
   is highly susceptible to errors, as compared to shotgun sequencing.
   Therefore, there is a peremptory demand to design fast and more accurate
   statistical and computational tools to analyze this data. We present
   HaShRECA, a new short-reads error correction algorithm based on
   probabilistic analysis of potential read errors that utilizes the Hadoop
   MapReduce framework. Experimental results show that HaShRECA is more
   accurate, as well as time and space efficient as compared to previous
   algorithms.</abstract><date>2015</date><author>Tahir, Muhammad
   Sardaraz, Muhammad
   Ikram, Ataul Aziz
   Bajwa, Hassan</author></paper><paper><title>Motion Aware Exposure Bracketing for HDR Video</title><abstract>Mobile phones and tablets are rapidly gaining significance as
   omnipresent image and video capture devices. In this context we present
   an algorithm that allows such devices to capture high dynamic range
   (HDR) video. The design of the algorithm was informed by a perceptual
   study that assesses the relative importance of motion and dynamic range.
   We found that ghosting artefacts are more visually disturbing than a
   reduction in dynamic range, even if a comparable number of pixels is
   affected by each. We incorporated these findings into a real-time,
   adaptive metering algorithm that seamlessly adjusts its settings to take
   exposures that will lead to minimal visual artefacts after recombination
   into an HDR sequence. It is uniquely suitable for real-time selection of
   exposure settings. Finally, we present an off-line HDR reconstruction
   algorithm that is matched to the adaptive nature of our real-time
   metering approach.</abstract><date>JUL 2015</date><author>Gryaditskaya, Yulia
   Pouli, Tania
   Reinhard, Erik
   Myszkowski, Karol
   Seidel, Hans-Peter</author></paper><paper><title>Large Scale Tissue Morphogenesis Simulation on Heterogenous Systems
   Based on a Flexible Biomechanical Cell Model</title><abstract>The complexity of biological tissue morphogenesis makes in silico
   simulations of such system very interesting in order to gain a better
   understanding of the underlying mechanisms ruling the development of
   multicellular tissues. This complexity is mainly due to two elements:
   firstly, biological tissues comprise a large amount of cells; secondly,
   these cells exhibit complex interactions and behaviors. To address these
   two issues, we propose two tools: the first one is a virtual cell model
   that comprise two main elements: firstly, a mechanical structure
   (membrane, cytoskeleton, and cortex) and secondly, the main behaviors
   exhibited by biological cells, i. e., mitosis, growth, differentiation,
   molecule consumption, and production as well as the consideration of the
   physical constraints issued from the environment. An artificial
   chemistry is also included in the model. This virtual cell model is
   coupled to an agent-based formalism. The second tool is a simulator that
   relies on the OpenCL framework. It allows efficient parallel simulations
   on heterogenous devices such as micro-processors or graphics processors.
   We present two case studies validating the implementation of our model
   in our simulator: cellular proliferation controlled by cell signalling
   and limb growth in a virtual organism.</abstract><date>SEP-OCT 2015</date><author>Jeannin-Girardon, Anne
   Ballet, Pascal
   Rodin, Vincent</author></paper><paper><title>Parallel Spatial-Spectral Hyperspectral Image Classification With Sparse
   Representation and Markov Random Fields on GPUs</title><abstract>Spatial-spectral classification is a very important topic in the field
   of remotely sensed hyperspectral imaging. In this work, we develop a
   parallel implementation of a novel supervised spectral-spatial
   classifier, which models the likelihood probability via l(1) - l(2)
   sparse representation and the spatial prior as a Gibbs distribution.
   This classifier takes advantage of the spatial piece-wise smoothness and
   correlation of neighboring pixels in the spatial domain, but its
   computational complexity is very high which makes its application to
   time-critical scenarios quite limited. In order to improve the
   computational efficiency of the algorithm, we optimized its serial
   version and developed a parallel implementation for commodity graphics
   processing units (GPUs). Our parallel spatial-spectral classifier with
   sparse representation and Markov random fields (SSC-SRMRF-P) exploits
   the low-level architecture of GPUs. The parallel optimization of the
   proposed method has been carried out using the compute unified device
   architecture (CUDA). The performance of the parallel implementation is
   evaluated and compared with the serial and multicore implementations on
   central processing units (CPUs). In fact, the proposed method has been
   designed to adequately exploit the massive data parallel capacities of
   GPUs together with the control and logic capacities of CPUs, thus
   resorting to a heterogeneous CPU-GPU framework in the design of the
   parallel algorithm. Experimental results using real hyperpsectral images
   demonstrate very high performance for the proposed CPU-GPU parallel
   method, both in terms of classification accuracy and computational
   performance.</abstract><date>JUN 2015</date><author>Wu, Zebin
   Wang, Qicong
   Plaza, Antonio
   Li, Jun
   Sun, Le
   Wei, Zhihui</author></paper><paper><title>hybridcheck: software for the rapid detection, visualization and dating
   of recombinant regions in genome sequence data</title><abstract>hybridcheck is a software package to visualize the recombination signal
   in large DNA sequence data set, and it can be used to analyse
   recombination, genetic introgression, hybridization and horizontal gene
   transfer. It can scan large (multiple kb) contigs and whole-genome
   sequences of three or more individuals. hybridcheck is written in the r
   software for OS X, Linux and Windows operating systems, and it has a
   simple graphical user interface. In addition, the r code can be readily
   incorporated in scripts and analysis pipelines. hybridcheck implements
   several ABBA-BABA tests and visualizes the effects of hybridization and
   the resulting mosaic-like genome structure in high-density graphics. The
   package also reports the following: (i) the breakpoint positions, (ii)
   the number of mutations in each introgressed block, (iii) the
   probability that the identified region is not caused by recombination
   and (iv) the estimated age of each recombination event. The divergence
   times between the donor and recombinant sequence are calculated using a
   JC, K80, F81, HKY or GTR correction, and the dating algorithm is
   exceedingly fast. By estimating the coalescence time of introgressed
   blocks, it is possible to distinguish between hybridization and
   incomplete lineage sorting. hybridcheck is libre software and it and its
   manual are free to download from .</abstract><date>MAR 2016</date><author>Ward, Ben J.
   van Oosterhout, Cock</author></paper><paper><title>Identification of minimal eukaryotic introns through GeneBase, a
   user-friendly tool for parsing the NCBI Gene databank</title><abstract>We have developed GeneBase, a full parser of the National Center for
   Biotechnology Information (NCBI) Gene database, which generates a fully
   structured local database with an intuitive user-friendly graphic
   interface for personal computers. Features of all the annotated
   eukaryotic genes are accessible through three main software tables,
   including for each entry details such as the gene summary, the gene
   exon/intron structure and the specific Gene Ontology attributions. The
   structuring of the data, the creation of additional calculation fields
   and the integration with nucleotide sequences allow users to make many
   types of comparisons and calculations that are useful for data retrieval
   and analysis. We provide an original example analysis of the existing
   introns across all the available species, through which the classic
   biological problem of the 'minimal intron' may find a solution using
   available data. Based on all currently available data, we can define the
   shortest known eukaryotic GT-AG intron length, setting the physical
   limit at the 30 base pair intron belonging to the human MST1L gene. This
   'model intron' will shed light on the minimal requirement elements of
   recognition used for conventional splicing functioning. Remarkably, this
   size is indeed consistent with the sum of the splicing consensus
   sequence lengths.</abstract><date>DEC 2015</date><author>Piovesan, Allison
   Caracausi, Maria
   Ricci, Marco
   Strippoli, Pierluigi
   Vitale, Lorenza
   Pelleri, Maria Chiara</author></paper><paper><title>RViz: a toolkit for real domain data visualization</title><abstract>In computational science and computer graphics, there is a strong
   requirement to represent and visualize information in the real domain,
   and many visualization data structures and algorithms have been proposed
   to achieve this aim. Unfortunately, the dataflow model that is often
   selected to address this issue in visualization systems is not flexible
   enough to visualize newly invented data structures and algorithms
   because this scheme can accept only specific data structures. To address
   this problem, we propose a new visualization tool, RViz, which is
   independent of the input information data structures. Since there is no
   requirement for additional efforts to manage the flow networks and the
   interface to abstracted information is simple in RViz, any scientific
   information visualization algorithms are easier to implement than the
   dataflow model. In this paper, we provide case studies in which we have
   successfully implemented new data structures and related algorithms
   using RViz, including geometry synthesis, distance field representation,
   and implicit surface reconstruction. Through these cases, we show how
   RViz helps users visualize and understand any hidden insights in input
   information.</abstract><date>OCT 2015</date><author>Kam, Hyeong Ryeol
   Lee, Sung-Ho
   Park, Taejung
   Kim, Chang-Hun</author></paper><paper><title>On Implementation High-Scalable CFD Solvers for Hybrid Clusters with
   Massively-Parallel Architectures</title><abstract>New approach for solving of compressible fluid dynamic problems with
   complex geometry on Cartesian grids is proposed. It leads to algorithmic
   uniformity for whole domain and structured memory accesses which are
   essential for effective implementations on massively-parallel
   architectures - GPUs. Methods used are based on implicit scheme and
   LU-SGS method. Novel parallel algorithm for last one is proposed.
   In-depth analysis of CUDA+MPI implementation (interoperability issues,
   libraries tuning) scalable up to hundreds GPUs is performed.</abstract><date>2015</date><author>Pavlukhin, Pavel
   Menshov, Igor</author></paper><paper><title>Quantitative multi-spectral oxygen saturation measurements independent
   of tissue optical properties</title><abstract>Imaging of tissue oxygenation is important in several applications
   associated with patient care. Optical sensing is commonly applied for
   assessing oxygen saturation but is often restricted to local
   measurements or else it requires spectral and spatial information at the
   expense of time. Many methods proposed so far require assumptions on the
   properties of measured tissue. In this study we investigated a
   computational method that uses only multispectral information and
   quantitatively computes tissue oxygen saturation independently of tissue
   optical properties. The method is based on linear transformations of
   measurements in three isosbestic points. We investigated the ideal
   isosbestic point combination out of six isosbestic points available for
   measurement in the visible and near-infrared region that enable accurate
   oxygen saturation computation. We demonstrate this method on controlled
   tissue mimicking phantoms having different optical properties and
   validated the measurements using a gas analyzer. A mean error of 2.9 +/-
   2.8% O(2)Sat was achieved. Finally, we performed pilot studies in
   tissues in-vivo by measuring dynamic changes in fingers subjected to
   vascular occlusion, the vasculature of mouse ears and exposed mouse
   organs.[GRAPHICS]Selected steps of spectral transformations applied to
   oxygenation spectra. The original reflectance spectrum M(lambda) is
   transformed in step 1 to overlap with reference spectra (grey) in three
   isosbestic points, resulting in M ''(lambda). In step 2, the gradient of
   M ''(lambda) is computed resulting in M-grad ''(lambda), which can be
   used for quantitative oxygenation computation.</abstract><date>JAN 2016</date><author>Radrich, Karin
   Ntziachristos, Vasilis</author></paper><paper><title>Multi-GPU and multi-CPU accelerated FDTD scheme for vibroacoustic
   applications</title><abstract>The Finite-Difference Time-Domain (FDTD) method is applied to the
   analysis of vibroacoustic problems and to study the propagation of
   longitudinal and transversal waves in a stratified media. The potential
   of the scheme and the relevance of each acceleration strategy for
   massively computations in FDTD are demonstrated in this work. In this
   paper, we propose two new specific implementations of the bidimensional
   scheme of the FDTD method using multi-CPU and multi-GPU, respectively.
   In the first implementation, an open source message passing interface
   (OMPI) has been included in order to massively exploit the resources of
   a biprocessor station with two Intel Xeon processors. Moreover,
   regarding CPU code version, the streaming SIMD extensions (SSE) and also
   the advanced vectorial extensions (AVX) have been included with shared
   memory approaches that take advantage of the multi-core platforms. On
   the other hand, the second implementation called the multi-GPU code
   version is based on Peer-to-Peer communications available in CUDA on two
   GPUs (NVIDIA GTX 670). Subsequently, this paper presents an accurate
   analysis of the influence of the different code versions including
   shared memory approaches, vector instructions and multi-processors (both
   CPU and GPU) and compares them in order to delimit the degree of
   improvement of using distributed solutions based on multi-CPU and
   multi-GPU. The performance of both approaches was analysed and it has
   been demonstrated that the addition of shared memory schemes to CPU
   computing improves substantially the performance of vector instructions
   enlarging the simulation sizes that use efficiently the cache memory of
   CPUs. In this case GPU computing is slightly twice times faster than the
   fine tuned CPU version in both cases one and two nodes. However, for
   massively computations explicit vector instructions do not worth it
   since the memory bandwidth is the limiting factor and the performance
   tends to be the same than the sequential version with auto-vectorisation
   and also shared memory approach. In this scenario GPU computing is the
   best option since it provides a homogeneous behaviour. More
   specifically, the speedup of GPU computing achieves an upper limit of 12
   for both one and two GPUs, whereas the performance reaches peak values
   of 80 GFlops and 146 GFlops for the performance for one GPU and two GPUs
   respectively. Finally, the method is applied to an earth crust profile
   in order to demonstrate the potential of our approach and the necessity
   of applying acceleration strategies in these type of applications. (C)
   2015 Elsevier B.V. All rights reserved.</abstract><date>JUN 2015</date><author>Frances, J.
   Otero, B.
   Bleda, S.
   Gallego, S.
   Neipp, C.
   Marquez, A.
   Belendez, A.</author></paper><paper><title>Strong scaling of general-purpose molecular dynamics simulations on GPUs</title><abstract>We describe a highly optimized implementation of MPI domain
   decomposition in a GPU-enabled, general-purpose molecular dynamics code,
   HOOMD-blue (Anderson and Glotzer, 2013). Our approach is inspired by a
   traditional CPU-based code, LAMMPS (Plimpton, 1995), but is implemented
   within a code that was designed for execution on GPUs from the start
   (Anderson et al., 2008). The software supports short-ranged pair force
   and bond force fields and achieves optimal GPU performance using an
   autotuning algorithm. We are able to demonstrate equivalent or superior
   scaling on up to 3375 GPUs in Lennard-Jones and dissipative particle
   dynamics (DPD) simulations of up to 108 million particles. GPUDirect
   RDMA capabilities in recent GPU generations provide better performance
   in full double precision calculations. For a representative polymer
   physics application, HOOMD-blue 1.0 provides an effective GPU vs. CPU
   node speed-up of 12.5x. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>JUL 2015</date><author>Glaser, Jens
   Trung Dac Nguyen
   Anderson, Joshua A.
   Lui, Pak
   Spiga, Filippo
   Millan, Jaime A.
   Morse, David C.
   Glotzer, Sharon C.</author></paper><paper><title>Analyzing the potential of GPGPUs for real-time explicit finite element
   analysis of soft tissue deformation using CUDA</title><abstract>As the presence of finite element implementations on General Purpose
   Graphics Processing Units (GPGPUs) is the literature increases, detailed
   and in-breadth testing of the hardware is somewhat lacking. We present
   an implementation and detailed analysis of an FE algorithm designed for
   real-time solution, particularly aimed at elasticity problems applied to
   soft tissue deformation.An efficient parallel implementation of Total
   Lagrangian Explicit Dynamics implementation is elucidated and the
   potential for real-time execution is examined. It is shown that in
   conjunction with modern computing architectures, solution times can be
   significantly reduced, depending on the solution strategy.The usability
   of the method is investigated by conducting a broad assay on ranging
   model sizes and different cards and comparing to an industry-proven FE
   code Abaqus. In doing so, we study the effect of using single/double
   precision computation, quantify and present error measurements as a
   function of the number of time-steps. We also examine the usage of a
   special texture memory space and its effect on computation for different
   devices. Adding material complexity in the form of a tissue damage model
   is presented and its computational impact elucidated. The aggregate
   results show that, for a particular set of problems, it is possible to
   compute a simple set of test cases 30-120 times faster than current
   commercial solutions.According to the speedups achieved, an indication
   is provided that the GPGPU technology shows promise in the undertaking
   of real-time FE computation. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>NOV 1 2015</date><author>Strbac, Vukasin
   Sloten, Jos Vander
   Famaey, Nele</author></paper><paper><title>Real-Time FFT Computation Using GPGPU for OFDM-Based Systems</title><abstract>In optical and wireless communications systems, the goal is to reach 10
   Gbps or above data rates. In order to support such extremely high data
   rates, the physical layer generally uses orthogonal frequency division
   multiplexing (OFDM) modulation. Unlike serial transmission of symbols,
   the OFDM modulation transmits data with many parallel sub-carriers,
   which help to provide high bandwidth. Field programmable gate arrays
   (FPGAs) and digital signal processors (DSPs) are usually employed to
   process OFDM blocks in real time. However, FPGAs and DSPs are not cost
   effective, and they are difficult to adapt to new standards. One of the
   most computationally intensive functions in OFDM systems is the fast
   Fourier transform (FFT) computation process. This paper aims to
   accelerate the FFT process to achieve high communication throughput in
   real time. Two parallel approaches are implemented for two different
   NVIDIA graphics processing unit (GPU) architectures. To obtain the best
   performance values, several optimizations are implemented. Our general
   purpose graphics processing unit (GPGPU)-based FFT computation achieves
   up to 24 Gbps throughput in real time.</abstract><date>MAR 2016</date><author>Cetin, Omer
   Keskin, Selcuk
   Kocak, Taskin</author></paper><paper><title>FLOWING BILATERAL FILTER: DEFINITION AND IMPLEMENTATIONS</title><abstract>The bilateral filter plays a key role in image processing applications
   due to its intuitive parameterization and its high quality filter
   result, smoothing homogeneous regions while preserving the edges of the
   objects. Considering the image as a topological relief, seeing pixel
   intensities as peaks and valleys, we introduce a way to control the
   tonal weighting coefficients, the flowing bilateral filter, reducing
   "halo" artifacts typically produced by the regular bilateral filter
   around a large peak surrounded by two valleys of lower values. In this
   paper we propose to investigate exact and approximated versions of CPU
   and parallel GPU (Graphical Processing Unit) based implementations of
   the regular and flowing bilateral filter using the NVidia CUDA API. Fast
   implementations of these filters are important for the processing of
   large 3D volumes up to several GB acquired by x-ray or electron
   tomography.</abstract><date>2015</date><author>Moreaud, Maxime
   Cokelaer, Francois</author></paper><paper><title>Fast filter bank convolution for three-dimensional wavelet transform by
   shared memory on mobile GPU computing</title><abstract>Mobile GPU applications usually constrain by the real-time requirement.
   However, FLOPS of mobile GPU is limited by the size and power supply of
   the SoC systems. Same to desktop GPUs, the mobile GPU consists of an
   on-chip memory hierarchy, and proper usage of memory hierarchy
   accelerates mobile GPU applications such as Discrete Wavelet Transform
   (DWT) to satisfy the real-time requirement. In this paper, by taking
   advantage of GPU shared memory in Tegra K1, a mobile GPU from Nvidia, we
   develop Bank Conflict Free Shared Memory Parallel DWT for mobile GPU
   applications. Computational results show that, with the display
   resolution of (EGA), Bank Conflict Free Shared Memory Parallel DWT is
   significantly faster than SoC CPU-based DWT. Computational results also
   show that, with the display resolution of (CGA), (VGA), (SVGA) and
   (XGA), Bank Conflict Free Shared Memory Parallel DWT can generally
   satisfy the real-time requirement.</abstract><date>SEP 2015</date><author>Zhao, Di</author></paper><paper><title>Rare plants distribution modeling using indirect environmental gradients</title><abstract>Knowledge of geographic and ecological pattern of rare and endangered
   species distribution is of essential importance for understanding human
   activities threatening biodiversity. This study aims to show how
   quantitative data for the abundance of rare plant species in a localized
   territory can be modeled successfully. Ordination and species
   distribution models (SDMs) combine field observation of species
   abundance data with environmental gradients, also measured on the field,
   to produce statistically tested for their accuracy, graphic (biplots,
   response surfaces, etc.) and numerical models which can then be used for
   conservation or management of rare plants. Species distribution
   modeling, shown in this study, can be successfully used for probability
   assessment for finding unknown populations of rare plants as well as
   guiding tool in their search, saving considerable resources and time.</abstract><date>JUL 2015</date><author>Dyakov, N. R.</author></paper><paper><title>Algorithms for GPU-Based Molecular Dynamics Simulations of Complex
   Fluids: Applications to Water, Mixtures, and Liquid Crystals</title><abstract>A custom code for molecular dynamics simulations has been designed to
   run on CUDA-enabled NVIDIA graphics processing units (GPUs). The
   double-precision code simulates multicomponent fluids, with
   intramolecular and intermolecular forces, coarse-grained and atomistic
   models, holonomic constraints, Nose-Hoover thermostats, and the
   generation of distribution functions. Algorithms to compute
   Lennard-Jones and Gay-Berne interactions, and the electrostatic force
   using Ewald summations, are discussed. A neighbor list is introduced to
   improve scaling with respect to system size. Three test systems are
   examined: SPC/E water; an n-hexane/2-propanol mixture; and a liquid
   crystal mesogen, 2-(4-butyloxyphenyl)-5-octyloxypyrimidine. Code
   performance is analyzed for each system. With one GPU, a 33-119 fold
   increase in performance is achieved compared with the serial code while
   the use of two GPUs leads to a 69-287 fold improvement and three GPUs
   yield a 101-377 fold speedup. (C) 2015 Wiley Periodicals, Inc.</abstract><date>SEP 15 2015</date><author>Kazachenko, Sergey
   Giovinazzo, Mark
   Hall, Kyle Wm.
   Cann, Natalie M.</author></paper><paper><title>Theoretical studies on the reaction of mono- and ditriflate derivatives
   of 1,4:3,6-dianhydro-D-mannitol with trimethylamine-Can a quaternary
   ammonium salt be a source of the methyl group?</title><abstract>DFT studies on the mechanism of the formation of "gemini" quaternary
   ammonium salts in the reaction of 1,4:3,6-dianhydro-d-mannitol
   ditriflate derivative with trimethylamine and its subsequent conversion
   to tertiary amine through the methyl-transfer reaction are discussed.
   Two alternative reaction pathways are presented in the gas phase and in
   ethanol. Additionally, the transformation of the monotriflate derivative
   of 1,4:3,6-dianhydro-d-mannitol into the single quaternary ammonium salt
   is presented. Two functionals (B3LYP, M062X) and two basis sets
   (6-31+G** and 6-311++G**) were used for the calculations. The effect of
   the substituent attached to the five-membered rings at the C2 (and/or
   C5) carbon atom on the activation barrier is described. The
   trimethylammonium group bond to the five-membered ring greatly reduces
   the activation barrier height. The preferred reaction pathway for the
   conversions was established. Including the London dispersion in the
   calculations increases the stabilization of all the points on the
   potential energy surface in relation to individual reactants.[GRAPHICS].</abstract><date>JAN 2016</date><author>Bednarko, Justyna
   Wielinska, Justyna
   Sikora, Karol
   Liberek, Beata
   Nowacki, Andrzej</author></paper><paper><title>A new teiid lizard from the Late Cretaceous of the Hateg Basin, Romania
   and its phylogenetic and palaeobiogeographical relationships</title><abstract>A new lizard genus and species is described based on a
   three-dimensionally preserved partial skull and associated lower jaws
   from the Pui Islaz locality (Late Cretaceous, early Maastrichtian) in
   the Hateg Basin, western Romania. Barbatteius vremiri gen. et sp. nov.
   is diagnosed by a unique combination of symplesiomorphies and
   synapomorphies. A nested set of synapomorphies support assigning
   Barbatteius to Teiidae as the first unambiguous Late Cretaceous record
   of this family from Laurasia. Barbatteius differs from other teiids by
   having more extensive osteodermal sculpture on the skull roof and
   suspensorium, and by a pentagonal occipital osteoscute exhibiting more
   or less parallel lateral margins. Barbatteius is a large-bodied lizard,
   estimated to be up to 800 mm in total length. It has weakly heterodont
   dentition, but without enlarged posterior crushing teeth, suggesting
   that it fed on arthropods, small vertebrates and plants. The mix of taxa
   with affinities to Euramerica (paramacellodid and borioteiioid lizards)
   and Gondwana (madtsoiid snakes and the teiid Barbatteius) currently
   known for the Maastrichtian squamate assemblage from Hateg Basin
   supports the growing realization that 'Hateg Island' has a complex
   palaeobiogeographical history.[GRAPHICS]</abstract><date>MAR 3 2016</date><author>Venczel, Marton
   Codrea, Vlad A.</author></paper><paper><title>Hyper-Real-Time Ice Simulation and Modeling Using GPGPU</title><abstract>This paper describes the design of an efficient parallel implementation
   of an ice simulator that simulates the behaviour of a ship operating in
   pack ice. The main idea of the method is to treat ice as a set of
   discrete objects with very simple properties, and to model the system
   mechanics mainly as a set of discrete contact and failure events. In
   this way it becomes possible to parallelize the problem, so that a very
   large number of ice floes can be modeled. This approach is called the
   Ice Event Mechanics Modeling (IEMM) method which builds a system
   solution from a large set of discrete events occurring between a large
   set of discrete objects. The simulator is developed using the NVIDIA
   Compute Unified Device Architecture (CUDA). This paper also describes
   the execution of experiments to evaluate the performance of the
   simulator and to validate the numerical modeling of ship operations in
   pack ice. Our results show speed up of 11 times, reducing simulation
   time for a large ice field (9,801 floes) from over 2 hours to about 12
   minutes.</abstract><date>DEC 2015</date><author>Alawneh, Shadi
   Dragt, Roelof
   Peters, Dennis
   Daley, Claude
   Bruneau, Stephen</author></paper><paper><title>Medical student preferences for self-directed study resources in gross
   anatomy.</title><abstract>Gross anatomy instruction in medical curricula involve a range of
   resources and activities including dissection, prosected specimens,
   anatomical models, radiological images, surface anatomy, textbooks,
   atlases, and computer-assisted learning (CAL). These resources and
   activities are underpinned by the expectation that students will
   actively engage in self-directed study (SDS) to enhance their knowledge
   and understanding of anatomy. To gain insight into preclinical versus
   clinical medical students' preferences for SDS resources for learning
   gross anatomy, and whether these vary on demographic characteristics and
   attitudes toward anatomy, students were surveyed at two Australian
   medical schools, one undergraduate-entry and the other graduate-entry.
   Lecture/tutorial/practical notes were ranked first by 33% of 156
   respondents (mean rank+-SD, 2.48+-1.38), textbooks by 26% (2.62+-1.35),
   atlases 20% (2.80+-1.44), videos 10% (4.34+-1.68), software 5%
   (4.78+-1.50), and websites 4% (4.24+-1.34). Among CAL resources, Wikipedia
   was ranked highest. The most important factor in selecting CAL resources
   was cost (ranked first by 46%), followed by self-assessment, ease of
   use, alignment with curriculum, and excellent graphics (each 6-9%).
   Compared with preclinical students, clinical students ranked software
   and Acland's Video Atlas of Human Anatomy higher and felt radiological
   images were more important in selecting CAL resources. Along with other
   studies reporting on the quality, features, and impact on learning of
   CAL resources, the diversity of students' preferences and opinions on
   usefulness and ease of use reported here can help guide faculty in
   selecting and recommending a range of CAL and other resources to their
   students to support their self-directed study. Anat Sci Educ 9: 150-160.
    2015 American Association of Anatomists. </abstract><date>2016-Mar</date><author>Choi-Lundberg, Derek L
   Low, Tze Feng
   Patman, Phillip
   Turner, Paul
   Sinha, Sankar N</author></paper><paper><title>Implementation of Viterbi Decoder toward GPU-Based SDR Receiver</title><abstract>Viterbi decoding is commonly used for several protocols, but
   computational cost is quite high and thus it is necessary to implement
   it effectively. This paper describes GPU implementation of Viterbi
   decoder utilizing three-point Viterbi decoding algorithm (TVDA), in
   which the received bits are divided into multiple chunks and several
   chunks are decoded simultaneously. Coalesced access and Warp Shuffle,
   which is new instruction introduced are also utilized in order to
   improve decoder performance. In addition, iterative execution of
   parallel chunks decoding reduces the latency of proposed Viterbi decoder
   in order to utilize the decoder as a part of GPU-based SDR transceiver.
   As the result, the throughput of proposed Viterbi decoder is improved by
   23.1%.</abstract><date>NOV 2015</date><author>Tomita, Kosuke
   Hatanaka, Masahide
   Onoye, Takao</author></paper><paper><title>Parallel Connected Component Labeling Based on the Selective Four
   Directional Label Search Using CUDA</title><abstract>Connected component labeling (CCL) is a mandatory step in image
   segmentation where objects are extracted and uniquely labeled. CCL is a
   computationally expensive operation and thus is often done in parallel
   processing framework to reduce execution time. Various parallel CCL
   methods have been proposed in the literature. Among them are NSZ label
   equivalence (NSZ-LE) method, modified 8 directional label selection
   (M8DLS) method, HYBRID1 method, and HYBRID2 method. Soh et al. showed
   that HYBRID2 outperforms the others and is the best so far. In this
   paper we propose a new hybrid parallel CCL algorithm termed as HYBRID3
   that combines selective four directional label search (S4DLS) with label
   backtracking (LB). We show that the average percentage speedup of the
   proposed over M8DLS is around 60% more than that of HYBRID2 over M8DLS
   for various kinds of images.</abstract><date>2015</date><author>???</author></paper><paper><title>Finite element modelling of elastic wave scattering within a
   polycrystalline material in two and three dimensions</title><abstract>Finite element modelling is a promising tool for further progressing the
   development of ultrasonic non-destructive evaluation of polycrystalline
   materials. Yet its widespread adoption has been held back due to a high
   computational cost, which has restricted current works to relatively
   small models and to two dimensions. However, the emergence of
   sufficiently powerful computing, such as highly efficient solutions on
   graphics processors, is enabling a step improvement in possibilities.
   This article aims to realise those capabilities to simulate ultrasonic
   scattering of longitudinal waves in an equiaxed polycrystalline material
   in both two (2D) and three dimensions (3D). The modelling relies on an
   established Voronoi approach to randomly generate a representative grain
   morphology. It is shown that both 2D and 3D numerical data show good
   agreement across a range of scattering regimes in comparison to
   well-established theoretical predictions for attenuation and phase
   velocity. In addition, 2D parametric studies illustrate the mesh
   sampling requirements for two different types of mesh to ensure
   modelling accuracy and present useful guidelines for future works.
   Modelling limitations are also shown. It is found that 2D models reduce
   the scattering mechanism in the Rayleigh regime. (C) 2015 Acoustical
   Society of America.</abstract><date>OCT 2015</date><author>Van Pamel, Anton
   Brett, Colin R.
   Huthwaite, Peter
   Lowe, Michael J. S.</author></paper><paper><title>Systolic genetic search, a systolic computing-based metaheuristic</title><abstract>In this paper, we propose a new parallel optimization algorithm that
   combines ideas from the fields of metaheuristics and Systolic Computing.
   The algorithm, called Systolic Genetic Search (SGS), is designed to
   explicitly exploit the high degree of parallelism available in modern
   Graphics Processing Unit (GPU) architectures. In SGS, solutions
   circulate synchronously through a grid of processing cells, which apply
   adapted evolutionary operators on their inputs to compute their outputs
   that are then ejected from the cells and continue moving through the
   grid. Four different variants of SGS are experimentally studied for
   solving two classical benchmarking problems and a real-world
   application. An extensive experimental analysis, which considered
   several instances for each problem, shows that three of the SGS variants
   designed are highly effective since they can obtain the optimal solution
   in almost every execution for the instances and problems studied, as
   well as they outperform a Random Search (sanity check) and two Genetic
   Algorithms. The parallel implementation on GPU of the proposed algorithm
   has achieved a high performance obtaining runtime reductions from the
   sequential implementation that, depending on the instance considered,
   can arrive to around a hundred times, and have also exhibited a good
   scalability behavior when solving highly dimensional problem instances.</abstract><date>JUL 2015</date><author>Pedemonte, Martin
   Luna, Francisco
   Alba, Enrique</author></paper><paper><title>Elastic moduli of simple mass spring models</title><abstract>Mass spring models (MSMs) are a popular choice for representation of
   soft bodies in computer graphics and virtual reality applications. In
   this paper, we investigate physical properties of the simplest MSMs
   composed of mass points and linear springs. The nodes are either placed
   on a cubic lattice or positioned randomly within the system. We
   calculate the elastic moduli for such models and relate the results to
   other studies. We show that there is a well-defined relationship between
   the geometric characteristics of the MSM systems and physical properties
   of the modeled materials. It is also demonstrated that these models
   exhibit a proper convergence to a unique solution upon mesh refinement
   and thus can represent elastic materials with a high precision.</abstract><date>OCT 2015</date><author>Kot, Maciej
   Nagahashi, Hiroshi
   Szymczak, Piotr</author></paper><paper><title>Cytopathology whole slide images and adaptive tutorials for postgraduate
   pathology trainees: a randomized crossover tirial</title><abstract>To determine whether cytopathology whole slide images and virtual
   microscopy adaptive tutorials aid learning by postgraduate trainees, we
   designed a randomized crossover trial to evaluate the quantitative and
   qualitative impact of whole slide images and virtual microscopy adaptive
   tutorials compared with traditional glass slide and textbook methods of
   learning cytopathology. Forty-three anatomical pathology registrars were
   recruited from Australia, New Zealand, and Malaysia. Online assessments
   were used to determine efficacy, whereas user experience and perceptions
   of efficiency were evaluated using online Likert scales and open-ended
   questions. Outcomes of online assessments indicated that, with respect
   to performance, learning with whole slide images and virtual microscopy
   adaptive tutorials was equivalent to using traditional methods.
   High-impact learning, efficiency, and equity of learning from virtual
   microscopy adaptive tutorials were strong themes identified in
   open-ended responses. Participants raised concern about the lack of
   z-axis capability in the cytopathology whole slide images, suggesting
   that delivery of z-stacked whole slide images online may be important
   for future educational development. In this trial, learning
   cytopathology with whole slide images and virtual microscopy adaptive
   tutorials was found to be as effective as and perceived as more
   efficient than learning from glass slides and textbooks. The use of
   whole slide images and virtual microscopy adaptive tutorials has the
   potential to provide equitable access to effective learning from
   teaching material of consistently high quality. It also has broader
   implications for continuing professional development and maintenance of
   competence and quality assurance in specialist practice. (C) 2015
   Elsevier Inc. All rights reserved.</abstract><date>SEP 2015</date><author>Van Es, Simone L.
   Kumar, Rakesh K.
   Pryor, Wendy M.
   Salisbury, Elizabeth L.
   Velan, Gary M.</author></paper><paper><title>Improving Performance of Image Retrieval Based on Fuzzy Colour
   Histograms by Using Hybrid Colour Model and Genetic Algorithm</title><abstract>A hybrid colour model is a colour descriptor formed by combining
   channels from several different colour models. Although rarely used in
   computer graphics applications due to redundancy, hybrid colour models
   may be of interest for the Content-Based Image Retrieval (CBIR). Best
   features of each colour model can be combined to obtain optimal
   retrieval performance. This paper evaluates several approaches to the
   construction of a hybrid colour model that is used to construct a fuzzy
   colour histogram of image as a compact feature for retrieval. By
   evaluating each channel separately, a colour model named HSY is
   proposed. Various parameters of fuzzy histogram are further improved
   using Genetic algorithm (GA). Using standard data sets and the Average
   Normalized Modified Retrieval Rank (ANMRR) as a metric for retrieval
   performance, it is shown that this novel approach can give an improved
   retrieval performance.</abstract><date>DEC 2015</date><author>Ljubovic, V.
   Supic, H.</author></paper><paper><title>Particle-based shallow water simulation for irregular and sparse
   simulation domains</title><abstract>We propose a shallow water simulation using a Lagrangian technique.
   Smoothed particle hydrodynamics are used to solve the shallow water
   equation, so we avoid discretization of the entire simulation domain and
   easily handle sparse and irregular simulation domains. In the context of
   shallow water equations, much less attention has been paid to Lagrangian
   simulation methods than to Eulerian methods. Therefore, many problems
   remained unsolved, which prevents the practical use of Lagrangian
   shallow water simulations in computer graphics. We concentrate on
   several issues associated with the simulation. First, we increase the
   accuracy of the smoothed particle hydrodynamics approximation by
   applying a correction to the kernel function that is used in the
   simulation. Second, we introduce a novel boundary handling algorithm
   that can handle arbitrary boundary domains; even irregular and
   complicated boundaries do not pose a problem and introduce only small
   computational overhead. Third, with the increased accuracy, we use the
   fluid height to generate a flat fluid surface. All the proposed methods
   can easily be integrated into the smoothed particle hydrodynamics
   framework. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>DEC 2015</date><author>Chladek, Michal
   Durikovic, Roman</author></paper><paper><title>GPU-based parallel algorithm for computing point visibility inside
   simple polygons</title><abstract>Given a simple polygon P in the plane, we present a parallel algorithm
   for computing the visibility polygon of an observer point q inside P. We
   use chain visibility concept and a bottom-up merge method for
   constructing the visibility polygon of point q. The algorithm is simple
   and mainly designed for GPU architectures, where it runs in O(log n)
   time using O(n) processors. This is the first work on designing a
   GPU-based parallel algorithm for the visibility problem. To the best of
   our knowledge, the presented algorithm is also the first suboptimal
   parallel algorithm for the visibility problem that can be implemented on
   existing parallel architectures. We evaluated a sample implementation of
   the algorithm on several large polygons. The experiments indicate that
   our algorithm can compute the visibility of a polygon having over 4M
   points in tenth of a second on an NVIDIA GTX 780 Ti GPU. (C) 2015
   Elsevier Ltd. All rights reserved.</abstract><date>JUN 2015</date><author>Shoja, Ehsan
   Ghodsi, Mohammad</author></paper><paper><title>Solutions to the st-connectivity problem using a GPU-based distributed
   BFS</title><abstract>The st-connectivity problem (ST-CON) is a decision problem that asks,
   for vertices s and t in a graph, if t is reachable from s. Although
   originally defined for directed graphs, it can also be studied on
   undirected graphs and used as a building block for solving more complex
   tasks on large scale graphs. We present solutions to ST-CON based on a
   high performance Breadth First Search (BFS) executed on clusters of
   Graphics Processing Units (GPUs) using the Nvidia CUDA platform. To
   measure performances, we use the number of ST-CONs per second. We
   present the results for two different implementations that highlight the
   impact of atomic operations in CUDA. (C) 2014 Elsevier Inc. All rights
   reserved.</abstract><date>FEB 2015</date><author>Bernaschi, Massimo
   Carbone, Giancarlo
   Mastrostefano, Enrico
   Vella, Flavio</author></paper><paper><title>GPGPU-MiniBench: Accelerating GPGPU Micro-Architecture Simulation</title><abstract>Graphics processing units (GPU), due to their massive computational
   power with up to thousands of concurrent threads and general-purpose GPU
   (GPGPU) programming models such as CUDA and OpenCL, have opened up new
   opportunities for speeding up general-purpose parallel applications.
   Unfortunately, pre-silicon architectural simulation of modern-day GPGPU
   architectures and workloads is extremely time-consuming. This paper
   addresses the GPGPU simulation challenge by proposing a framework,
   called GPGPU-MiniBench, for generating miniature, yet representative
   GPGPU workloads. GPGPU-MiniBench first summarizes the inherent execution
   behavior of existing GPGPU workloads in a profile. The central component
   in the profile is the Divergence Flow Statistics Graph (DFSG), which
   characterizes the dynamic control flow behavior including loops and
   branches of a GPGPU kernel. GPGPU-MiniBench generates a synthetic
   miniature GPGPU kernel that exhibits similar execution characteristics
   as the original workload, yet its execution time is much shorter thereby
   dramatically speeding up architectural simulation. Our experimental
   results show that GPGPU-MiniBench can speed up GPGPU architectural
   simulation by a factor of 49x on average and up to 589x, with an average
   IPC error of 4.7 percent across a broad set of GPGPU benchmarks from the
   CUDA SDK, Rodinia and Parboil benchmark suites. We also demonstrate the
   usefulness of GPGPU-MiniBench for driving GPU architecture exploration.</abstract><date>NOV 2015</date><author>Yu, Zhibin
   Eeckhout, Lieven
   Goswami, Nilanjan
   Li, Tao
   John, Lizy K.
   Jin, Hai
   Xu, Chengzhong
   Wu, Junmin</author></paper><paper><title>Parallel Implementation of WRF Double Moment 5-Class Cloud Microphysics
   Scheme on Multiple GPUs</title><abstract>The Weather Research and Forecast (WRF) Double Moment 5-class (WDM5)
   mixed ice microphysics scheme predicts the mixing ratio of hydrometeors
   and their number concentrations for warm rain species including clouds
   and rain. WDM5 can be computed in parallel in the horizontal domain
   using multi-core GPUs. In order to obtain a better GPU performance, we
   manually rewrite the original WDM5 Fortran module into a highly parallel
   CUDA C program. We explore the usage of coalesced memory access and
   asynchronous data transfer. Our GPU-based WDM5 module is scalable to run
   on multiple GPUs. By employing one NVIDIA Tesla K40 GPU, our GPU
   optimization effort on this scheme achieves a speedup of 252x with
   respect to its CPU counterpart Fortran code running on one CPU core of
   Intel Xeon E5-2603, whereas the speedup for one CPU socket (4 cores)
   with respect to one CPU core is only 4.2x. We can even boost the speedup
   of this scheme to 468x with respect to one CPU core when two NVIDIA
   Tesla K40 GPUs are applied.</abstract><date>2015</date><author>Huang, Melin
   Huang, Bormin
   Huang, Allen H. -L.</author></paper><paper><title>Real-time finite element structural analysis in augmented reality</title><abstract>Conventional finite element analysis (FEA) is usually carried out in
   offsite and virtual environments, i.e., computer-generated graphics,
   which does not promote a user's perception and interaction, and limits
   its applications. With the purpose of enhancing structural analysis with
   augmented reality (AR) technologies, the paper presents a system which
   integrates sensor measurement and real-time FEA simulation into an
   AR-based environment. By incorporating scientific visualization
   technologies, this system superimposes FEA results directly on
   real-world objects, and provides intuitive interfaces for enhanced data
   exploration. A wireless sensor network has been integrated into the
   system to acquire spatially distributed loads, and a method to register
   the sensors onsite has been developed. Real-time PEA methods are
   employed to generate fast solutions in response to load variations. As a
   case study, this system is applied to monitor the stresses of a step
   ladder under actual loading conditions. The relationships among
   accuracy, mesh resolution and frame rate are investigated. (C) 2015
   Elsevier Ltd. All rights reserved.</abstract><date>SEP 2015</date><author>Huang, J. M.
   Ong, S. K.
   Nee, A. Y. C.</author></paper><paper><title>Optimization Integrator for Large Time Steps</title><abstract>Practical time steps in today's state-of-the-art simulators typically
   rely on Newton's method to solve large systems of nonlinear equations.
   In practice, this works well for small time steps but is unreliable at
   large time steps at or near the frame rate, particularly for difficult
   or stiff simulations. We show that recasting backward Euler as a
   minimization problem allows Newton's method to be stabilized by standard
   optimization techniques with some novel improvements of our own. The
   resulting solver is capable of solving even the toughest simulations at
   the 24 Hz frame rate and beyond. We show how simple collisions can be
   incorporated directly into the solver through constrained minimization
   without sacrificing efficiency. We also present novel penalty collision
   formulations for self collisions and collisions against scripted bodies
   designed for the unique demands of this solver. Finally, we show that
   these techniques improve the behavior of Material Point Method (MPM)
   simulations by recasting it as an optimization problem.</abstract><date>OCT 2015</date><author>Gast, Theodore F.
   Schroeder, Craig
   Stomakhin, Alexey
   Jiang, Chenfanfu
   Teran, Joseph M.</author></paper><paper><title>3D Terrain Real-time Rendering Method Based on CUDA-OpenGL
   Interoperability</title><abstract>In the field of geographic information, fast surface modelling
   techniques are important. Usually, mesh model is used to describe the
   terrain. However, the large data of mesh model are a bottleneck of
   real-time rendering. Compared with the CPU, GPU has more execution units
   and memory units, which greatly improves its computing capacity and
   memory bandwidth. Meanwhile, in the field of image rendering, OpenGL has
   always played an important role. Since CUDA (compute unified device
   architecture) and OpenGL both run on GPU and share data through common
   memory, this paper gives a three-dimensional terrain real-time rendering
   method based on CUDA-OpenGL interoperability. First, we use CUDA C
   kernel function to calculate the vertex coordinates and normal vectors,
   then we pass the data to OpenGL buffer, and render it. In this paper, we
   use share memory and register, overlap strategy, the expansion of
   concurrency and the reasonable block size. The experimental results show
   that this algorithm can improve the computing speed greatly, with the
   speedup to 212.3x, meeting the needs of real-time rendering.</abstract><date>NOV 2 2015</date><author>Wu, Jiaji
   Deng, Long
   Paul, Anand</author></paper><paper><title>CUDA-accelerated fast Sauvola's method on Kepler architecture</title><abstract>Sauvola's method is one of the top binarization methods for degraded
   document images. High computational complexity, however, restricts it to
   non-time-sensitive applications. In this paper, we present a parallel
   implementation of Sauvola's method with integral image optimization,
   called fast Sauvola's method, on Nvidia Kepler architecture GPUs using
   CUDA 5.0. Our implementation is evaluated on a GTX 650 graphic card (384
   cores) and exhibits an average speedup of about 38 compared to a
   sequential implementation on a fast CPU, and computational complex of
   our implementation is constant for any size of local windows.</abstract><date>DEC 2015</date><author>Chen, Xin
   Gao, Yuefang
   Huang, Zhonghong</author></paper><paper><title>The Gaussian Bloom Filter</title><abstract>Modern databases tailored to highly distributed, fault tolerant
   management of information for big data applications exploit a classical
   data structure for reducing disk and network I/O as well as for managing
   data distribution: The Bloom filter. This data structure allows to
   encode small sets of elements, typically the keys in a key-value store,
   into a small, constant-size data structure. In order to reduce memory
   consumption, this data structure suffers from false positives which lead
   to additional I/O operations and are therefore only harmful with respect
   to performance. With this paper, we propose an extension to the
   classical Bloom filter construction which facilitates the use of
   floating point coprocessors and GPUs or additional main memory in order
   to reduce false positives. The proposed data structure is compatible
   with the classical construction in the sense that the classical Bloom
   filter can be extracted in time linear to the size of the data structure
   and that the Bloom filter is a special case of our construction. We show
   that the approach provides a relevant gain with respect to the false
   positive rate. Implementations for Apache Cassandra, C++, and NVIDIA
   CUDA are given and support the feasibility and results of the approach.</abstract><date>2015</date><author>Werner, Martin
   Schoenfeld, Mirco</author></paper><paper><title>Integrating rotation and angular velocity from curvature</title><abstract>The problem of integrating the rotational vector from a given angular
   velocity vector is met in such diverse fields as the navigation,
   robotics, computer graphics, optical tracking and non-linear dynamics of
   flexible beams. For example, if the numerical formulation of non-linear
   dynamics of flexible beams is based on the interpolation of curvature,
   one needs to derive the rotation from the assumed curvature field. The
   relation between the angular velocity and the rotation is described by
   the first-order quasilinear differential equation. If the rotation is
   given, the related angular velocity is obtained by the differentiation.
   By contrast, if the angular velocity is given, the related rotations are
   obtained by the integration. The exact closed-form solution for the
   rotation is only possible if the angular velocity is constant in time.
   In dynamics of non-linear flexible spatial. beams, the problem of
   integrating rotations from a given angular velocity becomes even more
   complex because both the angular velocity and the curvature need
   simultaneously be integrated and are both functions of space and time.
   As the angular velocity and the curvature are assumed to be analytic
   functions, they must satisfy certain integrability conditions to assure
   the unique rotation is obtained from the two differential equations. The
   objective of the present paper is to derive approximate, yet closed-form
   solutions of the following problem: for a given curvature vector,
   determine both the rotation and the angular velocity. In order to avoid
   the singularity of kinematic relations, the quatemions are used for the
   parametrization of rotations, and the integrations are partly performed
   in the four-dimensional quatemion space. The resulting closed-form
   expressions for the rotational and angular velocity quatemions are ready
   to be used in the finite-element formulations of the dynamics of
   flexible spatial beams as interpolating functions. The present novel
   solution is assessed by comparisons of the numerical results with
   analytical solutions for variety of oscillating curvature functions, as
   well as with the solutions of the quaternion-based midpoint integrator
   and the Runge-Kutta-based Crouch-Grossman geometrical methods CG3 and
   CG4. (C) 2015 The Authors. Published by Elsevier Ltd.</abstract><date>JUL 2015</date><author>Treven, A.
   Saje, M.</author></paper><paper><title>Vectorizing unstructured mesh computations for many-core architectures</title><abstract>Achieving optimal performance on the latest multi-core and many-core
   architectures increasingly depends on making efficient use of the
   hardware's vector units. This paper presents results on achieving high
   performance through vectorization on CPUs and the Xeon-Phi on a key
   class of irregular applications: unstructured mesh computations. Using
   single instruction multiple thread (SIMT) and single instruction
   multiple data (SIMD) programming models, we show how unstructured mesh
   computations map to OpenCL or vector intrinsics through the use of code
   generation techniques in the OP2 Domain Specific Library and explore how
   irregular memory accesses and race conditions can be organized on
   different hardware. We benchmark Intel Xeon CPUs and the Xeon-Phi, using
   a tsunami simulation and a representative CFD benchmark. Results are
   compared with previous work on CPUs and NVIDIA GPUs to provide a
   comparison of achievable performance on current many-core systems. We
   show that auto-vectorization and the OpenCL SIMT model do not map
   efficiently to CPU vector units because of vectorization issues and
   threading overheads. In contrast, using SIMD vector intrinsics imposes
   some restrictions and requires more involved programming techniques but
   results in efficient code and near-optimal performance, two times faster
   than non-vectorized code. We observe that the Xeon-Phi does not provide
   good performance for these applications but is still comparable with a
   pair of mid-range Xeon chips. Copyright (c) 2015 John Wiley &amp; Sons, Ltd.</abstract><date>FEB 2016</date><author>Reguly, Istvan Z.
   Laszlo, Endre
   Mudalige, Gihan R.
   Giles, Mike B.</author></paper><paper><title>GPU technology as a platform for accelerating physiological systems
   modeling based on Laguerre-Volterra networks.</title><abstract>The use of a GPGPU programming paradigm (running CUDA-enabled algorithms
   on GPU cards) in biomedical engineering and biology-related applications
   have shown promising results. GPU acceleration can be used to speedup
   computation-intensive models, such as the mathematical modeling of
   biological systems, which often requires the use of nonlinear modeling
   approaches with a large number of free parameters. In this context, we
   developed a CUDA-enabled version of a model which implements a nonlinear
   identification approach that combines basis expansions and
   polynomial-type networks, termed Laguerre-Volterra networks and can be
   used in diverse biological applications. The proposed software
   implementation uses the GPGPU programming paradigm to take advantage of
   the inherent parallel characteristics of the aforementioned modeling
   approach to execute the calculations on the GPU card of the host
   computer system. The initial results of the GPU-based model presented in
   this work, show performance improvements over the original MATLAB model.
   </abstract><date>2015-Aug</date><author>Papadopoulos, Agathoklis
   Kostoglou, Kyriaki
   Mitsis, Georgios D
   Theocharides, Theocharis</author></paper><paper><title>BrowserGenome.org: web-based RNA-seq data analysis and visualization</title><abstract></abstract><date>NOV 2015</date><author>Schmid-Burgk, Jonathan L.
   Hornung, Veit</author></paper><paper><title>A new multi-planar Reconstruction Method using Voxel based beamforming
   for 3D ultrasound Imaging</title><abstract>For multi-planar reconstruction in 3D ultrasound imaging, direct and
   separable 3D scan conversion (SC) have been used for transforming the
   ultrasound data acquired in the 3D polar coordinate system to the 3D
   Cartesian coordinate system. These 3D SC methods can visualize an
   arbitrary plane for 3D ultrasound volume data. However, they suffer from
   blurring and blocking artifacts due to resampling during SC. In this
   paper, a new multi-planar reconstruction method based on voxel based
   beamforming (VBF) is proposed for reducing blurring and blocking
   artifacts. In VBF, unlike direct and separable 3D SC, each voxel on an
   arbitrary imaging plane is directly reconstructed by applying the
   focusing delay to radio-frequency (RF) data so that the blurring and
   blocking artifacts can be removed. From the phantom study, the proposed
   VBF method showed the higher contrast and less blurring compared to the
   separable and direct 3D SC methods. This result is consistent with the
   measured information entropy contrast (IEC) values, i.e., 98.9 vs. 42.0
   vs. 47.9, respectively. In addition, the 3D SC methods and VBF method
   were implemented on a high-end GPU by using CUDA programming The
   execution times for the VBF and direct 3D SC methods are 1656.1ms,
   1633.3ms and 1631.4ms, which are I/O bounded. These results indicate
   that the proposed VBF method can improve image quality of 3D ultrasound
   B-mode imaging by removing blurring and blocking artifacts associated
   with 3D scan conversion and show the feasibility of pseudo-real-time
   operation.</abstract><date>2015</date><author>Ju, Hyunseok
   Kang, Jinbum
   Song, Ilseob
   Yoo, Yangmo</author></paper><paper><title>Parallel Key Frame Extraction for Surveillance Video Service in a Smart
   City</title><abstract>Surveillance video service (SVS) is one of the most important services
   provided in a smart city. It is very important for the utilization of
   SVS to provide design efficient surveillance video analysis techniques.
   Key frame extraction is a simple yet effective technique to achieve this
   goal. In surveillance video applications, key frames are typically used
   to summarize important video content. It is very important and essential
   to extract key frames accurately and efficiently. A novel approach is
   proposed to extract key frames from traffic surveillance videos based on
   GPU (graphics processing units) to ensure high efficiency and accuracy.
   For the determination of key frames, motion is a more salient feature in
   presenting actions or events, especially in surveillance videos. The
   motion feature is extracted in GPU to reduce running time. It is also
   smoothed to reduce noise, and the frames with local maxima of motion
   information are selected as the final key frames. The experimental
   results show that this approach can extract key frames more accurately
   and efficiently compared with several other methods.</abstract><date>AUG 18 2015</date><author>Zheng, Ran
   Yao, Chuanwei
   Jin, Hai
   Zhu, Lei
   Zhang, Qin
   Deng, Wei</author></paper><paper><title>A Deep Learning Network Approach to ab initio Protein Secondary
   Structure Prediction</title><abstract>Ab initio protein secondary structure (SS) predictions are utilized to
   generate tertiary structure predictions, which are increasingly demanded
   due to the rapid discovery of proteins. Although recent developments
   have slightly exceeded previous methods of SS prediction, accuracy has
   stagnated around 80 percent and many wonder if prediction cannot be
   advanced beyond this ceiling. Disciplines that have traditionally
   employed neural networks are experimenting with novel deep learning
   techniques in attempts to stimulate progress. Since neural networks have
   historically played an important role in SS prediction, we wanted to
   determine whether deep learning could contribute to the advancement of
   this field as well. We developed an SS predictor that makes use of the
   position-specific scoring matrix generated by PSI-BLAST and deep
   learning network architectures, which we call DNSS. Graphical processing
   units and CUDA software optimize the deep network architecture and
   efficiently train the deep networks. Optimal parameters for the training
   process were determined, and a workflow comprising three separately
   trained deep networks was constructed in order to make refined
   predictions. This deep learning network approach was used to predict SS
   for a fully independent test dataset of 198 proteins, achieving a Q(3)
   accuracy of 80.7 percent and a Sov accuracy of 74.2 percent.</abstract><date>JAN-FEB 2015</date><author>Spencer, Matt
   Eickholt, Jesse
   Cheng, Jianlin</author></paper><paper><title>ACTIVATING ROLE OF INTERACTIVE DIDACTIC MATERIALS IN TEACHING COMPUTER
   SUBJECTS</title><abstract>In the days of the visual culture a manner of the transmission of
   information plays a very important role. Adopting a technique of the
   join of text, graphics, sound and animation in frames of the uniform
   structure of presenting data, particularly in the education, it is
   possible to achieve good results in handing over of knowledge than at
   using only one of the media. The article presents the results of
   research devoted to the influence of visual and textual teaching
   materials, on the level of assimilation of knowledge subjects and their
   involvement in the assimilation of content. The analysis of the results
   showed that the visualization of teaching content is a factor
   significantly activating the educational process and affecting the level
   of knowledge assimilation.</abstract><date>DEC 2015</date><author>Lis, Renata</author></paper><paper><title>Real-Time and Temporal-Coherent Foreground Extraction With Commodity
   RGBD Camera</title><abstract>Foreground extraction from video stream is an important component in
   many multimedia applications. By exploiting commodity RGBD cameras, we
   could further extract dynamic foreground objects with 3D information in
   real-time, thereby enabling new forms of multimedia applications such as
   3D telepresence. However, one critical problem with existing methods for
   real-time foreground extraction is temporal coherency. They could
   exhibit severe flickering results for foreground objects such as human
   motion, thus affecting the visual quality as well as the image object
   analysis in the multimedia applications. This paper presents a new
   GPU-based real-time foreground extraction method with several novel
   techniques. First, we detect shadow and fill missing depth data
   accordingly in RGBD video, and then adaptively combine color and depth
   masks to form a trimap. After that, we formulate a novel closed-form
   matting model to improve the temporal coherency in foreground extraction
   while achieving real-time performance. Particularly, we propagate RGBD
   data across temporal domain to improve the visual coherence in the
   foreground object extraction, and take advantage of various CUDA
   strategies and spatial data structures to improve the speed. Experiments
   with a number of users on different scenarios show that, compared with
   state-of-the-art methods, our method can extract stabler foreground
   objects with higher visual quality as well as better temporal coherency,
   while still achieving real-time performance (experimentally, 30.3 frames
   per second on average).</abstract><date>APR 2015</date><author>Zhao, Mengyao
   Fu, Chi-Wing
   Cai, Jianfei
   Cham, Tat-Jen</author></paper><paper><title>Efficient 3D Transpositions in Graphics Processing Units</title><abstract>Matrix transposition is a basic operation for several computing tasks.
   Hence, transposing a matrix in a computer's main memory has been well
   studied since many years ago. More recently, the out-of-place matrix
   transposition has been performed efficiently in graphical processing
   units (GPU), which are broadly used today for general purpose computing.
   However, due to the particular architecture of GPUs, the adaptation of
   the matrix transposition operation to 3D arrays is not straightforward.
   In this paper, we describe efficient implementations for graphical
   processing units of the 5 possible out-of-place 3D transpositions.
   Moreover, we also include the transposition of the most basic in-place
   3D transpositions. The results show that the achieved bandwidth is close
   to a simple array copy and is similar to the 2D transposition.</abstract><date>OCT 2015</date><author>Jodra, Jose L.
   Gurrutxaga, Ibai
   Muguerza, Javier</author></paper><paper><title>Towards power plant output modelling and optimization using parallel
   Regression Random Forest</title><abstract>In this paper, we explore the possibilities of using the Random Forest
   algorithm in its regression version to predict the power output of a
   power plant based on hourly measured data. This is a task commonly
   leading to a optimization problem that is, in general, best solved using
   a bio-inspired technique. We extend the results already published on
   this topic and show that Regression Random Forest can be a better
   alternative to solve the problem. A comparison of the method with
   previously published results is included. In order to implement the
   algorithm in a way that is as efficient as possible, a massively
   parallel implementation using a Graphics Processing Unit was used and is
   also described. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>FEB 2016</date><author>Janousek, Jan
   Gajdos, Petr
   Dohnalek, Pavel
   Radecky, Michal</author></paper><paper><title>A decoupled energy stable scheme for a hydrodynamic phase-field model of
   mixtures of nematic liquid crystals and viscous fluids</title><abstract>We develop a linear, first-order, decoupled, energy-stable scheme for a
   binary hydrodynamic phase field model of mixtures of nematic liquid
   crystals and viscous fluids that satisfies an energy dissipation law. We
   show that the semi-discrete scheme in time satisfies an analogous,
   semi-discrete energy-dissipation law for any time-step and is therefore
   unconditionally stable. We then discretize the spatial operators in the
   scheme by a finite-difference method and implement the fully discrete
   scheme in a simplified version using CUDA on GPUs in 3 dimensions in
   space and time. Two numerical examples for rupture of nematic liquid
   crystal filaments immersed in a viscous fluid matrix are given,
   illustrating the effectiveness of this new scheme in resolving complex
   interfacial phenomena in free surface flows of nematic liquid crystals.
   (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>JAN 15 2016</date><author>Zhao, Jia
   Yang, Xiaofeng
   Shen, Jie
   Wang, Qi</author></paper><paper><title>A comparative study of an X-ray tomography reconstruction algorithm in
   accelerated and cloud computing systems</title><abstract>With the increase of resolution in medical image scanners and the need
   of faster reconstruction methods, new ways of exploiting the inherent
   parallelism of reconstruction algorithms have arisen. In this paper, we
   present Mangoose++, an application to perform X-ray computed tomography
   that supports multiple grades of parallelism. This parallelism is
   tackled with two different approaches: the usage of parallel nodes with
   multicore CPUs in a cloud environment and the usage of high-performance
   computing (HPC)-based parallel architectures such as general-purpose
   computing on graphics processing unit (GPGPU) or Intel Xeon Phi. In this
   paper, we show the design and implementation of the application in three
   types of platforms related to the previous mentioned approaches,
   comparing and analyzing the performance, resource utilization, and
   scalability of each platform. Accelerators offer high performance for
   data sizes that fit inside the accelerator memory. This is the main
   advantage of Intel Xeon Phi that, in this work, obtains similar
   performance results than compute unified device architecture
   (CUDA)-based GPGPU versions, comparing with cards with less memory
   capacity. In our evaluation experiments, we additionally analyze and
   discuss the costs and efficiency of Mangoose++ over Amazon Compute Cloud
   platform, demonstrating that lower times can be achieved in a reasonable
   price compared with owned HPC-based hardware. A comparison between
   distinct hardware configurations is provided for emphasizing on the
   advantages and disadvantages of each one. Copyright (C) 2015 John Wiley
   &amp; Sons, Ltd.</abstract><date>DEC 25 2015</date><author>Serrano, Estefania
   Garcia Blas, Javier
   Carretero, Jesus</author></paper><paper><title>MRI Image Segmentation Based on a GPU Shortest Path Algorithm</title><abstract>Dijkstra algorithm can be adopted to distinguish different parts of
   boundaries in medical image segmentation problem, which can be a
   reference for further segmentation operation. However, classical
   Dijkstra algorithm can hardly adapt to real time image segmentation
   problem owing to its exponentially O(n(2)) computing complexity,
   especially for the increasing number of nodes. In this paper, we
   designed and implemented a parallel shortest path algorithm accelerated
   by GPU for medical image segmentation problem. A dynamic relax approach
   is presented to optimize classical Dijkstra algorithm. Therefore, the
   new parallel Dijkstra algorithm can be easily applied to CUDA parallel
   framework without concerning about GPU hardware and CUDA optimize
   details. Two experiments have been conducted to evaluate the algorithm
   performance. The results show that our new Dijkstra algorithm can get 8
   speedup for 4096 points compared with the classical Dijkstra algorithm.
   Besides, an impressing result with two speed up for 128*128 points
   problems is demonstrated in parallel Dijkstra compared with parallel
   Moore. In conclusion, the new parallel Dijkstra algorithm can
   significantly improve the real-time performance of image segmentation.</abstract><date>2015</date><author>Wang, Jie
   Chen, Weihao</author></paper><paper><title>A CUDA Implementation of the High Performance Conjugate Gradient
   Benchmark</title><abstract>The High Performance Conjugate Gradient (HPCG) benchmark has been
   recently proposed as a complement to the High Performance Linpack (HPL)
   benchmark currently used to rank supercomputers in the Top500 list. This
   new benchmark solves a large sparse linear system using a multigrid
   preconditioned conjugate gradient (PCG) algorithm. The PCG algorithm
   contains the computational and communication patterns prevalent in the
   numerical solution of partial differential equations and is designed to
   better represent modern application workloads which rely more heavily on
   memory system and network performance than HPL. GPU accelerated
   supercomputers have proved to be very effective, especially with regard
   to power efficiency, for accelerating compute intensive applications
   like HPL. This paper will present the details of a CUDA implementation
   of HPCG, and the results obtained at full scale on the largest GPU
   supercomputers available: the Cray XK7 at ORNL and the Cray XC30 at
   CSCS. The results indicate that GPU accelerated supercomputers are also
   very effective for this type of workload.</abstract><date>2015</date><author>Phillips, Everett
   Fatica, Massimiliano</author></paper><paper><title>Optimal Diagnostic indices for idiopathic Normal Pressure Hydrocephalus
   Based on the 3D Quantitative Volumetric Analysis for the Cerebral
   Ventricle and Subarachnoid Space</title><abstract>BACKGROUND AND PURPOSE: Despite the remarkable progress of 3D graphics
   technology, the Evans index has been the most popular index for
   ventricular enlargement. We investigated a novel reliable index for the
   MR imaging features specified in idiopathic normal pressure
   hydrocephalus, rather than the Evans index.MATERIALS AND METHODS: The
   patients with suspected idiopathic normal pressure hydrocephalus on the
   basis of the ventriculomegaly and a triad of symptoms underwent the CSF
   tap test. CSF volumes were extracted from a T2-weighted 3D spin-echo
   sequence named "sampling perfection with application-optimized contrasts
   by using different flip angle evolutions (SPACE)" on 3T MR imaging and
   were quantified semiautomatically. Subarachnoid spaces were divided as
   follows: upper and lower parts and 4 compartments of frontal convexity,
   parietal convexity, Sylvian fissure and basal cistern, and posterior
   fossa. The maximum length of 3 axial directions in the bilateral
   ventricles and their frontal horns was measured. The "z-Evans Index" was
   defined as the maximum z-axial length of the frontal horns to the
   maximum cranial z-axial length. These parameters were evaluated for the
   predictive accuracy for the tap-positive groups compared with the
   tap-negative groups and age-adjusted odds ratios at the optimal
   thresholds.RESULTS: In this study, 24 patients with tap-positive
   idiopathic normal pressure hydrocephalus, 25 patients without response
   to the tap test, and 23 age-matched controls were included. The frontal
   horns of the bilateral ventricles were expanded, with the most excessive
   expansion being toward the z-direction. The CSF volume of the parietal
   convexity had the highest area under the receiver operating
   characteristic curve (0.768), the z-Evans Index was the second (0.758),
   and the upper-to-lower subarachnoid space ratio index was the third
   (0.723), to discriminate the tap-test response.CONCLUSIONS: The CSF
   volume of the parietal convexity of &lt;38 mL, upper-to-lower subarachnoid
   space ratio of &lt;0.33, and the z-Evans Index of &gt;0.42 were newly proposed
   useful indices for the idiopathic normal pressure hydrocephalus
   diagnosis, an alternative to the Evans Index.</abstract><date>DEC 2015</date><author>Yamada, S.
   Ishikawa, M.
   Yamamoto, K.</author></paper><paper><title>Numerical study of double-diffusive convection in a vertical cavity with
   Soret and Dufour effects by lattice Boltzmann method on GPU</title><abstract>Double diffusive flow in a cavity has attracted lots of attention due to
   its importance in many engineering fields such as ocean circulation,
   crystal growth, pollution transportation in air, metal manufacturing
   process and so on. When heat and mass transfer occur simultaneously in
   the double diffusive flow, the fluid flow is not only driven by the
   temperature gradient but also by the concentration gradient as well. In
   some cases, the Dufour and Soret effects will play a significant role in
   the double diffusive flow process. The energy flux created by the
   concentration gradient is called Dufour effect and the temperature
   gradient can cause the mass flux which is Soret effect. When taking the
   Soret and Dufour effects into account, the temperature and concentration
   equations become coupled with each other. However, the coupling
   diffusivities matrix can be diagonalized. The coupled system can then be
   transformed to two uncoupled diffusion-advection equations of two
   independent species. The temperature and concentration can be obtained
   by the inverse transformation of these two independent species. As a
   numerical method developed in the past two decades, lattice Boltzmann
   method (LBM) is powerful in simulating complex heat transfer and fluid
   mechanics problems. In the current study, a lattice Boltzmann model was
   developed and implemented for the double-diffusive convection with Soret
   and Dufour effects. Three distribution functions were used to compute
   the fluid velocity, specie 1, and specie 2, respectively. Specifically,
   a rectangular enclosure with horizontal temperature and concentration
   gradients was investigated. On the other hand, the graphics processing
   units (GPU) computing becomes popular since the advent of the NVIDIA's
   CUDA platform, which includes both hardware components and software
   programming environment. The developed LBM code was adapted on the CUDA
   platform to accelerate the computation for parametric studies. The GPU
   is responsible for the parallel tasks while CPU tackles the sequential
   steps in the computation. To verify the improvement on computation
   ability by using GPU, the ratio of the computational time between CPU
   code and CUDA code is presented by simulating the classical natural
   convection process in a cavity. The computational speed can be
   accelerated by more than 20 times when large number of nodes is used.
   The fluid flow, temperature field and concentration field are presented
   for different Rayleigh numbers, buoyancy ratios, Prandtl numbers, Lewis
   numbers, aspect ratios, as well as Soret and Dufour coefficients. In
   addition, the results of Nusselt and Sherwood numbers are shown for
   different parametric conditions. As a result, lattice Boltzmann method
   was demonstrated as a good option to study the complex double-diffusive
   convection with Soret and Dufour effects in a vertical cavity. (C) 2015
   Elsevier Ltd. All rights reserved.</abstract><date>FEB 2016</date><author>Ren, Qinlong
   Chan, Cho Lik</author></paper><paper><title>A GPU accelerated moving mesh correspondence algorithm with applications
   to RV segmentation.</title><abstract>This study proposes a parallel nonrigid registration algorithm to obtain
   point correspondence between a sequence of images. Several recent
   studies have shown that computation of point correspondence is an
   excellent way to delineate organs from a sequence of images, for
   example, delineation of cardiac right ventricle (RV) from a series of
   magnetic resonance (MR) images. However, nonrigid registration
   algorithms involve optimization of similarity functions, and are
   therefore, computationally expensive. We propose Graphics Processing
   Unit (GPU) computing to accelerate the algorithm. The proposed approach
   consists of two parallelization components: 1) parallel Compute Unified
   Device Architecture (CUDA) version of the non-rigid registration
   algorithm; and 2) application of an image concatenation approach to
   further parallelize the algorithm. The proposed approach was evaluated
   over a data set of 16 subjects and took an average of 4.36 seconds to
   segment a sequence of 19 MR images, a significant performance
   improvement over serial image registration approach. </abstract><date>2015-Aug</date><author>Punithakumar, Kumaradevan
   Noga, Michelle
   Boulanger, Pierre</author></paper><paper><title>An analysis of computational models for accelerating the subtractive
   pixel adjacency model computation</title><abstract>Detecting covert information in images by means of steganalysis
   techniques has become increasingly necessary due to the amount of data
   being transmitted mainly through the Internet. However, these techniques
   are computationally expensive and not much attention has been paid to
   reduce their cost by means of available parallel computational
   platforms. This article presents two computational models for the
   Subtractive Pixel Adjacency Model (SPAM) which has shown the best
   detection rates among several assessed steganalysis techniques. A
   hardware architecture tailored for reconfigurable fabrics is presented
   achieving high performance and fulfilling hard real-time constraints. On
   the other hand, a parallel computational model for the CUDA architecture
   is also proposed. This model presents high performance during the first
   stage but it faces a bottleneck during the second stage of the SPAM
   process. Both computational models are analyzed in detail in terms of
   their algorithmic structure and performance results. To the best of
   Authors' knowledge these are the first design proposals to accelerate
   the SPAM model calculation. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>APR 2015</date><author>Rodriguez-Perez, Marisol
   Morales-Reyes, Alicia
   Cumplido, Rene
   Feregrino-Uribe, Claudia</author></paper><paper><title>An exact general remeshing scheme applied to physically conservative
   voxelization</title><abstract>We present an exact general remeshing scheme to compute analytic
   integrals of polynomial functions over the intersections between convex
   polyhedral cells of old and new meshes. In physics applications this
   allows one to ensure global mass, momentum, and energy conservation
   while applying higher-order polynomial interpolation. We elaborate on
   applications of our algorithm arising in the analysis of cosmological
   N-body data, computer graphics, and continuum mechanics problems.We
   focus on the particular case of remeshing tetrahedral cells onto a
   Cartesian grid such that the volume integral of the polynomial density
   function given on the input mesh is guaranteed to equal the
   corresponding integral over the output mesh. We refer to this as
   "physically conservative voxelization."At the core of our method is an
   algorithm for intersecting two convex polyhedra by successively clipping
   one against the faces of the other. This algorithm is an implementation
   of the ideas presented abstractly by Sugihara [48], who suggests using
   the planar graph representations of convex polyhedra to ensure
   topological consistency of the output. This makes our implementation
   robust to geometric degeneracy in the input. We employ a simplicial
   decomposition to calculate moment integrals up to quadratic order over
   the resulting intersection domain.We also address practical issues
   arising in a software implementation, including numerical stability in
   geometric calculations, management of cancellation errors, and extension
   to two dimensions. In a comparison to recent work, we show substantial
   performance gains. We provide a C implementation intended to be a fast,
   accurate, and robust tool for geometric calculations on polyhedral mesh
   elements. (C) 2015 The Authors. Published by Elsevier Inc.</abstract><date>SEP 15 2015</date><author>Powell, Devon
   Abel, Tom</author></paper><paper><title>Implementation of Multi-GPU Based Lattice Boltzmann Method for Flow
   Through Porous Media</title><abstract>The lattice Boltzmann method (LBM) can gain a great amount of
   performance benefit by taking advantage of graphics processing unit
   (GPU) computing, and thus, the GPU, or multi-GPU based LBM can be
   considered as a promising and competent candidate in the study of
   large-scale fluid flows. However, the multi-GPU based lattice Boltzmann
   algorithm has not been studied extensively, especially for simulations
   of flow in complex geometries. In this paper, through coupling with the
   message passing interface (MPI) technique, we present an implementation
   of multi-GPU based LBM for fluid flow through porous media as well as
   some optimization strategies based on the data structure and layout,
   which can apparently reduce memory access and completely hide the
   communication time consumption. Then the performance of the algorithm is
   tested on a one-node cluster equipped with four Tesla C1060 GPU cards
   where up to 1732 MFLUPS is achieved for the Poiseuille flow and a nearly
   linear speedup with the number of GPUs is also observed.</abstract><date>FEB 2015</date><author>Huang, Changsheng
   Shi, Baochang
   He, Nanzhong
   Chai, Zhenhua</author></paper><paper><title>A new technique for ultimate limit state design of arbitrary shape RC
   sections under biaxial bending</title><abstract>The design of reinforced concrete sections of arbitrary shape, namely
   with variable geometry, holes as well as with arbitrary distribution of
   reinforcing steel bars, is a very common task in civil engineering,
   reinforced concrete structures. The design of these sections requires
   the integration of non-linear stress fields on complex shapes, because
   of the non-linear behavior of concrete in compression. In this paper, a
   novel algorithm is proposed to compute the ultimate strength of
   reinforced concrete sections under biaxial bending. The algorithm
   includes section subdivision into trapezoidal elements using the
   techniques of polygon clipping algorithm proposed by Weiler-Atherton.
   Exact numerical integration for normal strength concrete (f(ck) &lt;= 50
   MPa) is achieved, for each trapezoid, using the change of variables
   theorem followed by Gauss-Legendre integration. The proposed technique
   is hereafter referred to as WAGL (Weiler-Atherton, Gauss-Legendre). The
   verification of the proposed algorithm is performed by comparing
   analytical results between the WAGL technique and methods proposed by
   other authors (five examples). Additionally, the results obtained are
   also compared with experimental results available in the literature. The
   application of the WAGL technique is illustrated with two RC
   cross-section design examples. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>DEC 1 2015</date><author>Rodrigues, R. Vaz</author></paper><paper><title>Parallel Navier-Stokes simulations for high speed compressible flow past
   arbitrary geometries using FLASH</title><abstract>We report extensions to the FLASH code to enable high-speed compressible
   viscous flow simulation past arbitrary two- and three-dimensional
   stationary bodies. The body shape is embedded in a block-structured
   Cartesian adaptive mesh refinement grid by implementing appropriate
   computer graphics algorithms. A high mesh refinement level is required
   for an accurate body shape representation which results in large grid
   sizes especially for three-dimensional simulations. Simulations are done
   in parallel on IBM Blue Gene/Q computing system on which the code
   performance has been assessed in both pure MPI and hybrid MPI-OpenMP
   modes. We also implement appropriate wall boundary conditions in FLASH
   to model viscous-wall effects. Navier-Stokes (NS) solutions for various
   two-dimensional test cases like a shock boundary layer interaction
   problem as well as for hypersonic flow past blunted cone-cylinder-flare
   and double-cone geometries are shown. Three dimensional NS simulations
   of micro vortex generators employed in hypersonic flow control have also
   been carried out and the computed results have been found to be
   consistent with experimental results. (C) 2014 Elsevier Ltd. All rights
   reserved.</abstract><date>MAR 30 2015</date><author>John, Benzi
   Emerson, David R.
   Gu, Xiao-Jun</author></paper><paper><title>Using Adobe Flash Animations of electron transport chain to teach and
   learn biochemistry</title><abstract>Teaching the subject of the electron transport chain is one of the most
   challenging aspects of the chemistry curriculum at the high school
   level. This article presents an educational program called Electron
   Transport Chain which consists of 14 visual animations including a
   biochemistry quiz. The program was created in the Adobe Flash CS3
   Professional animation program and is designed for high school chemistry
   students. Our goal is to develop educational materials that facilitate
   the comprehension of this complex subject through dynamic animations
   which show the course of the electron transport chain and simultaneously
   explain its nature. We record the process of the electron transport
   chain, including connections with oxidative phosphorylation, in such a
   way as to minimize the occurrence of discrepancies in interpretation.
   The educational program was evaluated in high schools through the
   administration of a questionnaire, which contained 12 opened-ended items
   and which required participants to evaluate the graphics of the
   animations, chemical content, student preferences, and its suitability
   for high school biochemistry teaching. (c) 2015 by the International
   Union of Biochemistry and Molecular Biology, 43(4):294-299, 2015.</abstract><date>JUL-AUG 2015</date><author>Tepla, Milada
   Klimova, Helena</author></paper><paper><title>Using the comprehensive patent citation network (CPC) to evaluate patent
   value</title><abstract>Most approaches to patent citation network analysis are based on
   single-patent direct citation relation, which is an incomplete
   understanding of the nature of knowledge flow between patent pairs,
   which are incapable of objectively evaluating patent value. In this
   paper, four types of patent citation networks (direct citation, indirect
   citation, coupling and co-citation networks) are combined, filtered and
   recomposed based on relational algebra. Then, a method based on
   comprehensive patent citation (CPC) network for patent value evaluation
   is proposed, and empirical study of optical disk technology related
   patents has been conducted based on this method. The empirical study was
   carried out in two steps: observation of network characteristics over
   the entire process (citation time lag and topological and graphics
   characteristics), and measurement verification by independent proxies of
   patent value (patent family and patent duration). Our results show that
   the CPC network retains the advantages of patent direct citation, and
   performs better on topological structure, graphics features, centrality
   distribution, citation lag and sensitivity than a direct citation
   network; The verified results by the patent family and maintenance show
   that the proposed method covers more valuable patents than the
   traditional method.</abstract><date>DEC 2015</date><author>Yang, Guan-Can
   Li, Gang
   Li, Chun-Ya
   Zhao, Yun-Hua
   Zhang, Jing
   Liu, Tong
   Chen, Dar-Zen
   Huang, Mu-Hsuan</author></paper><paper><title>gem5-gpu: A Heterogeneous CPU-GPU Simulator</title><abstract>gem5-gpu is a new simulator that models tightly integrated CPU-GPU
   systems. It builds on gem5, a modular full-system CPU simulator, and
   GPGPU-Sim, a detailed GPGPU simulator. gem5-gpu routes most memory
   accesses through Ruby, which is a highly configurable memory system in
   gem5. By doing this, it is able to simulate many system configurations,
   ranging from a system with coherent caches and a single virtual address
   space across the CPU and GPU to a system that maintains separate GPU and
   CPU physical address spaces. gem5-gpu can run most unmodified CUDA 3.2
   source code. Applications can launch non-blocking kernels, allowing the
   CPU and GPU to execute simultaneously. We present gem5-gpu's software
   architecture and a brief performance validation. We also discuss
   possible extensions to the simulator. gem5-gpu is open source and
   available at gem5-gpu.cs.wisc.edu.</abstract><date>JAN-JUN 2015</date><author>Power, Jason
   Hestness, Joel
   Orr, Marc S.
   Hill, Mark D.
   Wood, David A.</author></paper><paper><title>A rapid GPU-based heat transfer and solidification model for dynamic
   computer simulations of continuous steel casting</title><abstract>The paper presents a GPU-based model for continuous casting of steel.
   The model provides rapid computation capabilities required for real-time
   use in the casting control and optimization. The fully three-dimensional
   formulation of the heat transfer and solidification model is based on
   the control volume method and it allows for very fast transient
   simulations of the thermal behaviour of cast strands. The developed
   model has been verified on Stefan problem and validated with industry
   measurements. Heat transfer conditions in the mould and secondary
   cooling were determined experimentally in lab-scale experiments. The
   computational model is implemented as highly-parallel with the use of
   the NVIDIA CUDA architecture, which enables to launch the model on
   graphics processing units (GPUs) allowing for its great acceleration.
   The acceleration can be evaluated with the use of the relative
   computational time, which is the dimensionless ratio between the
   computational time that the model needs to compute the simulation and
   the wall-clock time of the real casting process being simulated. The
   relative computational time of the presented GPU-based computational
   model is between 0.0016 for a coarse mesh and 0.27 for a very fine mesh.
   The corresponding multiple of the GPU-acceleration, which is the ratio
   between the computational time of the GPU-based model and of the
   CPU-based model for the identical simulation, is between 33 and 68. (C)
   2015 Elsevier B.V. All rights reserved.</abstract><date>DEC 2015</date><author>Klimes, Lubomir
   Stetina, Josef</author></paper><paper><title>Evaluation and Acceleration of High-Throughput Fixed-Point Object
   Detection on FPGAs</title><abstract>Reliance on object or people detection is rapidly growing beyond
   surveillance to industrial and social applications. The histogram of
   oriented gradients (HOG), one of the most popular object detection
   algorithms, achieves high detection accuracy but delivers just under 1
   frame/s on a high-end CPU. Field-programmable gate array (FPGA)
   accelerations of this algorithm are limited by the intensive
   floating-point computations. All current fixed-point HOG implementations
   use large bit width to maintain detection accuracy, or perform poorly at
   reduced data precision. In this paper, we introduce the full-image
   evaluation methodology to explore the FPGA implementation of HOG using
   reduced bit width. This approach lessens the required area resources on
   the FPGA, and increases the clock frequency and hence the throughput per
   device through increased parallelism. We evaluate the detection accuracy
   of the fixed-point HOG by applying state-of-the-art computer vision
   pedestrian detection evaluation metrics and show it performs as well as
   the original floating-point code from OpenCV. We then show our single
   FPGA implementation achieves a 68.7 x higher throughput than a high-end
   CPU, 5.1 x higher than a high-end graphics processing unit (GPU), and
   7.8 x higher than the same implementation using floating-point on the
   same FPGA. A power consumption comparison for different platforms shows
   our fixed-point FPGA implementation uses 130 x less power than CPU, and
   31 x less energy than GPU to process one image.</abstract><date>JUN 2015</date><author>Ma, Xiaoyin
   Najjar, Walid A.
   Roy-Chowdhury, Amit K.</author></paper><paper><title>GPU-accelerated adjoint algorithmic differentiation</title><abstract>Many scientific problems such as classifier training or medical image
   reconstruction can be expressed as minimization of differentiable
   real-valued cost functions and solved with iterative gradient-based
   methods. Adjoint algorithmic differentiation (AAD) enables automated
   computation of gradients of such cost functions implemented as computer
   programs. To backpropagate adjoint derivatives, excessive memory is
   potentially required to store the intermediate partial derivatives on a
   dedicated data structure, referred to as the "tape". Parallelization is
   difficult because threads need to synchronize their accesses during
   taping and backpropagation. This situation is aggravated for many-core
   architectures, such as Graphics Processing Units (GPUs), because of the
   large number of light-weight threads and the limited memory size in
   general as well as per thread. We show how these limitations can be
   mediated if the cost function is expressed using GPU-accelerated vector
   and matrix operations which are recognized as intrinsic functions by our
   AAD software. We compare this approach with naive and vectorized
   implementations for CPUs. We use four increasingly complex cost
   functions to evaluate the performance with respect to memory consumption
   and gradient computation times. Using vectorization, CPU and GPU memory
   consumption could be substantially reduced compared to the naive
   reference implementation, in some cases even by an order of complexity.
   The vectorization allowed usage of optimized parallel libraries during
   forward and reverse passes which resulted in high speedups for the
   vectorized CPU version compared to the naive reference implementation.
   The GPU version achieved an additional speedup of 7.5 +/- 4.4, showing
   that the processing power of GPUs can be utilized for AAD using this
   concept. Furthermore, we show how this software can be systematically
   extended for more complex problems such as nonlinear absorption
   reconstruction for fluorescence-mediated tomography.Program
   summaryProgram title: AD-GPUCatalogue identifier: AEYX_v1_0Program
   summary URL: http://cpc.cs.qub.ac.uk/summaries/AEYX_v1_0.htmlProgram
   obtainable from: CPC Program Library, Queen's University, Belfast, N.
   IrelandLicensing provisions: Standard CPC licence,
   http://cpc.cs.qub.ac.uk/licence/licence.htmlNo. of lines in distributed
   program, including test data, etc.: 16715No. of bytes in distributed
   program, including test data, etc.: 143683Distribution format:
   tar.gzProgramming language: C++ and CUDA.Computer: Any computer with a
   compatible C++ compiler and a GPU with CUDA capability 3.0 or
   higher.Operating system: Windows 7 or Linux.RAM: 16 GbyteClassification:
   4.9, 4.12, 6.1, 6.5.External routines: CUDA 6.5, Intel MKL (optional)
   and routines from BLAS, LAPACK and CUBLASNature of problem: Gradients
   are required for many optimization problems, e.g. classifier training or
   nonlinear image reconstruction. Often, the function, of which the
   gradient is required, can be implemented as a computer program. Then,
   algorithmic differentiation methods can be used to compute the gradient.
   Depending on the approach this may result in excessive requirements of
   computational resources, i.e. memory and arithmetic computations. GPUs
   provide massive computational resources but require special
   considerations to distribute the workload onto many light-weight
   threads.Solution method: Adjoint algorithmic differentiation allows
   efficient computation of gradients of cost functions given as computer
   programs. The gradient can be theoretically computed using a similar
   amount of arithmetic operations as one function evaluation. Optimal
   usage of parallel processors and limited memory is a major challenge
   which can be mediated by the use of vectorization.Restrictions: To use
   the GPU-accelerated adjoint algorithmic differentiation method, the cost
   function must be implemented using the provided AD-GPU intrinsics for
   matrix and vector operations. Unusual
   features:GPU-acceleration.Additional comments: The code uses some
   features of C++11, e.g. std::shared ptr. Alternatively, the boost
   library can be used.Running time: The time to run the example program is
   a few minutes or up to a few hours to reproduce the performance
   measurements. (C) 2015 The Authors. Published by Elsevier B.V.</abstract><date>MAR 2016</date><author>Gremse, Felix
   Hoefter, Andreas
   Razik, Lukas
   Kiessling, Fabian
   Naumann, Uwe</author></paper><paper><title>GPU Accelerated Belief Propagation Decoding of Non-Binary LDPC Codes
   with Parallel and Sequential Scheduling</title><abstract>Low-Density Parity-Check (LDPC) codes are very powerful channel coding
   schemes with a broad range of applications. The existence of low
   complexity (i.e., linear time) iterative message passing decoders with
   close to optimum error correction performance is one of the main
   strengths of LDPC codes. It has been shown that the performance of these
   decoders can be further enhanced if the LDPC codes are extended to
   higher order Galois fields, yielding so called non-binary LDPC codes.
   However, this performance gain comes at the cost of rapidly increasing
   decoding complexity. To deal with this increased complexity, we present
   an efficient implementation of a signed-log domain FFT decoder for
   non-binary irregular LDPC codes which exploits the inherent massive
   parallelization capabilities of message passing decoders. We employ
   Nvidia's Compute Unified Device Architecture (CUDA) to incorporate the
   available processing power of state-of-the-art Graphics Processing Units
   (GPUs). Furthermore, we present a CUDA implementation of the signed-log
   domain FFT decoder using the so-called layered update rule, in which
   check nodes are updated one after another. This sequential updating of
   nodes has been shown to converge about twice as fast as the traditional
   flooding scheme. To achieve a high speedup of the layered CUDA
   implementation, we employ quasi-cyclic non-binary LDPC codes since they
   allow to update multiple neighboring check nodes in parallel without any
   performance loss.</abstract><date>JAN 2015</date><author>Beermann, Moritz
   Monzo, Enrique
   Schmalen, Laurent
   Vary, Peter</author></paper><paper><title>Linear solvation energy relationship for the adsorption of synthetic
   organic compounds on single-walled carbon nanotubes in water</title><abstract>The linear solvation energy relationship (LSER) was applied to predict
   the adsorption coefficient (K) of synthetic organic compounds (SOCs) on
   single-walled carbon nanotubes (SWCNTs). A total of 40 log K values were
   used to develop and validate the LSER model. The adsorption data for 34
   SOCs were collected from 13 published articles and the other six were
   obtained in our experiment. The optimal model composed of four
   descriptors was developed by a stepwise multiple linear regression (MLR)
   method. The adjusted r(2) (r(adj)(2)) and root mean square error (RMSE)
   were 0.84 and 0.49, respectively, indicating good fitness. The
   leave-one-out cross-validation Q(2) ([GRAPHICS]) was 0.79, suggesting
   the robustness of the model was satisfactory. The external Q(2)
   ([GRAPHICS]) and RMSE (RMSEext) were 0.72 and 0.50, respectively,
   showing the model's strong predictive ability. Hydrogen bond donating
   interaction (bB) and cavity formation and dispersion interactions (vV)
   stood out as the two most influential factors controlling the adsorption
   of SOCs onto SWCNTs. The equilibrium concentration would affect the
   fitness and predictive ability of the model, while the coefficients
   varied slightly.</abstract><date>JAN 2 2016</date><author>Ding, H.
   Chen, C.
   Zhang, X.</author></paper><paper><title>Development of a Second-Generation Underwater Acoustic Ambient Noise
   Imaging Camera</title><abstract>A nominally circular 2-D broadband acoustic array of 1.3-m diameter,
   comprising 508 sensors and associated electronics, was designed, built,
   and tested for ambient noise imaging (ANI) potential in Singapore
   waters. The system, named Remotely Operated Mobile Ambient Noise Imaging
   System (ROMANIS), operates over 25-85 kHz, streaming real-time data at
   1.6 Gb/s over a fiber optic link. By using sensors that are much larger
   than half-wavelength at the highest frequency of interest, so with some
   directionality, good beamforming performance is obtained with a small
   number of sensors compared to a conventional half-wavelength-spaced
   array. A data acquisition system consisting of eight single-board
   computers enables synchronous data collection from all 508 sensors. A
   dry-coupled neoprene cover is used to encapsulate the ceramic elements
   as an alternative to potting or oil filling, for easier maintenance.
   Beamforming is performed in real-time using parallel computing on a
   graphics processing unit (GPU). Experiments conducted in Singapore
   waters yielded images of underwater objects at much larger ranges and
   with better resolution than any previous ANI system. Although ROMANIS
   was designed for ANI, the array may be valuable in many other
   applications requiring a broadband underwater acoustic receiving array.</abstract><date>JAN 2016</date><author>Pallayil, Venugopalan
   Chitre, Mandar
   Kuselan, Subash
   Raichur, Amogh
   Ignatius, Manu
   Potter, John R.</author></paper><paper><title>MSI.R scripts reveal volatile and semi-volatile features in
   low-temperature plasma mass spectrometry imaging (LTP-MSI) of chilli
   (Capsicum annuum)</title><abstract>In cartography, the combination of colour and contour lines is used to
   express a three-dimensional landscape on a two-dimensional map. We
   transferred this concept to the analysis of mass spectrometry imaging
   (MSI) data and developed a collection of R scripts for the efficient
   evaluation of .imzML archives in a four-step strategy: (1) calculation
   of the density distribution of mass-to-charge ratio (m/z) signals in the
   .imzML file and assembling of a pseudo-master spectrum with peak list,
   (2) automated generation of mass images for a defined scan range and
   subsequent visual inspection, (3) visualisation of individual ion
   distributions and export of relevant .mzML spectra and (4) creation of
   overlay graphics of ion images and photographies. The use of a
   Hue-Chroma-Luminance (HCL) colour model in MSI graphics takes into
   account the human perception for colours and supports the correct
   evaluation of signal intensities. Further, readers with colour blindness
   are supported. Contour maps promote the visual recognition of patterns
   in MSI data, which is particularly useful for noisy data sets. We
   demonstrate the scalability of MSI.R scripts by running them on
   different systems: on a personal computer, on Amazon Web Services (AWS)
   instances and on an institutional cluster. By implementing a parallel
   computing strategy, the execution speed for .imzML data scanning with
   image generation could be improved by more than an order of magnitude.
   Applying our MSI.R scripts to low-temperature plasma (LTP)-MSI data
   shows the localisation of volatile and semi-volatile compounds in the
   cross-cut of a chilli (Capsicum annuum) fruit. The subsequent
   identification of compounds by gas and liquid chromatography coupled to
   mass spectrometry (GC-MS, LC-MS) proves that LTP-MSI enables the direct
   measurement of volatile organic compound (VOC) distributions from
   biological tissues.</abstract><date>JUL 2015</date><author>Gamboa-Becerra, Roberto
   Ramirez-Chavez, Enrique
   Molina-Torres, Jorge
   Winkler, Robert</author></paper><paper><title>Determining protein structures by combining semireliable data with
   atomistic physical models by Bayesian inference</title><abstract>More than 100,000 protein structures are now known at atomic detail.
   However, far more are not yet known, particularly among large or complex
   proteins. Often, experimental information is only semireliable because
   it is uncertain, limited, or confusing in important ways. Some
   experiments give sparse information, some give ambiguous or nonspecific
   information, and others give uncertain information-where some is right,
   some is wrong, but we don't know which. We describe a method called
   Modeling Employing Limited Data (MELD) that can harness such problematic
   information in a physics-based, Bayesian framework for improved
   structure determination. We apply MELD to eight proteins of known
   structure for which such problematic structural data are available,
   including a sparse NMR dataset, two ambiguous EPR datasets, and four
   uncertain datasets taken from sequence evolution data. MELD gives
   excellent structures, indicating its promise for experimental
   biomolecule structure determination where only semireliable data are
   available.</abstract><date>JUN 2 2015</date><author>MacCallum, Justin L.
   Perez, Alberto
   Dill, Ken A.</author></paper><paper><title>A Parallel High Speed Lossless Data Compression Algorithm in Large-Scale
   Wireless Sensor Network</title><abstract>In large-scale wireless sensor networks, massive sensor data generated
   by a large number of sensor nodes call for being stored and disposed.
   Though limited by the energy and bandwidth, a large-scale wireless
   sensor network displays the disadvantages of fusing the data collected
   by the sensor nodes and compressing them at the sensor nodes. Thus the
   goals of reduction of bandwidth and a high speed of data processing
   should be achieved at the second-level sink nodes. Traditional
   compression technology is unable to appropriately meet the demands of
   processing massive sensor data with a high compression rate and low
   energy cost. In this paper, Parallel Matching
   Lempel-Ziv-Storer-Szymanski (PMLZSS), a high speed lossless data
   compression algorithm, making use of the CUDA framework at the
   second-level sink node is presented. The core idea of PMLZSS algorithm
   is parallel matrix matching. PMLZSS algorithm divides the data
   compression files into multiple compressed dictionary window strings and
   prereading window strings along the vertical and horizontal axes of the
   matrices, respectively. All of the matrices are parallel matched in the
   different thread blocks. Compared with LZSS and BZIP2 on the traditional
   serial CPU platforms, the compression speed of PMLZSS increases about 16
   times while, for BZIP2, the compression speed increases about 12 times
   when the basic compression rate unchanged.</abstract><date>2015</date><author>Zhou, Bin
   Jin, Hai
   Zheng, Ran</author></paper><paper><title>Evaluation of a Modified User Guide for Hearing Aid Management</title><abstract>Objectives: This study investigated if a hearing aid user guide modified
   using best practice principles for health literacy resulted in superior
   ability to perform hearing aid management tasks, compared with the user
   guide in the original form.Design: This research utilized a two-arm
   study design to compare the original manufacturer's user guide with a
   modified user guide for the same hearing aidan Oticon Acto
   behind-the-ear aid with an open dome. The modified user guide had a
   lower reading grade level (4.2 versus 10.5), used a larger font size,
   included more graphics, and had less technical information. Eighty-nine
   adults ages 55 years and over were included in the study; none had
   experience with hearing aid use or management. Participants were
   randomly assigned either the modified guide (n = 47) or the original
   guide (n = 42). All participants were administered the Hearing Aid
   Management test, designed for this study, which assessed their ability
   to perform seven management tasks (e.g., change battery) with their
   assigned user guide.Results: The regression analysis indicated that the
   type of user guide was significantly associated with performance on the
   Hearing Aid Management test, adjusting for 11 potential covariates. In
   addition, participants assigned the modified guide required
   significantly fewer prompts to perform tasks and were significantly more
   likely to perform four of the seven tasks without the need for prompts.
   The median time taken by those assigned the modified guide was also
   significantly shorter for three of the tasks. Other variables associated
   with performance on the Hearing Aid Management test were health literacy
   level, finger dexterity, and age.Conclusions: Findings indicate that the
   need to design hearing aid user guides in line with best practice
   principles of health literacy as a means of facilitating improved
   hearing aid management in older adults.</abstract><date>JAN-FEB 2016</date><author>Caposecco, Andrea
   Hickson, Louise
   Meyer, Carly
   Khan, Asaduzzaman</author></paper><paper><title>Genetic annealing with efficient strategies to improve the performance
   for the NP-hard and routing problems</title><abstract>&lt;inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink"
   xlink:href="teta_a_1020624_ilm0001.gif"&gt;&lt;/inline-graphic&gt; problem which
   cannot be solved in polynomial time for asymptotically large values of
   &lt;inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink"
   xlink:href="teta_a_1020624_ilm0002.gif"&gt;&lt;/inline-graphic&gt; and travelling
   salesman problem (TSP) is important in operations research and
   theoretical computer science. In this paper a balanced combination of
   genetic algorithm and simulated annealing has been applied. To improve
   the performance of finding an optimal solution from huge search space,
   we have incorporated the use of tournament and rank as selection
   operators, and inver-over operator mechanism for crossover and mutation.
   This proposed technique is applied for some routing resource problems in
   a chip design process and a best optimal solution was obtained, and the
   TSP appears as a sub-problem in many areas and is used as a benchmark
   for many optimisation methods.</abstract><date>NOV 2 2015</date><author>Eswarawaka, Rajesh
   Mahammad, S. K. Noor
   Reddy, B. Eswara</author></paper><paper><title>Ultrahigh-Resolution Spectral Domain Optical Coherence Tomography Based
   on a Linear-Wavenumber Spectrometer</title><abstract>In this study we demonstrate ultrahigh-resolution spectral domain
   optical coherence tomography (UHR SD-OCT) with a linear-wavenumber (k)
   spectrometer; to accelerate signal processing and to display
   two-dimensional (2-D) images in real time. First, we performed a
   numerical simulation to find the optimal parameters for the linear-k
   spectrometer to achieve ultrahigh axial resolution, such as the number
   of grooves in a grating, the material for a dispersive prism, and the
   rotational angle between the grating and the dispersive prism. We found
   that a grating with 1200 grooves and an F2 equilateral prism at a
   rotational angle of 26.07 degrees, in combination with a lens of focal
   length 85.1 mm, are suitable for UHR SD-OCT with the imaging depth range
   (limited by spectrometer resolution) set at 2.0 mm. As guided by the
   simulation results, we constructed the linear-k spectrometer needed to
   implement a UHR SD-OCT. The actual imaging depth range was measured to
   be approximately 2.1 mm, and axial resolution of 3.8 mu m in air was
   achieved, corresponding to 2.8 mu m in tissue (n = 1.35). The
   sensitivity was -91 dB with -10 dB roll-off at 1.5 mm depth. We
   demonstrated a 128.2 fps acquisition rate for OCT images with 800
   lines/frame, by taking advantage of NVIDIA's compute unified device
   architecture (CUDA) technology, which allowed for real-time signal
   processing compatible with the speed of the spectrometer's data
   acquisition.</abstract><date>FEB 2015</date><author>Lee, Sang-Won
   Kang, Heesung
   Park, Joo Hyun
   Lee, Tae Geol
   Lee, Eun Seong
   Lee, Jae Yong</author></paper><paper><title>Efficient Parallel CUDA Random Number Generator on NVIDIA GPUs</title><abstract>In this paper, we implemented a parallel random number generation
   program on GPU's, which are known for high performance computing, using
   LCG (Linear Congruential Generator). Random numbers are important in all
   fields requiring the use of randomness, and LCG is one of the most
   widely used methods for the generation of pseudo-random numbers. We
   explained the parallel program using the NVIDIA CUDA model and
   MPI(Message Passing Interface) and showed uniform distribution and
   performance results. We also used a Monte Carlo algorithm to calculate
   pi() comparing the parallel random number generator with cuRAND, which
   is a CUDA library function, and showed that our program is much more
   efficient. Finally we compared performance results using multi-GPU's
   with those of ideal speedups.</abstract><date>2015</date><author>???</author></paper><paper><title>Seeing is believing: good graphic design principles for medical research</title><abstract>Have you noticed when you browse a book, journal, study report, or
   product label how your eye is drawn to figures more than to words and
   tables? Statistical graphs are powerful ways to transparently and
   succinctly communicate the key points of medical research. Furthermore,
   the graphic design itself adds to the clarity of the messages in the
   data. The goal of this paper is to provide a mechanism for selecting the
   appropriate graph to thoughtfully construct quality deliverables using
   good graphic design principles. Examples are motivated by the efforts of
   a Safety Graphics Working Group that consisted of scientists from the
   pharmaceutical industry, Food and Drug Administration, and academic
   institutions. Copyright (c) 2015 John Wiley &amp; Sons, Ltd.</abstract><date>SEP 30 2015</date><author>Duke, Susan P.
   Bancken, Fabrice
   Crowe, Brenda
   Soukup, Mat
   Botsis, Taxiarchis
   Forshee, Richard</author></paper><paper><title>User-drawn sketch-based 3D object retrievalusing sparse coding</title><abstract>3D object retrieval from user-drawn (sketch) queries is one of the
   important research issues in the areas of pattern recognition and
   computer graphics for simulation, visualization, and Computer Aided
   Design. The performance of any content-based 3D object retrieval system
   crucially depends on the availability of effective descriptors and
   similarity measures for this kind of data. We present a sketch-based
   approach for improving 3D object retrieval effectiveness by optimizing
   the representation of one particular type of features (oriented
   gradients) using a sparse coding approach. We perform experiments, the
   results of which show that the retrieval quality improves over
   alternative features and codings. Based our findings, the coding can be
   proposed for sketch-based 3D object retrieval systems relying on
   oriented gradient features.</abstract><date>JUN 2015</date><author>Yoon, Sang Min
   Yoon, Gang-Joon
   Schreck, Tobias</author></paper><paper><title>Statistical shape analysis of temporal lobe in mesial temporal sclerosis
   patients</title><abstract>Surgery is regarded as a common treatment option for patients with
   mesial temporal lobe epilepsy due to hippocampal sclerosis but sometimes
   deciding this diagnosis can be very difficult. We aim to investigate the
   shape differences in the temporal lobe of mesial temporal sclerosis
   epilepsy patients compared with healthy controls, investigating the side
   difference and, if present, assessing the clinical application of this
   situation.The MRI scans of mesial TLE patients and controls were
   retrospectively reviewed. Temporal lobe data were collected from the
   two-dimensional digital images. Standard anthropometric landmarks were
   selected and marked on each digital image using TPSDIG 2.04 software.
   Eight anatomic landmarks were marked on images. A generalized Procrustes
   analysis was used to evaluate the shape difference. The shape
   deformation of the temporal lobe from control to patient was evaluated
   using the TPS method.There were statistically significant TL shape
   differences between groups. High level deformations for the left and
   right side from the control to patient group were seen in the TPS
   graphic. The highest deformation was determined at the inferior lateral
   temporal midpoint of the middle temporal gyri and superior temporal
   landmark points of both the right and left sides.Our study for the first
   time demonstrated temporal shape differences in TLE patients using a
   landmark-based geometrical morphometric method by taking into
   consideration the topographic distribution of TL.</abstract><date>NOV 2015</date><author>Ocakoglu, Gokhan
   Taskapilioglu, M. Ozgur
   Ercan, Ilker
   Demir, Aylin Bican
   Hakyemez, Bahattin
   Bekar, Ahmet
   Bora, Ibrahim</author></paper><paper><title>Parallelization of Cell Contour Line Extraction Algorithm</title><abstract>In this paper, a parallel cell contour line extraction algorithm using
   CUDA, which has no inner contour lines, is proposed. The contour of a
   cell is very important in a cell image analysis. It could be obtained by
   a conventional serial contour tracing algorithm or parallel morphology
   operation. However, the cell image has various damages in acquisition or
   dyeing process. They could be turn into several inner contours, which
   make a cell image analysis difficult. The proposed algorithm introduces
   a min-max coordinates table into each CUDA thread block, and removes the
   inner contour in parallel. It is 4.1 to 7.6 times faster than a
   conventional serial contour tracing algorithm.</abstract><date>2015</date><author>???
   ???
   ???</author></paper><paper><title>Scale-space feature extraction on digital surfaces</title><abstract>A classical problem in many computer graphics applications consists in
   extracting significant zones or points on an object surface, like loci
   of tangent discontinuity (edges), maxima or minima of curvatures,
   inflection points, etc. These places have specific local geometrical
   properties and often called generically features. An important problem
   is related to the scale, or range of scales, for which a feature is
   relevant. We propose a new robust method to detect features on digital
   data (surface of objects in Z(3), which exploits asymptotic properties
   of recent digital curvature estimators. In Coeurjolly et al [1] and
   Levallois et al. [1,2], authors have proposed curvature estimators
   (mean, principal and Gaussian) on 2D and 3D digitized shapes and have
   demonstrated their multigrid convergence (for C-3-smooth surfaces).
   Since such approaches integrate local information within a ball around
   points of interest the radius is a crucial parameter. In this paper, we
   consider the radius as a scale-space parameter. By analyzing the
   behavior of such curvature estimators as the ball radius tends to zero,
   we propose a tool to efficiently characterize and extract several
   relevant features (edges, smooth and flat parts) on digital surfaces.
   (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>OCT 2015</date><author>Levallois, Jeremy
   Coeurjolly, David
   Lachaud, Jacques-Olivier</author></paper><paper><title>Graphics processing unit (GPU) accelerated fast multipole BEM with
   level-skip M2L for 3D elasticity problems</title><abstract>In order to accelerate fast multipole boundary element method (FMBEM),
   in terms of the intrinsic parallelism of boundary elements and the FMBEM
   tree structure, a series of CUDA based GPU parallel algorithms for
   different parts of FMBEM with level-skip M2L for 3D elasticity are
   presented. A rigid body motion method (RBMM) for the FMBEM is proposed
   based on special displacement boundary conditions to deal with strongly
   singular integration and free term coefficients. The numerical example
   results show that our parallel algorithms obviously accelerates the
   FMBEM and can be used in large scale engineering problems with wide
   applications in the future. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>APR 2015</date><author>Wang, Yingjun
   Wang, Qifu
   Deng, Xiaowei
   Xia, Zhaohui
   Yan, Jinhui
   Xu, Hua</author></paper><paper><title>Potential Application Areas of GIS in Preconstruction Planning</title><abstract>The construction industry is experiencing changes in the tools and
   techniques used in the planning for high productivity, safety, and
   sustainability. These changes have not been seen much in the
   preconstruction planning (PCP) stage. The PCP is highly individualistic
   in nature, in which planners/contractors use their past experience to
   plan a project within the constraints of time, cost, and quality. The
   use of planning techniques in the PCP stage has been the subject of
   research, development, and implementation for a long time; however, at
   present, importance is being placed on the use of different tools. Tools
   like knowledge-based systems, computer graphics, and a combination of
   the two have been reported in the literature. Literature also suggests
   the use of geographic information systems (GIS) as a recent tool in the
   PCP stage. A structure that is to be developed becomes an important
   component of its neighborhood. It is closely related to all other
   existing facilities in its surroundings; hence, its construction should
   not be planned in isolation but in reference to its locality, existing
   facilities/utilities, topography, and so on. In the PCP stage,
   consideration should be given to its surroundings, in which the use of
   GIS has been found to be helpful. Therefore, the primary objective of
   the present study is to explore the potential application areas of GIS,
   as a tool, in the PCP stage. The present study informs practitioners in
   the construction industry about the application areas of GIS for their
   wider use in real-life projects. Finally, research areas worthy for
   further investigation are highlighted.(C) 2015 American Society of Civil
   Engineers.</abstract><date>JAN 2016</date><author>Bansal, V. K.</author></paper><paper><title>Accelerating MRI reconstruction via three-dimensional dual-dictionary
   learning using CUDA</title><abstract>Using undersampled k-space data for reconstruction is an effective way
   to accelerate data acquisition of magnetic resonance imaging (MRI). With
   the development of compressed sensing (CS) theory, many solutions have
   been proposed for undersampled data reconstruction. Moreover, dictionary
   learning method has shown good results in improving reconstruction
   quality. However, CS reconstruction algorithms are time consuming,
   especially at dictionary training and sparse coding step. The
   computation overhead is even higher for three-dimensional
   reconstruction. With a large number of slices, data size can be massive
   and more time consuming. In this paper, we use three-dimensional
   dual-dictionary learning scheme for the reconstruction procedure.
   Three-dimensional dictionaries train the dictionary atoms in image
   blocks and utilize spatial correlation among MR slices. Dual-dictionary
   learning method cooperates low-resolution dictionary and high-resolution
   dictionary for sparse coding and image updating, respectively. Compute
   unified device architecture (CUDA) is utilized to design the parallel
   algorithms on graphics processing unit (GPU). We mainly optimize
   dictionary learning algorithm and image updating. We also develop
   parallel CPU codes using OpenMP (Open Multi-Processing) and another
   version of CUDA codes with algorithmic optimization. Experimental
   results show that more than 324 times of speedup is achieved compared
   with CPU-only codes with 24 MRI slices and more than 40 times of
   acceleration compared with OpenMP codes.</abstract><date>JUL 2015</date><author>Li, Jiansen
   Sun, Jianqi
   Song, Ying
   Zhao, Jun</author></paper><paper><title>Efficient urban flood simulation using a GPU-accelerated SPH model</title><abstract>Urban flooding may lead to significant losses of properties and lives
   and numerical modelling can facilitate better flood risk management to
   reduce losses. Flood modelling generally involves seeking numerical
   solutions to the shallow water equations (SWEs) or one of the simplified
   forms using the traditional numerical methods including the finite
   difference method (FDM), finite volume method (FVM) and finite element
   method (FEM). Recently, a relatively new approach, smoothed particle
   hydrodynamics (SPH), has also been used to solve the SWEs and
   encouraging results have been reported. However, the SPH method is
   computationally too demanding for efficient simulations, which has been
   one of the major disadvantages dogging its wider applications. This work
   presents an SPH model that is computationally accelerated by modern
   graphic processing units (GPUs) for efficient urban flooding modelling.
   The model's predictive capability and enhanced computational efficiency
   are demonstrated by application to experimental and field-scale
   hypothetic urban flood events.</abstract><date>DEC 2015</date><author>Liang, Qiuhua
   Xia, Xilin
   Hou, Jingming</author></paper><paper><title>Multi-GPU Acceleration of Algebraic Multi-Grid Preconditioners for
   Elliptic Field Problems</title><abstract>In this contribution, a multi-graphic processing unit (GPU)
   implementation of Krylov sub-space methods with algebraic multi-grid
   preconditioners is proposed. It is used to solve large linear systems
   stemming from finite element or finite difference discretizations of
   elliptic problems as they occur, e.g., in electrostatics. The
   distribution of data across multiple GPUs and the effects on memory and
   speed are discussed when using an approach that preserves the effects of
   fine-grained parallelism with shared memory on the GPU while
   distributing data across multiple GPUs with minimal communication
   effort.</abstract><date>MAR 2015</date><author>Richter, Christian
   Schoeps, Sebastian
   Clemens, Markus</author></paper><paper><title>Mapping Cohesive Fracture and Fragmentation Simulations to Graphics
   Processor Units</title><abstract>A graphics processor units(GPU)-based computational framework is
   presented to deal with dynamic failure events simulated by means of
   cohesive zone elements. The work is divided into two parts. In the first
   part, we deal with pre-processing of the information and verify the
   effectiveness of dynamic insertion of cohesive elements in large meshes
   in parallel. To this effect, we employ a novel and simplified
   topological data structure specialized for meshes with triangles,
   designed to run efficiently and minimize memory occupancy on the GPU. In
   the second part, we present a parallel explicit dynamics code that
   implements an extrinsic cohesive zone formulation where the elements are
   inserted on-the-fly', when needed and where needed. The main challenge
   for implementing a GPU-based computational framework using an extrinsic
   cohesive zone formulation resides on being able to dynamically adapt the
   mesh, in a consistent way, by inserting cohesive elements on fractured
   facets. In order to handle that, we extend the conventional data
   structure used in the finite element method (based on element incidence)
   and store, for each element, references to the adjacent elements. This
   additional information suffices to consistently insert cohesive elements
   by duplicating nodes when needed. Currently, our data structure is
   specialized for triangular meshes, but an extension to tetrahedral
   meshes is feasible. The data structure is effective when used in
   conjunction with algorithms to traverse nodes and elements. Results from
   parallel simulations show an increase in performance when adopting
   strategies such as distributing different jobs among threads for the
   same element and launching many threads per element. To avoid
   concurrency on accessing shared entities, we employ graph coloring. In a
   pre-processing phase, each node of the dual graph (bulk elements of the
   mesh as graph nodes) is assigned a color different from the colors
   assigned to adjacent nodes. In that fashion, elements of the same color
   can be processed in parallel without concurrency. All the procedures
   needed for the insertion of cohesive elements along fracture facets and
   for computing nodal properties are performed by threads assigned to
   triangles, invoking one kernel per color. Computations on existing
   cohesive elements are also performed based on adjacent bulk elements.
   Experiments show that GPU speedup increases with the number of nodes and
   bulk elements. Copyright (c) 2015 John Wiley &amp; Sons, Ltd.</abstract><date>SEP 21 2015</date><author>Alhadeff, A.
   Celes, W.
   Paulino, G. H.</author></paper><paper><title>A Study on CUDA Computation Based on Forward Modeling and Inversion
   Algorithm of 2D Magnetotelluric</title><abstract>Magnetotelluric sounding method based on the difference of the rock's
   resistivity is an Exploration method about doing research in earth's
   resistivity and phase with the native electromagnetic field. The paper
   introduces the inversion algorithm of 2D magnetotelluric named conjugate
   gradient method. the method, which avoid solving the jacobi matrix is
   very effective, but the method is not effective enough when the model is
   divided into big grid. Because using the CUDA technology can do more
   computation in the limited time, it improves the effectivity of the
   forward modeling and inversion algorithm of 2D magnetotelluric.</abstract><date>2015</date><author>Wang, Mao
   Tan, Handong
   Yao, Changli
   Ma, Huan
   Yang, Miaoxin</author></paper><paper><title>Comparison of Neck Screw and Conventional Fixation Techniques in
   Mandibular Condyle Fractures Using 3-Dimensional Finite Element Analysis</title><abstract>Purpose: To compare the mechanical stress on the mandibular condyle
   after the reduction and fixation of mandibular condylar fractures using
   the neck screw and 2 other conventional techniques according to
   3-dimensional finite element analysis.Materials and Methods: A
   3-dimensional finite element model of a mandible was created and
   graphically simulated on a computer screen. The model was fixed with 3
   different techniques: a 2.0-mm plate with 4 screws, 2 plates (1 1.5-mm
   plate and 1 2.0-mm plate) with 4 screws, and a neck screw. Loads were
   applied that simulated muscular action, with restrictions of the upper
   movements of the mandible, differentiation of the cortical and medullary
   bone, and the virtual `` folds'' of the plates and screws so that they
   could adjust to the condylar surface. Afterward, the data were exported
   for graphic visualization of the results and quantitative analysis was
   performed.Results: The 2-plate technique exhibited better stability in
   regard to displacement of fractures, deformity of the synthesis
   materials, and minimum and maximum tension values. The results with the
   neck screw were satisfactory and were similar to those found when a
   miniplate was used.Conclusion: Although the study shows that 2 isolated
   plates yielded better results compared with the other groups using other
   fixation systems and methods, the neck screw could be an option for
   condylar fracture reduction. (C) 2015 American Association of Oral and
   Maxillofacial Surgeons</abstract><date>JUL 2015</date><author>Conci, Ricardo Augusto
   Silveira Tomazi, Flavio Henrique
   Noritomi, Pedro Yoshito
   Lopes da Silva, Jorge Vicente
   Fritscher, Guilherme Genehr
   Heitz, Claiton</author></paper><paper><title>Extending parallelization of the self-organizing map by combining data
   and network partitioned methods</title><abstract>High-dimensional data is pervasive in many fields such as engineering,
   geospatial, and medical. It is a constant challenge to build tools that
   help people in these fields understand the underlying complexities of
   their data. Many techniques perform dimensionality reduction or other
   "compression" to show views of data in either two or three dimensions,
   leaving the data analyst to infer relationships with remaining
   independent and dependent variables. Contextual self-organizing maps
   offer a way to represent and interact with all dimensions of a data set
   simultaneously. However, computational times needed to generate these
   representations limit their feasibility to realistic industry settings.
   Batch self-organizing maps provide a data-independent method that allows
   the training process to be parallelized and therefore sped up, saving
   time and money involved in processing data prior to analysis. This
   research parallelizes the batch self-organizing map by combining network
   partitioning and data partitioning methods with CUDA on the graphical
   processing unit to achieve significant training time reductions.
   Reductions in training times of up to twenty-five times were found while
   using map sizes where other implementations have shown weakness. The
   reduced training times open up the contextual self-organizing map as
   viable option for engineering data visualization. (C) 2015 Elsevier Ltd.
   All rights reserved.</abstract><date>OCT 2015</date><author>Richardson, Trevor
   Winer, Eliot</author></paper><paper><title>GPU Accelerated Self-Organizing Map for High Dimensional Data</title><abstract>The self-organizing map (SOM) model is an effective technique applicable
   in a wide range of areas, such as pattern recognition and image
   processing. In the SOM model, the most time-consuming procedure is the
   training process. It consists of two time-consuming parts. The first
   part is the calculation of the Euclidean distances between training
   vectors and codevectors. The second part is the update of the
   codevectors with the pre-defined neighborhood structure. This paper
   proposes a graphics processing unit (GPU) algorithm that accelerates
   these two parts using the graphics rendering ability of GPUs. The
   distance calculation is implemented in the form of matrix multiplication
   with compute shader, while the update process is treated as a
   point-rendering process with vertex shader and fragment shader.
   Experimental results show that our algorithm runs much faster than
   previous CUDA implementations, especially for the large neighborhood
   case. Also, our method can handle the case with large codebook size and
   high dimensional data.</abstract><date>JUN 2015</date><author>Xiao, Yi
   Feng, Rui-Bin
   Han, Zi-Fa
   Leung, Chi-Sing</author></paper><paper><title>Binary Image Segmentation based on Optimized Parallel K-means</title><abstract>K-means is a classic unsupervised learning clustering algorithm. In
   theory, it can work well in the field of image segmentation. But
   compared with other segmentation algorithms, this algorithm needs much
   more computation, and segmentation speed is slow. This limits its
   application. With the emergence of general-purpose computing on the GPU
   and the release of CUDA, some scholars try to implement K-means
   algorithm in parallel on the GPU, and applied to image segmentation at
   the same time. They have achieved some results, but the approach they
   use is not completely parallel, not take full advantage of GPU's super
   computing power. K-means algorithm has two core steps: label and update,
   in current parallel realization of K-means, only labeling is parallel,
   update operation is still serial. In this paper, both of the two steps
   in K-means will be parallel to improve the degree of parallelism and
   accelerate this algorithm. Experimental results show that this
   improvement has reached a much quicker speed than the previous research.</abstract><date>2015</date><author>Qiu, Xiao-bing
   Zhou, Yong
   Lin, Li</author></paper><paper><title>Optimizing seam carving on multi-GPU systems for real-time content-aware
   image resizing</title><abstract>Image resizing is increasingly important for picture sharing and
   exchanging between various personal electronic equipments. Seam Carving
   is a state-of-the-art approach for effective image resizing because of
   its content-aware characteristic. However, complex computation and
   memory access patterns make it time consuming and prevent its wide usage
   in real-time image processing. To address these problems, we propose a
   novel algorithm, called Non-Cumulative Seam Carving (NCSC), which
   removes main computation bottleneck. Furthermore, we also propose
   Partial update of Index Map (PIM) algorithm to reduce computation
   amount. Finally, we implement our algorithm on a multi-GPU platform.
   Results show that our approach achieves maximum speedup over the
   original seam carving implementation on a single-GPU system. It also
   presents maximum speedup on a two-GPU system over the single-thread CPU
   implementation of original seam carving algorithm. NCSC only takes 0.10
   s to reduce a image to 70 % in width on a two-GPU platform compared to
   11 s with the traditional seam carving on a single-thread CPU system.</abstract><date>SEP 2015</date><author>Kim, Ikjoon
   Zhai, Jidong
   Li, Yan
   Chen, Wenguang</author></paper><paper><title>A Publishing Method of Lightweight Three-Dimensional Assembly
   Instruction for Complex Products</title><abstract>In order to accurately guide on-site workers to quickly accomplish the
   assembly job of complex products, and reduce the deployment cost of
   assembly instruction, we propose a publishing method of lightweight 3D
   assembly instruction for complex products. In this paper, the key frames
   of assembly motion and the 3D technical annotations in the lightweight
   model are mapped to the time-based assembly process. Then, the annotated
   lightweight model and assembly process information are integrated and
   published into a single 3D assembly instruction document. An assembly
   instruction publishing example of satellite antenna feed component shows
   that the lightweight 3D assembly instruction is well instructive and
   affordable because it provides the interactive simulation of assembly
   process and time-based display of assembly technical annotations without
   using expensive computer-aided design (CAD) systems, graphics
   workstations, or virtual reality equipments. This method gives a full
   play to the advantages of model-based definition technology and
   lightweight model, and fills the gap between the process planning and
   the instruction publishing in the 3D virtual manufacturing environment.</abstract><date>SEP 2015</date><author>Geng, Junhao
   Zhang, Sumei
   Yang, Bin</author></paper><paper><title>GPU Accelerated Finding of Channels and Tunnels for a Protein Molecule</title><abstract>This paper proposes a novel method for computing the cavities and
   channels/tunnels in a protein molecule in interactive time without
   significant user effort. A sphere tree structure is used to represent a
   protein molecule, which provides a parallel architecture to access a
   Graphic Processing Unit (GPU) memory. The use of CUDA programming with a
   GPU then allows the proposed system to work in parallel on either a
   sphere tree structure of a molecule or a set of voxels composing the
   space. A real-time performance is achieved for proximity queries on a
   protein molecule, and an interactive time performance is realized for
   finding all the cavities and channel/tunnels without user effort. The
   proposed system also provides a method for approximating a convex hull
   of a molecule in a discrete space, and then generates the shortest path
   from a user selected or automatically chosen cavity to the exterior of
   the protein molecule. Experimental results in comparison with previous
   methods confirm the time efficiency of the proposed system.</abstract><date>FEB 2016</date><author>Kim, Byungjoo
   Lee, Jung Eun
   Kim, Young J.
   Kim, Ku-Jin</author></paper><paper><title>Real-time time-division color electroholography using a single GPU and a
   USB module for synchronizing reference light</title><abstract>We propose real-time time-division color electroholography using a
   single graphics processing unit (GPU) and a simple synchronization
   system of reference light. To facilitate real-time time-division color
   electroholography, we developed a light emitting diode (LED) controller
   with a universal serial bus (USB) module and the drive circuit for
   reference light. A one-chip RGB LED connected to a personal computer via
   an LED controller was used as the reference light. A single GPU
   calculates three computer-generated holograms (CGHs) suitable for red,
   green, and blue colors in each frame of a three-dimensional (3D) movie.
   After CGH calculation using a single GPU, the CPU can synchronize the
   CGH display with the color switching of the one-chip RGB LED via the LED
   controller. Consequently, we succeeded in real-time time-division color
   electroholography for a 3D object consisting of around 1000 points per
   color when an NVIDIA GeForce GTX TITAN was used as the GPU. Furthermore,
   we implemented the proposed method in various GPUs. The experimental
   results showed that the proposed method was effective for various GPUs.
   (C) 2015 Optical Society of America</abstract><date>DEC 1 2015</date><author>Araki, Hiromitsu
   Takada, Naoki
   Niwase, Hiroaki
   Ikawa, Shohei
   Fujiwara, Masato
   Nakayama, Hirotaka
   Kakue, Takashi
   Shimobaba, Tomoyoshi
   Ito, Tomoyoshi</author></paper><paper><title>Distinct roles for GABA across multiple timescales in mammalian
   circadian timekeeping</title><abstract>The suprachiasmatic nuclei (SCN), the central circadian pacemakers in
   mammals, comprise a multiscale neuronal system that times daily events.
   We use recent advances in graphics processing unit computing to generate
   a multiscale model for the SCN that resolves cellular electrical
   activity down to the timescale of individual action potentials and the
   intracellular molecular events that generate circadian rhythms. We use
   the model to study the role of the neurotransmitter GABA in
   synchronizing circadian rhythms among individual SCN neurons, a topic of
   much debate in the circadian community. The model predicts that GABA
   signaling has two components: phasic (fast) and tonic (slow). Phasic
   GABA postsynaptic currents are released after action potentials, and can
   both increase or decrease firing rate, depending on their timing in the
   interspike interval, a modeling hypothesis we experimentally validate;
   this allows flexibility in the timing of circadian output signals.
   Phasic GABA, however, does not significantly affect molecular
   timekeeping. The tonic GABA signal is released when cells become very
   excited and depolarized; it changes the excitability of neurons in the
   network, can shift molecular rhythms, and affects SCN synchrony. We
   measure which neurons are excited or inhibited by GABA across the day
   and find GABA-excited neurons are synchronized by-and GABA-inhibited
   neurons repelled from-this tonic GABA signal, which modulates the
   synchrony in the SCN provided by other signaling molecules. Our
   mathematical model also provides an important tool for circadian
   research, and a model computational system for the many multiscale
   projects currently studying brain function.</abstract><date>JUL 21 2015</date><author>DeWoskin, Daniel
   Myung, Jihwan
   Belle, Mino D. C.
   Piggins, Hugh D.
   Takumi, Toru
   Forger, Daniel B.</author></paper><paper><title>Spectral Decomposition Modeling Method and Its Application to EM
   Scattering Calculation of Large Rough Surface With SSA Method</title><abstract>The small slope approximation (SSA) method is a practical method to
   calculate the electromagnetic (EM) scattering from rough surfaces.
   However, the SSA method requires that the interval for sampling surfaces
   must be small enough, such as less than one-tenth of incident
   wavelength. This constraint condition will cause the problem of huge
   memory consumption and insufficient memory when the EM scattering of
   large rough surfaces is calculated. Although the hard disk has large
   space to keep data and can solve the insufficient memory problem, its
   read/write speed is still too slow. In addition, massive data
   transmission will reduce the computational efficiency for the compute
   unified device architecture (CUDA) parallel computation under some
   conditions. In this paper, the main idea of the spectral decomposition
   modeling method is that the whole spectrum of rough surface is divided
   into several parts and these parts can be used to generate
   different-scale rough surfaces. Then, by analyzing the different-scale
   rough surfaces, the large rough surface can be achieved and applied to
   the calculation of EM scattering with the SSA method. Due to the small
   memory consumption of different-scale rough surfaces, it takes less time
   to translate data for the different-scale rough surfaces than that for
   the standard large surface. Thus, the spectral decomposition modeling
   method could readily be applied to CUDA parallel computation.</abstract><date>APR 2015</date><author>Jiang, Wang-Qiang
   Zhang, Min
   Wei, Peng-Bo
   Nie, Ding</author></paper><paper><title>A cost-optimal parallel algorithm for the 0-1 knapsack problem and its
   performance on multicore CPU and GPU implementations</title><abstract>The 0-1 knapsack problem has been extensively studied in the past years
   due to its immediate applications in industry and financial management,
   such as cargo loading, stock cutting, and budget control. Many
   algorithms have been proposed to solve this problem, most of which are
   heuristic, as the problem is well-known to be NP-hard. Only a few
   optimal algorithms have been designed to solve this problem but with
   high time complexity. This paper proposes the cost-optimal parallel
   algorithm (COPA) on an EREW PRAM model with shared memory to solve this
   problem. COPA is scalable and yields optimal solutions consuming less
   computational time. Furthermore, this paper implements COPA on two
   scenarios - multicore CPU based architectures using Open MP and GPU
   based configurations using CUDA. A series of experiments are conducted
   to examine the performance of COPA under two different test platforms.
   The experimental results show that COPA could reduce a significant
   amount of execution time. Our approach achieves the speedups of up to
   10.26 on multicore CPU implementations and 17.53 on GPU implementations
   when the sequential dynamic programming algorithm for KP01 is considered
   as a baseline. Importantly, GPU implementations outstand themselves in
   the experimental results. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>MAR 2015</date><author>Li, Kenli
   Liu, Jing
   Wan, Lanjun
   Yin, Shu
   Li, Keqin</author></paper><paper><title>Efficient matrix-free GPU implementation of Fixed Grid Finite Element
   Analysis</title><abstract>This paper proposes a strategy for the efficient implementation of Fixed
   Grid Finite Element Analysis (FGFEA) method on Graphics Processing Units
   (GPUs). Such a strategy makes use of grid regularity of FGFEA to reduce
   drastically both the memory required by the implementation and the
   memory transactions to perform the operations with the common elemental
   stiffness matrix. The matrix-free method is adopted (i) to reduce the
   memory requirements obviating the assembly process of FEA and (ii) to
   exploit the parallelization potential of GPU architectures performing
   matrix-vector products at the Degree of Freedom (DoF) level. The
   underlying idea is to exploit data locality and maximize the use of
   on-chip memory, which increase notably the performance of GPU computing.
   The numerical experiments show that the proposed matrix-free CPU
   instance of FGFEA can achieve significant speedup over classical
   sparse-matrix CPU implementation using similar iterative solver. (C)
   2015 Elsevier B.V. All rights reserved</abstract><date>OCT 15 2015</date><author>Martinez-Frutos, Jesus
   Herrero-Perez, David</author></paper><paper><title>Interactive Near-Field Illumination for Photorealistic Augmented Reality
   with Varying Materials on Mobile Devices</title><abstract>At present, photorealistic augmentation is not yet possible since the
   computational power of mobile devices is insufficient. Even streaming
   solutions from stationary PCs cause a latency that affects user
   interactions considerably. Therefore, we introduce a differential
   rendering method that allows for a consistent illumination of the
   inserted virtual objects on mobile devices, avoiding delays. The
   computation effort is shared between a stationary PC and the mobile
   devices to make use of the capacities available on both sides. The
   method is designed such that only a minimum amount of data has to be
   transferred asynchronously between the participants. This allows for an
   interactive illumination of virtual objects with a consistent appearance
   under both temporally and spatially varying real illumination
   conditions. To describe the complex near-field illumination in an indoor
   scenario, HDR video cameras are used to capture the illumination from
   multiple directions. In this way, sources of illumination can be
   considered that are not directly visible to the mobile device because of
   occlusions and the limited field of view. While our method focuses on
   Lambertian materials, we also provide some initial approaches to
   approximate non-diffuse virtual objects and thereby allow for a wider
   field of application at nearly the same cost.</abstract><date>DEC 2015</date><author>Rohmer, Kai
   Bueschel, Wolfgang
   Dachselt, Raimund
   Grosch, Thorsten</author></paper><paper><title>Algorithms for classification of astronomical object spectra</title><abstract>Obtaining interesting celestial objects from tens of thousands or even
   millions of recorded optical-ultraviolet spectra depends not only on the
   data quality but also on the accuracy of spectra decomposition.
   Additionally rapidly growing data volumes demandes higher computing
   power and/or more efficient algorithms implementations. In this paper we
   speed up the process of substracting iron transitions and fitting
   gaussian functions to emission peaks utilising C++ and OpenCL methods
   together with the NOSQL database. In this paper we implemented typical
   astronomical methods of detecting peaks in comparison to our previous
   hybrid methods implemented with CUDA.</abstract><date>2015</date><author>Wasiewicz, P.
   Szuppe, J.
   Hryniewicz, K.</author></paper><paper><title>Real-time 3D Visualization Method of Landslide disaster prediction
   Simulation using GPU</title><abstract>In this paper, we propose a GPU-based interactive and plausible
   visualization method for the silt and landslide simulation results
   computed with SPH. By empirical experiments, we verify that our
   GPU-accelerated screen space mesh method can be effectively used for
   visualizing the landslide disaster simulation.  The method proposed in
   this paper make it possible to overcome the limitation of previous
   simulations where the experience obtained by trials and errors plays the
   most important roles. Because the realtime visualization enables
   interactive observation of simulation results and efficient data
   assimilation, the accuracy of the simulation can be significantly
   improved in an efficient way.</abstract><date>2015</date><author>???
   ???</author></paper><paper><title>Working with the HL7 metamodel in a Model Driven Engineering context</title><abstract>HL7 (Health Level 7) International is an organization that defines
   health information standards. Most HL7 domain information models have
   been designed according to a proprietary graphic language whose domain
   models are based on the HL7 metamodel. Many researchers have considered
   using HL7 in the MDE (Model-Driven Engineering) context. A limitation
   has been identified: all MDE tools support UML (Unified Modeling
   Language), which is a standard model language, but most do not support
   the HL7 proprietary model language. We want to support software
   engineers without HL7 experience, thus realworld problems would be
   modeled by them by defining system requirements in UML that are
   compliant with HL7 domain models transparently. The objective of the
   present research is to connect HL7 with software analysis using a
   generic model-based approach. This paper introduces a first approach to
   an HL7 MDE solution that considers the MIF (Model Interchange Format)
   metamodel proposed by HL7 by making use of a plug-in developed in the EA
   (Enterprise Architect) tool. (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>OCT 2015</date><author>Martinez-Garcia, A.
   Garcia-Garcia, J. A.
   Escalona, M. J.
   Parra-Calderon, C. L.</author></paper><paper><title>Fast analytical scatter estimation using graphics processing units</title><abstract>PURPOSE: To develop a fast patient-specific analytical estimator of
   first-order Compton and Rayleigh scatter in cone-beam computed
   tomography, implemented using graphics processing units.METHODS: The
   authors developed an analytical estimator for first-order Compton and
   Rayleigh scatter in a cone-beam computed tomography geometry. The
   estimator was coded using NVIDIA's CUDA environment for execution on an
   NVIDIA graphics processing unit. Performance of the analytical estimator
   was validated by comparison with high-count Monte Carlo simulations for
   two different numerical phantoms. Monoenergetic analytical simulations
   were compared with monoenergetic and polyenergetic Monte Carlo
   simulations. Analytical and Monte Carlo scatter estimates were compared
   both qualitatively, from visual inspection of images and profiles, and
   quantitatively, using a scaled root-mean-square difference metric.
   Reconstruction of simulated cone-beam projection data of an
   anthropomorphic breast phantom illustrated the potential of this method
   as a component of a scatter correction algorithm.RESULTS: The
   monoenergetic analytical and Monte Carlo scatter estimates showed very
   good agreement. The monoenergetic analytical estimates showed good
   agreement for Compton single scatter and reasonable agreement for
   Rayleigh single scatter when compared with polyenergetic Monte Carlo
   estimates. For a voxelized phantom with dimensions 128 x 128 x 128
   voxels and a detector with 256 x 256 pixels, the analytical estimator
   required 669 seconds for a single projection, using a single NVIDIA 9800
   GX2 video card. Accounting for first order scatter in cone-beam image
   reconstruction improves the contrast to noise ratio of the reconstructed
   images.CONCLUSION: The analytical scatter estimator, implemented using
   graphics processing units, provides rapid and accurate estimates of
   single scatter and with further acceleration and a method to account for
   multiple scatter may be useful for practical scatter correction schemes.</abstract><date>2015</date><author>Ingleby, Harry
   Lippuner, Jonas
   Rickey, Daniel W.
   Li, Yue
   Elbakri, Idris</author></paper><paper><title>Aerobic biodegradation of organic compounds in hydraulic fracturing
   fluids</title><abstract>Little is known of the attenuation of chemical mixtures created for
   hydraulic fracturing within the natural environment. A synthetic
   hydraulic fracturing fluid was developed from disclosed industry
   formulas and produced for laboratory experiments using commercial
   additives in use by Marcellus shale field crews. The experiments
   employed an internationally accepted standard method (OECD 301A) to
   evaluate aerobic biodegradation potential of the fluid mixture by
   monitoring the removal of dissolved organic carbon (DOC) from an aqueous
   solution by activated sludge and lake water microbial consortia for two
   substrate concentrations and four salinities. Microbial degradation
   removed from 57 % to more than 90 % of added DOC within 6.5 days, with
   higher removal efficiency at more dilute concentrations and little
   difference in overall removal extent between sludge and lake microbe
   treatments. The alcohols isopropanol and octanol were degraded to levels
   below detection limits while the solvent acetone accumulated in
   biological treatments through time. Salinity concentrations of 40 g/L or
   more completely inhibited degradation during the first 6.5 days of
   incubation with the synthetic hydraulic fracturing fluid even though
   communities were pre-acclimated to salt. Initially diverse microbial
   communities became dominated by 16S rRNA sequences affiliated with
   Pseudomonas and other Pseudomonadaceae after incubation with the
   synthetic fracturing fluid, taxa which may be involved in acetone
   production. These data expand our understanding of constraints on the
   biodegradation potential of organic compounds in hydraulic fracturing
   fluids under aerobic conditions in the event that they are accidentally
   released to surface waters and shallow soils.[GRAPHICS].</abstract><date>JUL 2015</date><author>Kekacs, Daniel
   Drollette, Brian D.
   Brooker, Michael
   Plata, Desiree L.
   Mouser, Paula J.</author></paper><paper><title>Computing Locally Injective Mappings by Advanced MIPS</title><abstract>Computing locally injective mappings with low distortion in an efficient
   way is a fundamental task in computer graphics. By revisiting the
   well-known MIPS (Most-Isometric ParameterizationS) method, we introduce
   an advanced MIPS method that inherits the local injectivity of MIPS,
   achieves as low as possible distortions compared to the state-of-the-art
   locally injective mapping techniques, and performs one to two orders of
   magnitude faster in computing a mesh-based mapping. The success of our
   method relies on two key components. The first one is an enhanced MIPS
   energy function that penalizes the maximal distortion significantly and
   distributes the distortion evenly over the domain for both mesh-based
   and meshless mappings. The second is a use of the inexact block
   coordinate descent method in mesh-based mapping in a way that
   efficiently minimizes the distortion with the capability not to be
   trapped early by the local minimum. We demonstrate the capability and
   superiority of our method in various applications including mesh
   parameterization, mesh-based and meshless deformation, and mesh
   improvement.</abstract><date>AUG 2015</date><author>Fu, Xiao-Ming
   Liu, Yang
   Guo, Baining</author></paper><paper><title>Computational performance of a smoothed particle hydrodynamics
   simulation for shared-memory parallel computing</title><abstract>The computational performance of a smoothed particle hydrodynamics (SPH)
   simulation is investigated for three types of current shared-memory
   parallel computer devices: many integrated core (MIC) processors,
   graphics processing units (GPUs), and multi-core CPUs. We are especially
   interested in efficient shared-memory allocation methods for each
   chipset, because the efficient data access patterns differ between
   compute unified device architecture (CUDA) programming for GPUs and
   OpenMP programming for MIC processors and multi-core CPUs. We first
   introduce several parallel implementation techniques for the SPH code,
   and then examine these on our target computer architectures to determine
   the most effective algorithms for each processor unit. In addition, we
   evaluate the effective computing performance and power efficiency of the
   SPH simulation on each architecture, as these are critical metrics for
   overall performance in a multi-device environment. In our benchmark
   test, the GPU is found to produce the best arithmetic performance as a
   standalone device unit, and gives the most efficient power consumption.
   The multi-core CPU obtains the most effective computing performance. The
   computational speed of the MIC processor on Xeon Phi approached that of
   two Xeon CPUs. This indicates that using MICs is an attractive choice
   for existing SPH codes on multi-core CPUs parallelized by OpenMP, as it
   gains computational acceleration without the need for significant
   changes to the source code. (C) 2015 The Authors. Published by Elsevier
   B.V.</abstract><date>SEP 2015</date><author>Nishiura, Daisuke
   Furuichi, Mikito
   Sakaguchi, Hide</author></paper><paper><title>A GEMM interface and implementation on NVIDIA GPUs for multiple small
   matrices</title><abstract>We present an interface and an implementation of the General Matrix
   Multiply (GEMM) routine for multiple small matrices processed
   simultaneously on NVIDIA graphics processing units (GPUs). We focus on
   matrix sizes under 16. The implementation can be easily extended to
   larger sizes. For single precision matrices, our implementation is 30%
   to 600% faster than the batched cuBLAS implementation distributed in the
   CUDA Toolkit 5.0 on NVIDIA Tesla K20c. For example, we obtain 104
   GFlop/s and 216 GFlop/s when multiplying 100,000 independent matrix
   pairs of size 10 and 16, respectively. Similar improvement in
   performance is obtained for other sizes, in single and double precisions
   for real and complex types, and when the number of matrices is smaller.
   Apart from our implementation, our different function interface also
   plays an important role in the improved performance. Applications of
   this software include finite element computation on GPUs. (C) 2014
   Elsevier Inc. All rights reserved.</abstract><date>JAN 2015</date><author>Jhurani, Chetan
   Mullowney, Paul</author></paper><paper><title>Accelerating the Smith-Waterman algorithm with interpair pruning and
   band optimization for the all-pairs comparison of base sequences</title><abstract>Background: The Smith-Waterman algorithm is known to be a more sensitive
   approach than heuristic algorithms for local sequence alignment
   algorithms. Despite its sensitivity, a greater time complexity
   associated with the Smith-Waterman algorithm prevents its application to
   the all-pairs comparisons of base sequences, which aids in the
   construction of accurate phylogenetic trees. The aim of this study is to
   achieve greater acceleration using the Smith-Waterman algorithm (by
   realizing interpair block pruning and band optimization) compared with
   that achieved using a previous method that performs intrapair block
   pruning on graphics processing units (GPUs).Results: We present an
   interpair optimization method for the Smith-Waterman algorithm with the
   aim of accelerating the all-pairs comparison of base sequences. Given
   the results of the pairs of sequences, our method realizes efficient
   block pruning by computing a lower bound for other pairs that have not
   yet been processed. This lower bound is further used for band
   optimization. We integrated our interpair optimization method into SW#,
   a previous GPU-based implementation that employs variants of a banded
   Smith-Waterman algorithm and a banded Myers-Miller algorithm. Evaluation
   using the six genomes of Bacillus anthracis shows that our method pruned
   88 % of the matrix cells on a single GPU and 73 % of the matrix cells on
   two GPUs. For the genomes of the human chromosome 21, the alignment
   performance reached 202 giga-cell updates per second (GCUPS) on two
   Tesla K40 GPUs.Conclusions: Efficient interpair pruning and band
   optimization makes it possible to complete the all-pairs comparisons of
   the sequences of the same species 1.2 times faster than the intrapair
   pruning method. This acceleration was achieved at the first phase of
   SW#, where our method significantly improved the initial lower bound.
   However, our interpair optimization was not effective for the comparison
   of the sequences of different species such as comparing human,
   chimpanzee, and gorilla. Consequently, our method is useful in
   accelerating the applications that require optimal local alignments
   scores for the same species. The source code is available for download
   from http://www-hagi.ist.osaka-u.ac.jp/research/code/.</abstract><date>OCT 6 2015</date><author>Okada, Daiki
   Ino, Fumihiko
   Hagihara, Kenichi</author></paper><paper><title>Indian scorpions collected in Karnataka: maintenance in captivity, venom
   extraction and toxicity studies</title><abstract>Background: Maintenance of scorpions under laboratory conditions is
   ideal for long-term venom collection to explore the therapeutic
   applications of scorpion venom. Collection of venom by electrical
   stimulation requires a reliable stimulator and effective restrainer.
   Thus, the present study was conducted to develop a convenient method to
   maintain scorpions and to extract their venom for toxicity studies via a
   modified restrainer and stimulator.Methods: Four different scorpion
   species were collected, among which three species were maintained in the
   laboratory in containers that mimic their natural habitat. Venom was
   extracted from Hottentotta rugiscutis by electrical stimulation at 8 V
   for 18 months and LD50 was estimated by the graphic method of Miller and
   Tainter.Results: A total of 373 scorpions including Hottentotta
   rugiscutis, Hottentotta tamulus, Lychas tricarinatus and Heterometrus
   swammerdami were collected, identified and maintained successfully,
   achieving a 97 % survival rate. Hottentotta rugiscutis yielded 6.0 mL of
   venom by electrical stimulation. The LD50 of H. rugiscutis venom was
   estimated to be 3.02 mg/kg of body weight in female Swiss albino
   mice.Conclusions: Scorpions were successfully maintained for 18 months.
   Herein we have also documented a simple, cost-effective method of venom
   extraction by electrical stimulation using a modified restrainer.
   Furthermore, Hottentotta rugiscutis was reported for the first time in
   Karnataka.</abstract><date>DEC 4 2015</date><author>Nagaraj, Santhosh Kambaiah
   Dattatreya, Pavana
   Boramuthi, Thippeswamy Nayaka</author></paper><paper><title>Parallel Optimization of 3D Cardiac Electrophysiological Model Using GPU</title><abstract>Large-scale 3D virtual heart model simulations are highly demanding in
   computational resources. This imposes a big challenge to the traditional
   computation resources based on CPU environment, which already cannot
   meet the requirement of the whole computation demands or are not easily
   available due to expensive costs. GPU as a parallel computing
   environment therefore provides an alternative to solve the large-scale
   computational problems of whole heart modeling. In this study, using a
   3D sheep atrial model as a test bed, we developed a GPU-based simulation
   algorithm to simulate the conduction of electrical excitation waves in
   the 3D atria. In the GPU algorithm, a multicellular tissue model was
   split into two components: one is the single cell model (ordinary
   differential equation) and the other is the diffusion term of the
   monodomain model (partial differential equation). Such a decoupling
   enabled realization of the GPU parallel algorithm. Furthermore, several
   optimization strategies were proposed based on the features of the
   virtual heart model, which enabled a 200-fold speedup as compared to a
   CPU implementation. In conclusion, an optimized GPU algorithm has been
   developed that provides an economic and powerful platform for 3D whole
   heart simulations.</abstract><date>2015</date><author>Xia, Yong
   Wang, Kuanquan
   Zhang, Henggui</author></paper><paper><title>Multi-style video stylization based on texture advection</title><abstract>Artistic video stylization, which is widely used in multimedia
   entertainment, transforms a given video into different artistic styles.
   Most of the existing video stylization algorithms can simulate single or
   limited video artistic styles. Although some algorithms can achieve
   multi-style video processing, these algorithms are complex and difficult
   to implement. To solve this problem, we propose a multi-styled video
   stylization algorithm based on texture advection, where different
   artistic styles are synthesized and transferred from user-specified
   texture samples of desired styles. We use the direction field-guided
   texture synthesis to compute the texture layer that represents the
   artistic style. Painterly directional video styles are simulated
   competently by the orientation changes in the synthesized anisotropic
   textures. There appeared local distorted region of the texture layer
   during texture advection under the optical flow field. To address this
   issue, we propose the texture inpaint to synthesize the limited
   distorted region and make the stylized video temporally coherent. We
   also accelerate the video stylization by using the CUDA parallel
   computing framework that parallelly computes the morphological
   operations used for video abstraction. Finally, we produce stylized
   videos of multiple artistic styles with satisfactory experimental
   results, including the styles of oil painting, watercolor painting and
   stylized lines drawing.</abstract><date>NOV 2015</date><author>Tang Ying
   Zhang Yan
   Shi XiaoYing
   Fan Jing</author></paper><paper><title>Parallel Implementation of Desirability Function-Based Scalarization
   Approach for Multiobjective Optimization Problems</title><abstract>Scalarization approaches are the simplest methods for solving the
   multiobjective problems. The idea of scalarization is based on
   decomposition of multiobjective problems into single objective
   sub-problems. Every one of these sub-problems can be solved in a
   parallel manner since they are independent with each other. Hence, as a
   scalarization approach, systematically modification on the desirability
   levels of the objective values of multiobjective problems can be
   employed for solving these problems. In this study, desirability
   function-based scalarization approach is converted into parallel
   algorithm and applied into seven benchmark problems. The performance of
   parallel algorithm with respect to sequential one is evaluated based on
   execution time on different graphical processing units and central
   processing units. The results show that even the accuracy of parallel
   and sequential codes are same, the execution time of parallel algorithm
   is up to 24.5-times faster than the sequential algorithm (8.25-times
   faster on average) with respect to the complexity of the problem.</abstract><date>JUN 2015</date><author>Altinoz, O. Tolga
   Akca, Eren
   Yilmaz, A. Egemen
   Duca, Anton
   Ciuprina, Gabriela</author></paper><paper><title>Implications of expansin-like 3 gene in Dictyostelium morphogenesis</title><abstract>Dictyostelium harbors multiple expansin-like genes with generally
   unknown functions. Thus, we analyzed the expansin-like 3 (expL3) gene
   and found that its expression was reduced in a null mutant for a STATa
   gene encoding a transcription factor. The expression of expL3 was
   developmentally regulated and its transcript was spliced only in the
   multicellular stages. The expL3 promoter was activated in the anterior
   prestalk region of the parental strain and downregulated in the STATa
   null slug, although the expL3 promoter was still expressed in the
   prestalk region. The expL3 overexpressing strain exhibited delayed
   development and occasionally formed an aberrant structure, i.e., a
   fruiting body-like structure with a short stalk. The ExpL3-myc protein
   bound cellulose.</abstract><date>APR 19 2015</date><author>Kawata, Takefumi
   Nakamura, Yuri
   Saga, Yukika
   Iwade, Yumi
   Ishikawa, Megumi
   Sakurai, Aya
   Shimada, Nao</author></paper><paper><title>CUDA-based Analytic Programming by Means of SOMA Algorithm</title><abstract>Analytic programming is one of methods of symbolic regression that is
   composing simple elements into more complex units. This process can be
   used e.g. for approximation of measured data with suitable mathematical
   formula. To find the most suitable mathematical formula, it is necessary
   to use an evolutionary algorithm. The constructed formulas can consist
   of mathematical operators, functions, variables and constants. Since
   values of these constants are not known at the time of construction of
   the formula, it is necessary to estimate the values by means of another
   evolutionary algorithm. Unfortunately, due to this estimation, the whole
   process becomes too slow. Therefore, this algorithm is implemented in
   one of the most widespread programming architecture NVIDIA CUDA and the
   results in terms of execution time are compared.</abstract><date>2015</date><author>Kojecky, Lumir
   Zelinka, Ivan</author></paper><paper><title>Performance improvement of data mining in Weka through multi-core and
   GPU acceleration: opportunities and pitfalls</title><abstract>Data mining tools may be computationally demanding, which leads to an
   increasing interest on parallel computing strategies in order to improve
   their performance. While multi-core processors and Graphics Processing
   Units (GPUs) accelerators increased the computing power of current
   desktop computers, we observe that desktop-based data mining tools do
   not take full advantage of these architectures yet. This paper
   investigates strategies to improve the performance of Weka, a popular
   data mining tool, through multi-core and GPU acceleration. Using
   performance profiling of Weka, we identify operations that could improve
   the data mining performance when parallelized. We selected two of these
   operations, and analyze the impact of their parallel execution on Weka's
   performance. These experiments demonstrate that while significant
   speedups can be achieved, all operations are not prone to be
   parallelized, which reinforces the need for a careful and well-studied
   selection of the candidates.</abstract><date>AUG 2015</date><author>Engel, Tiago Augusto
   Charao, Andrea Schwertner
   Kirsch-Pinheiro, Manuele
   Steffenel, Luiz-Angelo</author></paper><paper><title>Active learning for sketch recognition</title><abstract>The increasing availability of pen-based tablets, and pen-based
   interfaces opened the avenue for computer graphics applications that can
   utilize sketch recognition technologies for natural interaction. This
   has led to an increasing interest in sketch recognition algorithms
   within the computer graphics community. However, a key problem getting
   in the way of building accurate sketch recognizers has been the
   necessity of creating large amounts of annotated training data. Several
   authors have attempted to address this issue by creating synthetic data,
   or by building easy-to-use annotation tools. In this paper, we take a
   different approach, and demonstrate that the active learning technology
   can be used to reduce the amount of manual annotation required to
   achieve a target recognition accuracy. In particular, we show that by
   annotating few, but carefully selected examples, we can surpass
   accuracies achievable with equal number of arbitrarily selected
   examples. This work is the first comprehensive study on the use of
   active learning for sketch recognition. We present results of extensive
   analyses and show that the utility of active learning depends on a
   number of practical factors that require careful consideration. These
   factors include the choices of informativeness measures, batch selection
   strategies, seed size, and domain-specific factors such as feature
   representation and the choice of database. Our results imply that the
   Margin based informativeness measure consistently outperforms other
   measures. We also show that active learning brings definitive advantages
   in challenging databases when accompanied with powerful feature
   representations. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>NOV 2015</date><author>Yanik, Erelcan
   Sezgin, Tevfik Metin</author></paper><paper><title>A rational cubic clipping method for computing real roots of a
   polynomial</title><abstract>Many problems in computer aided geometric design and computer graphics
   can be turned into a root-finding problem of a polynomial equation.
   Among various solutions, clipping methods based on the Bernstein-Bezier
   form usually have good numerical stability. A traditional clipping
   method using polynomials of degree r can achieve a convergence rate of r
   + 1 for a single root. It utilizes two polynomials of degree r to bound
   the given polynomial f(t) of degree n, where r = 2, 3, and the roots of
   the bounding polynomials are used for clipping off the subintervals
   containing no roots of f(t). This paper presents a rational cubic
   clipping method for finding the roots of a polynomial f(t) within an
   interval. The bounding rational cubics can achieve an approximation
   order of 7 and the corresponding convergence rate for finding a single
   root is also 7. In addition, differently from the traditional cubic
   clipping method solving the two bounding polynomials in O (n(2)), the
   new method directly constructs the two rational cubics in O(n) which can
   be used for bounding f (t) in many cases. Some examples are provided to
   show the efficiency, the approximation effect and the convergence rate
   of the new method. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>OCT 2015</date><author>Chen, Xiao-Diao
   Ma, Weiyin
   Ye, Yangtian</author></paper><paper><title>CUDAMPF: a multi-tiered parallel framework for accelerating protein
   sequence search in HMMER on CUDA-enabled GPU</title><abstract>Background: HMMER software suite is widely used for analysis of
   homologous protein and nucleotide sequences with high sensitivity. The
   latest version of hmmsearch in HMMER 3.x, utilizes heuristic-pipeline
   which consists of MSV/SSV (Multiple/Single ungapped Segment Viterbi)
   stage, P7Viterbi stage and the Forward scoring stage to accelerate
   homology detection. Since the latest version is highly optimized for
   performance on modern multi-core CPUs with SSE capabilities, only a few
   acceleration attempts report speedup. However, the most compute
   intensive tasks within the pipeline (viz., MSV/SSV and P7Viterbi stages)
   still stand to benefit from the computational capabilities of massively
   parallel processors.Results: A Multi-Tiered Parallel Framework (CUDAMPF)
   implemented on CUDA-enabled GPUs presented here, offers a finer-grained
   parallelism for MSV/SSV and Viterbi algorithms. We couple SIMT (Single
   Instruction Multiple Threads) mechanism with SIMD (Single Instructions
   Multiple Data) video instructions with warp-synchronism to achieve
   high-throughput processing and eliminate thread idling. We also propose
   a hardware-aware optimal allocation scheme of scarce resources like
   on-chip memory and caches in order to boost performance and scalability
   of CUDAMPF. In addition, runtime compilation via NVRTC available with
   CUDA 7.0 is incorporated into the presented framework that not only
   helps unroll innermost loop to yield upto 2 to 3-fold speedup than
   static compilation but also enables dynamic loading and switching of
   kernels depending on the query model size, in order to achieve optimal
   performance.Conclusions: CUDAMPF is designed as a hardware-aware
   parallel framework for accelerating computational hotspots within the
   hmmsearch pipeline as well as other sequence alignment applications. It
   achieves significant speedup by exploiting hierarchical parallelism on
   single GPU and takes full advantage of limited resources based on their
   own performance features. In addition to exceeding performance of other
   acceleration attempts, comprehensive evaluations against high-end CPUs
   (Intel i5, i7 and Xeon) shows that CUDAMPF yields upto 440 GCUPS for
   SSV, 277 GCUPS for MSV and 14.3 GCUPS for P7Viterbi all with 100%
   accuracy, which translates to a maximum speedup of 37.5, 23.1 and
   11.6-fold for MSV, SSV and P7Viterbi respectively.</abstract><date>FEB 27 2016</date><author>Jiang, Hanyu
   Ganesan, Narayan</author></paper><paper><title>The Design and Implementation of a Verification Technique for GPU
   Kernels</title><abstract>We present a technique for the formal verification of GPU kernels,
   addressing two classes of correctness properties: data races and barrier
   divergence. Our approach is founded on a novel formal operational
   semantics for GPU kernels termed synchronous, delayed visibility (SDV)
   semantics, which captures the execution of a GPU kernel by multiple
   groups of threads. The SDV semantics provides operational definitions
   for barrier divergence and for both inter- and intra-group data races.
   We build on the semantics to develop a method for reducing the task of
   verifying a massively parallel GPU kernel to that of verifying a
   sequential program. This completely avoids the need to reason about
   thread interleavings, and allows existing techniques for sequential
   program verification to be leveraged. We describe an efficient encoding
   of data race detection and propose a method for automatically inferring
   the loop invariants that are required for verification. We have
   implemented these techniques as a practical verification tool,
   GPUVerify, that can be applied directly to OpenCL and CUDA source code.
   We evaluate GPUVerify with respect to a set of 162 kernels drawn from
   public and commercial sources. Our evaluation demonstrates that
   GPUVerify is capable of efficient, automatic verification of a large
   number of real-world kernels.</abstract><date>JUN 2015</date><author>Betts, Adam
   Chong, Nathan
   Donaldson, Alastair F.
   Ketema, Jeroen
   Qadeer, Shaz
   Thomson, Paul
   Wickerson, John</author></paper><paper><title>GPU-Based Calculation of Lightning-Generated Electromagnetic Fields in
   3-D Problems With Statistically Defined Uncertainties</title><abstract>A complete computational framework for the efficient study of
   lightning-induced electromagnetic fields and solution of pertinent
   problems with uncertainties in realistic environments is presented in
   this paper. The latter often involve various factors, such as material
   inhomogeneities, rough terrain surfaces, and irregular lightning
   channels that may inhibit the utilization of simplified approaches. To
   deal with these situations of augmented complexity, the
   finite-difference time-domain method is applied in 3-D curvilinear
   formulation, ensuring that all the important details are taken into
   account. As the study of real-life lightning problems involves intense
   computations, the algorithm is accelerated by exploiting the computing
   capabilities of contemporary graphics processing units. Our
   implementation relies on a massive parallelization approach, introduces
   several new optimized practices, and ensures significant shortening of
   the simulations' duration. Hence, the investigation of configurations
   with uncertainties and the extraction of statistical features are
   greatly facilitated. In other words, the proposed approach comprises an
   instructive contribution toward the foundation of a useful tool for the
   in-depth investigation of lightning-related phenomena.</abstract><date>DEC 2015</date><author>Pyrialakos, Georgios G.
   Zygiridis, Theodoros T.
   Kantartzis, Nikolaos V.
   Tsiboukis, Theodoros D.</author></paper><paper><title>A Dataset for Visual Navigation with Neuromorphic Methods</title><abstract>Standardized benchmarks in Computer Vision have greatly contributed to
   the advance of approaches to many problems in the field. If we want to
   enhance the visibility of event-driven vision and increase its impact,
   we will need benchmarks that allow comparison among different
   neuromorphic methods as well as comparison to Computer Vision
   conventional approaches. We present datasets to evaluate the accuracy of
   frame-free and frame-based approaches for tasks of visual navigation.
   Similar to conventional Computer Vision datasets, we provide synthetic
   and real scenes, with the synthetic data created with graphics packages,
   and the real data recorded using a mobile robotic platform carrying a
   dynamic and active pixel vision sensor (DAVIS) and an RGB+Depth sensor.
   For both datasets the cameras move with a rigid motion in a static
   scene, and the data includes the images, events, optic flow, 3D camera
   motion, and the depth of the scene, along with calibration procedures.
   Finally, we also provide simulated event data generated synthetically
   from well-known frame-based optical flow datasets.</abstract><date>FEB 23 2016</date><author>Barranco, Francisco
   Fermuller, Cornelia
   Aloimonos, Yiannis
   Delbruck, Tobi</author></paper><paper><title>A GPU-based Parallel Ant Colony Algorithm for Scientific Workflow
   Scheduling</title><abstract>Scientific workflow scheduling problem is a combinatorial optimization
   problem. In the real application, the scientific workflow generally has
   thousands of task nodes. Scheduling large-scale workflow has huge
   computational overhead. In this paper, a parallel algorithm for
   scientific workflow scheduling is proposed so that the computing speed
   can be improved greatly. Our method used ant colony optimization
   approaches on the GPU. Thousands of GPU threads can parallel construct
   solutions. The parallel ant colony algorithm for workflow scheduling was
   implemented with CUDA C language. Scheduling problem instances with
   different scales were tested both in our parallel algorithm and CPU
   sequential algorithm. The experimental results on NVIDIA Tesla M2070 GPU
   show that our implementation for 1000 task nodes runs in 5 seconds,
   while a conventional sequential algorithm implementation runs in 104
   seconds on Intel Xeon X5650 CPU. Thus, our GPU-based parallel algorithm
   implementation attains a speed-up factor of 20.7.</abstract><date>AUG 2015</date><author>Wang, Pengfei
   Li, Huifang
   Zhang, Baihai</author></paper><paper><title>A Rapid Parallelization of Cone-beam Projection and Back-projection
   Operator Based on Texture Fetching Interpolation</title><abstract>Projection and back-projection are the most computational consuming
   parts in Computed Tomography (CT) reconstruction. Parallelization
   strategies using GPU computing techniques have been introduced. We in
   this paper present a new parallelization scheme for both projection and
   back-projection. The proposed method is based on CUDA technology carried
   out by NVIDIA Corporation. Instead of build complex model, we aimed on
   optimizing the existing algorithm and make it suitable for CUDA
   implementation so as to gain fast computation speed. Besides making use
   of texture fetching operation which helps gain faster interpolation
   speed, we fixed sampling numbers in the computation of projection, to
   ensure the synchronization of blocks and threads, thus prevents the
   latency caused by inconsistent computation complexity. Experiment
   results have proven the computational efficiency and imaging quality of
   the proposed method.</abstract><date>2015</date><author>Xie, Lizhe
   Hu, Yining
   Chen, Yang
   Shi, Luyao</author></paper><paper><title>Fast algorithm for calculating the radiological path in fan-beam CT
   image reconstruction</title><abstract>Algebraic reconstruction is more suitable for image reconstruction than
   analytic reconstruction as its high contrast and precision for
   incomplete projecting data. The calculation of projection coefficients
   is the key step of algebraic reconstruction, which has crucial effect on
   the reconstructed quality and speed. In this paper, an efficient method
   to calculate projection coefficients is proposed for cone-beam computed
   tomography (CBCT), referring to the structure characteristics of
   graphics processing unit (GPU) and intersection law of ray and grids.
   Firstly, an array is used to store the projection coefficient value
   under different intersections of grids and ray. Then, another array is
   designed to correct it in another direction. Finally, the projection
   coefficients can be solved by combining the two arrays. The proposed
   method has the advantage of less calculation amount and less program
   branches, and is suitable for parallel acceleration based on compute
   unified device architecture (CUDA). The calculated speed can be
   accelerated significantly. Experimental results show that the proposed
   algorithm is 10 times faster than Siddon algorithm, and almost 30 times
   faster in GPU. (C) 2015 Elsevier GmbH. All rights reserved.</abstract><date>2016</date><author>Li, Shihu
   Wang, Mingquan
   Hou, Huiling
   Yang, Juan
   Wang, Xin</author></paper><paper><title>Robust 5DOF Transesophageal Echo Probe Tracking at Fluoroscopic Frame
   Rates</title><abstract>Registration between transesophageal echocardiography (TEE) and x-ray
   fluoroscopy (XRF) has recently been introduced as a potentially useful
   tool for advanced image guidance of structural heart interventions.
   Algorithms for registration at fluoroscopic imaging frame rates (15-30
   fps) have yet to be reported, despite the fact that probe movement
   resulting from cardiorespiratory motion and physician manipulation can
   introduce non-trivial registration errors during untracked image frames.
   In this work, we present a novel algorithm for GPU-accelerated 2D/3D
   registration and apply it to the problem of TEE probe tracking in XRF
   sequences. Implementation in CUDA C resulted in an extremely fast
   similarity computation of &lt; 80 mu s, which in turn enabled registration
   frame rates ranging from 23.6-92.3 fps. The method was validated on
   simulated and clinical datasets and achieved target registration errors
   comparable to previously reported methods but at much faster
   registration speeds. Our results show, for the first time, the ability
   to accurately register TEE and XRF coordinate systems at fluoroscopic
   frame rates without the need for external hardware. The algorithm is
   generic and can potentially be applied to other 2D/3D registration
   problems where real-time performance is required.</abstract><date>2015</date><author>Hatt, Charles R.
   Speidel, Michael A.
   Raval, Amish N.</author></paper><paper><title>A multimodal parallel architecture: A cognitive framework for multimodal
   interactions</title><abstract>Human communication is naturally multimodal, and substantial focus has
   examined the semantic correspondences in speech-gesture and text-image
   relationships. However, visual narratives, like those in comics, provide
   an interesting challenge to multimodal communication because the words
   and/or images can guide the overall meaning, and both modalities can
   appear in complicated "grammatical" sequences: sentences use a syntactic
   structure and sequential images use a narrative structure. These dual
   structures create complexity beyond those typically addressed by
   theories of multimodality where only a single form uses combinatorial
   structure, and also poses challenges for models of the linguistic system
   that focus on single modalities. This paper outlines a broad theoretical
   framework for multimodal interactions by expanding on Jackendoffs (2002)
   parallel architecture for language. Multimodal interactions are
   characterized in terms of their component cognitive structures: whether
   a particular modality (verbal, bodily, visual) is present, whether it
   uses a grammatical structure (syntax, narrative), and whether it
   "dominates" the semantics of the overall expression. Altogether, this
   approach integrates multimodal interactions into an existing framework
   of language and cognition, and characterizes interactions between
   varying complexity in the verbal, bodily, and graphic domains. The
   resulting theoretical model presents an expanded consideration of the
   boundaries of the "linguistic" system and its involvement in multimodal
   interactions, with a framework that can benefit research on corpus
   analyses, experimentation, and the educational benefits of
   multimodality. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>JAN 2016</date><author>Cohn, Neil</author></paper><paper><title>CyberSoLIM: A cyber platform for digital soil mapping</title><abstract>In recent years, new demands and trends have emerged in the digital soil
   mapping (DSM) field: the range of applications as well as the range of
   users has become much diverse. Users of DSM include not only experts in
   soil science community but also those from relevant domains (e.g.,
   hydrology, ecology). In addition, the rapid expansion of areas for DSM
   and the ever increasing spatial resolution of covariates call for an
   accelerated level of computation. These new trends have raised the bar
   for DSM software platforms. This paper presents CyberSoLIM, a prototype
   system to illustrate an idea of easy-of-use and high performance enabled
   cyber environment for DSM. CyberSoLIM is implemented to have five main
   features: (1) heuristic modeling, which allows digital soil mappers to
   construct DSM workflow easily; (2) visualized modeling with which the
   conceptual workflow of DSM is expressed by graphic icons; (3) workflow
   reuse, which increases the efficiency of DSM deployment; (4) online
   execution and high-performance computing, which can use the advantage of
   cyber infrastructure and high-performance computing; and (5) web service
   enabled, which provides effective and easy means to share and integrate
   models and algorithms. As an illustration of such environment,
   CyberSoLIM was used to infer the silt content in topsoil (0-20 cm) in
   China's Anhui Province. The case study confirms that software platform
   as illustrated by CyberSoLIM is easy to use and efficient in terms of
   mapping productivity. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>FEB 1 2016</date><author>Jiang, Jingchao
   Zhu, A-Xing
   Qin, Cheng-Zhi
   Zhu, Tongxin
   Liu, Junzhi
   Du, Fei
   Liu, Jing
   Zhang, Guiming
   An, Yiming</author></paper><paper><title>Very High Frame Rate Volumetric Integration of Depth Images on Mobile
   Devices</title><abstract>Volumetric methods provide efficient, flexible and simple ways of
   integrating multiple depth images into a full 3D model. They provide
   dense and photorealistic 3D reconstructions, and parallelised
   implementations on GPUs achieve real-time performance on modern graphics
   hardware. To run such methods on mobile devices, providing users with
   freedom of movement and instantaneous reconstruction feedback, remains
   challenging however. In this paper we present a range of modifications
   to existing volumetric integration methods based on voxel block hashing,
   considerably improving their performance and making them applicable to
   tablet computer applications. We present (i) optimisations for the basic
   data structure, and its allocation and integration; (ii) a highly
   optimised raycasting pipeline; and (iii) extensions to the camera
   tracker to incorporate IMU data. In total, our system thus achieves
   frame rates up 47 Hz on a Nvidia Shield Tablet and 910 Hz on a Nvidia
   GTX Titan X GPU, or even beyond 1.1 kHz without visualisation.</abstract><date>NOV 2015</date><author>Kaehler, Olaf
   Prisacariu, Victor Adrian
   Ren, Carl Yuheng
   Sun, Xin
   Torr, Philip
   Murray, David</author></paper><paper><title>A model evaluation study for treatment planning of laser-induced thermal
   therapy</title><abstract>A cross-validation analysis evaluating computer model prediction
   accuracy for a priori planning magnetic resonance-guided laser-induced
   thermal therapy (MRgLITT) procedures in treating focal diseased brain
   tissue is presented. Two mathematical models are considered. (1) A
   spectral element discretisation of the transient Pennes bioheat transfer
   equation is implemented to predict the laser-induced heating in perfused
   tissue. (2) A closed-form algorithm for predicting the steady-state heat
   transfer from a linear superposition of analytic point source heating
   functions is also considered. Prediction accuracy is retrospectively
   evaluated via leave-one-out cross-validation (LOOCV). Modelling
   predictions are quantitatively evaluated in terms of a Dice similarity
   coefficient (DSC) between the simulated thermal dose and thermal dose
   information contained within N=22 MR thermometry datasets. During LOOCV
   analysis, the transient model's DSC mean and median are 0.7323 and
   0.8001 respectively, with 15 of 22 DSC values exceeding the success
   criterion of DSC0.7. The steady-state model's DSC mean and median are
   0.6431 and 0.6770 respectively, with 10 of 22 passing. A one-sample,
   one-sided Wilcoxon signed-rank test indicates that the transient finite
   element method model achieves the prediction success criteria, DSC0.7,
   at a statistically significant level.</abstract><date>OCT 3 2015</date><author>Fahrenholtz, Samuel J.
   Moon, Tim Y.
   Franco, Michael
   Medina, David
   Danish, Shabbar
   Gowda, Ashok
   Shetty, Anil
   Maier, Florian
   Hazle, John D.
   Stafford, Roger J.
   Warburton, Tim
   Fuentes, David</author></paper><paper><title>Single particle tomography in EMAN2</title><abstract>Single particle tomography (SPT or subtomogram averaging) offers a
   powerful alternative to traditional 2-D single particle reconstruction
   for studying conformationally or compositionally heterogeneous
   macromolecules. It can also provide direct observation (without labeling
   or staining) of complexes inside cells at nanometer resolution. The
   development of computational methods and tools for SPT remains an area
   of active research. Here we present the EMAN2.1 SPT toolbox, which
   offers a full SPT processing pipeline, from particle picking to
   post-alignment analysis of subtomogram averages, automating most steps.
   Different algorithm combinations can be applied at each step, providing
   versatility and allowing for procedural cross-testing and
   specimen-specific strategies. Alignment methods include all-vs-all,
   binary tree, iterative single-model refinement, multiple-model
   refinement, and self-symmetry alignment. An efficient angular search,
   Graphic Processing Unit (GPU) acceleration and both threaded and
   distributed parallelism are provided to speed up processing. Finally,
   automated simulations, per particle reconstruction of subtiltseries, and
   per-particle Contrast Transfer Function (CTF) correction have been
   implemented. Processing examples using both real and simulated data are
   shown for several structures. (C) 2015 Elsevier Inc. All rights
   reserved.</abstract><date>JUN 2015</date><author>Galaz-Montoya, Jesus G.
   Flanagan, John
   Schmid, Michael F.
   Ludtke, Steven J.</author></paper><paper><title>Accelerating Multiple Compound Comparison Using LINGO-Based
   Load-Balancing Strategies on Multi-GPUs.</title><abstract>Compound comparison is an important task for the computational
   chemistry. By the comparison results, potential inhibitors can be found
   and then used for the pharmacy experiments. The time complexity of a
   pairwise compound comparison is O(n (2)), where n is the maximal length
   of compounds. In general, the length of compounds is tens to hundreds,
   and the computation time is small. However, more and more compounds have
   been synthesized and extracted now, even more than tens of millions.
   Therefore, it still will be time-consuming when comparing with a large
   amount of compounds (seen as a multiple compound comparison problem,
   abbreviated to MCC). The intrinsic time complexity of MCC problem is O(k
   (2) n (2)) with k compounds of maximal length n. In this paper, we
   propose a GPU-based algorithm for MCC problem, called CUDA-MCC, on
   single- and multi-GPUs. Four LINGO-based load-balancing strategies are
   considered in CUDA-MCC in order to accelerate the computation speed
   among thread blocks on GPUs. CUDA-MCC was implemented by C+OpenMP+CUDA.
   CUDA-MCC achieved 45 times and 391 times faster than its CPU version on
   a single NVIDIA Tesla K20m GPU card and a dual-NVIDIA Tesla K20m GPU
   card, respectively, under the experimental results. </abstract><date>2015</date><author>Lin, Chun-Yuan
   Wang, Chung-Hung
   Hung, Che-Lun
   Lin, Yu-Shiang</author></paper><paper><title>Fast computation of seabed spherical-wave reflection coefficients in
   geoacoustic inversion</title><abstract>This paper develops a fast numerical approach to computing
   spherical-wave reflection coefficients (SWRCs) for layered seabeds,
   which provides substantial savings in computation time when used as the
   forward model for geoacoustic inversion of broadband seabed reflectivity
   data. The approach exploits the Sommerfeld-integral representation of
   SWRCs as the Hankel transform of a function proportional to the
   plane-wave reflection coefficient (PWRC), and applies Levin integration
   to the rapidly oscillating integrand cast as the product of a
   (pre-computed) media-independent matrix and a vector involving PWRCs at
   a sparse sampling of integration angles. Compared to conventional
   Simpson's rule integration for computation of the SWRC, the Levin
   integration yields speed-up factors of an order of magnitude or more.
   Further, it results in reduced memory requirements for storage of
   pre-computed quantities, a desirable property when a graphics processing
   unit (GPU) is used for parallel computation of SWRCs. The paper applies
   trans-dimensional Bayesian inversion to investigate the impact of
   forward modeling in terms of PWRCs and SWRCs on the estimation of
   geoacoustic parameters and uncertainties. Model comparisons are
   quantified in simulated-and measured-data inversions by comparing the
   estimated geoacoustic parameters to the true parameters or core
   measurements, respectively, and by calculating the deviance information
   criterion for model selection. (C) 2015 Acoustical Society of America.</abstract><date>OCT 2015</date><author>Quijano, Jorge E.
   Dosso, Stan E.
   Dettmer, Jan
   Holland, Charles W.</author></paper><paper><title>CUDA programs for solving the time-dependent dipolar Gross-Pitaevskii
   equation in an anisotropic trap</title><abstract>In this paper we present new versions of previously published numerical
   programs for solving the dipolar Gross-Pitaevskii (GP) equation
   including the contact interaction in two and three spatial dimensions in
   imaginary and in real time, yielding both stationary and non-stationary
   solutions. New versions of programs were developed using CUDA toolkit
   and can make use of Nvidia GPU devices. The algorithm used is the same
   split-step semi-implicit Crank-Nicolson method as in the previous
   version (Kishor Kumar et al., 2015), which is here implemented as a
   series of CUDA kernels that compute the solution on the GPU. In
   addition, the Fast Fourier Transform (FFT) library used in the previous
   version is replaced by cuFFT library, which works on CUDA-enabled GPUs.
   We present speedup test results obtained using new versions of programs
   and demonstrate an average speedup of 12-25, depending on the program
   and input size.</abstract><date>MAR 2016</date><author>Loncar, Vladimir
   Balaz, Antun
   Boojevic, Aleksandar
   Skrbic, Srdjan
   Muruganandam, Paulsamy
   Adhikari, Sadhan K.</author></paper><paper><title>Numerical Simulation of Long Wave Runup for Breaking and Nonbreaking
   Waves</title><abstract>Tsunamis produce a wealth of quantitative data that can be used to
   improve tsunami hazard awareness and to increase the preparedness of the
   population at risk. These data also allow for a performance evaluation
   of the coastal infrastructure and observations of sediment transport,
   erosion, and deposition. The interaction of the tsunami with coastal
   infrastructures and with the movable sediment bed is a three-dimensional
   process. Therefore, for runup and inundation prediction,
   three-dimensional numerical models must be employed. In this study, we
   have employed Smoothed Particle Hydrodynamics (SPH) to simulate tsunami
   runup on idealized geometries for the validation and exploration of
   three-dimensional flow structures in tsunamis. We make use of the
   canonical experiments for long-wave runup for breaking and nonbreaking
   waves. The results of our study prove that SPH is able to reproduce the
   runup of long waves for different initial and geometric conditions. We
   have also investigated the applicability and the effectiveness of
   different viscous terms that are available in the SPH literature.
   Additionally, a new breaking criterion based on numerical experiments is
   introduced, and its similarities and differences with existing criteria
   are discussed.</abstract><date>MAR 2015</date><author>Shadloo, Mostafa S.
   Weiss, Robert
   Yildiz, Mehmet
   Dalrymple, Robert A.</author></paper><paper><title>A literature review of bounding volumes hierarchy focused on collision
   detection</title><abstract>A bounding volume is a common method to simplify object representation
   by using the composition of geometrical shapes that enclose the object;
   it encapsulates complex objects by means of simple volumes and it is
   widely useful in collision detection applications and ray tracing for
   rendering algorithms. They are popular in computer graphics and
   computational geometry. Most popular bounding volumes are spheres,
   Oriented-Bounding Boxes (OBB's), Axis-Aligned Bounding Boxes (AABB's);
   moreover, the literature review includes ellipsoids, cylinders, sphere
   packing, sphere shells, k-DOP's, convex hulls, cloud of points, and
   minimal bounding boxes, among others. A Bounding Volume Hierarchy is
   usually a tree in which the complete object is represented tighter
   fitting every level of the hierarchy. Additionally, each bounding volume
   has a cost associated to construction, update, and interference tests.
   For instance, spheres are invariant to rotation and translations, then
   they do not require being updated; their constructions and interference
   tests are more straightforward then OBB's; however, their tightness is
   lower than other bounding volumes. Finally, three comparisons between
   two polyhedra; seven different algorithms were used, of which five are
   public libraries for collision detection.</abstract><date>2015</date><author>Dinas, Simena
   Banon, Jose M.</author></paper><paper><title>A fast GPU-based Monte Carlo simulation of proton transport with
   detailed modeling of nonelastic interactions.</title><abstract>PURPOSE: Very fast Monte Carlo (MC) simulations of proton transport have
   been implemented recently on graphics processing units (GPUs). However,
   these MCs usually use simplified models for nonelastic proton-nucleus
   interactions. Our primary goal is to build a GPU-based proton transport
   MC with detailed modeling of elastic and nonelastic proton-nucleus
   collisions.METHODS: Using the cuda framework, the authors implemented
   GPU kernels for the following tasks: (1) simulation of beam spots from
   our possible scanning nozzle configurations, (2) proton propagation
   through CT geometry, taking into account nuclear elastic scattering,
   multiple scattering, and energy loss straggling, (3) modeling of the
   intranuclear cascade stage of nonelastic interactions when they occur,
   (4) simulation of nuclear evaporation, and (5) statistical error
   estimates on the dose. To validate our MC, the authors performed (1)
   secondary particle yield calculations in proton collisions with
   therapeutically relevant nuclei, (2) dose calculations in homogeneous
   phantoms, (3) recalculations of complex head and neck treatment plans
   from a commercially available treatment planning system, and compared
   with (GEANT)4.9.6p2/TOPAS.RESULTS: Yields, energy, and angular
   distributions of secondaries from nonelastic collisions on various
   nuclei are in good agreement with the (GEANT)4.9.6p2 Bertini and Binary
   cascade models. The 3D-gamma pass rate at 2%-2 mm for treatment plan
   simulations is typically 98%. The net computational time on a NVIDIA
   GTX680 card, including all CPU-GPU data transfers, is  20 s for 1 *
   10(7) proton histories.CONCLUSIONS: Our GPU-based MC is the first of its
   kind to include a detailed nuclear model to handle nonelastic
   interactions of protons with any nucleus. Dosimetric calculations are in
   very good agreement with (GEANT)4.9.6p2/TOPAS. Our MC is being
   integrated into a framework to perform fast routine clinical QA of
   pencil-beam based treatment plans, and is being used as the dose
   calculation engine in a clinically applicable MC-based IMPT treatment
   planning system. The detailed nuclear modeling will allow us to perform
   very fast linear energy transfer and neutron dose estimates on the GPU.</abstract><date>2015-Jun</date><author>Wan Chan Tseung, H
   Ma, J
   Beltran, C</author></paper><paper><title>CUDA based parallel design of a shot change detection algorithm using
   frame segmentation and object movement</title><abstract>This paper proposes the parallel design of a shot change detection
   algorithm using frame segmentation and moving blocks. In the proposed
   approach, the high parallel processing components, such as frame
   histogram calculation, block histogram calculation, Otsu threshold
   setting function, frame moving operation, and block histogram
   comparison, are designed in parallel for NVIDIA GPU. In order to
   minimize memory access delay time and guarantee fast computation, the
   output of a GPU kernel becomes the input data of another kernel in a
   pipeline way using the shared memory of GPU. In addition, the optimal
   sizes of CUDA processing blocks and threads are estimated through the
   prior experiments. In the experimental test of the proposed shot change
   detection algorithm, the detection rate of the GPU based parallel
   algorithm is the same as that of the CPU based algorithm, but the
   average of processing time speeds up about 6~8 times.</abstract><date>2015</date><author>???
   ???</author></paper><paper><title>Higher order numerical simulation of unsteady viscous incompressible
   flows using kinetically reduced local Navier-Stokes equations on a GPU</title><abstract>Higher order approach of Kinetically Reduced Local Navier Stokes (KRLNS)
   equations are applied for two-dimensional (2-D) simulations of Womersley
   problem and doubly periodic shear layers in order to demonstrate the
   accuracy, efficiency and the capability to capture the correct transient
   behavior for unsteady incompressible viscous flows. The numerical
   results obtained by the KRLNS equations using higher order difference
   approximations are in excellent agreement with those obtained by the
   Lattice Boltzmann method (LBM) and the pseudo-spectral method (PSM),
   which is a standard approach to incompressible viscous flows. It is
   confirmed that the KRLNS method can capture the correct transient
   behavior without use of sub-iterations due to a smoothing effect
   introduced by using the Grand potential in the continuity equation, and
   keep the fluctuation of velocity divergence at small level by taking
   sufficiently low Mach number. Parallel computations are carried out on a
   GPU based on NVIDIA Tesla C1060 system and the provided CUDA library.
   High values of speedup are obtained for three methods, the KRLNS
   equations, PSM and LBM. (C) 2014 Elsevier Ltd. All rights reserved.</abstract><date>MAR 30 2015</date><author>Hashimoto, T.
   Tanno, I.
   Yasuda, T.
   Tanaka, Y.
   Morinishi, K.
   Satofuka, N.</author></paper><paper><title>Big Data Approaches for the Analysis of Large-Scale fMRI Data Using
   Apache Spark and GPU Processing: A Demonstration on Resting-State fMRI
   Data from the Human Connectome Project</title><abstract>Technologies for scalable analysis of very large datasets have emerged
   in the domain of internet computing, but are still rarely used in
   neuroimaging despite the existence of data and research questions in
   need of efficient computation tools especially in fMRI. In this work, we
   present software tools for the application of Apache Spark and Graphics
   Processing Units (GPUs) to neuroimaging datasets, in particular
   providing distributed file input for 4D NIfTI fMRI datasets in Scala for
   use in an Apache Spark environment. Examples for using this Big Data
   platform in graph analysis of fMRI datasets are shown to illustrate how
   processing pipelines employing it can be developed. With more tools for
   the convenient integration of neuroimaging file formats and typical
   processing steps, big data technologies could find wider endorsement in
   the community, leading to a range of potentially useful applications
   especially in view of the current collaborative creation of a wealth of
   large data repositories including thousands of individual fMRI datasets.</abstract><date>JAN 6 2016</date><author>Boubela, Roland N.
   Kalcher, Klaudius
   Huf, Wolfgang
   Nasel, Christian
   Moser, Ewald</author></paper><paper><title>MOC-Based Parallel Preprocessing of ZY-3 Satellite Images</title><abstract>The launch of the ZY-3 surveying and mapping satellite (ZSMS) by China
   has resulted in a significant increase in the volume of image data
   collected for subsequent processing. In this letter, we present our
   research on the message passing interface (MPI), open multiprocessing
   (OpenMP), and compute unified device architecture (CUDA)-based
   (MOC-based) preprocessing of ZSMS images in a system that consists of
   multiple central processing units (CPUs) and graphics processing units
   (GPUs). First, CPUs and GPUs in the system are organized into atomic
   computing resources (ACRs) by means of an MPI. Then, three cooperative
   methods are proposed for potential performance improvement of the
   processors with OpenMP and CUDA. The input/output (I/O) overhead is also
   addressed in this letter. The experimental results show that the total
   execution time of the 12 ZSMS nadir images with four ACRs is reduced to
   86.10 s, which could provide near-real-time response for the
   time-critical applications that follow.</abstract><date>FEB 2015</date><author>Fang, Liuyang
   Wang, Mi
   Li, Deren
   Pan, Jun</author></paper><paper><title>Algorithmic choices in WARP - A framework for continuous energy Monte
   Carlo neutron transport in general 3D geometries on GPUs</title><abstract>In recent supercomputers, general purpose graphics processing units
   (GPGPUs) are a significant faction of the supercomputer's total
   computational power. GPGPUs have different architectures compared to
   central processing units (CPUs), and for Monte Carlo neutron transport
   codes used in nuclear engineering to take advantage of these coprocessor
   cards, transport algorithms must be changed to execute efficiently on
   them. WARP is a continuous energy Monte Carlo neutron transport code
   that has been written to do this. The main thrust of WARP is to adapt
   previous event-based transport algorithms to the new CPU hardware; the
   algorithmic choices for all parts of which are presented in this paper.
   It is found that remapping history data references increases the CPU
   processing rate when histories start to complete. The main reason for
   this is that completed data are eliminated from the address space,
   threads are kept busy, and memory bandwidth is not wasted on checking
   completed data. Remapping also allows the interaction kernels to be
   launched concurrently, improving efficiency. The OptiX ray tracing
   framework and CUDPP library are used for geometry representation and
   parallel dataset-side operations, ensuring high performance and
   reliability. (C) 2014 Elsevier Ltd. All rights reserved.</abstract><date>MAR 2015</date><author>Bergmann, Ryan M.
   Vujic, Jasmina L.</author></paper><paper><title>GPU-Based Ray Tracing Algorithm for High-Speed Propagation Prediction in
   Multi-Room Indoor Environments</title><abstract>A novel ray tracing algorithm for high-speed propagation prediction in
   multi-room indoor environments is proposed in this paper, whose
   theoretical foundations are geometrical optics(GO) and the uniform
   theory of diffraction(UTD). Taking the geometrical and electromagnetic
   information of the complex indoor scene into account, some acceleration
   techniques are adopted to raise the efficiency of the ray tracing
   algorithm. The simulation results indicate that the runtime of the ray
   tracing algorithm will sharply increase when the number of the objects
   in multi-room buildings is large enough. Therefore, GPU acceleration
   technology is used to solve that problem. Finally, a typical multi-room
   indoor environment with several objects in each room is simulated by
   using the serial ray tracing algorithm and the parallel one
   respectively. It can be found easily from the results that compared with
   the serial algorithm, the GPU-based one can achieve greater efficiency.</abstract><date>2015</date><author>Guan, Xiaowei
   Guo, Lixin
   Liu, Zhongyu</author></paper><paper><title>Reconstruction using 'triangular approximation' of bone grafts for
   orbital blowout fractures</title><abstract>There are many orbital wall reconstruction materials that can be used in
   surgery for orbital blowout fractures. We consider autogenous bone
   grafts to have the best overall characteristics among these materials
   and use thinned, inner cortical tables of the ilium. A bone bender is
   normally used to shape the inner iliac table to match the orbital shape.
   Since orbital walls curve three-dimensionally, processing of bone grafts
   is not easy and often requires much time and effort.We applied a
   triangular approximation method to the processing of bone grafts.
   Triangular approximation is a concept used in computer graphics for
   polygon processing. In this method, the shape of an object is
   represented as combinations of polygons, mainly triangles. In this
   study, the inner iliac table was used as a bone graft, and cuts or
   scores were made to create triangular sections. These triangular
   sections were designed three-dimensionally so that the shape of the
   resulting graft approximated to the three-dimensional orbital shape.
   This method was used in 12 patients with orbital blowout fractures,
   which included orbital floor fractures, medial wall fractures, and
   combined inferior and medial wall fractures. In all patients, bone
   grafts conformed to the orbital shape and good results were
   obtained.This simple method uses a reasonable and easy-to-understand
   approach and is useful in the treatment of bone defects in orbital
   blowout fractures when using a hard graft material. (C) 2015 European
   Association for Cranio-Maxillo-Facial Surgery. Published by Elsevier
   Ltd. All rights reserved.</abstract><date>OCT 2015</date><author>Saiga, Atsuomi
   Mitsukawa, Nobuyuki
   Yamaji, Yoshihisa</author></paper><paper><title>Design and Implementation of a Graphic 3D Simulator for the Study of
   Control Techniques Applied to Cooperative Robots</title><abstract>A new methodology is proposed for the design and implementation of
   three-dimensional simulations of manipulator robots, in individual as
   well as cooperative tasks, using several programming and 3D-design
   tools. To obtain easily the dynamic-mathematical models of movement for
   the represented systems, regardless of the kind of manipulator robot to
   be considered, the development of a computer algorithm is also
   presented. Besides, through the use of the Bullet library, we
   incorporated the detection of collisions between constitutive elements
   of the system, considering elements external to the robots, collisions
   between links of the same robot, or between two robots. Using a C++
   programming platform, we also designed and implemented a flexible and
   intuitive graphic user interface. The dynamic-mathematical performance
   of the cooperative interaction between two actual robots, designed and
   implemented in the Departamento de Ingenieria Electrica of the
   Universidad de Santiago de Chile (DIE-UdeSantiago de Chile), was
   modeled, plotted and three-dimensionally simulated, by using the new
   algorithm proposed for this purpose. Finally, the performance results
   are presented and analyzed.</abstract><date>DEC 2015</date><author>Urrea, Claudio
   Paul Coltters, Jean</author></paper><paper><title>PSkel: A stencil programming framework for CPU-GPU systems</title><abstract>The use of Graphics Processing Units (GPUs) for high-performance
   computing has gained growing momentum in recent years. Unfortunately,
   GPU-programming platforms like Compute Unified Device Architecture
   (CUDA) are complex, user unfriendly, and increase the complexity of
   developing high-performance parallel applications. In addition, runtime
   systems that execute those applications often fail to fully utilize the
   parallelism of modern CPU-GPU systems. Typically, parallel kernels run
   entirely on the most powerful device available, leaving other devices
   idle. These observations sparked research in two directions: (1)
   high-level approaches to software development for GPUs, which strike a
   balance between performance and ease of programming; and (2) task
   partitioning to fully utilize the available devices. In this paper, we
   propose a framework, called PSkel, that provides a single high-level
   abstraction for stencil programming on heterogeneous CPU-GPU systems,
   while allowing the programmer to partition and assign data and
   computation to both CPU and GPU. Our current implementation uses
   parallel skeletons to transparently leverage Intel Threading Building
   Blocks (Intel Corporation, Santa Clara, CA, USA) and NVIDIA CUDA (Nvidia
   Corporation, Santa Clara, CA, USA). In our experiments, we observed that
   parallel applications with task partitioning can improve average
   performance by up to 76% and 28% compared with CPU-only and GPU-only
   parallel applications, respectively. Copyright (c) 2015John Wiley &amp;
   Sons, Ltd.</abstract><date>DEC 10 2015</date><author>Pereira, Alyson D.
   Ramos, Luiz
   Goes, Luis F. W.</author></paper><paper><title>High-speed parallel implementations of the rainbow method based on
   perfect tables in a heterogeneous system</title><abstract>The computing power of graphics processing units (GPU) has increased
   rapidly, and there has been extensive research on general-purpose
   computing on GPU (GPGPU) for cryptographic algorithms such as RSA,
   Elliptic Curve Cryptosystem (ECC), NTRU, and Advanced Encryption
   Standard. With the rise of GPGPU, commodity computers have become
   complex heterogeneous GPU+CPU systems. This new architecture poses new
   challenges and opportunities in high-performance computing. In this
   paper, we present high-speed parallel implementations of the rainbow
   method based on perfect tables, which is known as the most efficient
   time-memory trade-off, in the heterogeneous GPU+CPU system. We give a
   complete analysis of the effect of multiple checkpoints on reducing the
   cost of false alarms and take advantage of it for load balancing between
   GPU and CPU. For GTX460, our implementation is about 1.86 and 3.25 times
   faster than other GPU-accelerated implementations, RainbowCrack and
   Cryptohaze, respectively, and for GTX580, 1.53 and 2.40 times faster.
   Copyright (c) 2014 John Wiley &amp; Sons, Ltd.</abstract><date>JUN 2015</date><author>Kim, Jung Woo
   Seo, Jungjoo
   Hong, Jin
   Park, Kunsoo
   Kim, Sung-Ryul</author></paper><paper><title>Towards an ultra efficient kinetic scheme. Part III:
   High-performance-computing</title><abstract>In this paper we demonstrate the capability of the fast semi-Lagrangian
   scheme developed in [20] and [21] to deal with parallel architectures.
   First, we will present the behaviors of such scheme on a classical
   architecture using OpenMP and then on GPU (Graphics Processing Unit)
   architecture using CUDA. The goal is to prove that this new scheme is
   well adapted to these types of parallelizations, and, moreover that the
   gain in CPU time is substantial on nowadays affordable computers. We
   first present the sequential version of our high-order kinetic scheme
   and focus on important details for an effective parallel implementation.
   Then, we introduce the specific treatments and algorithms which have
   been developed for an OpenMP and CUDA parallelizations. Numerical tests
   are shown for the full 3D/3D simulations. These assess the important
   speed-up factor of the method gained between the sequential code and the
   parallel versions and its very good scalability which makes this
   approach a real competitor with respect to existing schemes for the
   solution of multidimensional kinetic models. (C) 2014 Elsevier Inc. All
   rights reserved.</abstract><date>MAR 1 2015</date><author>Dimarco, Giacomo
   Loubere, Raphal
   Narski, Jacek</author></paper><paper><title>Quantitative visualization of alternative exon expression from RNA-seq
   data</title><abstract>Motivation: Analysis of RNA sequencing (RNA-Seq) data revealed that the
   vast majority of human genes express multiple mRNA isoforms, produced by
   alternative pre-mRNA splicing and other mechanisms, and that most
   alternative isoforms vary in expression between human tissues. As
   RNA-Seq datasets grow in size, it remains challenging to visualize
   isoform expression across multiple samples.Results: To help address this
   problem, we present Sashimi plots, a quantitative visualization of
   aligned RNA-Seq reads that enables quantitative comparison of exon usage
   across samples or experimental conditions. Sashimi plots can be made
   using the Broad Integrated Genome Viewer or with a stand-alone command
   line program.</abstract><date>JUL 15 2015</date><author>Katz, Yarden
   Wang, Eric T.
   Silterra, Jacob
   Schwartz, Schraga
   Wong, Bang
   Thorvaldsdottir, Helga
   Robinson, James T.
   Mesirov, Jill P.
   Airoldi, Edoardo M.
   Burge, Christopher B.</author></paper><paper><title>MultiElec: A MATLAB Based Application for MEA Data Analysis</title><abstract>We present MultiElec, an open source MATLAB based application for data
   analysis of microelectrode array (MEA) recordings. MultiElec displays an
   extremely user-friendly graphic user interface (GUI) that allows the
   simultaneous display and analysis of voltage traces for 60 electrodes
   and includes functions for activation-time determination, the production
   of activation-time heat maps with activation time and isoline display.
   Furthermore, local conduction velocities are semi-automatically
   calculated along with their corresponding vector plots. MultiElec allows
   ad hoc signal suppression, enabling the user to easily and efficiently
   handle signal artefacts and for incomplete data sets to be analysed.
   Voltage traces and heat maps can be simply exported for figure
   production and presentation. In addition, our platform is able to
   produce 3D videos of signal progression over all 60 electrodes.
   Functions are controlled entirely by a single GUI with no need for
   command line input or any understanding of MATLAB code. MultiElec is
   open source under the terms of the GNU General Public License as
   published by the Free Software Foundation, version 3. Both the program
   and source code are available to download from
   http://www.cancer.manchester.ac.uk/MultiElec/.</abstract><date>JUN 15 2015</date><author>Georgiadis, Vassilis
   Stephanou, Anastasis
   Townsend, Paul A.
   Jackson, Thomas R.</author></paper><paper><title>A computational comparison of scaling techniques for linear optimization
   problems on a graphical processing unit</title><abstract>Preconditioning techniques are important in solving linear problems, as
   they improve their computational properties. Scaling is the most widely
   used preconditioning technique in linear optimization algorithms and is
   used to reduce the condition number of the constraint matrix, to improve
   the numerical behavior of the algorithms and to reduce the number of
   iterations required to solve linear problems. Graphical processing units
   (GPUs) have gained a lot of popularity in the recent years and have been
   applied for the solution of linear optimization problems. In this paper,
   we review and implement ten scaling techniques with a focus on the
   parallel implementation of them on GPUs. All these techniques have been
   implemented under the MATLAB and CUDA environment. Finally, a
   computational study on the Netlib set is presented to establish the
   practical value of GPU-based implementations. On average the speedup
   gained from the GPU implementations of all scaling methods is about 7x.</abstract><date>FEB 1 2015</date><author>Ploskas, Nikolaos
   Samaras, Nikolaos</author></paper><paper><title>Towards the extrapolation of the valence-valence electron partial
   structure factor for liquid Mg near freezing from a combination of
   theory and experiment</title><abstract>Egelstaff, March, and McGill (1973) proposed the extraction of electron
   correlation functions in liquids from scattering data. Here, we appeal
   to computer simulation by de Wijs et al. (1995) on the partial structure
   factor[GRAPHICS]between ions (i) and valence electrons (v) for liquid Mg
   near freezing, to write the valence-valence partial structure
   factor[GRAPHICS]in terms of[GRAPHICS]and the neutron structure
   factor[GRAPHICS], to high accuracy.</abstract><date>SEP 3 2015</date><author>March, N. H.
   Angilella, G. G. N.</author></paper><paper><title>Dynamics Modeling and Control Simulation of an Autonomous Underwater
   Vehicle</title><abstract>A dynamics model of an open-shelf Autonomous underwater vehicle (AUV) is
   described in this paper. The virtual prototype technology and the
   control simulation software are used to build the virtual prototype
   model of AUV, and AUV dynamic location control arithmetic is simulated
   based on analyzing motion and hydrodynamic mathematical model of the
   virtual prototype. The simulation results indicate that the virtual
   prototype system has the function of simulation demo and performance
   validation, and can provide one kind of new method for AUV graphic
   simulation, and has very important practical meaning on AUV design and
   control research.</abstract><date>WIN 2015</date><author>Liu, Guijie
   Chen, Gong
   Jiao, Jianbo
   Jiang, Ruilin</author></paper><paper><title>3D hybrid-domain full waveform inversion on GPU</title><abstract>The traditional frequency-domain full waveform inversion (FWI) method
   has limited application for 3D case due to its significant computational
   challenges and huge memory cost. The time-frequency hybrid-domain FWI is
   thus utilized to combine the computational efficiency of time-domain FWI
   and multiscale inversion of frequency-domain FWI. We present a
   simplified hybrid-domain FWI method. Compared with the previous
   hybrid-domain FWI, our method can take one less Discrete Fourier
   Transform (DFT) and one less inverse Discrete Fourier Transform (IDFT)
   for single iteration and achieved the equivalent inversion effect.
   Meanwhile, the hybrid-domain FWI is very suitable for fine-grained
   parallel computation, and the application of graphic processing unit
   (GPU) has significantly enhanced the computation efficiency. Modeling,
   boundary absorption and DFT are the most time-consuming modules. The
   occupancy of the three kernels achieved a good level as Compute Unified
   Device Architecture (CUDA) Visual Profile shows and the bandwidth usage
   has been much improved by introducing the tiling method for 3D finite
   difference problem. Finally, our hybrid-domain FWI is applied to a 3D
   model on our personal computer equipped with GTX 680 graphic card, and
   complexity analysis of our algorithm is presented. The results further
   confirm the feasibility of this technique. (C) 2015 Elsevier Ltd. All
   rights reserved.</abstract><date>OCT 2015</date><author>Liu, Lu
   Ding, Renwei
   Liu, Hongwei
   Liu, Hong</author></paper><paper><title>Parallel multi-objective multi-robot coalition formation</title><abstract>In the quest for greater autonomy, there is an increasing need for
   solutions that would enable a large set of robots to coalesce and
   perform complicated multi-robot tasks. This problem, also known as the
   multi-robot coalition formation problem has been traditionally
   approached as a single objective optimization problem. However, robots
   in the real world have to optimize multiple conflicting criteria such as
   battery life, number of completed tasks, and distance traveled.
   Researchers have only recently addressed the robot coalition formation
   problem as a multi-objective optimization problem, however the proposed
   solutions have computational bottlenecks that make them unsuitable for
   real time robotic applications. In this paper we address the issue of
   scalability by proposing parallelized algorithms in the CUDA programming
   framework. NSGA-II and PAES algorithm have been parallelized due to
   their suitability to the coalition formation domain as outlined in our
   previous work. The parallelized versions of these algorithms have been
   applied to both the additive and non-additive coalition formation
   environments. Simulations have been performed in the player/stage
   environment to validate the applicability of our approach to real robot
   situations. Results establish that the multi-point PAES parallel variant
   yields significant performance gains in terms of running time and
   solution quality when the problem is scaled to deal with large inputs.
   This suggests that the algorithm may be viable for real time robotic
   applications. Experiments demonstrate significant speedup when the
   proposed parallel algorithms were compared with the serial solutions
   proposed earlier. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>NOV 30 2015</date><author>Agarwal, Manoj
   Agrawal, Nitin
   Sharma, Shikhar
   Vig, Lovekesh
   Kumar, Naveen</author></paper><paper><title>A Total Order Heuristic-Based Convex Hull Algorithm for Points in the
   Plane</title><abstract>Computing the convex hull of a set of points is a fundamental operation
   in many research fields, including geometric computing, computer
   graphics, computer vision, robotics, and so forth. This problem is
   particularly challenging when the number of points goes beyond some
   millions. In this article, we describe a very fast algorithm that copes
   with millions of points in a short period of time without using any kind
   of parallel computing. This has been made possible because the algorithm
   reduces to a sorting problem of the input point set, what dramatically
   minimizes the geometric computations (e.g., angles, distances, and so
   forth) that are typical in other algorithms. When compared with popular
   convex hull algorithms (namely, Graham's scan, Andrew's monotone chain,
   Jarvis' gift wrapping, Chan's, and Quickhull), our algorithm is capable
   of generating the convex hull of a point set in the plane much faster
   than those five algorithms without penalties in memory space. (C) 2015
   Elsevier Ltd. All rights reserved.</abstract><date>JAN 2016</date><author>Gomes, Abel J. P.</author></paper><paper><title>Multimodal interaction with virtual worlds XMMVR: eXtensible language
   for MultiModal interaction with virtual reality worlds</title><abstract>Based on a philosophy of integrating components from multimodal
   interaction applications with 3D graphical environments, reusing already
   defined markup language for describing graphics, graphical and spoken
   interactions based on the interactive movie metaphor, a markup language
   for modeling scenes, behavior and interaction is sought. With the
   definition of this language, we hope to have a common framework for
   developing applications that allow multimodal interaction at 3D stages.
   Thus we have defined the basis of an architecture that allows us to
   integrate the components of such multimodal interaction applications in
   3D virtual environments.</abstract><date>SEP 2015</date><author>Olmedo, Hector
   Escudero, David
   Cardenoso, Valentin</author></paper><paper><title>Unsafe Floating-point to Unsigned Integer Casting Check for GPU Programs</title><abstract>Numerical programs usually include type-casting instructions which
   convert data among different types. Identifying unsafe type-casting is
   important for preventing undefined program behaviors which cause serious
   problems such as security vulnerabilities and result
   non-reproducibility. While many tools had been proposed for handling
   sequential programs, to our best knowledge, there isn't a tool geared
   toward GPUs. In this paper, we propose a static analysis based method
   which points out all potentially unsafe type-casting instructions in a
   program. To reduce false alarms (which are commonly raised by static
   analysis), we employ two techniques, manual hints and pre-defined
   function contracts, and we empirically show that these techniques are
   effective in practice. We evaluated our method with artificial programs
   and samples from CUDA SDK. Our implementation is currently being
   integrated into a GPU program analysis framework called GKLEE. We plan
   to integrate dynamic unsafe type-casting checks also in our future work.</abstract><date>NOV 18 2015</date><author>Chiang, Wei-Fan
   Gopalakrishnan, Ganesh
   Rakamaric, Zvonimir</author></paper><paper><title>Parallel Super-Resolution Reconstruction Based on Neighbor Embedding
   Technique</title><abstract>Super Resolution (SR) is a technique to recover a high-resolution (HR)
   image from different noisy low resolution (LR) images. The missing
   high-frequency components in LR images should be restored correctly in
   HR image. Because of the extensive size of satellite images, the utilize
   to parallel algorithms can accomplish results more quickly with accurate
   results. This paper proposes an accelerated parallel implementation for
   an example based super-resolution algorithm, Neighbor Embedding (NE),
   using GPU. The NE trains the dictionary with patches obtained from a
   single image in the training phase. Euclidean distances are used to
   obtain the optimal weights that will be used in the construction of
   high-resolution images. Compute Device Unified Architecture (CUDA) by
   NVidia's has been used to implement the proposed parallel NE. Different
   experiments have been carried out on a synthetic test image and
   satellite test image. The proposed GPU implementation of the NE was
   benchmarked against the serial implementation. The experimental results
   show that the speed of the implementation depends on the image size. The
   speed of the GPU implementation compared to the serial one using CPU
   ranged from 20x for small images to more than 30x for large image size.</abstract><date>2015</date><author>Moustafa, Marwa
   Ebied, Hala M.
   Helmy, Ashraf
   Nazamy, Taymoor M.
   Tolba, Mohamed F.</author></paper><paper><title>RootGraph: a graphic optimization tool for automated image analysis of
   plant roots</title><abstract>This paper outlines a numerical scheme for accurate, detailed, and
   high-throughput image analysis of plant roots. In contrast to existing
   root image analysis tools that focus on root system-average traits, a
   novel, fully automated and robust approach for the detailed
   characterization of root traits, based on a graph optimization process
   is presented. The scheme, firstly, distinguishes primary roots from
   lateral roots and, secondly, quantifies a broad spectrum of root traits
   for each identified primary and lateral root. Thirdly, it associates
   lateral roots and their properties with the specific primary root from
   which the laterals emerge. The performance of this approach was
   evaluated through comparisons with other automated and semi-automated
   software solutions as well as against results based on manual
   measurements. The comparisons and subsequent application of the
   algorithm to an array of experimental data demonstrate that this method
   outperforms existing methods in terms of accuracy, robustness, and the
   ability to process root images under high-throughput conditions.</abstract><date>NOV 2015</date><author>Cai, Jinhai
   Zeng, Zhanghui
   Connor, Jason N.
   Huang, Chun Yuan
   Melino, Vanessa
   Kumar, Pankaj
   Miklavcic, Stanley J.</author></paper><paper><title>A lightweight and cross-platform Web3D system for casting process based
   on virtual reality technology using WebGL</title><abstract>With the advances in computer technology nowadays, the virtual
   manufacturing technology has provided a new way in casting process
   design. In this study, a virtual reality system of casting production
   process named VR-Casting has been developed. The full range of virtual
   display about the casting process has been revealed under the network
   environment. Several key technologies applied in VR-Casting were
   introduced in this paper, such as a novel network running environment,
   the latest Web3D drawing standard named WebGL, levels of detail
   technology used for rendering on demand, and the detection and updating
   technology in the casting motions. Based on the above technologies,
   different 3D models and the virtual panoramic scene were established.
   Motion schema and user interface of the casting process was delicately
   designed to enable the system a better interactivity. The latest
   internet graphics standard WebGL was used to render the models. As
   VR-Casting is characterized as lightweight and cross-platform, it offers
   access to the visualization for various platforms and devices. The
   demonstration delivers VR-Casting has a broad application prospect like
   exhibition, education, training, and process analysis. Some tests were
   implemented on different devices, and the results demonstrated
   VR-Casting has a splendid performance when conducting tests on rendering
   models. When using VR-Casting, observer has a certain sense of immersion
   with arbitrarily adjusting of the observation angle and even watches
   deeply into the interior of the casts. Thus, observer can master the
   details in a more comprehensive and diverse way during the casting
   process.</abstract><date>SEP 2015</date><author>Sun, Fei
   Zhang, Zhaochuang
   Liao, Dunming
   Chen, Tao
   Zhou, Jianxin</author></paper><paper><title>FPGA Implementation of Real-Time Compressive Sensing with Partial
   Fourier Dictionary</title><abstract>This paper presents a novel real-time compressive sensing (CS)
   reconstruction which employs high density field-programmable gate array
   (FPGA) for hardware acceleration. Traditionally, CS can be implemented
   using a high-level computer language in a personal computer (PC) or
   multicore platforms, such as graphics processing units (GPUs) and
   Digital Signal Processors (DSPs). However, reconstruction algorithms are
   computing demanding and software implementation of these algorithms is
   extremely slow and power consuming. In this paper, the orthogonal
   matching pursuit (OMP) algorithm is refined to solve the sparse
   decomposition optimization for partial Fourier dictionary, which is
   always adopted in radar imaging and detection application. OMP
   reconstruction can be divided into two main stages: optimization which
   finds the closely correlated vectors and least square problem. For large
   scale dictionary, the implementation of correlation is time consuming
   since it often requires a large number of matrix multiplications. Also
   solving the least square problem always needs a scalable matrix
   decomposition operation. To solve these problems efficiently, the
   correlation optimization is implemented by fast Fourier transform (FFT)
   and the large scale least square problem is implemented by Conjugate
   Gradient (CG) technique, respectively. The proposed method is verified
   by FPGA (Xilinx Virtex-7 XC7VX690T) realization, revealing its
   effectiveness in real-time applications.</abstract><date>2016</date><author>Quan, Yinghui
   Li, Yachao
   Gao, Xiaoxiao
   Xing, Mengdao</author></paper><paper><title>Accelerating Smith-Waterman Alignment for Protein Database Search Using
   Frequency Distance Filtration Scheme Based on CPU-GPU Collaborative
   System</title><abstract>The Smith-Waterman (SW) algorithm has been widely utilized for searching
   biological sequence databases in bioinformatics. Recently, several works
   have adopted the graphic card with Graphic Processing Units (GPUs) and
   their associated CUDA model to enhance the performance of SW
   computations. However, these works mainly focused on the protein
   database search by using the intertask parallelization technique, and
   only using the GPU capability to do the SW computations one by one.
   Hence, in this paper, we will propose an efficient SW alignment method,
   called CUDA-SWfr, for the protein database search by using the intratask
   parallelization technique based on a CPU-GPU collaborative system.
   Before doing the SW computations on GPU, a procedure is applied on CPU
   by using the frequency distance filtration scheme (FDFS) to eliminate
   the unnecessary alignments. The experimental results indicate that
   CUDA-SWfr runs 9.6 times and 96 times faster than the CPU-based SW
   method without and with FDFS, respectively.</abstract><date>2015</date><author>Liu, Yu
   Hong, Yang
   Lin, Chun-Yuan
   Hung, Che-Lun</author></paper><paper><title>CUDA accelerated uniform re-sampling for non-Cartesian MR reconstruction</title><abstract>A grid-driven gridding (GDG) method is proposed to uniformly re-sample
   non-Cartesian raw data acquired in PROPELLER, in which a trajectory
   window for each Cartesian grid is first computed. The intensity of the
   reconstructed image at this grid is the weighted average of raw data in
   this window. Taking consider of the single instruction multiple data
   (SIMD) property of the proposed GDG, a CUDA accelerated method is then
   proposed to improve the performance of the proposed GDG. Two groups of
   raw data sampled by PROPELLER in two resolutions are reconstructed by
   the proposed method. To balance computation resources of the GPU and
   obtain the best performance improvement, four thread-block strategies
   are adopted. Experimental results demonstrate that although the proposed
   GDG is more time consuming than traditional DDG, the CUDA accelerated
   GDG is almost 10 times faster than traditional DDG.</abstract><date>2015</date><author>Feng, Chaolu
   Zhao, Dazhe</author></paper><paper><title>A case study of CUDA FORTRAN and OpenACC for an atmospheric climate
   kernel</title><abstract>The porting of a key kernel in the tracer advection routines of the
   Community Atmosphere Model Spectral Element (CAM-SE) to use Graphics
   Processing Units (GPUs) using OpenACC is considered in comparison to an
   existing CUDA FORTRAN port. The development of the OpenACC kernel for
   GPUs was substantially simpler than that of the CUDA port. Also, OpenACC
   performance was about 1.5 x slower than the optimized CUDA version.
   Particular focus is given to compiler maturity regarding OpenACC
   implementation for modern FORTRAN, and it is found that the Cray
   implementation is currently more mature than the PGI implementation.
   Still, for the case that ran successfully on PGI, the PGI OpenACC
   runtime was slightly faster than Cray. The results show encouraging
   performance for OpenACC implementation compared to CUDA while also
   exposing some issues that may be necessary before the implementations
   are suitable for porting all of CAM-SE. Most notable are that GPU shared
   memory should be used by future OpenACC implementations and that derived
   type support should be expanded. (C) 2015 Published by Elsevier B.V.</abstract><date>JUL 2015</date><author>Norman, Matthew
   Larkin, Jeffrey
   Vose, Aaron
   Evans, Katherine</author></paper><paper><title>APES-based Procedure for Super-resolution SAR Imagery with GPU Parallel
   Computing</title><abstract>The amplitude and phase estimation (APES) algorithm is widely used in
   modern spectral analysis. Compared with conventional Fourier transform
   (FFT), APES results in lower sidelobes and narrower spectral peaks.
   However, in synthetic aperture radar (SAR) imaging with large scene,
   without parallel computation, it is difficult to apply APES directly to
   super-resolution radar image processing due to its great amount of
   calculation. In this paper, a procedure is proposed to achieve target
   extraction and parallel computing of APES for super-resolution SAR
   imaging. Numerical experimental are carried out on Tesla K40C with 745
   MHz GPU clock rate and 2880 CUDA cores. Results of SAR image with GPU
   parallel computing show that the parallel APES is remarkably more
   efficient than that of CPU-based with the same super-resolution.</abstract><date>2015</date><author>Jia, Weiwei
   Xu, Xiaojian
   Xu, Guangyao</author></paper><paper><title>Stable Anisotropic Materials</title><abstract>The Finite Element Method (FEM) is commonly used to simulate isotropic
   deformable objects in computer graphics. Several applications (wood,
   plants, muscles) require modeling the directional dependence of the
   material elastic properties in three orthogonal directions. We
   investigate linear orthotropic materials, a special class of linear
   anisotropic materials where the shear stresses are decoupled from normal
   stresses, as well as general linear (non-orthotropic) anisotropic
   materials. Orthotropic materials generalize transversely isotropic
   materials, by exhibiting different stiffness in three orthogonal
   directions. Orthotropic materials are, however, parameterized by nine
   values that are difficult to tune in practice, as poorly adjusted
   settings easily lead to simulation instabilities. We present a
   user-friendly approach to setting these parameters that is guaranteed to
   be stable. Our approach is intuitive as it extends the familiar
   intuition known from isotropic materials. Similarly to linear
   orthotropic materials, we also derive a stability condition for a subset
   of general linear anisotropic materials, and give intuitive approaches
   to tuning them. In order to simulate large deformations, we augment
   linear corotational FEM simulations with our orthotropic and general
   anisotropic materials.</abstract><date>OCT 2015</date><author>Li, Yijing
   Barbic, Jernej</author></paper><paper><title>Computer simulation implementations and optimization of the right atrium
   of the heart based on GPU</title><abstract>For heart right atrium electrophysiology simulation computation huge,
   time-consuming problem, a method based on high-performance graphics
   processing unit (GPU) to achieve parallel computing and optimization.
   First, consider the differences between right atrial heart cell center
   and the edge of the constructed one-dimensional non-homogeneous cardiac
   tissue model right atrium; operator split method enables solution of the
   model calculation task with parallelism. Presents three parallel
   strategies based on the specific solver process, and which takes the
   shortest thread blocks from the policy settings were further optimized
   data storage mode switching frequencies and so on. The results show
   that: for simulation 500 cells CUDA program execution time than the
   serial program fell by 60%, after further optimized CUDA program
   execution time can be reduced by 84%; the greater the right atrial
   cardiac tissue, GPU acceleration effect more obviously. The results
   verify the GPU acceleration solution method can significantly increase
   the heart rate right atrium solver model, reduce the actual execution
   time.</abstract><date>2015</date><author>Zhao Chunxi</author></paper><paper><title>A new test and graphical tool to assess the goodness of fit of logistic
   regression models</title><abstract>A prognostic model is well calibrated when it accurately predicts event
   rates. This is first determined by testing for goodness of fit with the
   development dataset. All existing tests and graphic tools designed for
   the purpose suffer several drawbacks, related mainly to the subgrouping
   of observations or to heavy dependence on arbitrary parameters. We
   propose a statistical test and a graphical method to assess the goodness
   of fit of logistic regression models, obtained through an extension of
   similar techniques developed for external validation. We analytically
   computed and numerically verified the distribution of the underlying
   statistic. Simulations on a set of realistic scenarios show that this
   test and the well-known Hosmer-Lemeshow approach have similar type I
   error rates. The main advantage of this new approach is that the
   relationship between model predictions and outcome rates across the
   range of probabilities can be represented in the calibration belt plot,
   together with its statistical confidence. By readily spotting any
   deviations from the perfect fit, this new graphical tool is designed to
   identify, during the process of model development, poorly modeled
   variables that call for further investigation. This is illustrated
   through an example based on real data. Copyright (c) 2015 John Wiley &amp;
   Sons, Ltd.</abstract><date>FEB 28 2016</date><author>Nattino, Giovanni
   Finazzi, Stefano
   Bertolini, Guido</author></paper><paper><title>Eliciting prior distributions for extra parameters in some generalized
   linear models</title><abstract>To elicit an informative prior distribution for a normal linear model or
   a gamma generalized linear model (GLM), expert opinion must be
   quantified about both the regression coefficients and the extra
   parameters of these models. The latter task has attracted comparatively
   little attention. In this article, we introduce two elicitation methods
   that aim to complete the prior structure of the normal and gamma GLMs.
   First, we develop a method of assessing a conjugate prior distribution
   for the error variance in normal linear models. The method quantifies an
   expert's opinions through assessments of a median and conditional
   medians. Second, we propose a novel method for eliciting a lognormal
   prior distribution for the scale parameter of gamma GLMs. Given the mean
   value of a gamma distributed response variable, the method is based on
   conditional quartile assessments. It can also be used to quantify an
   expert's opinion about the prior distribution for the shape parameter of
   any gamma random variable, if the mean of the distribution has been
   elicited or is assumed to be known. In the context of GLMs, the mean
   value is determined by the regression coefficients. Interactive graphics
   is the medium through which assessments for the two proposed methods are
   elicited. Examples illustrating use of the methods are given. Computer
   programs that implement both methods are available.</abstract><date>AUG 2015</date><author>Elfadaly, Fadlalla G.
   Garthwaite, Paul H.</author></paper><paper><title>Evaluating Performance Portability of OpenACC</title><abstract>Accelerator-based heterogeneous computing is gaining momentum in High
   Performance Computing arena. However, the increased complexity of the
   accelerator architectures demands more generic, high-level programming
   models. OpenACC is one such attempt to tackle the problem. While the
   abstraction endowed by OpenACC offers productivity, it raises questions
   on its portability. This paper evaluates the performance portability
   obtained by OpenACC on twelve OpenACC programs on NVIDIA CUDA, AMD GCN,
   and Intel MIC architectures. We study the effects of various compiler
   optimizations and OpenACC program settings on these architectures to
   provide insights into the achieved performance portability.</abstract><date>2015</date><author>Sabne, Amit
   Sakdhnagool, Putt
   Lee, Seyong
   Vetter, Jeffrey S.</author></paper><paper><title>Large-scale fingerprint identification on GPU</title><abstract>This paper proposes a new parallel algorithm to speed up fingerprint
   identification using GPUs. A careful design of the algorithm and data
   structures, guided by well-defined optimization goals, yields a speed-up
   of 1946 x over a baseline sequential CPU implementation and of 207 x
   over a CPU implementation optimized with SIMD instructions. The proposed
   algorithm enables a medium-scale AFIS (Automated Fingerprint
   Identification System) to run on a simple PC with four Tesla C2075 GPUs.
   On a benchmark with 250 000 fingerprints and 100 000 queries, the
   proposed system yields state-of-the-art biometric accuracy with a
   throughput of more than 35 million fingerprint matches per second. The
   proposed approach can be easily scaled-up, thus making possible the
   implementation of a large-scale AFIS (i.e., with a database of hundred
   million fingerprints) on inexpensive hardware. (C) 2015 Elsevier Inc.
   All rights reserved.</abstract><date>JUN 10 2015</date><author>Cappelli, Raffaele
   Ferrara, Matteo
   Maltoni, Davide</author></paper><paper><title>An adaptive approach for texture enhancement based on a fractional
   differential operator with non-integer step and order</title><abstract>Image texture enhancement is an important topic in computer graphics,
   computer vision and pattern recognition. By applying the fractional
   derivative to analyze texture characteristics, a new fractional
   differential operator mask with adaptive non-integral step and order is
   proposed in this paper to enhance texture images. A non-regular
   self-similar support region is constructed based on a local texture
   similarity measure, which can effectively exclude pixels with low
   correlation and noise. Then, through applying sub-pixel division and
   introducing a local linear piecewise model to estimate the gray value in
   between the pixels, the resulting non-integral steps can improve the
   characterization of self-similarity that is inherent in many image
   types. Moreover, with in-depth understanding of the local texture
   pattern distribution in the support region, adaptive selection of the
   fractional derivative order is also performed to deal with complex
   texture details. Finally, the non-regular fractional differential
   operator mask which incorporates adaptive non-integral step and order is
   constructed. Experimental results show that, for images with rich
   texture contents, the effective characterization of the degree of
   self-similarity in the texture patterns based on our proposed approach
   leads to improved image enhancement results when compared with
   conventional approaches. (C) 2014 Elsevier B.V. All rights reserved.</abstract><date>JUN 22 2015</date><author>Hu, Fuyuan
   Si, Shaohui
   Wong, Hau San
   Fu, Baochuan
   Si, MaoXin
   Luo, Heng</author></paper><paper><title>Montblanc(1): GPU accelerated radio interferometer measurement equations
   in support of Bayesian inference for radio observations</title><abstract>We present Montblanc, a GPU implementation of the Radio interferometer
   measurement equation (RIME) in support of the Bayesian inference for
   radio observations (BIRO) technique. BIRO uses Bayesian inference to
   select sky models that best match the visibilities observed by a radio
   interferometer. To accomplish this, BIRO evaluates the RIME multiple
   times, varying sky model parameters to produce multiple model
   visibilities. chi(2) values computed from the model and observed
   visibilities are used as likelihood values to drive the Bayesian
   sampling process and select the best sky model.As most of the elements
   of the RIME and chi(2) calculation are independent of one another, they
   are highly amenable to parallel computation. Additionally, Montblanc
   caters for iterative RIME evaluation to produce multiple chi(2) values.
   Modified model parameters are transferred to the GPU between each
   iteration.We implemented Montblanc as a Python package based upon
   NVIDIA's CUDA architecture. As such, it is easy to extend and implement
   different pipelines. At present, Montblanc supports point and Gaussian
   morphologies, but is designed for easy addition of new source
   profiles.Montblanc's RIME implementation is performant: On an NVIDIA
   K40, it is approximately 250 times faster than MEQTREES on a dual
   hexacore Intel E5-2620v2 CPU. Compared to the OSKAR simulator's
   GPU-implemented RIME components it is 7.7 and 12 times faster on the
   same K40 for single and double-precision floating point respectively.
   However, OSKAR's RIME implementation is more general than Montblanc's
   BIRO-tailored RIME.Theoretical analysis of Montblanc's dominant CUDA
   kernel suggests that it is memory bound. In practice, profiling shows
   that is balanced between compute and memory, as much of the data
   required by the problem is retained in L1 and L2 caches. (C) 2015
   Elsevier B.V. All rights reserved.</abstract><date>SEP 2015</date><author>Perkins, S. J.
   Marais, P. C.
   Zwart, J. T. L.
   Natarajan, I.
   Tasse, C.
   Smirnov, O.</author></paper><paper><title>Rendering Pacioli's rhombicuboctahedron</title><abstract>We analyse the glass rhombicuboctahedron (RCO) appearing in a famous
   painting of Pacioli (1495), considering the extent to which it might
   agree with a physically correct rendering of a corresponding glass
   container half filled with water. This investigation shows that it is
   unlikely that the painter of the RCO was looking at such a physical
   object. We then ask what a proper rendering of such an object might look
   like. Our computer renderings, which take into account multiple internal
   and external reflections and refractions, yield visual effects that
   differ strongly from their depictions in the painting. Nevertheless, the
   painter of the RCO has clearly succeeded in providing a rendering that
   appears plausible and awe-inspiring to almost all observers.</abstract><date>OCT 2 2015</date><author>Sequin, Carlo H.
   Shiau, Raymond</author></paper><paper><title>Analysis of the second order accurate uniform equilibrium flux method
   and its graphics processing unit acceleration</title><abstract>The extension of the Uniform Equilibrium Flux Method (UEFM) to second
   order accuracy in space is presented. This extension is made possible
   through the recasting of the original UEFM flux expressions from a
   volumetric flux to a surface flux, allowing for reconstruction through a
   Taylor series expansion of the resulting split surface fluxes at the
   cell interfaces. By doing so, we avoid the difficulties associated with
   integration of the gradient terms over velocity and physical space as
   required by the original UEFM fluxes. Analysis of the dissipative
   qualities of the renewed direction split UEFM flux expressions
   demonstrate that the numerical dissipation is a function of Mach number,
   with increasing amounts of dissipation present with increasing Mach
   numbers. Following this analysis, the higher order UEFM fluxes are
   applied to large scale parallel computation using Graphics Processing
   Units, or GPUs, through the use of CUDA. The vector split nature of the
   UEFM fluxes are well suited to GPU computation due to the high degree of
   locality. This parallelization is performed using a cell-based parallel
   paradigm through the creation of several key CUDA kernels for the
   calculation of split fluxes, gradient of split fluxes and state related
   computations. The algorithm is executed entirely on the GPU device, with
   the host remaining idle during the computation stage. The GPU
   accelerated UEFM algorithm is then applied to the solution of several
   two dimensional benchmark problems. Speedup of approximately 200 and 171
   times for first order accuracy and second order accuracy respectively is
   demonstrated when using an Nvidia Tesla C2075 computing GPU compared to
   that of a single core of an Intel Xeon E5-2760 CPU. (C) 2014 Elsevier
   Ltd. All rights reserved.</abstract><date>MAR 30 2015</date><author>Chen, Yen-Chih
   Smith, Matthew R.
   Ferguson, Alexander</author></paper><paper><title>Presentation and response timing accuracy in Adobe Flash and
   HTML5/JavaScript Web experiments</title><abstract>Web-based research is becoming ubiquitous in the behavioral sciences,
   facilitated by convenient, readily available participant pools and
   relatively straightforward ways of running experiments: most recently,
   through the development of the HTML5 standard. Although in most studies
   participants give untimed responses, there is a growing interest in
   being able to record response times online. Existing data on the
   accuracy and cross-machine variability of online timing measures are
   limited, and generally they have compared behavioral data gathered on
   the Web with similar data gathered in the lab. For this article, we took
   a more direct approach, examining two ways of running experiments
   online-Adobe Flash and HTML5 with CSS3 and JavaScript-across 19
   different computer systems. We used specialist hardware to measure
   stimulus display durations and to generate precise response times to
   visual stimuli in order to assess measurement accuracy, examining
   effects of duration, browser, and system-to-system variability (such as
   across different Windows versions), as well as effects of processing
   power and graphics capability. We found that (a) Flash and JavaScript's
   presentation and response time measurement accuracy are similar; (b)
   within-system variability is generally small, even in low-powered
   machines under high load; (c) the variability of measured response times
   across systems is somewhat larger; and (d) browser type and system
   hardware appear to have relatively small effects on measured response
   times. Modeling of the effects of this technical variability suggests
   that for most within-and between-subjects experiments, Flash and
   JavaScript can both be used to accurately detect differences in response
   times across conditions. Concerns are, however, noted about using some
   correlational or longitudinal designs online.</abstract><date>JUN 2015</date><author>Reimers, Stian
   Stewart, Neil</author></paper><paper><title>Surface trees - Representation of boundary surfaces using a tree
   descriptor</title><abstract>Many applications in fields as diverse as computer graphics, medical
   imaging or pattern recognition require the usage of the boundary of
   digital objects, or discrete surface. A discrete surface is a set of
   orthogonal quadrilaterals connected to each other that is typically
   represented either as a face adjacency graph or as a polygon mesh. In
   this work we propose a new method, named surface trees, to represent
   discrete surfaces. Surface trees allow the representation of any
   discrete surface by coding a tree structure contained in the face
   adjacency graph. This method uses an alphabet of nine symbols, in
   addition to the parenthesis notation, to codify trees of maximum degree
   four. Surface trees are a compact way of representing any discrete
   surface at the same time they preserve geometrical information and
   provide invariance under translation and rotation. We demonstrate our
   method on synthetic surfaces as well as others obtained from real data.
   (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>AUG 2015</date><author>Lemus, Eduardo
   Bribiesca, Ernesto
   Garduno, Edgar</author></paper><paper><title>An analytical GPU performance model for 3D stencil computations from the
   angle of data traffic</title><abstract>The achievable GPU performance of many scientific computations is not
   determined by a GPU's peak floating-point rate, but rather how fast data
   are moved through different stages of the entire memory hierarchy. We
   take low-order 3D stencil computations as a representative class to
   study the reachable GPU performance from the angle of data traffic.
   Specifically, we propose a simple analytical model to estimate the
   execution time based on quantifying the data traffic volume at three
   stages: (1) between registers and on-streaming multiprocessor (SMX)
   storage, (2) between on-SMX storage and L2 cache, (3) between L2 cache
   and GPU's device memory. Three associated granularities are used: a CUDA
   thread, a thread block, and a set of simultaneously active thread
   blocks. For four chosen 3D stencil computations, NVIDIA's profiling
   tools are used to verify the accuracy of the quantified data traffic
   volumes, by examining a large number of executions with different
   problem sizes and thread block configurations. Moreover, by introducing
   an imbalance coefficient, together with the known realistic memory
   bandwidths, we can predict the execution time usage based on the
   quantified data traffic volumes. For the four 3D stencils, the average
   error of the time predictions is 6.9 % for a baseline implementation
   approach, whereas for a blocking implementation approach the average
   prediction error is 9.5 %.</abstract><date>JUL 2015</date><author>Su, Huayou
   Cai, Xing
   Wen, Mei
   Zhang, Chunyuan</author></paper><paper><title>On spatio-temporal feature point detection for animated meshes</title><abstract>Although automatic feature detection has been a long-sought subject by
   researchers in computer graphics and computer vision, feature extraction
   on deforming models remains a relatively unexplored area. In this paper,
   we develop a new method for automatic detection of spatio-temporal
   feature points on animated meshes. Our algorithm consists of three main
   parts. We first define local deformation characteristics, based on
   strain and curvature values computed for each point at each frame. Next,
   we construct multi-resolution space-time Gaussians and
   difference-of-Gaussian (DoG) pyramids on the deformation characteristics
   representing the input animated mesh, where each level contains 3D
   smoothed and subsampled representation of the previous level. Finally,
   we estimate locations and scales of spatio-temporal feature points by
   using a scale-normalized differential operator. A new, precise
   approximation of spatio-temporal scale-normalized Laplacian has been
   introduced, based on the space-time DoG. We have experimentally verified
   our algorithm on a number of examples and conclude that our technique
   allows to detect spatio and temporal feature points in a reliable
   manner.</abstract><date>NOV 2015</date><author>Mykhalchuk, Vasyl
   Seo, Hyewon
   Cordier, Frederic</author></paper><paper><title>Performance Analysis of a High-Level Abstractions-Based Hydrocode on
   Future Computing Systems</title><abstract>In this paper we present research on applying a domain specific
   high-level abstractions (HLA) development strategy with the aim to
   "future-proof" a key class of high performance computing (HPC)
   applications that simulate hydrodynamics computations at AWE plc. We
   build on an existing high-level abstraction framework, OPS, that is
   being developed for the solution of multi-block structured mesh-based
   applications at the University of Oxford. OPS uses an "active library"
   approach where a single application code written using the OPS API can
   be transformed into different highly optimized parallel implementations
   which can then be linked against the appropriate parallel library
   enabling execution on different back-end hardware platforms. The target
   application in this work is the CloverLeaf mini-app from Sandia National
   Laboratory's Mantevo suite of codes that consists of algorithms of
   interest from hydrodynamics workloads. Specifically, we present (1) the
   lessons learnt in re-engineering an industrial representative
   hydro-dynamics application to utilize the OPS high-level framework and
   subsequent code generation to obtain a range of parallel
   implementations, and (2) the performance of the auto-generated OPS
   versions of CloverLeaf compared to that of the performance of the
   hand-coded original CloverLeaf implementations on a range of platforms.
   Benchmarked systems include Intel multi-core CPUs and NVIDIA GPUs, the
   Archer (Cray XC30) CPU cluster and the Titan (Cray XK7) GPU cluster with
   different parallelizations (OpenMP, OpenACC, CUDA, OpenCL and MPI). Our
   results show that the development of parallel HPC applications using a
   high-level framework such as OPS is no more time consuming nor difficult
   than writing a one-off parallel program targeting only a single parallel
   implementation. However the OPS strategy pays off with a highly
   maintainable single application source, through which multiple
   parallelizations can be realized, without compromising performance
   portability on a range of parallel systems.</abstract><date>2015</date><author>Mudalige, G. R.
   Reguly, I. Z.
   Giles, M. B.
   Mallinson, A. C.
   Gaudin, W. P.
   Herdman, J. A.</author></paper><paper><title>Parallel Implementation of MAFFT on CUDA-Enabled Graphics Hardware</title><abstract>Multiple sequence alignment (MSA) constitutes an extremely powerful tool
   for many biological applications including phylogenetic tree estimation,
   secondary structure prediction, and critical residue identification.
   However, aligning large biological sequences with popular tools such as
   MAFFT requires long runtimes on sequential architectures. Due to the
   ever increasing sizes of sequence databases, there is increasing demand
   to accelerate this task. In this paper, we demonstrate how graphic
   processing units (GPUs), powered by the compute unified device
   architecture (CUDA), can be used as an efficient computational platform
   to accelerate the MAFFT algorithm. To fully exploit the GPU's
   capabilities for accelerating MAFFT, we have optimized the sequence data
   organization to eliminate the bandwidth bottleneck of memory access,
   designed a memory allocation and reuse strategy to make full use of
   limited memory of GPUs, proposed a new modified-run-length encoding
   (MRLE) scheme to reduce memory consumption, and used high-performance
   shared memory to speed up I/O operations. Our implementation tested in
   three NVIDIA GPUs achieves speedup up to 11.28 on a Tesla K20m GPU
   compared to the sequential MAFFT 7.015.</abstract><date>JAN-FEB 2015</date><author>Zhu, Xiangyuan
   Li, Kenli
   Salah, Ahmad
   Shi, Lin
   Li, Keqin</author></paper><paper><title>A two-tier design space exploration algorithm to construct GPU
   performance model</title><abstract>Graphics Processing Units (GPUs) have a large and complex design space
   that needs to be explored in order to optimize the performance of future
   GPUs. Statistical techniques are useful tools to help computer
   architects to predict performance of complex processors. In this study,
   these methods are utilized to build a model which predicts the GPU
   performance efficiently. The design space of targeted Fermi GPU has more
   than 8 million points which cause exploring this huge design space a
   challenging process. In order to build an accurate model, we propose a
   two-tier algorithm in our algorithm which builds a multiple linear
   regression model from a small set of simulated data. In this algorithm
   the Plackett Burman design is used to find the key parameters of the
   GPU, and further simulations are guided by a fractional factorial design
   for the most important parameters. Our algorithm is able to construct a
   GPU performance predictor which can predict the performance of any point
   in the design space with an average prediction error between 1% and 5%
   for different benchmark applications. In addition, in comparison to
   other methods which need a large number of sampling points, the accuracy
   in our method is achieved by only sampling between 0.0003% and 0.0015%
   of the full design space. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Mirsoleimani, S. Ali
   Khunjush, Farshad
   Karami, Ali</author></paper><paper><title>Parallel Implementation of Sparse Representation Classifiers for
   Hyperspectral Imagery on GPUs</title><abstract>Classification is one of the most important analysis techniques for
   hyperspectral image analysis. Sparse representation is an extremely
   powerful tool for this purpose, but the high computational complexity of
   sparse representation-based classification techniques limits their
   application in time-critical scenarios. To improve the efficiency and
   performance of sparse representation classification techniques for
   hyperspectral image analysis, this paper develops a new parallel
   implementation on graphics processing units (GPUs). First, an optimized
   sparse representation model based on spatial correlation regularization
   and a spectral fidelity term is introduced. Then, we use this approach
   as a case study to illustrate the advantages and potential challenges of
   applying GPU parallel optimization principles to the considered problem.
   The first GPU optimization algorithm for sparse representation
   classification (SRCSC_P) of hyperspectral images is proposed in this
   paper, and a parallel implementation of the proposed method is developed
   using compute unified device architecture (CUDA) on GPUs. The GPU
   parallel implementation is compared with the serial and multicore
   implementations on CPUs. Experimental results based on real
   hyperspectral datasets show that the average speedup of SRCSC_P is more
   than 130x, and the proposed approach is able to provide results
   accurately and fast, which is appealing for computationally efficient
   hyperspectral data processing.</abstract><date>JUN 2015</date><author>Wu, Zebin
   Wang, Qicong
   Plaza, Antonio
   Li, Jun
   Liu, Jianjun
   Wei, Zhihui</author></paper><paper><title>Fragment-based similarity searching with infinite color space</title><abstract>Fragment-based searching and abstract representation of molecular
   features through reduced graphs have separately been used for virtual
   screening. Here, we combine these two approaches and apply the algorithm
   RedFrag to virtual screens retrospectively and prospectively. It uses a
   new type of reduced graph that does not suffer from information loss
   during its construction and bypasses the necessity of feature
   definitions. Built upon chemical epitopes resulting from molecule
   fragmentation, the reduced graph embodies physico-chemical and
   2D-structural properties of a molecule. Reduced graphs are compared with
   a continuous-similarity-distance-driven maximal common subgraph
   algorithm, which calculates similarity at the fragmental and topological
   levels. The performance of the algorithm is evaluated by retrieval
   experiments utilizing precompiled validation sets. By predicting and
   experimentally testing ligands for endothiapepsin, a challenging model
   protease, the method is assessed in a prospective setting. Here, we
   identified five novel ligands with affinities as low as 2.08 M. (c) 2015
   Wiley Periodicals, Inc.</abstract><date>AUG 5 2015</date><author>Gunera, Jakub
   Kolb, Peter</author></paper><paper><title>Explicit GPU Based Second-Order Finite-Difference Modeling on a High
   Resolution Surface, Feather River, California</title><abstract>An explicit second-order finite-difference computer model was developed
   and optimized for solution of the Shallow Water Equations. The model was
   applied to the Feather River below the Oroville Dam and Thermalito
   Afterbay near Gridley, California. Two versions of the computer model
   were constructed to run on either Central Processing Units or Graphical
   Processing Units, utilizing Fortran, C, C++, and the NVIDIA Compute
   Unified Device Architecture (CUDA) parallel computing platform. The
   underlying algorithm utilizes a structured grid and is capable of
   handling the wetting and drying of cells. It was developed with view to
   maximizing stability while maintaining accuracy, and allowing for
   flexibility of the computational domain. Comparisons with analytical and
   observed results showed the proposed methodology to be robust, accurate,
   and efficient. The models were applied to a section of the Feather River
   where observations of flow depths and volumetric flow rates are
   available for multiple flood events. The domain surface was partially
   developed using high-resolution photogrammetric data obtained through
   use of unmanned aerial vehicles. Runtimes and results were compared to
   the United States Bureau of Reclamations' implicit finite-volume
   numerical method and with field observation with generally good
   correspondence.</abstract><date>JAN 2016</date><author>Ransom, Owen T.
   Younis, Bassam A.</author></paper><paper><title>3D-Model-Based Video Analysis for Computer Generated Faces
   Identification</title><abstract>Modern computer graphics technologies brought realism in
   computer-generated characters, making them achieve truly natural
   appearance. Besides traditional virtual reality applications such as
   avatars, games, or cinema, these synthetic characters may be used to
   generate realistic fakes, which may lead to improper use of the
   technology. This fact raises the demand for advanced tools able to
   discriminate real and artificial human faces in digital media. In this
   paper, we propose a method to distinguish between computer generated and
   natural faces by modeling and evaluating their dynamic behavior. Because
   of a 3D-model-based video analysis, the proposed technique allows
   identifying synthetic characters by detecting their more limited
   variability over time. Experimental results demonstrate the
   effectiveness of the proposed approach also on very challenging and
   realistic video sequences.</abstract><date>AUG 2015</date><author>Duc-Tien Dang-Nguyen
   Boato, Giulia
   De Natale, Francesco G. B.</author></paper><paper><title>iDrug-Target: predicting the interactions between drug compounds and
   target proteins in cellular networking via benchmark dataset
   optimization approach</title><abstract>Information about the interactions of drug compounds with proteins in
   cellular networking is very important for drug development.
   Unfortunately, all the existing predictors for identifying drug-protein
   interactions were trained by a skewed benchmark data-set where the
   number of non-interactive drug-protein pairs is overwhelmingly larger
   than that of the interactive ones. Using this kind of highly unbalanced
   benchmark data-set to train predictors would lead to the outcome that
   many interactive drug-protein pairs might be mispredicted as
   non-interactive. Since the minority interactive pairs often contain the
   most important information for drug design, it is necessary to minimize
   this kind of misprediction. In this study, we adopted the neighborhood
   cleaning rule and synthetic minority over-sampling technique to treat
   the skewed benchmark datasets and balance the positive and negative
   subsets. The new benchmark datasets thus obtained are called the
   optimized benchmark datasets, based on which a new predictor called
   iDrug-Target was developed that contains four sub-predictors:
   iDrug-GPCR, iDrug-Chl, iDrug-Ezy, and iDrug-NR, specialized for
   identifying the interactions of drug compounds with GPCRs
   (G-protein-coupled receptors), ion channels, enzymes, and NR (nuclear
   receptors), respectively. Rigorous cross-validations on a set of
   experiment-confirmed datasets have indicated that these new predictors
   remarkably outperformed the existing ones for the same purpose. To
   maximize users' convenience, a public accessible Web server for
   iDrug-Target has been established at[GRAPHICS], by which users can
   easily get their desired results. It has not escaped our notice that the
   aforementioned strategy can be widely used in many other areas as well.</abstract><date>OCT 3 2015</date><author>Xiao, Xuan
   Min, Jian-Liang
   Lin, Wei-Zhong
   Liu, Zi
   Cheng, Xiang
   Chou, Kuo-Chen</author></paper><paper><title>Accelerating RSA with Fine-Grained Parallelism Using GPU</title><abstract>RSA is a public key cryptography widely used for end-to-end
   authentication and key exchange in various Internet protocols, such as
   SSL and TLS. Compared with symmetric cryptography, the cryptographic
   operations in RSA is much more time consuming. This brings pressure on
   performance to service providers using secure protocols, and hinders
   these protocols from being more widely used. Graphics Processing Units
   (GPUs) are increasingly used for intensive data parallelism general
   purpose computing. GPUs often provide better throughput than CPUs at the
   same cost. In this paper, we propose a new approach to parallelize
   Montgomery multiplication under the Single Instruction Multiple Thread
   (SIMT) threading model of GPUs, and construct a parallel RSA
   implementation based on this approach, combining with other optimization
   techniques both in the algorithmic level and implementation level. The
   performance evaluation shows our RSA implementation achieves a
   record-breaking latency for RSA decryption implementations on GPUs: 2.6
   ms for RSA-1024 and 6.5 ms for RSA-2048. The peak throughtput of
   decryptions per second of our implementation reaches 5,244 for RSA-2048
   and 34,981 for RSA-1024 respectively, which is much faster than existing
   integer-based implementations. The peak throughput of our implementation
   is slightly slower than the fastest floating-point based implementation,
   while the latency of our implementation is 3 times faster.</abstract><date>2015</date><author>Yang, Yang
   Guan, Zhi
   Sun, Huiping
   Chen, Zhong</author></paper><paper><title>Speeding up multiple instance learning classification rules on GPUs</title><abstract>Multiple instance learning is a challenging task in supervised learning
   and data mining. However, algorithm performance becomes slow when
   learning from large-scale and high-dimensional data sets. Graphics
   processing units (GPUs) are being used for reducing computing time of
   algorithms. This paper presents an implementation of the G3P-MI
   algorithm on GPUs for solving multiple instance problems using
   classification rules. The GPU model proposed is distributable to
   multiple GPUs, seeking for its scalability across large-scale and
   high-dimensional data sets. The proposal is compared to the
   multi-threaded CPU algorithm with streaming SIMD extensions parallelism
   over a series of data sets. Experimental results report that the
   computation time can be significantly reduced and its scalability
   improved. Specifically, an speedup of up to 149 can be achieved over the
   multi-threaded CPU algorithm when using four GPUs, and the rules
   interpreter achieves great efficiency and runs over 108 billion genetic
   programming operations per second.</abstract><date>JUL 2015</date><author>Cano, Alberto
   Zafra, Amelia
   Ventura, Sebastian</author></paper><paper><title>Implementation of K-shortest path algorithm in GPU using CUDA</title><abstract>K-shortest path algorithm is generalization of the shortest path
   algorithm. K-shortest path is used in various fields like sequence
   alignment problem in molecular bioinformatics, robot motion planning,
   path finding in gene network where speed to calculate paths plays a
   vital role. Parallel implementation is one of the best ways to fulfill
   the requirement of these applications. A GPU based parallel algorithm is
   developed to find k number of shortest path in a positive edge-weighted
   directed large graph. In calculated shortest path repetition of the
   vertices is not allowed. Implemented algorithm calculates a k-shortest
   path between two pair of vertices of a graph with n nodes and m
   vertices. This approach is based on Yen's algorithm to find k-shortest
   loopless path. We implemented our algorithms in Nvidia's GPU using
   Compute Unified Device Architecture (CUDA). This paper presents
   comparative analysis between CPU and GPU based implementation of Yen's
   Algorithm. Our approach achieves the 6 time speed up in comparison of
   serial algorithm. (C) 2015 The Authors. Published by Elsevier B.V.</abstract><date>2015</date><author>AvadheshPratapSingh
   DhirendraPratapSingh</author></paper><paper><title>Accelerating electrostatic interaction calculations with graphical
   processing units based on new developments of ewald method using
   non-uniform fast fourier transform</title><abstract>We present new algorithms to improve the performance of ENUF method (F.
   Hedman, A. Laaksonen, Chem. Phys. Lett. 425, 2006, 142) which is
   essentially Ewald summation using Non-Uniform FFT (NFFT) technique. A
   NearDistance algorithm is developed to extensively reduce the neighbor
   list size in real-space computation. In reciprocal-space computation, a
   new algorithm is developed for NFFT for the evaluations of electrostatic
   interaction energies and forces. Both real-space and reciprocal-space
   computations are further accelerated by using graphical processing units
   (GPU) with CUDA technology. Especially, the use of CUNFFT (NFFT based on
   CUDA) very much reduces the reciprocal-space computation. In order to
   reach the best performance of this method, we propose a procedure for
   the selection of optimal parameters with controlled accuracies. With the
   choice of suitable parameters, we show that our method is a good
   alternative to the standard Ewald method with the same computational
   precision but a dramatically higher computational efficiency. (c) 2015
   Wiley Periodicals, Inc.</abstract><date>JAN 30 2016</date><author>Yang, Sheng-Chun
   Wang, Yong-Lei
   Jiao, Gui-Sheng
   Qian, Hu-Jun
   Lu, Zhong-Yuan</author></paper><paper><title>Accelerating earthquake simulations on general-purpose graphics
   processors</title><abstract>Parallelization strategies are presented for Virtual Quake, a numerical
   simulation code for earthquakes based on topologically realistic systems
   of interacting earthquake faults. One of the demands placed upon the
   simulation is the accurate reproduction of the observed earthquake
   statistics over three to four decades. This requires the use of a
   high-resolution fault model in computations, which demands computational
   power that is well beyond the scope of off-the-shelf multi-core CPU
   computers. However, the recent advances in general-purpose graphic
   processing units have the potential to address this problem at moderate
   cost increments. A functional decomposition of Virtual Quake is
   performed, and opportunities for parallelization are discussed in this
   work. Computationally intensive modules are identified, and these are
   implemented on graphics processing units, significantly speeding up
   earthquake simulations. In the current best case scenario, a computer
   with six graphics processing units can simulate 500years of fault
   activity in California at 1.5kmx1.5km element resolution in less than
   1hour, whereas a single CPU requires more than 2days to perform the same
   simulation. Copyright (c) 2015 John Wiley &amp; Sons, Ltd.</abstract><date>DEC 10 2015</date><author>Sengupta, Prasenjit
   Nguyen, Jimmy
   Kwan, Jason
   Menon, Padmanabhan K.
   Heien, Eric M.
   Rundle, John B.</author></paper><paper><title>BCVEGPY2.2: A newly upgraded version for hadronic production of the
   meson B-c and its excited states</title><abstract>A newly upgraded version of the BCVEGPY, a generator for hadronic
   production of the meson Be and its excited states, is available. In
   comparison with the previous one (Chang et al., 2006), the new version
   is to apply an improved hit-and-miss technology to generating the
   un-weighted events much more efficiently under various simulation
   environments. The codes for production of 2S-wave B-c states are also
   given here.New version program summaryTitle of program:
   BCVEGPY2.2Catalogue identifier: ADTJ_v2_3Program obtained from: CPC
   Program Library, Queen's University, Belfast, N. IrelandLicensing
   provisions: Standard CPC licence,
   http://cpc.cs.qub.ac.uk/licence/licence.htmlNo. of lines in distributed
   program, including test data, etc.: 323731No. of bytes in distributed
   program, including test data, etc.: 4498602Distribution format:
   tar.gzComputer: Any LINUX based on PC with FORTRAN 77 or FORTRAN 90 and
   GNU C compiler as well Operating systems: LINUXProgramming language
   used: FORTRAN 77/90Memory required to execute with typical data: About
   2.0 MBClassification: 11.2, 11.5Catalogue identifier of previous
   version: ADTJ_v2_2Journal reference of previous version: Comput. Phys.
   Commun. 183 (2012) 442Does the new version supersede the old version?:
   YesNature of physical problem: Hadronic Production of B-c meson and its
   excited states.Method of solution: To generate un-weighted events of B-c
   meson and its excited states by using an improved hit-and-miss
   technology.Reasons for new version: Responding to the feedback from
   users, such as those from CMS and LHCb groups, we create a new
   hit-and-miss algorithm for generating the un-weighted events.
   Furthermore, the relevant codes for generating the 2S-excited state of
   B-c meson are added, because the excited state production may be sizable
   in the new LHC run.Typical running time: it depends on which option is
   chosen to match PYTHIA when generating the full events and also on which
   state of B-c meson, either its ground state or its excited states, is to
   be generated. Typically on a 2.27GHz Intel Xeon E5520 processor machine,
   for producing the B-c meson ground state: I) If setting [IDWTUP=3 and
   unwght =.true.], it shall adopt the new hit-and-miss technology to
   generate the un-weighted events, and to generate 10(5) events takes 30
   minutes; II) If setting [IDWTUP=3 and unwght =.false.] or [IDWTUP=1 and
   IGENERATE=0], it shall generate the weighted events, and to generate 105
   events takes 2 minutes only (the fastest way, for theoretical purpose
   only); Ill) As a comparison, if setting [IDWTUP=1 and IGENERATE=1], it
   shall, as the same as the previous version, adopt the PYTHIA inner
   hit-and-miss technology to generate the un-weighted events, and to
   generate 1000 events takes about 22 hours. Thus, the efficiency (and
   accuracy also) for generating the un-weighted events obviously is
   greatly increased.Keywords: Event generator; Hadronic production; B-c
   meson; Un-weighted eventsSummary of revisions: 1). We improve the
   approach for generating un-weighted events. 2). Responding to the
   feedback from users, we adjust part of the codes to make it work more
   user-friendly. More specifically, we explain main changes in the
   following :Event generation.If each simulated event comes with a weight,
   it will make the data analysis much more complicated. Thus the
   un-weighted events are usually adopted for Monte Carlo simulations. As
   an external process of PYTHIA, the generator BCVEGPY [1-4] shall call
   the PYTHIA inner hit-and-miss mechanism to generate the un-weighted
   events by setting IDWGTUP=1 and IGENERATE=1 [5], i.e. the Von Neumann
   method is used for generating the un-weighted B-c events.Every events
   bearing a weight (xwgtup) respectively, when inputting them to PYTHIA,
   they are suffered from being accepted or rejected, all the fully
   generated events at the output become to have a common weight. The Von
   Neumann method states that the event should be accepted by the PYTHIA
   subroutine PYEVNT with a probability R = xwgtup/xmaxup. This can be
   achieved by comparing R with a random number that is uniformly
   distributed within the region of [0, 1]. Namely if R is bigger than such
   a random number then the event is accepted, otherwise it should be
   rejected. Here xmaxup stands for the maximum event weight.The von
   Neumann method works effectively for the cases when all the weights of
   input events are moderate in the whole phase-space. However if the input
   events' weights vary greatly, such as varying logarithmically, thee its
   efficiency shall be greatly depressed, since too much time shall be
   wasted for calculating xwgtup of the rejected events. Thus it is helpful
   to find a new method for generating unweighted events.We will adopt the
   new hit-and-miss strategy suggested by Ref.[6] to do the Bc meson
   un-weight simulation. Extra switches for calling this new technology are
   added to BCVEGPY, e.g. the new hit-and-miss technology shall be called
   by setting IDWTUP=3 and unwght =.true.. Details for this new technology
   can be found in Ref. [6]. For self-consistency, we repeat its main idea
   here.To be different from previous versions, BCVEGPY2.2 uses the VEGAS
   [7] and the MINT [8] as a combined way to generate the un-weighted
   events. The whole phase space shall be separated to a multidimensional
   phase-space grid. The main purpose of VEGAS [7] is to perform the
   adaptive Monte Carlo multi-dimensional integration, which uses the
   importance-sampling method to improve the integration efficiency. Each
   event shall generally result in a different weight, recorded by xwgtup,
   and the maximum weight within each grid shall be simultaneously recorded
   into the importance-sampling grid file (with the suffix.grid). Then
   following the idea of MINT, the Von Neumann method is used in each
   phase-space grid. Within this small grid region, the von Neumann
   algorithm works effectively, thus the efficiency for generating
   un-weighted events are greatly increased.To implement the new
   hit-and-miss algorithm into BCVEGPY2.2, we change the original VEGAS
   subroutine as vegas (fxn, ndim, ncall, itmx, nprn, xint xmax,
   imode)Three new variables xint, xmax and imode are added in the VEGAS
   subroutine. The xmax array is used to record the maximum weights in all
   cells and mode is a flag. xint stands for the output cross-section when
   setting imode=0, which shall be used to initialize the xmax array when
   setting imode=1. For convenience, the generated xmax array will be
   stored in the same grid file in which the importance sampling function
   is stored.In the initialization stage, the VEGAS subroutine shall be
   called by the subroutine evntinit twice by setting imode=0 and imode=1
   respectively to generate both the upper bound grid xmax for all cells
   and the importance sampling function.A subroutine gem (fxn, ndim, Xmax,
   i mode) is defined in the file vegas. F with the purpose to generate the
   un-weighted events. Three options for calling gem subroutine are
   defined: jmode=0 is to initializes the parameter; jmode=3 is to print
   the generation statistics; jmode=1 is the key option, which is to use
   the new hit-and-miss technology to generate the un-weighted events. More
   explicitly, by calling gen (fxn, ndim,mmax, jmode=1), three steps shall
   be executed:1. Call the phase_gen subroutine to generate a random
   phase-space point and to calculate its weight xwgtup.2. Judge the point
   locates in which cell and read from the xmax array and get the upper
   bound value xmaxup for this particular cell.3. Judge whether such point
   be kept or not by using the Von Neumann method with the help of the
   probability xwgtup/xmaxup.To be more flexible, we add one parameter
   igenmode for generating or using the existed. grid files. When setting
   igenmode=1, the VEGAS subroutine shall be called to generate the. grid
   files. When setting igenmode=2, the VEGAS subroutine shall be called to
   generate more accurate. grid files from the existed. grid files. When
   setting igenmode=3, one can directly use the existed. grid files to
   generate events without running VEGAS. Importantly, before using the
   existed. grid files, one must ensure all the parameters be the same as
   the previous generation.A script for setting the parameters and a
   cross-check of the un-weighted events.We put an additional file,
   bcvegpy_set_par.nam, in the new version for setting the parameters. This
   way the user does not need to compile the program again if only the
   parameter values are changed.[GRAPHICS]Fig. 1. Comparison of the
   normalized B-c transverse momentum (PT) and rapidity (y) distributions
   derived by setting unwght=.true. (events) and unwght=.false.
   (differential distributions), which are represented by solid and dotted
   lines, respectively.As a cross-check of the new technology, we compare
   the un-weighted Bc event distributions derived by setting unwght=.true.
   with the weighted Bc differential distributions derived by setting
   unwght=.false.. The results are shown in Fig. 1. Those two distributions
   after proper normalization agree well with each other, that shows our
   present scheme for un-weighted events is correct.Bc(2S) generation.In
   2014 the ATLAS collaboration reported an observation about an excited
   state of Bc meson, which most probably is Bc (2S) state [9]. With more
   data being collected at LHC detectors, it is hopeful that more
   observations on the excited Bc states will be issued. Therefore in
   addition to the production via color-singlet B-c(1S), B-c(1P) and
   color-octet B-c(1S) states, the B-c(2S) production is involved in
   BCVEGPY2.2. It is achieved by replacing the 1S-wave bound-state
   parameters pmb, pmc and fbc with those of the 2S-wave one. Here pmb, pmc
   and fbc are for b-quark mass, c-quark mass and the radial wave function
   at the zero (vertical bar R(0)1), respectively. For the 2S-wave case,
   their default values are set as pmb =5.234 GeV, pmc =1.633 GeV and fbc
   =0.991 GeV3/2 [10] if the mass of the 2S-wave Bc state is 6.867 GeV.More
   explicitly, two new values for ibcstate are added: ibcstate =9 is to
   generate 2(1)S(0) state and ibcstate =10 is to generate 2(3)S(1) state.
   Detailed technologies for deriving the production properties of all the
   mentioned ten Bc meson states can be found in Refs.[11-13]. Furthermore,
   the values for mix_type are rearranged. mix_type=1 is to generate the
   mixing events for all mentioned states. mix_type=2 is to generate the
   mixing events for 1(1)S(0) and 1(3)S(1) states. mix_type=3 is to
   generate the mixing events for the four 1P-wave states and the two
   color-octet 1(1)S(0) and 1(3)S(1) states. mix_type=4 is to generate the
   mixing events for 2(1)S(0) and 2(3)S(1) states. (C) 2015 Elsevier B.V.
   All rights reserved.</abstract><date>DEC 2015</date><author>Chang, Chao-Hsi
   Wang, Xian-You
   Wu, Xing-Gang</author></paper><paper><title>Orbifold Tutte Embeddings</title><abstract>Injective parameterizations of surface meshes are vital for many
   applications in Computer Graphics, Geometry Processing and related
   fields. Tutte's embedding, and its generalization to convex combination
   maps, are among the most popular approaches for computing
   parameterizations of surface meshes into the plane, as they guarantee
   injectivity, and their computation only requires solving a sparse linear
   system. However, they are only applicable to disk-type and toric surface
   meshes.In this paper we suggest a generalization of Tutte's embedding to
   other surface topologies, and in particular the common, yet untreated
   case, of sphere-type surfaces. The basic idea is to enforce certain
   boundary conditions on the parameterization so as to achieve a Euclidean
   orbifold structure. The orbifold-Tutte embedding is a seamless, globally
   bijective parameterization that, similarly to the classic Tutte
   embedding, only requires solving a sparse linear system for its
   computation.In case the cotangent weights are used, the orbifold-Tutte
   embedding globally minimizes the Dirichlet energy and is shown to
   approximate conformal and four-point quasiconformal mappings. As far as
   we are aware, this is the first fully-linear method that produces
   bijective approximations to conformal mappings.Aside from
   parameterizations, the orbifold-Tutte embedding can be used to generate
   bijective inter-surface mappings with three or four landmarks and
   symmetric patterns on sphere-type surfaces.</abstract><date>NOV 2015</date><author>Aigerman, Noam
   Lipman, Yaron</author></paper><paper><title>Vacancy-related diffusion correlation effects in a simple cubic random
   alloy and on the Na-K sublattice of alkali feldspar</title><abstract>Motivated by the need to analyse experimental data on ionic conductivity
   in alkali feldspar, we performed Monte Carlo (MC) simulations of vacancy
   diffusion in random binary systems. We employed an efficient procedure
   for the calculation of the vacancy correlation factor[GRAPHICS], which
   includes the computation of the associated partial correlation factors
   (PCFs)[GRAPHICS]and[GRAPHICS]. Test simulations on a simple cubic
   lattice show the improvements compared to previous MC data and the
   discrepancies with the Manning model. Vacancy correlation factors on the
   Na-K sublattice in the monoclinic structure of alkali feldspar proved to
   be dependent on crystal orientation. For the[GRAPHICS]-direction, PCFs
   related to the four different jump types were calculated. We also
   examined the percolation behaviour for extreme ratios of the atomic jump
   frequencies. The results are found to agree with known data for the
   simple cubic lattice. In the case of feldspar, we provide the first
   useful estimates for the percolation threshold and the associated
   critical exponent using a simplified set of jump frequencies.</abstract><date>JUL 23 2015</date><author>Wilangowski, F.
   Stolwijk, N. A.</author></paper><paper><title>THE TYPES OF COMPUTER GRAPHICS AND THEIR APPLICATION AT DIFFERENT LEVELS
   OF KNOWLEDGE</title><abstract>In this article we introduce the concept of computer graphics and
   graphical application C.a.R (Compasses and Ruler), its basic commands
   and several examples associated with the geometry. This subject is at
   all levels of knowledge in various stages of development. We will
   present the C. a. R possibilities that can be used in secondary schools,
   high schools and colleges. We show function graphs of varying degrees of
   difficulty that are too complicated for the human imagination.</abstract><date>DEC 2015</date><author>Makarewicz, Anna
   Korga, Sylwester
   Rosa, Wojciech</author></paper><paper><title>A Distributed PTX Virtual Machine on Hybrid CPU/GPU Clusters</title><abstract>Hybrid CPU/GPU cluster recently has drawn lots of attention from high
   performance computing because of excellent execution performance and
   energy efficiency. Many supercomputing sites in the newest TOP 500 and
   Green 500 are built by hybrid CPU/GPU clusters instead of CPU clusters.
   However, the programming complexity of hybrid CPU/GPU clusters is so
   high such that most of users usually hesitate to move toward to this new
   cluster computing platform. To resolve this problem, we propose a
   distributed PTX virtual machine called BigGPU on heterogeneous clusters
   in this paper. As named, this virtual machine physically is a
   distributed system which is aimed at parallel re-compiling and executing
   the PTX codes by aggregating CPUs and GPUs available in a computational
   cluster. With the support of this virtual machine, users can regard a
   hybrid CPU/GPU as a single large-scale GPU. Consequently, they can
   develop applications by using only CUDA without combining MPI and
   multithreading APIs while can simultaneously use distributed CPUs and
   GPUs for resolving the same problem. Moreover, they need not handle the
   problem of load balance among heterogeneous processors and the
   constraints of device memory and thread configuration existing in
   physical GPUs because BigGPU supports large-scale virtual device memory
   space and thread configuration. On the other hand, we have evaluated the
   execution performance of BigGPU in this paper. Our experimental results
   have shown that BigGPU indeed can effectively exploit the computational
   power of CPUs and GPUs for enhancing the execution performance of user's
   CUDA programs. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>JAN 2016</date><author>Liang, Tyng-Yeu
   Li, Hung-Fu
   Lin, Yu-Jie
   Chen, Bi-Shing</author></paper><paper><title>Editing, Publishing and Aggregating Video Articles: Do We Need a
   Scholarly Approach?</title><abstract>The article supports the idea of providing infrastructure and training
   for preparing and publishing quality video articles. Properly edited,
   formatted, and verified video items can present graphic contents of
   interest to the global scientific community. It is suggested to apply
   traditional attributes of scholarly articles to video items and
   aggregate them on a specifically designed editing, publishing, and
   indexing platform, called PubTube. As a mega platform, PubTube may
   provide space for a variety of open-access sources of information,
   ranging from short audio-video presentations to research protocols and
   educational lectures. Video articles on the platform have to pass
   quality checks by skilled reviewers. Global editorial associations
   should be prepared to improving the whole process of publishing and
   aggregating video articles.</abstract><date>SEP 2015</date><author>Assadi, Reza
   Gasparyan, Armen Yuri</author></paper><paper><title>Intraoperative on-the-fly organ-mosaicking for laparoscopic surgery.</title><abstract>The goal of computer-assisted surgery is to provide the surgeon with
   guidance during an intervention, e.g.,using augmented reality. To
   display preoperative data, soft tissue deformations that occur during
   surgery have to be taken into consideration. Laparoscopic sensors, such
   as stereo endoscopes, can be used to create a three-dimensional
   reconstruction of stereo frames for registration. Due to the small field
   of view and the homogeneous structure of tissue, reconstructing just one
   frame, in general, will not provide enough detail to register
   preoperative data, since every frame only contains a part of an organ
   surface. A correct assignment to the preoperative model is possible only
   if the patch geometry can be unambiguously matched to a part of the
   preoperative surface. We propose and evaluate a system that combines
   multiple smaller reconstructions from different viewpoints to segment
   and reconstruct a large model of an organ. Using graphics processing
   unit-based methods, we achieved four frames per second. We evaluated the
   system with in silico, phantom, ex vivo, and in vivo (porcine) data,
   using different methods for estimating the camera pose (optical
   tracking, iterative closest point, and a combination). The results
   indicate that the proposed method is promising for on-the-fly organ
   reconstruction and registration. </abstract><date>2015-Oct</date><author>Reichard, Daniel
   Bodenstedt, Sebastian
   Suwelack, Stefan
   Mayer, Benjamin
   Preukschas, Anas
   Wagner, Martin
   Kenngott, Hannes
   Muller-Stich, Beat
   Dillmann, Rudiger
   Speidel, Stefanie</author></paper><paper><title>Teaching Introductory Parallel Computing Course with Hands-On Experience</title><abstract>This paper presents an innovative course designed to teach parallel
   computing to undergraduate students with significant hands-on
   experience. This course represents an introduction to the main topics of
   parallel, distributed and high-performance computing (HPC). The course
   introduces main concepts and architectures used in parallel computing
   today, and improve students' skills to develop parallel programs using
   major parallel programming paradigms: MPI (Message Passing Interface),
   OpenMP (Open-Multiprocessing). The main objective of the course is to
   teach practical parallel programming tools and techniques for MIMD with
   shared memory, MIMD with distributed memory and SIMD. Each of these
   software tools can be used to give students experience with
   parallelization strategies, and ability to rate the quality and
   effectiveness of parallel programs. We evaluate the success of our
   approach through the use of testing and survey and provide directions
   for further improvements in teaching parallel programming.</abstract><date>2015</date><author>Stojanovic, Natalija
   Milovanovic, Emina</author></paper><paper><title>CHANGING TECHNIQUES OF ARCHITECTURAL DESIGN PRESENTATION</title><abstract>The aim of this paper is to highlight contemporary grey area in fair
   presentation of architectural project. Historical evolution of depicting
   techniques shows adequacy of architectural "visions" to the
   manifestations of art in subsequent historical periods. A breakthrough
   in the presentation of projects turned out to be the departure from
   realism in Art. Building designs were presented in a similar, abstract
   manner. At first it was a domain of groups of avant-garde artists and
   architects. Situation has changed after World War II followed by the
   division of Europe into two political and economy zones, and formation
   of the so-called people's democracy countries. In state-owned, big
   design offices and contractor companies, design drawings were delivering
   professional information to the professionals. At the same time, in
   capitalist countries, presentations of designs were aimed at private
   investors, majority of whom were not architects. Therefore, for
   practical reasons they took on decidedly realistic forms. A computer
   with graphic software offers now almost unlimited possibilities of
   presenting hiperrealistic and thus seductive images. Flair and
   creativity are sometimes substituted with software manipulations.
   Professional ethics should become the principal virtue in contemporary
   design rendering.</abstract><date>SEP 2015</date><author>Bardzinska-Bonenberg, Teresa
   Swit-Jankowska, Barbara</author></paper><paper><title>Non-Photorealistic Rendering Using CUDA-Based Image Segmentation</title><abstract>When rendering both three-dimensional objects and photo images together,
   the non-photorealistic rendering results are in visual discord since the
   two contents have their own independent color distributions. This paper
   proposes a non-photorealistic rendering technique which renders both
   three-dimensional objects and photo images such as cartoons and
   sketches. The proposed technique computes the color distribution
   property of the photo images and reduces the number of colors of both
   photo images and 3D objects. NPR is performed based on the reduced
   colormaps and edge features. To enhance the natural scene presentation,
   the image region segmentation process is preferred when extracting and
   applying colormaps. However, the image segmentation technique needs a
   lot of computational operations. It takes a long time for
   non-photorealistic rendering for large size frames. To speed up the
   time-consuming segmentation procedure, we use GPGPU for the parallel
   computing using the GPU. As a result, we significantly improve the
   execution speed of the algorithm.</abstract><date>2015</date><author>Park, Jong-Seung
   Yoon, Hyun-Cheol</author></paper><paper><title>Future Directions: How Virtual Reality Can Further Improve the
   Assessment and Treatment of Eating Disorders and Obesity.</title><abstract>Transdisciplinary efforts for further elucidating the etiology of eating
   and weight disorders and improving the effectiveness of the available
   evidence-based interventions are imperative at this time. Recent studies
   indicate that computer-generated graphic environments-virtual reality
   (VR)-can integrate and extend existing treatments for eating and weight
   disorders (EWDs). Future possibilities for VR to improve actual
   approaches include its use for altering in real time the experience of
   the body (embodiment) and as a cue exposure tool for reducing food
   craving. </abstract><date>2016-Feb</date><author>Gutierrez-Maldonado, Jose
   Wiederhold, Brenda K
   Riva, Giuseppe</author></paper><paper><title>GPU acceleration of the stochastic grid bundling method for
   early-exercise options</title><abstract>In this work, a parallel graphics processing units (GPU) version of the
   Monte Carlo stochastic grid bundling method (SGBM) for pricing
   multi-dimensional early-exercise options is presented. To extend the
   method's applicability, the problem dimensions and the number of bundles
   will be increased drastically. This makes SGBM very expensive in terms
   of computational costs on conventional hardware systems based on central
   processing units. A parallelization strategy of the method is developed
   and the general purpose computing on graphics processing units paradigm
   is used to reduce the execution time. An improved technique for bundling
   asset paths, which is more efficient on parallel hardware is introduced.
   Thanks to the performance of the GPU version of SGBM, a general approach
   for computing the early-exercise policy is proposed. Comparisons between
   sequential and GPU parallel versions are presented.</abstract><date>DEC 2 2015</date><author>Leitao, Alvaro
   Oosterlee, Cornelis W.</author></paper><paper><title>Parallel algorithm for computing points on a computation front
   hyperplane</title><abstract>A parallel algorithm for computing points on a computation front
   hyperplane is described. This task arises in the computation of a
   quantity defined on a multidimensional rectangular domain.
   Three-dimensional domains are usually discussed, but the material is
   given in the general form when the number of measurements is at least
   two. When the values of a quantity at different points are internally
   independent (which is frequently the case), the corresponding
   computations are independent as well and can be performed in parallel.
   However, if there are internal dependences (as, for example, in the
   Gauss-Seidel method for systems of linear equations), then the order of
   scanning points of the domain is an important issue. A conventional
   approach in this case is to form a computation front hyperplane (a usual
   plane in the three-dimensional case and a line in the two-dimensional
   case) that moves linearly across the domain at a certain angle. At every
   step in the course of motion of this hyperplane, its intersection points
   with the domain can be treated independently and, hence, in parallel,
   but the steps themselves are executed sequentially. At different steps,
   the intersection of the hyperplane with the entire domain can have a
   rather complex geometry and the search for all points of the domain
   lying on the hyperplane at a given step is a nontrivial problem. This
   problem (i.e., the computation of the coordinates of points lying in the
   intersection of the domain with the hyperplane at a given step in the
   course of hyperplane motion) is addressed below. The computations over
   the points of the hyperplane can be executed in parallel.</abstract><date>JAN 2015</date><author>Krasnov, M. M.</author></paper><paper><title>Kernel Specialization Provides Adaptable GPU Code for Particle Image
   Velocimetry</title><abstract>Graphics Processing Units (GPUs) are increasingly used to accelerate
   scientific applications. The state-of-the-art limits the adaptability of
   GPU kernels to both problem parameters and hardware characteristics.
   This makes writing high performance libraries for GPUs challenging. We
   address these challenges through Kernel Specialization (KS) which
   supports both user and hardware parameters and produces highly optimized
   GPU code. We apply KS to Particle Image Velocimetry (PIV), a technique
   used to obtain instantaneous velocity measurements in fluids for such
   diverse applications as aircraft design and artificial heart design. KS
   helps the user search PIV's highly non-linear design space, supports a
   wide range of PIV parameters, and results in improved acceleration times
   over existing kernels.</abstract><date>APR 2015</date><author>Moore, Nicholas
   Leeser, Miriam
   King, Laurie Smith</author></paper><paper><title>Automated protein motif generation in the structure-based protein
   function prediction tool ProMOL</title><abstract>ProMOL, a plugin for the PyMOL molecular graphics system, is a
   structure-based protein function prediction tool. ProMOL includes a set
   of routines for building motif templates that are used for screening
   query structures for enzyme active sites. Previously, each motif
   template was generated manually and required supervision in the
   optimization of parameters for sensitivity and selectivity. We developed
   an algorithm and workflow for the automation of motif building and
   testing routines in ProMOL. The algorithm uses a set of empirically
   derived parameters for optimization and requires little user
   intervention. The automated motif generation algorithm was first tested
   in a performance comparison with a set of manually generated motifs
   based on identical active sites from the same 112 PDB entries. The two
   sets of motifs were equally effective in identifying alignments with
   homologs and in rejecting alignments with unrelated structures. A second
   set of 296 active site motifs were generated automatically, based on
   Catalytic Site Atlas entries with literature citations, as an expansion
   of the library of existing manually generated motif templates. The new
   motif templates exhibited comparable performance to the existing ones in
   terms of hit rates against native structures, homologs with the same EC
   and Pfam designations, and randomly selected unrelated structures with a
   different EC designation at the first EC digit, as well as in terms of
   RMSD values obtained from local structural alignments of motifs and
   query structures. This research is supported by NIH grant GM078077.</abstract><date>DEC 2015</date><author>Osipovitch, Mikhail
   Lambrecht, Mitchell
   Baker, Cameron
   Madha, Shariq
   Mills, Jeffrey L.
   Craig, Paul A.
   Bernstein, Herbert J.</author></paper><paper><title>Topology optimization design of 3D electrothermomechanical actuators by
   using GPU as a co-processor</title><abstract>The topology optimization method (TOM) requires high computational
   resources to be solved, especially in multiphysics problems. The high
   number of computational requirements is because TOM is an iterative
   technique, in which the iterations go from tens to thousands.
   Furthermore, at each TOM iteration, it is necessary to execute several
   routines such as the finite element method (FEM), the optimizer, the
   calculation of the objective function and filter, and other less
   intensive computations. Consequently, several topology optimization
   problems have been restricted in the dimensionality and/or in the
   complexity considered, or addressed in powerful computers of restricted
   access due to their cost. Hence, in order to deal with complex and
   large-scale problem in standard computers, a methodology based on
   parallel computing on graphics processing unit (GPU) has been proposed
   by the scientific community. However, so far, this kind of approach has
   been mostly investigated for the traditional mean compliance
   optimization problem. In this work, TOM is applied to designing
   three-dimensional (3D) electrothermomechanical (ETM) actuators by using
   GPU as a co-processor in the most intensive and intrinsic parallel tasks
   of the method. The TOM is based on the SIMP material model and solved by
   sequential linear programming. The code is programmed in Matlab and
   CUDA, and is tested for obtaining a 3D microactuator. Considerable
   speedup is gained with the GPU; the whole TOM process is achieved up to
   35 times faster than that obtained with the sequential code version. (C)
   2016 Elsevier B.V. All rights reserved.</abstract><date>APR 15 2016</date><author>Javier Ramirez-Gil, Francisco
   Nelli Silva, Emilio Carlos
   Montealegre-Rubio, Wilfredo</author></paper><paper><title>Radiometric Transfer: Example-based Radiometric Linearization of
   Photographs</title><abstract>We present an example-based approach for radiometrically linearizing
   photographs that takes as input a radiometrically linear exemplar image
   and a target regular uncalibrated image of the same scene, possibly from
   a different viewpoint and/or under different lighting. The output of our
   method is a radiometrically linearized version of the target image.
   Modeling the change in appearance of a small image patch seen from a
   different viewpoint and/or under different lighting as a linear 1D
   subspace, allows us to recast radiometric transfer in a form similar to
   classic radiometric calibration from exposure stacks. The resulting
   radiometric transfer method is lightweight and easy to implement. We
   demonstrate the accuracy and validity of our method on a variety of
   scenes.</abstract><date>JUL 2015</date><author>Li, Han
   Peers, Pieter</author></paper><paper><title>Performance Analysis of Multi-GPU Implementations of Krylov-Subspace
   Methods Applied to FEA of Electromagnetic Phenomena</title><abstract>We present a comparison of performances of various graphic processing
   unit (GPU)-CUDA implementations of preconditioned Krylov-subspace
   methods, showing the impact of using a multi-GPU configuration. We aim
   to show that this resource allows the massively parallelized solution of
   large-scale real-world problems in state-of-the-art desktop PCs, since
   it overcomes the low-memory capacity of a single GPU, while still
   providing significant speedups when compared with either the usual
   sequential execution on a single-core CPU or an OpenMP implementation
   with four cores. The methods were selected based on their suitability to
   solve large-scale systems of equations arising from the 3-D
   finite-element analysis of open-bound earth current diffusion problems,
   both in steady state and under time-harmonic loading. In the latter, an
   ungauged harmonic edge-element formulation using the magnetic vector
   potential and the electric scalar potential was used. As the
   preconditioners suitable to this case, based on incomplete
   factorizations, are not appropriate for parallelization, we propose a
   hybrid CPU-GPU scheme to solve such problems, which still exhibits a
   competitive performance in low-cost PC desktops.</abstract><date>MAR 2015</date><author>Peixoto de Camargos, Ana Flavia
   Silva, Viviane Cristine</author></paper><paper><title>Development of GPU-accelerated localization system for autonomous mobile
   robot</title><abstract>We present a localization system for autonomous mobile robot, that
   operates in conditions of well - known environment. In our work we use
   particle - based Monte - Carlo localization. This algorithm has many
   applications in mobile robotics, but it is computationally expensive.
   Due to high level of parallelism in this algorithm, we had an
   opportunity to accelerate its execution on graphical processing unit
   (GPU) using CUDA parallel computing architecture from NVIDIA
   Corporation. We use Microsoft Kinect sensor and robot odometry to
   perform indoor localization. Developed system was tested on
   three-wheeled mobile robot. NVIDIA Jetson TK1 board performed all
   necessary computations.</abstract><date>2015</date><author>Rud, Maxim N.
   Pantiykchin, Alexander R.</author></paper><paper><title>The Hydrological Status Concept: Application at a Temporary River
   (Candelaro, Italy)</title><abstract>In achieving the final objective of the European Water Framework
   Directive, the evaluation of the hydrological status' of a water body in
   a catchment is of the utmost importance. It represents the divergence of
   the actual hydrological regime from its natural' condition and may thus
   provide crucial information about the ecological status of a river. In
   this paper, a new approach in evaluating the hydrological status of a
   temporary river was tested. The flow regime of a river has been
   classified through the analysis of two metrics: the permanence of flow
   and the predictability of no-flow conditions that were evaluated on
   monthly streamflow data. This method was applied to the Candelaro river
   basin (Puglia, Italy) where we had to face the problem of limited data
   availability. The Soil and Water Assessment Tool model was used when
   streamflow data were not available, and a geographic information system
   procedure was applied to estimate potential water abstractions from the
   river. Four types of rivers were identified whose regimes may exert a
   control on aquatic life. By using the two metrics as coordinates in a
   plot, a graphic representation of the regime can be visualized in a
   point. Hydrological perturbations associated with water abstractions,
   point discharges and the presence of a reservoir were assessed by
   comparing the position of the two points representing the regime before
   and after the impacts. The method is intended to be used with biological
   metrics in order to define the ecological status of a stream, and it
   could also be used in planning the measures' aimed at fulfilling the
   Water Framework Directive goals. Copyright (c) 2014 John Wiley &amp; Sons,
   Ltd.</abstract><date>SEP 2015</date><author>De Girolamo, A. M.
   Lo Porto, A.
   Pappagallo, G.
   Tzoraki, O.
   Gallart, F.</author></paper><paper><title>Stereoscopic neuroanatomy lectures using a three-dimensional virtual
   reality environment</title><abstract>Introduction: Three-dimensional (3D) computer graphics are increasingly
   used to supplement the teaching of anatomy. While most systems consist
   of a program which produces 3D renderings on a workstation with a
   standard screen, the Dextrobeam virtual reality VR environment allows
   the presentation of spatial neuroanatomical models to larger groups of
   students through a stereoscopic projection system.Materials and methods:
   Second-year medical students (n = 169) were randomly allocated to
   receive a standardised pre-recorded audio lecture detailing the anatomy
   of the third ventricle accompanied by either a two-dimensional (2D)
   PowerPoint presentation (n = 80) or a 3D animated tour of the third
   ventricle with the DextroBeam. Students completed a 10-question
   multiple-choice exam based on the content learned and a subjective
   evaluation of the teaching method immediately after the lecture.Results:
   Students in the 2D group achieved a mean score of 5.19 (+/- 2.12)
   compared to 5.45 (+/- 2.16) in the 3D group, with the results in the 3D
   group statistically non-inferior to those of the 2D group (p &lt; 0.0001).
   The students rated the 3D method superior to 2D teaching in four domains
   (spatial understanding, application in future anatomy classes,
   effectiveness, enjoyableness) (p &lt; 0.01).Conclusion: Stereoscopically
   enhanced 3D lectures are valid methods of imparting neuroanatomical
   knowledge and are well received by students. More research is required
   to define and develop the role of large-group VR systems in modern
   neuroanatomy curricula. (C) 2015 Elsevier GmbH. All rights reserved.</abstract><date>2015</date><author>Kockro, Ralf A.
   Amaxopoulou, Christina
   Killeen, Tim
   Wagner, Wolfgang
   Reisch, Robert
   Schwandt, Eike
   Gutenberg, Angelika
   Giese, Alf
   Stofft, Eckart
   Stadie, Axel T.</author></paper><paper><title>A review of the new HGNC gene family resource</title><abstract>The HUGO Gene Nomenclature Committee (HGNC) approves unique gene symbols
   and names for human loci. As well as naming genomic loci, we manually
   curate genes into family sets based on shared characteristics such as
   function, homology or phenotype. Each HGNC gene family has its own
   dedicated gene family report on our website, www.genenames.org. We have
   recently redesigned these reports to support the visualisation and
   browsing of complex relationships between families and to provide extra
   curated information such as family descriptions, protein domain graphics
   and gene family aliases. Here, we review how our gene families are
   curated and explain how to view, search and download the gene family
   data.</abstract><date>FEB 3 2016</date><author>Gray, Kristian A.
   Seal, Ruth L.
   Tweedie, Susan
   Wright, Mathew W.
   Bruford, Elspeth A.</author></paper><paper><title>AMGX: A LIBRARY FOR GPU ACCELERATED ALGEBRAIC MULTIGRID AND
   PRECONDITIONED ITERATIVE METHODS</title><abstract>The solution of large sparse linear systems arises in many applications,
   such as computational fluid dynamics and oil reservoir simulation. In
   realistic cases the matrices are often so large that they require large
   scale distributed parallel computing to obtain the solution of interest
   in a reasonable time. In this paper we discuss the design and
   implementation of the AmgX library, which provides drop-in GPU
   acceleration of distributed algebraic multigrid (AMG) and preconditioned
   iterative methods. The AmgX library implements both classical and
   aggregation-based AMG methods with different selector and interpolation
   strategies, along with a variety of smoothers and preconditioners,
   including block-Jacobi, Gauss-Seidel, and incomplete-LU factorization.
   The library contains many of the standard and flexible preconditioned
   Krylov subspace iterative methods, which can be combined with any of the
   available multigrid methods or simpler preconditioners. The parallelism
   in the aggregation scheme exploits parallel graph matching techniques,
   while the smoothers and preconditioners often rely on parallel graph
   coloring algorithms. The AMG algorithm implemented in the AmgX library
   achieves 2-5x speedup on a single GPU against a competitive
   implementation on the CPU. As will be shown in the numerical experiments
   section, both setup and solve phases scale well across multiple nodes,
   sustaining this performance advantage.</abstract><date>2015</date><author>Naumov, M.
   Arsaev, M.
   Castonguay, P.
   Cohen, J.
   Demouth, J.
   Eaton, J.
   Layton, S.
   Markovskiy, N.
   Reguly, I.
   Sakharnykh, N.
   Sellappan, V.
   Strzodka, R.</author></paper><paper><title>High performance GPU based optimized feature matching for computer
   vision applications</title><abstract>In this paper, a graphics processing unit (GPU) based matching technique
   is proposed to perform fast feature matching between different images
   under various image conditions with viewpoint changes. Most recently,
   general-purpose graphics processing units (GPGPUs or GPUs) have become
   commonplace within high performance supercomputers. GPUs allow
   developers to effectively exploit the computational power for high
   performance computing. This paper focuses on improving the performance
   of feature matching based on self-organizing map by porting it onto the
   GPUs. GPU optimization has been applied for the fast computation of
   keypoints to make the system fast and efficient. This scheme has
   enhanced the overall performance and is much more efficient compared to
   other methods without degradation of detection results. The proposed
   matching algorithm is partitioned between the CPU and GPU in a way that
   best exploits the parallelism and perform matching between the different
   images. Experimental results demonstrate that fast feature matching is
   achieved using the graphics processing units, and its computational
   efficiency is checked by comparing its results and run times with those
   of some standard software (MATLAB) run on central processing unit (CPU).
   (c) 2015 Elsevier GmbH. All rights reserved.</abstract><date>2016</date><author>Sharma, Kajal</author></paper><paper><title>Analysis of cholera epidemics with bacterial growth and spatial movement</title><abstract>In this work, we propose novel epidemic models (named,
   susceptible-infected-recovered-susceptible-bacteria) for cholera
   dynamics by incorporating a general formulation of bacteria growth and
   spatial variation. In the first part, a generalized ordinary
   differential equation (ODE) model is presented and it is found that
   bacterial growth contributes to the increase in the basic reproduction
   number, &lt;inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink"
   xlink:href="tjbd_a_974696_ilm0001.gif"&gt;&lt;/inline-graphic&gt;. With the
   derived basic reproduction number, we analyse the local and global
   dynamics of the model. Particularly, we give a rigorous proof on the
   endemic global stability by employing the geometric approach. In the
   second part, we extend the ODE model to a partial differential equation
   (PDE) model with the inclusion of diffusion to capture the movement of
   human hosts and bacteria in a heterogeneous environment. The disease
   threshold of this PDE model is studied again by using the basic
   reproduction number. The results on the threshold dynamics of the ODE
   and PDE models are compared, and verified through numerical simulation.
   Additionally, our analysis shows that incorporating diffusive spatial
   spread does not produce a Turing instability when &lt;inline-graphic
   xmlns:xlink="http://www.w3.org/1999/xlink"
   xlink:href="tjbd_a_974696_ilm0002.gif"&gt;&lt;/inline-graphic&gt; associated with
   the ODE model is less than the unity.</abstract><date>JUN 30 2015</date><author>Wang, Xueying
   Wang, Jin</author></paper><paper><title>Directive-based GPU programming for computational fluid dynamics</title><abstract>Directive-based programming of graphics processing units (CPUs) has
   recently appeared as a.viable alter. native to using specialized
   low-level languages such as CUDA C and OpenCL for general-purpose GPU
   programming. This technique, which uses "directive" or "pragma"
   statements to annotate source codes written in traditional high-level
   languages, is designed to permit a unified code base to serve multiple
   computational platforms. In this work we analyze the popular OpenACC
   programming standard, as implemented by the PGI compiler suite, in order
   to evaluate its utility and performance potential in computational fluid
   dynamics (CFD) applications. We examine the process of applying the
   OpenACC Fortran API to a test CFD code that serves as a proxy for a
   full-scale research code developed at Virginia Tech; this test code is
   used to asses the performance improvements attainable for our CFD
   algorithm on common GPU platforms, as well as to determine the
   modifications that must be made to the original source code in order to
   run efficiently on the GPU. Performance is measured on several recent
   GPU architectures from NVIDIA and AMD (using both double and single
   precision arithmetic) and the accelerator code is bench-marked against a
   multithreaded CPU version constructed from the same Fortran source code
   using OpenMP directives. A single NVIDIA Kepler CPU card is found to
   perform approximately 20x faster than a single CPU core and more than 2x
   faster than a 16-core Xeon server. An analysis of optimization
   techniques for OpenACC reveals cases in which manual intervention by the
   programmer can improve accelerator performance by up to 30% over the
   default compiler heuristics, although these optimizations are relevant
   only for specific platforms. Additionally, the use of multiple
   accelerators with OpenACC is investigated, including an experimental
   high-level interface for multi-GPU programming that automates scheduling
   tasks across multiple devices. While the overall performance of the
   OpenACC code is found to be satisfactory, we also observe some
   significant limitations and restrictions imposed by the OpenACC API
   regarding certain useful features of modern Fortran (2003/8); these are
   sufficient for us to conclude that it would not be practical to apply
   OpenACC to our full research code at this time due to the amount of
   refactoring required. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>JUL 2 2015</date><author>Pickering, Brent P.
   Jackson, Charles W.
   Scogland, Thomas R. W.
   Feng, Wu-Chun
   Roy, Christopher J.</author></paper><paper><title>GPU programming for biomedical imaging</title><abstract>Scientific computing is rapidly advancing due to the introduction of
   powerful new computing hardware, such as graphics processing units
   (CPUs). Affordable thanks to mass production, GPU processors enable the
   transition to efficient parallel computing by bringing the performance
   of a supercomputer to a workstation. We elaborate on some of the
   capabilities and benefits that GPU technology offers to the field of
   biomedical imaging. As practical examples, we consider a GPU algorithm
   for the estimation of position of interaction from photomultiplier (PMT)
   tube data, as well as a GPU implementation of the MLEM algorithm for
   iterative image reconstruction.</abstract><date>2015</date><author>Caucci, Luca
   Furenlid, Lars R.</author></paper><paper><title>Simulating P Systems on GPU Devices: A Survey</title><abstract>P systems have been proven to be useful as modeling tools in many
   fields, such as Systems Biology and Ecological Modeling. For such
   applications, the acceleration of P system simulation is often desired,
   given the computational needs derived from these kinds of models. One
   promising solution is to implement the inherent parallelism of P systems
   on platforms with parallel architectures.In this respect, GPU computing
   proved to be an alternative to more classic approaches in Parallel
   Computing. It provides a low cost, and a manycore platform with a high
   level of parallelism. The GPU has been already employed to speedup the
   simulation of P systems.In this paper, we look over the available
   parallel P systems simulators on the GPU, with special emphasis on those
   included in the PMCGPU project, and analyze some useful guidelines for
   future implementations and developments.</abstract><date>2015</date><author>Martinez-del-Amor, Miguel A.
   Garcia-Quismondo, Manuel
   Macias-Ramos, Luis F.
   Valencia-Cabrera, Luis
   Riscos-Nunez, Agustin
   Perez-Jimenez, Mario J.</author></paper><paper><title>HLog(n)GP: A parallel computation model for GPU clusters</title><abstract>Parallel computation model is an abstraction for the performance
   characteristics of parallel computers, and should evolve with the
   development of computational infrastructure. The heterogeneous
   CPU/Graphics Processing Unit (GPU) systems have been and will be
   important platforms for scientific computing, which introduces an urgent
   demand for new parallel computation models targeting this kind of
   supercomputers. In this research, we propose a parallel computation
   model called HLog(n)GP to abstract the computation and communication
   features of heterogeneous platforms like TH-1A. All the substantial
   parameters of HLog(n)GP are in vector form and deal with the new
   features in GPU clusters. A simplified version HLog(3)GP of the proposed
   model is mapped to a specific GPU cluster and verified with two typical
   benchmarks. Experimental results show that HLog(3)GP outperforms the
   other two evaluated models and can well model the new particularities of
   GPU clusters. Copyright (c) 2015 John Wiley &amp; Sons, Ltd.</abstract><date>DEC 10 2015</date><author>Lu, Fengshun
   Song, Junqiang
   Pang, Yufei</author></paper><paper><title>A GPU Simulation Tool for Training and Optimisation in 2D Digital X-Ray
   Imaging</title><abstract>Conventional radiology is performed by means of digital detectors, with
   various types of technology and different performance in terms of
   efficiency and image quality. Following the arrival of a new digital
   detector in a radiology department, all the staff involved should adapt
   the procedure parameters to the properties of the detector, in order to
   achieve an optimal result in terms of correct diagnostic information and
   minimum radiation risks for the patient. The aim of this study was to
   develop and validate a software capable of simulating a digital Xray
   imaging system, using graphics processing unit computing. All
   radiological image components were implemented in this application: an
   X-ray tube with primary beam, a virtual patient, noise, scatter
   radiation, a grid and a digital detector. Three different digital
   detectors (two digital radiography and a computed radiography systems)
   were implemented. In order to validate the software, we carried out a
   quantitative comparison of geometrical and anthropomorphic phantom
   simulated images with those acquired. In terms of average pixel values,
   the maximum differences were below 15%, while the noise values were in
   agreement with a maximum difference of 20%. The relative trends of
   contrast to noise ratio versus beamenergy and intensity were well
   simulated. Total calculation times were below 3 seconds for clinical
   images with pixel size of actual dimensions less than 0.2 mm. The
   application proved to be efficient and realistic. Short calculation
   times and the accuracy of the results obtained make this software a
   useful tool for training operators and dose optimisation studies.</abstract><date>NOV 6 2015</date><author>Gallio, Elena
   Rampado, Osvaldo
   Gianaria, Elena
   Bianchi, Silvio Diego
   Ropolo, Roberto</author></paper><paper><title>Two Dimensional Yau-Hausdorff Distance with Applications on Comparison
   of DNA and Protein Sequences</title><abstract>Comparing DNA or protein sequences plays an important role in the
   functional analysis of genomes. Despite many methods available for
   sequences comparison, few methods retain the information content of
   sequences. We propose a new approach, the Yau-Hausdorff method, which
   considers all translations and rotations when seeking the best match of
   graphical curves of DNA or protein sequences. The complexity of this
   method is lower than that of any other two dimensional minimum Hausdorff
   algorithm. The Yau-Hausdorff method can be used for measuring the
   similarity of DNA sequences based on two important tools: the
   Yau-Hausdorff distance and graphical representation of DNA sequences.
   The graphical representations of DNA sequences conserve all sequence
   information and the Yau-Hausdorff distance is mathematically proved as a
   true metric. Therefore, the proposed distance can preciously measure the
   similarity of DNA sequences. The phylogenetic analyses of DNA sequences
   by the Yau-Hausdorff distance show the accuracy and stability of our
   approach in similarity comparison of DNA or protein sequences. This
   study demonstrates that Yau-Hausdorff distance is a natural metric for
   DNA and protein sequences with high level of stability. The approach can
   be also applied to similarity analysis of protein sequences by graphic
   representations, as well as general two dimensional shape matching.</abstract><date>SEP 18 2015</date><author>Tian, Kun
   Yang, Xiaoqian
   Kong, Qin
   Yin, Changchuan
   He, Rong L.
   Yau, Stephen S. -T.</author></paper><paper><title>Augment Your Reality.</title><abstract>This article features some of the latest advances and applications in
   computer graphics technology. </abstract><date>2016 Jan-Feb</date><author>[Anonymous]</author></paper><paper><title>Validation of a novel open-source work-flow for the simulation of
   packed-bed reactors</title><abstract>The simulation of flow and transport in packed-bed (catalytic and
   non-catalytic) reactors is of paramount importance in the chemical
   industry. Different tools have been developed in the last decades for
   generating particle packings, such as the Discrete Element Method (DEM),
   whereas Computational Fluid Dynamics (CFD) is generally employed for
   simulating fluid flow and scalar dispersion. This work-flow presents the
   main drawbacks of being computationally expensive, as most packing
   generation algorithms deal with non-convex objects, such as trilobes,
   with cumbersome strategies, and of making use of in-house or commercial
   codes, that are either difficult to access or costly. In this paper a
   novel open-source and easily accessible work-flow based on Blender, a
   rigid-body simulation tool developed for computer graphics applications,
   and OpenFOAM a very well-known CFD code, is presented. The approach,
   which presents the main advantage of being computationally fast, is
   validated by comparison with experimental data for global bulk porosity,
   particle orientation, local porosity and velocity distributions, and
   pressure drop. To our knowledge this is the very first application of
   Blender for the simulation of packed-bed reactors. (C) 2015 Elsevier
   B.V. All rights reserved.</abstract><date>NOV 1 2015</date><author>Boccardo, Gianluca
   Augier, Frederic
   Haroun, Yacine
   Ferre, Daniel
   Marchisio, Daniele L.</author></paper><paper><title>Modification of common Fourier computer generated hologram's
   representation methods from sequential to parallel computing</title><abstract>The continuous improvement of GPUs for general purpose parallel
   computing has made it tempting to modify many physical computational
   techniques from sequential to parallel computation. By adding the
   capability of using CUDA-enabled GPU to MATLAB, a friendlier programming
   environment became available to physicists and researchers for
   implementing GPU parallel computing in their scientific calculations.
   This paper is dedicated to modify four methods of Fourier computer
   generated holography to utilize parallel computation of GPU. discussion
   of benefits, limitations and a speed-up comparison is presented. (C)
   2015 Elsevier GmbH. All rights reserved.</abstract><date>2015</date><author>Makey, Ghaith
   El-Daher, Moustafa Sayem
   Al-Shufi, Kanj</author></paper><paper><title>Aging at the spin-glass/ferromagnet transition: Monte Carlo simulations
   using graphics processing units</title><abstract>We study the nonequilibrium aging behavior of the +/- J Edwards-Anderson
   model in three dimensions for samples of size up to N = 128(3) and for
   up to 10(8) Monte Carlo sweeps. In particular we are interested in the
   change of the aging when crossing from the spin-glass phase to the
   ferromagnetic phase. The necessary long simulation times are reached by
   employing a CUDA-based GPU implementation, which allows for single-spin
   flip times as small as 8 ps. We measure typical spin-glass correlation
   functions in space and time to determine the growing length scale and
   extract the constituting exponents. We observe a clear signature of the
   disorder-driven equilibrium transition in the nonequilibrium behavior.</abstract><date>MAY 26 2015</date><author>Manssen, Markus
   Hartmann, Alexander K.</author></paper><paper><title>Efficient graphics processing unit based layered decoders for
   quasicyclic low-density parity-check codes</title><abstract>Because layered low-density parity-check (LDPC) decoding algorithm was
   proposed, one can exploit the diversity gain to achieve performance
   comparable to the traditional two-phase message passing (TPMP) decoding
   but with about twice faster decoding convergence compared to TPMP. In
   order to reduce the decoding time of layered LDPC decoder, a graphics
   processing unit (GPU) is exploited as the modem processor so that the
   decoding procedure can be processed in parallel using numerous threads
   in the GPU. In this paper, we present the parallel algorithms and
   efficient implementations on the GPU for two different layered message
   passing schemes, the row-layered and column-layered decoding. In the
   experiments, the quasicyclic LDPC codes for WiFi (802.11n) and WiMAX
   (802.16e) are decoded by the proposed layered LDPC decoders. The
   experimental results show that our decoder has good bit error ratio
   (BER) performance comparable to TPMP decoder. The peak throughput is 712
   Mbps, which is about two orders of magnitude faster than that of CPU
   implementation and comparable to the dedicated hardware solutions.
   Compared to the existing fastest GPU-based implementation, the presented
   decoder can achieve a performance improvement of 2.3 times. Copyright
   (C) 2013 John Wiley &amp; Sons, Ltd.</abstract><date>JAN 2015</date><author>Li, Rongchun
   Dou, Yong
   Zou, Dan
   Wang, Shi
   Zhang, Ying</author></paper><paper><title>CUDA Approach for Meshless Local Petrov-Galerkin Method</title><abstract>In this paper, a strategy to parallelize the meshless local
   Petrov-Galerkin (MLPG) method is developed. It is executed in a high
   parallel architecture, the well known graphics processing unit. The MLPG
   algorithm has many variations depending on which combination of trial
   and test functions is used. Two types of interpolation schemes are
   explored in this paper to approximate the trial functions and a
   Heaviside step function is used as test function. The first scheme
   approximates the trial function through a moving least squares
   interpolation, and the second interpolates using the radial point
   interpolation method with polynomial reproduction (RPIMp). To compare
   these two approaches, a simple electromagnetic problem is solved, and
   the number of nodes in the domain is increased while the time to
   assemble the system of equations is obtained. Results shows that with
   the parallel version of the algorithm it is possible to achieve an
   execution time 20 times smaller than the CPU execution time, for the
   MLPG using RPIMp versions of the method.</abstract><date>MAR 2015</date><author>Correa, Bruno C.
   Mesquita, Renato C.
   Amorim, Lucas P.</author></paper><paper><title>Targeted-Tracking With Pointing Devices</title><abstract>Targeting and tracking in graphical user interfaces have been widely
   studied, but attempts to model targeted-tracking are few.
   Targeted-tracking is essentially a two component task of tracking
   followed by targeting, where either or both components may dominate
   depending on the levels of difficulty in each component. The
   applicability of an empirical model based on computer mouse use is
   unknown with respect to other devices. In order to confirm the model
   validity for other input devices, experiments were carried out using a
   mouse, a pen mouse, a touch screen, and a graphics tablet. Fourteen
   participants were tested on 48 experimental conditions that included
   four difficulty levels and 12 conditions with varying track width (P),
   track length (D), and target width (W). Movement time, error rate, index
   of performance, and throughput were compared. Repeated-measures ANOVA
   indicated that factors in the targeted-tracking model were significant
   (p &lt; 0.05) and movement time data were a good fit (R-2 &gt; 0.8) to the
   model, confirming the generality of the model. A principal component
   analysis showed that a mouse is relatively superior in terms of both
   movement time and error rate. Thus, the targeted-tracking model is an
   effective way to compare and evaluate input devices.</abstract><date>AUG 2015</date><author>Senanayake, Ransalu
   Goonetilleke, Ravindra S.
   Hoffmann, Errol R.</author></paper><paper><title>Computation of Fresnel holograms and diffraction-specific coherent
   panoramagrams for full-color holographic displays based on anisotropic
   leaky-mode modulators</title><abstract>We have previously introduced a computational architecture suitable for
   driving full-color holographic display systems based around anisotropic
   leaky-mode modulators; this architecture appropriately handles
   single-sideband modulation and frequency-division multiplexing of
   spectral bands that correspond to the independent red, green, and blue
   color channels in the display output. In this paper, we describe an
   implementation for driving the MIT/BYU Mark IV holographic display
   system with such a computational approach, in cases of both pre-computed
   Fresnel CGHs and real-time, GPU-based diffraction-specific coherent
   panoramagrams. Real-time holographic images of nearly VGA-resolution
   (468 lines) are generated via three dual-head NVIDIA GPUs via a
   CUDA-based implementation that encompasses the requisite orthographic
   view generation from 3-D data sources, parallel vector-based fringe
   computation per hogel and per color, single-sideband modulation, and
   frequency-division multiplexing. We present the first results of this
   scheme and review the resulting performance metrics.</abstract><date>2015</date><author>Jolly, Sundeep
   Dreshaj, Ermal
   Bove, V. Michael, Jr.</author></paper><paper><title>Parallel Computation for Extended Edit Distances Using the Shared Memory
   on GPU</title><abstract>Given two strings ? and ? (???? ?, ???? ? ) over an alphabet ?, the
   extended edit distance between ? and ? can be computed using dynamic
   programming in ? ??? ? time and space. Recently, a parallel algorithm
   that takes ? ?? ? ? ? time and ? ???? space using ? threads to compute
   the extended edit distance between ? and ? was presented. In this paper,
   we present an improved parallel algorithm using the shared memory on
   GPU. The experimental results show that our parallel algorithm runs
   about 19?25 times faster than the previous parallel algorithm.</abstract><date>2015</date><author>???</author></paper><paper><title>HETEROGENEOUS PARALLEL COMPUTING USING CUDA FOR CHEMICAL PROCESS</title><abstract>CUDA (Compute Unified Device Architecture) is a parallel computing
   platform and programming model created by NVIDIA and implemented by the
   graphics processing units (GPUs) that they produce. Using CUDA, the GPUs
   can be used for general purpose processing which involves parallel
   computation. CUDA has been used to accelerate non-graphical applications
   in computational biology, cryptography and other fields by an order of
   magnitude or more. Chemical processes need validation of their
   experimental data. It was found that Chemical process could become one
   such application where CUDA can be efficiently used. These validations
   of Chemical processes normally involve calculation of many coefficients.
   The chemical process that has been chosen for parallelizing is Heat
   Transfer process. This process involves calculation of coefficients for
   multiple iterations. As each of these iterations is independent of one
   another, CUDA was used to parallelize the calculation process. The
   execution time analysis shows that though CPU outperforms GPU when the
   numbers of iterations are less, when the number of iterations increase
   the GPU outperforms CPU greatly. (C) 2015 The Authors. Published by
   Elsevier B.V.</abstract><date>2015</date><author>Sosutha, S.
   Mohana, D.</author></paper><paper><title>Survey of Physically Based Simulation of Cuts in Deformable Bodies</title><abstract>Virtual cutting of deformable bodies has been an important and active
   research topic in physically based modelling and simulation for more
   than a decade. A particular challenge in virtual cutting is the robust
   and efficient incorporation of cuts into an accurate computational model
   that is used for the simulation of the deformable body. This report
   presents a coherent summary of the state of the art in virtual cutting
   of deformable bodies, focusing on the distinct geometrical and
   topological representations of the deformable body, as well as the
   specific numerical discretizations of the governing equations of motion.
   In particular, we discuss virtual cutting based on tetrahedral,
   hexahedral and polyhedral meshes, in combination with standard,
   polyhedral, composite and extended finite element discretizations. A
   separate section is devoted to meshfree methods. Furthermore, we discuss
   cutting-related research problems such as collision detection and haptic
   rendering in the context of interactive cutting scenarios. The report is
   complemented with an application study to assess the performance of
   virtual cutting simulators.</abstract><date>SEP 2015</date><author>Wu, Jun
   Westermann, Ruediger
   Dick, Christian</author></paper><paper><title>GPU Computations on Hadoop Clusters for Massive Data Processing</title><abstract>Hadoop is a well-designed approach for handling massive amount of data.
   Comprised at the core of the Hadoop File System and MapReduce, it
   schedules the processing by orchestrating the distributed servers,
   providing redundancy and fault tolerance. In terms of performance,
   Hadoop is still behind high performance capacity due to CPUs' limited
   parallelism, though. GPU accelerated computing involves the use of a GPU
   together with a CPU to accelerate applications to data processing on GPU
   cluster toward higher efficiency. However, GPU cluster has low level
   data storage capacity. In this chapter, we exploit the hybrid model of
   GPU and Hadoop to make best use of both capabilities, and the design and
   implementation of application using Hadoop and CUDA is presented through
   two interfaces: Hadoop Streaming and Hadoop Pipes. Experimental results
   on K-means algorithm are presented as well as their performance results
   are discussed.</abstract><date>2016</date><author>Chen, Wenbo
   Xu, Shungou
   Jiang, Hai
   Weng, Tien-Hsiung
   Marino, Mario Donato
   Chen, Yi-Siang
   Li, Kuan-Ching</author></paper><paper><title>Fast point-based method of a computer-generated hologram for a
   triangle-patch model by using a graphics processing unit</title><abstract>The point-based method and fast-Fourier-transform-based method are
   commonly used for calculation methods of computer-generation holograms.
   This paper proposes a novel fast calculation method for a patch model,
   which uses the point-based method. The method provides a calculation
   time that is proportional to the number of patches but not to that of
   the point light sources. This means that the method is suitable for
   calculating a wide area covered by patches quickly. Experiments using a
   graphics processing unit indicated that the proposed method is about 8
   times or more faster than the ordinary point-based method. (C) 2016
   Optical Society of America</abstract><date>JAN 20 2016</date><author>Sugawara, Takuya
   Ogihara, Yuki
   Sakamoto, Yuji</author></paper><paper><title>Hough Forests for Real-Time, Automatic Device Localization in
   Fluoroscopic Images: Application to TAVR</title><abstract>A method for real-time localization of devices in fluoroscopic images is
   presented. Device pose is estimated using a Hough forest based detection
   framework. The method was applied to two types of devices used for
   transcatheter aortic valve replacement: a transesophageal echo (TEE)
   probe and prosthetic valve (PV). Validation was performed on clinical
   datasets, where both the TEE probe and PV were successfully detected in
   95.8% and 90.1% of images, respectively. TEE probe position and
   orientation errors were 1.42 +/- 0.79 mm and 2.59 degrees +/- 1.87
   degrees, while PV position and orientation errors were 1.04 +/- 0.77 mm
   and 2.90 degrees +/- 2.37 degrees. The Hough forest was implemented in
   CUDA C, and was able to generate device location hypotheses in less than
   50 ms for all experiments.</abstract><date>2015</date><author>Hatt, Charles R.
   Speidel, Michael A.
   Raval, Amish N.</author></paper><paper><title>HyperMix: An Open-Source Tool for Fast Spectral Unmixing on Graphics
   Processing Units</title><abstract>Spectral unmixing has been a popular technique for analyzing remotely
   sensed hyperspectral images. The goal of unmixing is to find a
   collection of pure spectral constituents (called endmembers) that can
   explain each (possibly mixed) pixel of the scene as a combination of
   endmembers, weighted by their coverage fractions in the pixel or
   abundances. Over the last years, many algorithms have been presented to
   address the three main parts of the spectral unmixing chain: 1)
   estimation of the number of endmembers; 2) identification of the
   endmember signatures; and 3) estimation of the per-pixel fractional
   abundances. However, to date, there is no standardized tool that
   integrates these algorithms in a unified framework. In this letter, we
   present HyperMix, an open-source tool for spectral unmixing that
   integrates different approaches for spectral unmixing and allows
   building unmixing chains in graphical fashion, so that the end-user can
   define one or several spectral unmixing chains in fully configurable
   mode. HyperMix provides efficient implementations of most of the
   algorithms used for spectral unmixing, so that the tool automatically
   recognizes if the computer has a graphics processing unit (GPU)
   available and optimizes the execution of these algorithms in the GPU.
   This allows for the execution of spectral unmixing chains on large
   hyperspectral scenes in computationally efficient fashion. The tool is
   available online from http://hypercomphypermix.blogspot.com.es and has
   been validated with real hyperspectral scenes, providing
   state-of-the-art unmixing results.</abstract><date>SEP 2015</date><author>Ignacio Jimenez, Luis
   Plaza, Antonio</author></paper><paper><title>An Improved Method for Predicting Linear B-cell Epitope Using Deep
   Maxout Networks</title><abstract>To establish a relation between an protein amino acid sequence and its
   tendencies to generate antibody response, and to investigate an improved
   in silico method for linear B-cell epitope (LBE) prediction. We present
   a sequence-based LBE predictor developed using deep maxout network (DMN)
   with dropout training techniques. A graphics processing unit (GPU) was
   used to reduce the training time of the model. A 10-fold
   cross-validation test on a large, non-redundant and experimentally
   verified dataset (Lbtope_Fixed_non_redundant) was performed to evaluate
   the performance. DMN-LBE achieved an accuracy of 68.33% and an area
   under the receiver operating characteristic curve (AUC) of 0.743,
   outperforming other prediction methods in the field. A web server,
   DMN-LBE, of the improved prediction model has been provided for public
   free use. We anticipate that DMN-LBE will be beneficial to vaccine
   development, antibody production, disease diagnosis, and therapy.</abstract><date>JUN 2015</date><author>Lian Yao
   Huang Ze Chi
   Ge Meng
   Pan Xian Ming</author></paper><paper><title>Hydrochemical characteristics of groundwater movement and evolution in
   the Xinli deposit of the Sanshandao gold mine using FCM and PCA methods</title><abstract>The objective of this study is to characterize the physicochemical
   properties of groundwater in the Sanshandao gold deposit, analyze the
   laws of motion and evolution of the groundwater system, and provide
   evidence for the design of undersea mining and safety warnings, using
   combined fuzzy c-means (FCM) and principal component analysis (PCA)
   methods. Thirteen physicochemical indicators and two isotopic indicators
   were surveyed from 100 fissure water samples and 13 typical water
   samples from Xinli, a deposit of the Sanshandao gold mine in China.
   First, PCA was used to extract four PCs (with explained variances 48,
   25, 10 and 7 %) and the characteristics of the hydrochemistry indicators
   of each PC were examined. The four PCs are identified as the processes
   of concentration and dilution, sericite rock orientation and
   carbonatization, potash feldspathization, and dissolution of carbon
   dioxide. Next, using the four PCs developed from PCA, 100 fissure water
   samples were grouped into four spatially continuous clusters using the
   FCM cluster method. With the aid of graphic technology, the four fuzzy
   clusters were determined as deep saline water, shallow saline water,
   bedrock brine and mixed wastewater, following careful analysis of the
   hydrochemical characteristics of each cluster combined with a related
   hydrogeological report. Finally, further analysis of the PCA and FCM
   results determined factors constituting the groundwater system,
   including sources of water, flow path, mixing zone, local fissure zone,
   and impact of mining on the groundwater system. This study shows that
   the combination of PCA and FCM is important for understanding
   hydrochemical characteristics of a groundwater system.</abstract><date>JUN 2015</date><author>Peng, Kang
   Li, Xibing
   Wang, Zewei</author></paper><paper><title>Participatory role of zinc in structural and functional characterization
   of bioremediase: a unique thermostable microbial silica leaching protein</title><abstract>A unique protein, bioremediase (UniProt Knowledgebase Accession No.:
   P86277), isolated from a hot spring bacterium BKH1 (GenBank Accession
   No.: FJ177512), has shown to exhibit silica leaching activity when
   incorporated to prepare bio-concrete material. Matrix-assisted laser
   desorption ionization mass spectrometry analysis suggests that
   bioremediase is 78 % homologous to bovine carbonic anhydrase II though
   it does not exhibit carbonic anhydrase-like activity. Bioinformatics
   study is performed for understanding the various physical and chemical
   parameters of the protein which predicts the involvement of zinc
   encircled by three histidine residues (His94, His96 and His119) at the
   active site of the protein. Isothermal titration calorimetric-based
   thermodynamic study on diethyl pyrocarbonate-modified protein recognizes
   the presence of Zn2+ in the enzyme moiety. Exothermic to endothermic
   transition as observed during titration of the protein with Zn2+
   discloses that there are at least two binding sites for zinc within the
   protein moiety. Addition of Zn2+ regains the activity of EDTA chelated
   bioremediase confirming the presence of extra binding site of Zn2+ in
   the protein moiety. Revival of folding pattern of completely unfolded
   urea-treated protein by Zn2+ explains the participatory role of zinc in
   structural stability of the protein. Restoration of the lambda (max) in
   intrinsic fluorescence emission study of the urea-treated protein by
   Zn2+ similarly confirms the involvement of Zn in the refolding of the
   protein. The utility of bioremediase for silica nanoparticles
   preparation is observed by field emission scanning electron
   microscopy.[GRAPHICS].</abstract><date>JUL 2015</date><author>Chowdhury, Trinath
   Sarkar, Manas
   Chaudhuri, Biswadeep
   Chattopadhyay, Brajadulal
   Halder, Umesh Chandra</author></paper><paper><title>An optimisation-based model for full-body upright reaching movements</title><abstract>An optimal simulation 3D model for full-body upright reaching movements
   was developed using graphic-based modelling tools (SimMechanics) to
   generate an inverse dynamics model of the skeleton and using
   parameterisation methods for a sensory motor controller. The adaptive
   weight coefficient of the cost function based on the final motor task
   error (i.e. distance between end-effector and target at the end of
   movement) was used to correct motor task error and physiological
   measurements (e.g. joint power, centre of mass displacement, etc.). The
   output of the simulation models using various cost functions were
   compared to experimental data from 15 healthy participants performing
   full-body upright reaching movements. The proposed method can reasonably
   predict full-body voluntary movements in terms of final posture, joint
   power, and movement of the centre of mass (COM) using simple algebraic
   calculations of inverse dynamics and forward kinematics instead of the
   complicated integrals of the forward dynamics. We found that the
   combination of several control strategies, i.e. minimising end-effector
   error, total joint power and body COM produced the best fit of the
   full-body reaching task.</abstract><date>JUN 11 2015</date><author>Sha, Daohang
   Thomas, James S.</author></paper><paper><title>Solving Graph Coloring Problem by Parallel Genetic Algorithm Using
   Compute Unified Device Architecture</title><abstract>Graph coloring problem (GCP) is a well-known NP-hard combinatorial
   optimization problem in graph theory. Solution for GCP often finds its
   applications to various engineering fields. So it is very important to
   find a feasible solution quickly. Recent years, Compute Unified Device
   Architecture (CUDA) show tremendous computational power by allowing
   parallel high performance computing. In this paper, we present a novel
   parallel genetic algorithm to solve the GCP based on CUDA. The
   initialization, crossover, mutation and selection operators are designed
   parallel in threads. Moreover, the performance of our algorithm is
   compared with the other graph coloring methods using standard DIMACS
   benchmarking graphs, and the comparison result shows that our algorithm
   is more competitive with computation time and graph instances size.</abstract><date>JUL 2015</date><author>Zhang Kai
   Qiu Ming
   Li Lin
   Liu Xiaoming</author></paper><paper><title>Biomechanical comparison of osteosynthesis with poly-L-lactic acid and
   titanium screw in intracapsular condylar fracture fixation: An
   experimental study</title><abstract>Background and Aims: The aim of this study was to compare the
   biomechanical stability of poly-L-lactic acid and titanium screws in the
   fixation of intracapsular condylar fractures, in 10 polyurethane
   hemimandibles. Materials and Methods: Artificial intracapsular fractures
   were created with a steel disk and electronic micromotor. The first
   group was fixed with 15 mm long self-tapping 2.0 mm system titanium
   screws and the second group was fixed with 15 mm long 2.4 mm
   bioresorbable screws. Linear loads of 25, 50, 75, 100 N was applied in
   anteroposterior direction to the hemimandibles and the data were
   transmitted directly from the load cell to a computer that shows
   emergent results of material characteristics under same forces as a
   graphic containing force and displacement. Results: The results show
   that there were no significant differences between the two methods, with
   25 N of loading. (P &gt; 0,05) The difference became significant with a
   higher value of loading. Conclusion: The results suggest that treatment
   with a single resorbable screw is not functionally stable as a single
   titanium screw.</abstract><date>SEP-OCT 2015</date><author>Omezli, M. M.
   Torul, D.
   Polat, M. E.
   Dayi, E.</author></paper><paper><title>A block-wise approximate parallel implementation for ART algorithm on
   CUDA-enabled GPU</title><abstract>Computed tomography (CT) has been widely used to acquire volumetric
   anatomical information in the diagnosis and treatment of illnesses in
   many clinics. However, the ART algorithm for reconstruction from
   under-sampled and noisy projection is still time-consuming. It is the
   goal of our work to improve a block-wise approximate parallel
   implementation for the ART algorithm on CUDA-enabled GPU to make the ART
   algorithm applicable to the clinical environment. The resulting method
   has several compelling features: (1) the rays are allotted into blocks,
   making the rays in the same block parallel; (2) GPU implementation
   caters to the actual industrial and medical application demand. We test
   the algorithm on a digital shepp-logan phantom, and the results indicate
   that our method is more efficient than the existing CPU implementation.
   The high computation efficiency achieved in our algorithm makes it
   possible for clinicians to obtain real-time 3D images.</abstract><date>2015</date><author>Fan, Zhongyin
   Xie, Yaoqin</author></paper><paper><title>GPU Parallel Implementation of Support Vector Machines for Hyperspectral
   Image Classification</title><abstract>Support vector machine (SVM) is considered as one of the most powerful
   classifiers for hyperspectral remote sensing images. However, it has
   high computational cost. In this paper, we propose a novel two-level
   parallel computing framework to accelerate the SVM-based classification
   by utilizing CUDA and OpenMP. For a binary SVM classifier, the kernel
   function is optimized on GPU, and then a second-order working set
   selection (WSS) procedure is employed and optimized especially for GPU
   to reduce the cost of communication between GPU and host. In addition to
   the parallel binary SVM classifier on GPU as data-processing level
   parallelization, a multiclass SVM is addressed by a "one-against-one"
   approach in OpenMP, and several binary SVM classifiers are run
   simultaneously to conduct task-level parallelization. The experimental
   results show that the solver in this framework offered a speedup of
   18.5x over the popular LIBSVM software in the training process for data
   with 200 bands, 13 classes, and 95 597 training samples, and 81.9x in
   the testing process for data with 103 bands, 9 classes, 1892 support
   vectors (SVs), and 42 776 testing samples.</abstract><date>OCT 2015</date><author>Tan, Kun
   Zhang, Junpeng
   Du, Qian
   Wang, Xuesong</author></paper><paper><title>Specific rehabilitation exercise for the treatment of patients with
   chronic low back pain</title><abstract>[Purpose] To evaluate the efficacy of our special rehabilitation method
   for patients with low back pain (LBP). [Subjects and Methods] All
   participants (n=33) received at least five individual 30-minute therapy
   sessions per week using the INFINITY method (R) and six group therapy
   sessions per week in a gymnasium and swimming pool, each lasting 30
   minutes and including the INFINITY method (R). The treatment lasted
   between four to seven weeks. Plantar function using a graphic method
   (computer plantography), graphical quantification of postural control
   during static standing (posturography), and pain were measured and
   evaluated before and after rehabilitation therapy. The INFINITY method
   (R) is a special rehabilitation method for patients with musculoskeletal
   problems. The method focuses on stabilization and strengthening of the
   trunk, dorsal and abdominal muscles, including the deep stabilization
   system which is closely linked with diaphragmatic breathing. It teaches
   the central nervous system to control muscles more precisely. [Results]
   Plantar functions, postural control in the upright stance and pain of
   LBP patients were significantly improved by 4-7 weeks of rehabilitation
   treatment with the INFINITY method (R). There were significant
   differences in all measured dependent variables of the patients between
   before and after treatment. [Conclusion] Rehabilitation therapy with the
   INFINITY method (R) positively influences body stabilization and pain in
   patients with problems of the lumbar spine. This method presents a new
   improved approach (with enhanced effect) to rehabilitation therapy for
   LBP patients.</abstract><date>AUG 2015</date><author>Tomanova, Michaela
   Lippert-Gruener, Marcela
   Lhotska, Lenka</author></paper><paper><title>Nonlinear Material Design Using Principal Stretches</title><abstract>The Finite Element Method is widely used for solid deformable object
   simulation in film, computer games, virtual reality and medicine.
   Previous applications of nonlinear solid elasticity employed materials
   from a few standard families such as linear corotational, nonlinear St.
   Venant-Kirchhoff, Neo-Hookean, Ogden or Mooney-Rivlin materials.
   However, the spaces of all nonlinear isotropic and anisotropic materials
   are infinite-dimensional and much broader than these standard materials.
   In this paper, we demonstrate how to intuitively explore the space of
   isotropic and anisotropic nonlinear materials, for design of animations
   in computer graphics and related fields. In order to do so, we first
   formulate the internal elastic forces and tangent stiffness matrices in
   the space of the principal stretches of the material. We then
   demonstrate how to design new isotropic materials by editing a single
   stress-strain curve, using a spline interface. Similarly, anisotropic
   (orthotropic) materials can be designed by editing three curves, one for
   each material direction. We demonstrate that modifying these curves
   using our proposed interface has an intuitive, visual, effect on the
   simulation. Our materials accelerate simulation design and enable visual
   effects that are difficult or impossible to achieve with standard
   nonlinear materials.</abstract><date>AUG 2015</date><author>Xu, Hongyi
   Sin, Funshing
   Zhu, Yufeng
   Barbic, Jernej</author></paper><paper><title>A digital tutor for learning fashion design</title><abstract>Computer-aided instruction (CAI) is an interactive instructional
   technique that effectively improves the students' understandings.
   However, CAI tool is currently lack of use in teaching and learning
   fashion design. The present research hence proposes a tool functions as
   a digital tutor poviding an intuitive visual aid for learning
   principles. The interfaces are designed based on two viewpoints: fashion
   style and design elements. Style Cognition Space and Style Cluster View
   are designed for visualizing fashion style knowledge, while Comparison
   View of Design Elements Effects and Elementary Attribute Space are
   designed for visualizing knowledge of design elements. Several skills of
   graphic user interface and information visualization are used, such as
   Details on demand, Portals, and Zoom. To obtain evaluations from users,
   three workshops of fashion design were held. Using questionnaires, all
   participants were asked to answer several questions about ease of use,
   effectiveness, and satisfaction. Whether professional lecturers or
   inexperienced novices at fashion, all users rate the system as highly
   satisfactory after use.</abstract><date>NOV 2015</date><author>Cheng, Ching-I
   Liu, Damon Shing-Min
   Lin, Charles Chia-Hsu</author></paper><paper><title>Position-Based Skinning for Soft Articulated Characters</title><abstract>In this paper, we introduce a two-layered approach addressing the
   problem of creating believable mesh-based skin deformation. For each
   frame, the skin is first deformed with a classic linear blend skinning
   approach, which usually leads to unsightly artefacts such as the
   well-known candy-wrapper effect and volume loss. Then we enforce some
   geometric constraints which displace the positions of the vertices to
   mimic the behaviour of the skin and achieve effects like volume
   preservation and jiggling. We allow the artist to control the amount of
   jiggling and the area of the skin affected by it. The geometric
   constraints are solved using a position-based dynamics (PBDs) schema. We
   employ a graph colouring algorithm for parallelizing the computation of
   the constraints. Being based on PBDs guarantees efficiency and real-time
   performances while enduring robustness and unconditional stability. We
   demonstrate the visual quality and the performance of our approach with
   a variety of skeleton-driven soft body characters.</abstract><date>SEP 2015</date><author>Abu Rumman, Nadine
   Fratarcangeli, Marco</author></paper><paper><title>A comparison of native GPU computing versus OpenACC for implementing
   flow-routing algorithms in hydrological applications</title><abstract>In recent years GPU computing has gained wide acceptance as a simple
   low-cost solution for speeding up computationally expensive processing
   in many scientific and engineering applications. However, in most cases
   accelerating a traditional CPU implementation for a GPU is a non-trivial
   task that requires a thorough refactorization of the code and specific
   optimizations that depend on the architecture of the device. OpenACC is
   a promising technology that aims at reducing the effort required to
   accelerate C/C+ +/Fortran code on an attached multicore device.
   Virtually with this technology the CPU code only has to be augmented
   with a few compiler directives to identify the areas to be accelerated
   and the way in which data has to be moved between the CPU and GPU. Its
   potential benefits are multiple: better code readability, less
   development time, lower risk of errors and less dependency on the
   underlying architecture and future evolution of the GPU technology. Our
   aim with this work is to evaluate the pros and cons of using OpenACC
   against native GPU implementations in computationally expensive
   hydrological applications, using the classic D8 algorithm of O'Callaghan
   and Mark for river network extraction as case-study. We implemented the
   flow accumulation step of this algorithm in CPU, using OpenACC and two
   different CUDA versions, comparing the length and complexity of the code
   and its performance with different datasets. We advance that although
   OpenACC can not match the performance of a CUDA optimized implementation
   (x 3.5 slower in average), it provides a significant performance
   improvement against a CPU implementation (x 2-6) with by far a simpler
   code and less implementation effort. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>FEB 2016</date><author>Rueda, Antonio J.
   Noguera, Jose M.
   Luque, Adrian</author></paper><paper><title>G-SHOT: GPU accelerated 3D local descriptor for surface matching</title><abstract>Signature of histogram of orientations (SHOT) as a novel 3D object local
   descriptor can achieves a good balance between descriptiveness and
   robustness in surface matching. However, its computation workload is
   much higher than the other 3D local descriptors. This paper investigates
   the development of suitable massively parallel algorithms on the
   graphics processing unit (GPU) for computation of high density and large
   scale 3D object local descriptors through two alternative parallel
   algorithms; one exact, and one approximate. Both algorithms exhibit
   outstanding speedup performance. The exact parallel descriptor comes at
   no cost to the descriptiveness, with a speedup factor of up to 40.70,
   with respect to the serial SHOT on the central processing unit (CPU).
   The approximate version achieves a corresponding speedup factor of up to
   54 with minor degradation in descriptiveness. The proposed algorithms
   are integrated into point cloud library (PCL), a open source project for
   image and point cloud. (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>JUL 2015</date><author>Hu, Linjia
   Nooshabadi, Saeid</author></paper><paper><title>Unifying Color and Texture Transfer for Predictive Appearance
   Manipulation</title><abstract>Recent color transfer methods use local information to learn the
   transformation from a source to an exemplar image, and then transfer
   this appearance change to a target image. These solutions achieve very
   successful results for general mood changes, e.g., changing the
   appearance of an image from sunny to overcast. However, such methods
   have a hard time creating new image content, such as leaves on a bare
   tree. Texture transfer, on the other hand, can synthesize such content
   but tends to destroy image structure. We propose the first algorithm
   that unifies color and texture transfer, outperforming both by
   leveraging their respective strengths. A key novelty in our approach
   resides in teasing apart appearance changes that can be modeled simply
   as changes in color versus those that require new image content to be
   generated. Our method starts with an analysis phase which evaluates the
   success of color transfer by comparing the exemplar with the source.
   This analysis then drives a selective, iterative texture transfer
   algorithm that simultaneously predicts the success of color transfer on
   the target and synthesizes new content where needed. We demonstrate our
   unified algorithm by transferring large temporal changes between
   photographs, such as change of season - e.g., leaves on bare trees or
   piles of snow on a street - and flooding.</abstract><date>JUL 2015</date><author>Okura, Fumio
   Vanhoey, Kenneth
   Bousseau, Adrien
   Efros, Alexei A.
   Drettakis, George</author></paper><paper><title>Solving the initial value problem of discrete geodesics</title><abstract>Computing geodesic paths and distances is a common operation in computer
   graphics and computeraided geometric design. The existing discrete
   geodesic algorithms are mainly designed to solve the boundary value
   problem, i.e., to find the shortest path between two given points. In
   this paper, we focus on the initial value problem, i.e., finding a
   uniquely determined geodesic path from a given point in any direction.
   Since the shortest paths do not provide the unique solution on triangle
   meshes, we solve the initial value problem in an indirect manner: given
   a fixed point and an initial tangent direction on a triangle mesh M, we
   first compute a geodesic curve (gamma) over cap on a piecewise smooth
   surface (M) over cap, which well approximates the input mesh M and can
   be constructed at little cost. Then, we solve a first-order ODE of the
   tangent vector using the fourth-order Runge-Kutta method, and parallel
   transport it along (gamma) over cap. When the geodesic curve reaches the
   boundary of the current patch, its tangent can be directly transported
   to the neighboring patch, thanks to the G(1)-continuity along the common
   boundary of two adjacent patches. Finally, once the geodesic curve
   (gamma) over cap is available, we project it onto the underlying mesh M,
   producing the discrete geodesic path gamma, which is guaranteed to be
   unique on M. It is worth noting that our method is different from the
   conventional methods of directly solving the geodesic equation (i.e., a
   secondorder ODE of the position) on piecewise smooth surfaces, which are
   difficult to implement due to the complicated representation of the
   geodesic equation involving Christoffel symbols. The proposed method,
   based on the first-order ODE of the tangent vector, is intuitive and
   easy for implementation. Our method is particularly useful for computing
   geodesic paths on low-resolution meshes which may have large and/or
   skinny triangles, since the conventional straightest geodesic paths are
   usually far from the ground truth. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>JAN 2016</date><author>Cheng, Peng
   Miao, Chunyan
   Liu, Yong-Jin
   Tu, Changhe
   He, Ying</author></paper><paper><title>Show Me My Health Plans: a study protocol of a randomized trial testing
   a decision support tool for the federal health insurance marketplace in
   Missouri</title><abstract>Background: The implementation of the ACA has improved access to quality
   health insurance, a necessary first step to improving health outcomes.
   However, access must be supplemented by education to help individuals
   make informed choices for plans that meet their individual financial and
   health needs.Methods/Design: Drawing on a model of information
   processing and on prior research, we developed a health insurance
   decision support tool called Show Me My Health Plans. Developed with
   extensive stakeholder input, the current tool (1) simplifies information
   through plain language and graphics in an educational component; (2)
   assesses and reviews knowledge interactively to ensure comprehension of
   key material; (3) incorporates individual and/or family health status to
   personalize out-of-pocket cost estimates; (4) assesses preferences for
   plan features; and (5) helps individuals weigh information appropriate
   to their interests and needs through a summary page with "good fit"
   plans generated from a tailored algorithm. The current study will
   evaluate whether the online decision support tool improves health
   insurance decisions compared to a usual care condition (the healthcare.
   gov marketplace website). The trial will include 362 individuals (181 in
   each group) from rural, suburban, and urban settings within a 90 mile
   radius around St. Louis. Eligibility criteria includes English-speaking
   individuals 18-64 years old who are eligible for the ACA marketplace
   plans. They will be computer randomized to view the intervention or
   usual care condition.Discussion: Presenting individuals with options
   that they can understand tailored to their needs and preferences could
   help improve decision quality. By helping individuals narrow down the
   complexity of health insurance plan options, decision support tools such
   as this one could prepare individuals to better navigate enrollment in a
   plan that meets their individual needs.</abstract><date>FEB 16 2016</date><author>Politi, Mary C.
   Barker, Abigail R.
   Kaphingst, Kimberly A.
   McBride, Timothy
   Shacham, Enbal
   Kebodeaux, Carey S.</author></paper><paper><title>Common-mask guided image reconstruction (c-MGIR) for enhanced 4D
   cone-beam computed tomography</title><abstract>Compared to 3D cone beam computed tomography (3D CBCT), the image
   quality of commercially available four-dimensional (4D) CBCT is severely
   impaired due to the insufficient amount of projection data available for
   each phase. Since the traditional Feldkamp-Davis-Kress (FDK)-based
   algorithm is infeasible for reconstructing high quality 4D CBCT images
   with limited projections, investigators had developed several
   compress-sensing (CS) based algorithms to improve image quality. The aim
   of this study is to develop a novel algorithm which can provide better
   image quality than the FDK and other CS based algorithms with limited
   projections. We named this algorithm 'the common mask guided image
   reconstruction' (c-MGIR).In c-MGIR, the unknown CBCT volume is
   mathematically modeled as a combination of phase-specific motion vectors
   and phase-independent static vectors. The common-mask matrix, which is
   the key concept behind the c-MGIR algorithm, separates the common static
   part across all phase images from the possible moving part in each phase
   image. The moving part and the static part of the volumes were then
   alternatively updated by solving two subminimization problems
   iteratively. As the novel mathematical transformation allows the static
   volume and moving volumes to be updated (during each iteration) with
   global projections and 'well' solved static volume respectively, the
   algorithm was able to reduce the noise and under-sampling artifact (an
   issue faced by other algorithms) to the maximum extent. To evaluate the
   performance of our proposed c-MGIR, we utilized imaging data from both
   numerical phantoms and a lung cancer patient. The qualities of the
   images reconstructed with c-MGIR were compared with (1) standard FDK
   algorithm, (2) conventional total variation (CTV) based algorithm, (3)
   prior image constrained compressed sensing (PICCS) algorithm, and (4)
   motion-map constrained image reconstruction (MCIR) algorithm,
   respectively. To improve the efficiency of the algorithm, the code was
   implemented with a graphic processing unit for parallel processing
   purposes.Root mean square error (RMSE) between the ground truth and
   reconstructed volumes of the numerical phantom were in the descending
   order of FDK, CTV, PICCS, MCIR, and c-MGIR for all phases. Specifically,
   the means and the standard deviations of the RMSE of FDK, CTV, PICCS,
   MCIR and c-MGIR for all phases were 42.64 +/- 6.5%, 3.63 +/- 0.83%,
   1.31% +/- 0.09%, 0.86% +/- 0.11% and 0.52 % +/- 0.02%, respectively. The
   image quality of the patient case also indicated the superiority of
   c-MGIR compared to other algorithms.The results indicated that
   clinically viable 4D CBCT images can be reconstructed while requiring no
   more projection data than a typical clinical 3D CBCT scan. This makes
   c-MGIR a potential online reconstruction algorithm for 4D CBCT, which
   can provide much better image quality than other available algorithms,
   while requiring less dose and potentially less scanning time.</abstract><date>DEC 7 2015</date><author>Park, Justin C.
   Zhang, Hao
   Chen, Yunmei
   Fan, Qiyong
   Li, Jonathan G.
   Liu, Chihray
   Lu, Bo</author></paper><paper><title>Introducing CURRENNT: The Munich Open-Source CUDA RecurREnt Neural
   Network Toolkit</title><abstract>In this article, we introduce CURRENNT, an open-source parallel
   implementation of deep recurrent neural networks (RNNs) supporting
   graphics processing units (GPUs) through NVIDIA's Computed Unified
   Device Architecture (CUDA). CURRENNT supports uni- and bidirectional
   RNNs with Long Short-Term Memory (LSTM) memory cells which overcome the
   vanishing gradient problem. To our knowledge, CURRENNT is the first
   publicly available parallel implementation of deep LSTM-RNNs. Benchmarks
   are given on a noisy speech recognition task from the 2013 2nd CHiME
   Speech Separation and Recognition Challenge, where LSTM-RNNs have been
   shown to deliver best performance. In the result, double digit speedups
   in bidirectional LSTM training are achieved with respect to a reference
   single-threaded CPU implementation. CURRENNT is available under the GNU
   General Public License from http://sourceforge.net/p/currennt.</abstract><date>MAR 2015</date><author>Weninger, Felix
   Bergmann, Johannes
   Schuller, Bjoern</author></paper><paper><title>Roofline Model Toolkit: A Practical Tool for Architectural and Program
   Analysis</title><abstract>We present preliminary results of the Roofline Toolkit for multicore,
   manycore, and accelerated architectures. This paper focuses on the
   processor architecture characterization engine, a collection of portable
   instrumented micro benchmarks implemented with Message Passing Interface
   (MPI), and OpenMP used to express thread-level parallelism. These
   benchmarks are specialized to quantify the behavior of different
   architectural features. Compared to previous work on performance
   characterization, these microbenchmarks focus on capturing the
   performance of each level of the memory hierarchy, along with
   thread-level parallelism, instruction-level parallelism and explicit
   SIMD parallelism, measured in the context of the compilers and run-time
   environments. We also measure sustained PCIe throughput with four GPU
   memory managed mechanisms. By combining results from the architecture
   characterization with the Roofline model based solely on architectural
   specifications, this work offers insights for performance prediction of
   current and future architectures and their software systems. To that
   end, we instrument three applications and plot their resultant
   performance on the corresponding Roofline model when run on a Blue
   Gene/Q architecture.</abstract><date>2015</date><author>Lo, Yu Jung
   Williams, Samuel
   Van Straalen, Brian
   Ligocki, Terry J.
   Cordery, Matthew J.
   Wright, Nicholas J.
   Hall, Mary W.
   Oliker, Leonid</author></paper><paper><title>Real-Time Calibration-Free C-Scan Images of the Eye Fundus Using Master
   Slave Swept Source Optical Coherence Tomography</title><abstract>Recently, we introduced a novel Optical Coherence Tomography (OCT)
   method, termed as Master Slave OCT (MS-OCT), specialized for delivering
   en-face images. This method uses principles of spectral domain
   interferometry in two stages. MS-OCT operates like a time domain OCT,
   selecting only signals from a chosen depth only while scanning the laser
   beam across the eye. Time domain OCT allows real time production of an
   en-face image, although relatively slowly. As a major advance, the
   Master Slave method allows collection of signals from any number of
   depths, as required by the user. The tremendous advantage in terms of
   parallel provision of data from numerous depths could not be fully
   employed by using multi core processors only. The data processing
   required to generate images at multiple depths simultaneously is not
   achievable with commodity multicore processors only. We compare here the
   major improvement in processing and display, brought about by using
   graphic cards. We demonstrate images obtained with a swept source at 100
   kHz (which determines an acquisition time [T-a] for a frame of 200x200
   pixels(2) of T-a =1.6 s). By the end of the acquired frame being
   scanned, using our computing capacity, 4 simultaneous en-face images
   could be created in T = 0.8 s. We demonstrate that by using graphic
   cards, 32 en-face images can be displayed in Td 0.3 s. Other faster
   swept source engines can be used with no difference in terms of T-d.
   With 32 images (or more), volumes can be created for 3D display, using
   en-face images, as opposed to the current technology where volumes are
   created using cross section OCT images.</abstract><date>2015</date><author>Bradu, Adrian
   Kapinchev, Konstantin
   Barnes, Fred
   Garway-Heath, David F.
   Rajendran, Ranjan
   Keane, Pearce
   Podoleanu, Adrian Gh</author></paper><paper><title>PrinCCes: Continuity-based geometric decomposition and systematic
   visualization of the void repertoire of proteins</title><abstract>Grooves and pockets on the surface, channels through the protein, the
   chambers or cavities, and the tunnels connecting the internal points to
   each other or to the external fluid environment are fundamental
   determinants of a wide range of biological functions. PrinCCes (Protein
   internal Channel 82 Cavity estimation) is a computer program supporting
   the visualization of voids. It includes a novel algorithm for the
   decomposition of the entire void volume of the protein or protein
   complex to individual entities. The decomposition is based on
   continuity. An individual void is defined by uninterrupted extension in
   space: a spherical probe can freely move between any two internal
   locations of a continuous void. Continuous voids are detected
   irrespective of their topological complexity, they may contain any
   number of holes and bifurcations. The voids of a protein can be
   visualized one by one or in combinations as triangulated surfaces. The
   output is automatically exported to free VMD (Visual Molecular Dynamics)
   or Chimera software, allowing the 3D rotation of the surfaces and the
   production of publication quality images. PrinCCes with graphic user
   interface and command line versions are available for MS Windows and
   Linux. (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>NOV 2015</date><author>Czirjak, Gabor</author></paper><paper><title>Contour Gradient Tree for Automatic Extraction of Salient Object
   Surfaces from 3D Imaging Data</title><abstract>Isosurface extraction is one of the most popular techniques for
   visualizing scalar volume data. However, volume data contains infinitely
   many isosurfaces. Furthermore, a single isosurface might contain many
   connected components, or contours, with each representing a different
   object surface. Hence, it is often a tedious and time-consuming manual
   process to find and extract contours that are interesting to users. This
   paper describes a novel method for automatically extracting salient
   contours from volume data. For this purpose, we propose a contour
   gradient tree (CGT) that contains the information of salient contours
   and their saliency magnitude. We organize the CGT in a hierarchical way
   to generate a sequence of contours in saliency order. Our method was
   applied to various medical datasets. Experimental results show that our
   method can automatically extract salient contours that represent regions
   of interest in the data.</abstract><date>NOV 2015</date><author>Sohn, Bong-Soo</author></paper><paper><title>Massively Parallel Change Detection with Application to Visual Quality
   Control</title><abstract>Our aim in this paper is to extend the results on parallel change
   detection recently discussed in [15], where such a detector has been
   proposed. Here, we emphasize its adaptive abilities to follow changing
   background and relax some theoretical assumptions on random errors,
   extending possible applications of the detector. We also discuss its
   implementation in NVidia CUDA technology and provide results of its
   extensive testing when applied to copper visual quality control, which
   is a challenge due to the need for massively parallel calculations in
   real-time.</abstract><date>2015</date><author>Rafajlowicz, Ewaryst
   Nizynski, Karol</author></paper><paper><title>Consistent Scene Editing by Progressive Difference Images</title><abstract>Even though much research was dedicated to the acceleration of
   consistent, progressive light transport simulations, the computation of
   fully converged images is still very time-consuming. This is
   problematic, as for the practical use in production pipelines, the rapid
   editing of lighting effects is important. While previous approaches
   restart the simulation with every scene manipulation, we make use of the
   coherence between frames before and after a modification in order to
   accelerate convergence of the context that remained similar. This is
   especially beneficial if a scene is edited that has already been
   converging for a long time, because much of the previous result can be
   reused, e.g., sharp caustics cast or received by the unedited scene
   parts. In its essence, our method performs the scene modification
   stochastically by predicting and accounting for the difference image. In
   addition, we employ two heuristics to handle cases in which stochastic
   removal is likely to lead to strong noise. Typical scene interactions
   can be broken down into object adding and removal, material
   substitution, camera movement and light editing, which we all examine in
   a number of test scenes both qualitatively and quantitatively. As we
   focus on caustics, we chose stochastic progressive photon mapping as the
   underlying light transport algorithm. Further, we show preliminary
   results of bidirectional path tracing and vertex connection and merging.</abstract><date>JUL 2015</date><author>Guenther, Tobias
   Grosch, Thorsten</author></paper><paper><title>Anisotropic hyperelastic behavior of soft biological tissues</title><abstract>Constitutive laws are fundamental to the studies of the mechanically
   dominated clinical interventions involving soft biological tissues which
   show a highly anisotropic hyperelastic mechanical properties. The
   purpose of this paper was to develop an improved constitutive law based
   on the Holzapfel-Gasser-Ogden's model: to replace the isotropic part
   with Gent constitutive law so as to model the noncollagenous matrix of
   the media due to its generality and capability to reproduce the
   Neo-Hookean model. This model is implemented into an in-house finite
   element program. A uniaxial tension test is considered to study the
   influence of material parameter[GRAPHICS]in Gent model
   and[GRAPHICS]which represents the angle between the collagen fibers and
   the circumferential direction. A simulation of an adventitial strip
   specimen under tension is performed to show the applicability of this
   constitutive law.</abstract><date>OCT 3 2015</date><author>Chen, Z. -W.
   Joli, P.
   Feng, Z. -Q.</author></paper><paper><title>Modeling Human Serum Albumin Tertiary Structure To Teach Upper-Division
   Chemistry Students Bioinformatics and Homology Modeling Basics</title><abstract>A homology modeling laboratory experiment has been developed for an
   introductory molecular modeling course for upper-division undergraduate
   chemistry students. With this experiment, students gain practical
   experience in homology model preparation and assessment as well as in
   protein visualization using the educational version of PyMOL
   state-of-the-art molecular graphics. Students create a human serum
   albumin homology model with relatively high resolution at 1.77 angstrom
   (heavy atom model) and with reasonable values for bond lengths, angles,
   and dihedrals. The suggested tasks integrate different fundamental
   aspects of structural biology and protein modeling. Ramachandran plots
   and side chain rotamers are discussed. The assignments are shown to be
   good not only to teach homology modeling basics, but also to introduce
   several other concepts of structural bioinformatics such as protein
   sequence data mining, basic local alignment search tool (BLAST), and
   multiple sequence alignment. Homology modeling is demonstrated to be a
   cornerstone in molecular docking and drug design projects if the crystal
   structure of the protein of interest is not yet determined.</abstract><date>JUL 2015</date><author>Petrovic, Dusan
   Zlatovic, Mario</author></paper><paper><title>Interactively Inspection Layers of CT Datasets on CUDA-Based Volume
   Rendering</title><abstract>With the fast growing size and dimensionality of scientific datasets,
   inspection and rendering data features has become an important topic
   when traversing through it. We propose opacity transfer function with
   trapezoid shape having an overlapping region to extract different layers
   when using Compute unified device architecture (CUDA)-based volume
   ray-casting. We define different tools such as probe for 3D inspection
   and virtual lenses for 2D inspection of inner layers in
   Region-of-interest (ROI). We verify the effectiveness which allowing
   inspection of structures both interior layers in ROI and exterior
   semi-transparent ones at interactive frame rates.</abstract><date>APR 2015</date><author>Luo Yanhong
   Tian Jun
   Luo Yanlin</author></paper><paper><title>Investigation of highly efficient algorithms for solving linear
   equations in the discontinuous deformation analysis method</title><abstract>Large-scale engineering computing using the discontinuous deformation
   analysis (DDA) method is time-consuming, which hinders the application
   of the DDA method. The simulation result of a typical numerical example
   indicates that the linear equation solver is a key factor that affects
   the efficiency of the DDA method. In this paper, highly efficient
   algorithms for solving linear equations are investigated, and two
   modifications of the DDA programme are presented. The first modification
   is a linear equation solver with high efficiency. The block Jacobi (BJ)
   iterative method and the block conjugate gradient with Jacobi
   pre-processing (Jacobi-PCG) iterative method are introduced, and the key
   operations are detailed, including the matrix-vector product and the
   diagonal matrix inversion. Another modification consists of a parallel
   linear equation solver, which is separately constructed based on the
   multi-thread and CPU-GPU heterogeneous platforms with OpenMP and CUDA,
   respectively. The simulation results from several numerical examples
   using the modified DDA programme demonstrate that the Jacobi-PCG is a
   better iterative method for large-scale engineering computing and that
   adoptive parallel strategies can greatly enhance computational
   efficiency. Copyright (c) 2015 John Wiley &amp; Sons, Ltd.</abstract><date>MAR 2016</date><author>Fu, Xiaodong
   Sheng, Qian
   Zhang, Yonghui
   Chen, Jian</author></paper><paper><title>Inflammation Markers and Major Depressive Disorder in Patients With
   Chronic Heart Failure: Results From the Sertraline Against Depression
   and Heart Disease in Chronic Heart Failure Study</title><abstract>BackgroundMajor depressive disorder (MDD) and chronic heart failure
   (CHF) have in common heightening states of inflammation, manifested by
   elevated inflammation markers such as C-reactive protein. This study
   compared inflammatory biomarker profiles in patients with CHF and MDD to
   those without MDD.MethodsThe study recruited patients admitted to
   inpatient care for acute heart failure exacerbations, after psychiatric
   diagnostic interview. Patients with Beck Depression Inventory (BDI)
   scores lower than 10 and with no history of depression served as the
   nondepressed reference group (n = 25). MDD severity was defined as
   follows: mild (BDI 10-15; n = 48), moderate (BDI 16-23; n = 51), and
   severe (BDI &gt;= 24; n = 33). A Bio-Plex assay measured 18 inflammation
   markers. Ordinal logistic models were used to examine the association of
   MDD severity and biomarker levels.ResultsAdjusting for age, sex, statin
   use, body mass index, left ventricular ejection fraction, tobacco use,
   and New York Heart Association class, the MDD overall group variable was
   significantly associated with elevated interleukin (IL)-2 (p = .019),
   IL-4 (p = .020), IL-6 (p = .026), interferon-gamma (p = .010), monocyte
   chemoattractant protein 1 (p = .002), macrophage inflammatory protein 1
   beta (p = .003), and tumor necrosis factor alpha (p = .004). MDD
   severity subgroups had a greater probability of elevated IL-6, IL-8,
   interferon-gamma, monocyte chemoattractant protein 1, macrophage
   inflammatory protein 1 beta, and tumor necrosis factor alpha compared
   with nondepressed group. The nondepressed group had greater probability
   of elevated IL-17 (p &lt; .001) and IL-1 beta (p &lt; .01).ConclusionsMDD in
   patients with CHF was associated with altered inflammation marker levels
   compared with patients with CHF who had no depression. Whether effective
   depression treatment will normalize the altered inflammation marker
   levels requires further study.Trial Registration:
   ClinicalTrials.gov[GRAPHICS].</abstract><date>SEP 2015</date><author>Xiong, Glen L.
   Prybol, Kevin
   Boyle, Stephen H.
   Hall, Russell
   Streilein, Robert D.
   Steffens, David C.
   Krishnan, Ranga
   Rogers, Joseph G.
   O'Connor, Christopher M.
   Jiang, Wei</author></paper><paper><title>Lead optimization attrition analysis (LOAA): a novel and general
   methodology for medicinal chemistry</title><abstract>Herein, we report a novel and general method, lead optimization
   attrition analysis (LOAA), to benchmark two distinct small-molecule lead
   series using a relatively unbiased, simple technique and commercially
   available software. We illustrate this approach with data collected
   during lead optimization of two independent oncology programs as a case
   study. Easily generated graphics and attrition curves enabled us to
   calibrate progress and support go/no go decisions on each program. We
   believe that this data-driven technique could be used broadly by
   medicinal chemists and management to guide strategic decisions during
   drug discovery.</abstract><date>AUG 2015</date><author>Munson, Mark
   Lieberman, Harvey
   Tserlin, Elina
   Rocnik, Jennifer
   Ge, Jie
   Fitzgerald, Maria
   Patel, Vinod
   Garcia-Echeverria, Carlos</author></paper><paper><title>BPLG: A Tuned Butterfly Processing Library for GPU Architectures</title><abstract>In order to increase the efficiency of existing software many works are
   incorporating GPU processing. However, despite the current advances in
   GPU languages and tools, taking advantage of their parallel architecture
   is still far more complex than programming standard multi-core CPUs. In
   this work, we present a library based on a set of building blocks that
   enable to easily design well-known algorithms with little effort. More
   specifically, we implement butterfly algorithms with this library, that
   is, a set of orthogonal signal transforms and an algorithm to solve
   tridiagonal equations systems. Thanks to the parametrization of the
   building blocks, the library can be easily tuned depending on the
   desired GPU architecture. This generic approach can be used to easily
   design these GPU algorithms while obtaining competitive performance on
   two recent NVIDIA GPU architectures, which results specially interesting
   from the productivity point of view.</abstract><date>DEC 2015</date><author>Lobeiras, J.
   Amor, M.
   Doallo, R.</author></paper><paper><title>The effects of static avatars on impression formation across different
   contexts on social networking sites</title><abstract>When making judgments about others, people use whatever social
   information is available in online environments. Such is the case for
   forming impressions of others. One type of such social information is a
   user's avatar. This study examines different types of avatars
   (photographs, cartoon humans, and nonhumans) created for task, social or
   dating/romantic situations to study the effect of avatar type on
   judgments of uncertainty and task-specific attractiveness. Data suggest
   various patterns of uncertainty and attractiveness in these situations.
   Both the graphic from of an avatar and the context of impression
   formation have effects on subsequent impression formation. Judgments of
   uncertainty and attraction were affected by both the graphic from of
   avatar and by the consistency between the context of impression
   formation and the attractiveness cues of the avatar. These findings are
   discussed as are implications for future research. (C) 2015 Elsevier
   Ltd. All rights reserved.</abstract><date>DEC 2015</date><author>Westerman, David
   Tamborini, Ron
   Bowman, Nicholas David</author></paper><paper><title>SPARSE: Seed Point Auto-Generation for Random Walks Segmentation
   Enhancement in medical inhomogeneous targets delineation of
   morphological MR and CT images</title><abstract>In medical image processing, robust segmentation of inhomogeneous
   targets is a challenging problem. Because of the complexity and
   diversity in medical images, the commonly used semiautomatic
   segmentation algorithms usually fail in the segmentation of
   inhomogeneous objects. In this study, we propose a novel algorithm
   imbedded with a seed point autogeneration for random walks segmentation
   enhancement, namely SPARSE, for better segmentation of inhomogeneous
   objects. With a few user-labeled points, SPARSE is able to generate
   extended seed points by estimating the probability of each voxel with
   respect to the labels. The random walks algorithm is then applied upon
   the extended seed points to achieve improved segmentation result. SPARSE
   is implemented under the compute unified device architecture (CUDA)
   programming environment on graphic processing unit (GPU) hardware
   platform. Quantitative evaluations are performed using clinical
   homogeneous and inhomogeneous cases. It is found that the SPARSE can
   greatly decrease the sensitiveness to initial seed points in terms of
   location and quantity, as well as the freedom of selecting parameters in
   edge weighting function. The evaluation results of SPARSE also
   demonstrate substantial improvements in accuracy and robustness to
   inhomogeneous target segmentation over the original random walks
   algorithm.</abstract><date>2015</date><author>Chen, Haibin
   Zhen, Xin
   Gu, Xuejun
   Yan, Hao
   Cervino, Laura
   Xiao, Yang
   Zhou, Linghong</author></paper><paper><title>CUDA-based High-performance computing of the S-BPF Algorithm with
   No-waiting Pipelining</title><abstract>The backprojection-filtration (BPF) algorithm has become a good solution
   for local reconstruction in cone-beam computed tomography (CBCT).
   However, the reconstruction speed of BPF is a severe limitation for
   clinical applications. The selective-backprojection filtration (S-BPF)
   algorithm is developed to improve the parallel performance of BPF by
   selective backprojection. Furthermore, the general-purpose graphics
   processing unit (GP-GPU) is a popular tool for accelerating the
   reconstruction. Much work has been performed aiming for the optimization
   of the cone-beam back-projection. As the cone-beam back-projection
   process becomes faster, the data transportation holds a much bigger time
   proportion in the reconstruction than before. This paper focuses on
   minimizing the total time in the reconstruction with the S-BPF algorithm
   by hiding the data transportation among hard disk, CPU and GPU. And
   based on the analysis of the S-BPF algorithm, some strategies are
   implemented: (1) the asynchronous calls are used to overlap the
   implemention of CPU and GPU, (2) an innovative strategy is applied to
   obtain the DBP image to hide the transport time effectively, (3) two
   streams for data transportation and calculation are synchronized by the
   cudaEvent in the inverse of finite Hilbert transform on GPU. Our main
   contribution is a smart reconstruction of the S-BPF algorithm with GPU's
   continuous calculation and no data transportation time cost. a 512(3)
   volume is reconstructed in less than 0.7 second on a single Tesla-based
   K20 GPU from 182 views projection with 512(2) pixel per projection. The
   time cost of our implementation is about a half of that without the
   overlap behavior.</abstract><date>2015</date><author>Deng Lin
   Yan Bin
   Chang Qingmei
   Han Yu
   Zhang Xiang
   Xi Xiaoqi
   Li Lei</author></paper><paper><title>Illumination-driven Mesh Reduction for Accelerating Light Transport
   Simulations</title><abstract>Progressive light transport simulations aspire a physically-based,
   consistent rendering to obtain visually appealing illumination effects,
   depth and realism. Thereby, the handling of large scenes is a difficult
   problem, as in typical scene subdivision approaches the parallel
   processing requires frequent synchronization due to the bouncing of
   light throughout the scene. In practice, however, only few object parts
   noticeably contribute to the radiance observable in the image, whereas
   large areas play only a minor role. In fact, a mesh simplification of
   the latter can go unnoticed by the human eye. This particular importance
   to the visible radiance in the image calls for an output-sensitive mesh
   reduction that allows to render originally out-of-core scenes on a
   single machine without swapping of memory. Thus, in this paper, we
   present a preprocessing step that reduces the scene size under the
   constraint of radiance preservation with focus on high-frequency effects
   such as caustics. For this, we perform a small number of preliminary
   light transport simulation iterations. Thereby, we identify mesh parts
   that contribute significantly to the visible radiance in the scene, and
   which we thus preserve during mesh reduction.</abstract><date>JUL 2015</date><author>Reich, Andreas
   Guenther, Tobias
   Grosch, Thorsten</author></paper><paper><title>Communication and computation optimization of concurrent kernels using
   kernel coalesce on a GPU</title><abstract>General purpose computation on graphics processing unit (GPU) is rapidly
   entering into various scientific and engineering fields. Many
   applications are being ported onto GPUs for better performance. Various
   optimizations, frameworks, and tools are being developed for effective
   programming of GPU. As part of communication and computation
   optimizations for GPUs, this paper proposes and implements an
   optimization method called as kernel coalesce that further enhances GPU
   performance and also optimizes CPU to GPU communication time. With
   kernel coalesce methods, proposed in this paper, the kernel launch
   overheads are reduced by coalescing the concurrent kernels and data
   transfers are reduced incase of intermediate data generated and used
   among kernels. Computation optimization on a device (GPU) is performed
   by optimizing the number of blocks and threads launched by tuning it to
   the architecture. Block level kernel coalesce method resulted in
   prominent performance improvement on a device without the support for
   concurrent kernels. Thread level kernel coalesce method is better than
   block level kernel coalesce method when the design of a grid structure
   (i.e., number of blocks and threads) is not optimal to the device
   architecture that leads to underutilization of the device resources.
   Both the methods perform similar when the number of threads per block is
   approximately the same in different kernels, and the total number of
   threads across blocks fills the streaming multiprocessor (SM) capacity
   of the device. Thread multi-clock cycle coalesce method can be chosen if
   the programmer wants to coalesce more than two concurrent kernels that
   together or individually exceed the thread capacity of the device. If
   the kernels have light weight thread computations, multi clock cycle
   kernel coalesce method gives better performance than thread and block
   level kernel coalesce methods. If the kernels to be coalesced are a
   combination of compute intensive and memory intensive kernels, warp
   interleaving gives higher device occupancy and improves the performance.
   Multi clock cycle kernel coalesce method for micro-benchmark1 considered
   in this paper resulted in 10-40% and 80-92% improvement compared with
   separate kernel launch, without and with shared input and intermediate
   data among the kernels, respectively, on a Fermi architecture device,
   that is, GTX 470. A nearest neighbor (NN) kernel from Rodinia benchmark
   is coalesced to itself using thread level kernel coalesce method and
   warp interleaving giving 131.9% and 152.3% improvement compared with
   separate kernel launch and 39.5% and 36.8% improvement compared with
   block level kernel coalesce method, respectively. Copyright (C) 2013
   John Wiley &amp; Sons, Ltd.</abstract><date>JAN 2015</date><author>Neelima, B.
   Reddy, G. Ram Mohana
   Raghavendra, Prakash S.</author></paper><paper><title>Fast computation of eikonal and transport equations on graphics
   processing units computer architectures</title><abstract>Eikonal models have been widely used for traveltime computations in the
   field of seismic imaging, but they are often criticized for having low
   accuracy and poor resolution of the output image. Including amplitude
   information can provide higher model resolution and accuracy of the
   images. We have developed a new approach for computing eikonal
   traveltimes and amplitudes, and we implemented it for multicore computer
   processing unit and graphics processing unit computer architectures.
   Traveltimes and amplitudes are computed simultaneously in iterations of
   the 3D velocity model. This is achieved by extending a fast sweeping
   method by computing amplitudes directly after the traveltimes with
   upwind numerical difference schemes. By performing the extra
   computations simultaneously with the traveltimes, the additional cost
   for the amplitude and raypaths is low. We tested our method on synthetic
   3D data sets to compute traveltimes, amplitudes, and raypaths, from
   which the Helmholtz Green's function was assembled. Using a grid of 1243
   nodes, the computations were performed in less than 1 s. The proposed
   method could work as a feasible alternative to full waveform modeling in
   seismic applications, which suffers from demanding computations because
   it requires several order of magnitudes shorter computing times.</abstract><date>SEP-OCT 2015</date><author>Noack, Marcus
   Gillberg, Tor</author></paper><paper><title>Hybrid Local FEM/Global LISA Modeling of Guided Wave Propagation and
   Interaction with Damage in Composite Structures</title><abstract>This paper presents a hybrid modeling technique for the efficient
   simulation of guided wave propagation and interaction with damage in
   composite structures. This hybrid approach uses a local finite element
   model (FEM) to compute the excitability of guided waves generated by
   piezoelectric transducers, while the global domain wave propagation,
   wave-damage interaction, and boundary reflections are modeled with the
   local interaction simulation approach (LISA).A small-size multi-physics
   FEM with non-reflective boundaries (NRB) was built to obtain the
   excitability information of guided waves generated by the transmitter.
   Frequency-domain harmonic analysis was carried out to obtain the
   solution for all the frequencies of interest. Fourier and inverse
   Fourier transform and frequency domain convolution techniques are used
   to obtain the time domain 3-D displacement field underneath the
   transmitter under an arbitrary excitation. This 3-D displacement field
   is then fed into the highly efficient time domain LISA simulation module
   to compute guided wave propagation, interaction with damage, and
   reflections at structural boundaries. The damping effect of composite
   materials was considered in the modified LISA formulation. The grids for
   complex structures were generated using commercial FEM preprocessors and
   converted to LISA connectivity format. Parallelization of the global
   LISA solution was achieved through Compute Unified Design Architecture
   (CUDA) running on Graphical Processing Unit (GPU). The multi-physics
   local FEM can reliably capture the detailed dimensions and local
   dynamics of the piezoelectric transducers. The global domain LISA can
   accurately solve the 3-D elastodynamic wave equations in a highly
   efficient manner. By combining the local FEM with global LISA, the
   efficient and accurate simulation of guided wave structural health
   monitoring procedure is achieved. Two numerical case studies are
   presented: (1) wave propagation in a unidirectional CFRP composite
   plate; (2) wave propagation in a stiffened cross-ply CFRP plate with
   delamination.</abstract><date>2015</date><author>Shen, Yanfeng
   Cesnik, Carlos E. S.</author></paper><paper><title>Shuffle up and deal: accelerating GPGPU Monte Carlo simulation with
   application to option pricing</title><abstract>In this paper, we demonstrate some speedup opportunity regarding Monte
   Carlo simulation on graphic processing unit architecture, with financial
   application. We leverage on the possibility of reducing the volume of
   actually generated random numbers, by replacing the generation phase
   with some shuffling using Compute Unified Device Architecture's built-in
   shuffle instructions. We will study various shuffling patterns and
   duration, elect the best among them with regard to induced correlation,
   using Granger causality test. We will then study the accuracy and
   variance of results actually achieved by our general-purpose computing
   on graphic processing unit shuffled Monte-Carlo, exhibiting a
   computational time reduced by half while error remains marginal.
   Copyright (c) 2015John Wiley &amp; Sons, Ltd.</abstract><date>DEC 10 2015</date><author>Cassagnes, Aurelien
   Chen, Yu
   Ohashi, Hirotada</author></paper><paper><title>Wavelets-based smoothness comparisons for volume data</title><abstract>In this study, the authors describe an objective smoothness assessment
   method for volume data. The metric can predict the extent of the
   difference in smoothness between a reference model, which may not be of
   perfect quality, and a distorted version. The proposed metric is based
   on the wavelet characterisation of Besov function spaces. The comparison
   of Besov norms between two models can resolve the global and local
   differences in smoothness between them. Experimental results from volume
   datasets with smoothing and sharpening operations demonstrate its
   effectiveness. By comparing direct volume rendered images, the
   experimental results show that the proposed smoothness index correlates
   well with human perceived vision. Finally, the metric can help the
   analyse compression distortions when they compare volume data with
   different smoothness.</abstract><date>DEC 2015</date><author>Lee, Mong-Shu
   Ueng, Shyh-Kuang
   Lin, Jhih-Jhong</author></paper><paper><title>ANALYSIS AND RESTITUTION VIA COMPUTER GRAPHICS OF THE ARCHITECTURAL
   SPACE OF THE MAIN STAIRCASE OF THE CASTLE OF VELEZ BLANCO (ALMERIA)</title><abstract>The aim of this project was to analyze and restore the architectural
   space of the main staircase of the castle of Velez using Rendering (a
   process of generating an image from a model by means of computer
   programs) in order to recreate the spatial, light and material qualities
   of the original room. Virtually the only intact structures that remain
   in the building are the walls. Therefore, the main focus of this
   research was the identification and creation of a virtual reconstruction
   of all the architectural elements that initially formed part of the main
   staircase space. The author has carried out a historical study and has
   made a three-dimensional survey of the current state of the remains of
   the staircase along with the pieces of marble from this room that have
   been preserved but disassembled in other areas of the castle.</abstract><date>JUL-SEP 2015</date><author>Motos Diaz, Ismael</author></paper><paper><title>Interoperability of product and manufacturing information using ontology</title><abstract>The communication among different computer-aided design/computer-aided
   manufacturing/analysis systems usually involves huge amount of
   information and heterogeneous data formats. Inefficient communication
   between computer-aided design and computer-aided design/computer-aided
   manufacturing/analysis systems leads to information loss, which will
   lead to design and fabrication errors. Therefore, a bridge is needed to
   be made between the systems so that models can be transported easily and
   efficiently from the computer-aided design system to another system. To
   carry out successful manufacturing operation, not only parametric
   information and machining information but also non-parametric
   information is needed to be transported. The non-parametric information
   are geometric dimensioning and tolerance, notes, and other annotation to
   three-dimensional models which, altogether, are regarded as product and
   manufacturing information. Exchange of product and manufacturing
   information is to be done not only syntactically but also semantically.
   It is necessary to identify ontology and semantics early in the process
   so that the system is able to view, understand, and define the concepts
   relating to parametric and non-parametric information to improve
   interoperability and overcome data sharing problems. Neutral formats
   such as STEP, Initial Graphics Exchange Specifications, or prominent
   interoperability-related research works offer integration of other
   systems with computer-aided design. However, exchange of non-parametric
   information or product and manufacturing information has not been
   considered in these research works. To achieve semantic integration of
   information between computer-aided design and computer-aided
   design/computer-aided manufacturing/analysis systems, this article
   proposes a data integration method for bothparametric information of a
   model and non-parametric information (product and manufacturing
   information)by constructing a neutral platform using OWL ontology, and
   also shows a pilot implementation that verifies interoperability between
   commercial computer-aided design and computer-aided
   design/computer-aided manufacturing/analysis systems.</abstract><date>SEP 2015</date><author>Ahmed, Fahim
   Han, Soonhung</author></paper><paper><title>MoleCollar and Tunnel Heat Map Visualizations for Conveying
   Spatio-Temporo-Chemical Properties Across and Along Protein Voids</title><abstract>Studying the characteristics of proteins and their inner void space,
   including their geometry, physico-chemical properties and dynamics are
   instrumental for evaluating the reactivity of the protein with other
   small molecules. The analysis of long simulations of molecular dynamics
   produces a large number of voids which have to be further explored and
   evaluated. In this paper we propose three new methods: two of them
   convey important properties along the long axis of a selected void
   during molecular dynamics and one provides a comprehensive picture
   across the void. The first two proposed methods use a specific heat map
   to present two types of information: an overview of all detected tunnels
   in the dynamics and their bottleneck width and stability over time, and
   an overview of a specific tunnel in the dynamics showing the bottleneck
   position and changes of the tunnel length over time. These methods help
   to select a small subset of tunnels, which are explored individually and
   in detail. For this stage we propose the third method, which shows in
   one static image the temporal evolvement of the shape of the most
   critical tunnel part, i.e., its bottleneck. This view is enriched with
   abstract depictions of different physicochemical properties of the amino
   acids surrounding the bottleneck. The usefulness of our newly proposed
   methods is demonstrated on a case study and the feedback from the domain
   experts is included. The biochemists confirmed that our novel methods
   help to convey the information about the appearance and properties of
   tunnels in a very intuitive and comprehensible manner.</abstract><date>JUN 2015</date><author>Byska, J.
   Jurcik, A.
   Groeller, M. E.
   Viola, I.
   Kozlikova, B.</author></paper><paper><title>Parallel Hyperspectral Coded Aperture for Compressive Sensing on GPUs</title><abstract>The application of compressive sensing (CS) to hyperspectral images is
   an active area of research over the past few years, both in terms of the
   hardware and the signal processing algorithms. However, CS algorithms
   can be computationally very expensive due to the extremely large volumes
   of data collected by imaging spectrometers, a fact that compromises
   their use in applications under real-time constraints. This paper
   proposes four efficient implementations of hyperspectral coded aperture
   (HYCA) for CS, two of them termed P-HYCA and P-HYCA-FAST and two
   additional implementations for its constrained version (CHYCA), termed
   P-CHYCA and P-CHYCA-FAST on commodity graphics processing units (GPUs).
   HYCA algorithm exploits the high correlation existing among the spectral
   bands of the hyperspectral data sets and the generally low number of
   end-members needed to explain the data, which largely reduces the number
   of measurements necessary to correctly reconstruct the original data.
   The proposed P-HYCA and P-CHYCA implementations have been developed
   using the compute unified device architecture (CUDA) and the cuFFT
   library. Moreover, this library has been replaced by a fast iterative
   method in the P-HYCA-FAST and P-CHYCA-FAST implementations that leads to
   very significant speedup factors in order to achieve real-time
   requirements. The proposed algorithms are evaluated not only in terms of
   reconstruction error for different compressions ratios but also in terms
   of computational performance using two different GPU architectures by
   NVIDIA: 1) GeForce GTX 590; and 2) GeForce GTX TITAN. Experiments are
   conducted using both simulated and real data revealing considerable
   acceleration factors and obtaining good results in the task of
   compressing remotely sensed hyperspectral data sets.</abstract><date>FEB 2016</date><author>Bernabe, Sergio
   Martin, Gabriel
   Nascimento, Jose M. P.
   Bioucas-Dias, Jose M.
   Plaza, Antonio
   Silva, Vitor</author></paper><paper><title>Accurate crossarchitecture performance modeling for sparse matrixvector
   multiplication (SpMV) on GPUs</title><abstract>This paper presents an integrated analytical and profilebased
   crossarchitecture performance modeling tool to specifically provide
   interarchitecture performance prediction for Sparse MatrixVector
   Multiplication (SpMV) on NVIDIA GPU architectures. To design and
   construct the tool, we investigate the interarchitecture relative
   performance for multiple SpMV kernels. For a sparse matrix, based on its
   SpMV kernel performance measured on a reference architecture, our
   crossarchitecture performance modeling tool can accurately predict its
   SpMV kernel performance on a target architecture. The prediction results
   can effectively assist researchers in making choice of an appropriate
   architecture that best fits their needs from a wide range of available
   computing architectures. We evaluate our tool with 14 widelyused sparse
   matrices on four GPU architectures: NVIDIA Tesla C2050, Tesla M2090,
   Tesla K20m, and GeForce GTX 295. In our experiments, Tesla C2050 works
   as the reference architecture, the other three are used as the target
   architectures. For Tesla M2090, the average performance differences
   between the predicted and measured SpMV kernel execution times for CSR,
   ELL, COO, and HYB SpMV kernels are 3.1%, 5.1%, 1.6%, and 5.6%,
   respectively. For Tesla K20m, they are 6.9%, 5.9%, 4.0%, and 6.6% on the
   average, respectively. For GeForce GTX 295, they are 5.9%, 5.8%, 3.8%,
   and 5.9% on the average, respectively. Copyright (c) 2014 John Wiley &amp;
   Sons, Ltd.</abstract><date>SEP 10 2015</date><author>Guo, Ping
   Wang, Liqiang</author></paper><paper><title>Massive Parallelization of the WRF GCE Model Toward a GPU-Based
   End-to-End Satellite Data Simulator Unit</title><abstract>Modern weather satellites provide more detailed observations of cloud
   and precipitation processes. To harness these observations for better
   satellite data assimilations, a cloud-resolving model, known as the
   Goddard Cumulus Ensemble (GCE) model, was developed and used by the
   Goddard Satellite Data Simulator Unit (G-SDSU). The GCE model has also
   been incorporated as part of the widely used weather research and
   forecasting (WRF) model. The computation of the cloud-resolving GCE
   model is time-consuming. This paper details our massively parallel
   design of GPU-based WRF GCE scheme. With one NVIDIA Tesla K40 GPU, the
   GPU-based GCE scheme achieves a speedup of 361 x as compared to its
   original Fortran counterpart running on one CPU core, whereas the
   speedup for one CPU socket (four cores) with respect to one CPU core is
   only 3.9 x.</abstract><date>MAY 2015</date><author>Huang, Melin
   Huang, Bormin
   Li, Xiaojie
   Huang, Allen Hung-Lung
   Goldberg, Mitchell D.
   Mehta, Ajay</author></paper><paper><title>Shape Context for soft biometrics in person re-identification and
   database retrieval</title><abstract>We introduce a novel descriptor for the analysis of pedestrians and its
   applications to person re-identification and database retrieval. A Shape
   Context descriptor of the head-torso region of persons' silhouettes is
   shown to have a very good discrimination ability and application to
   re-identification. For database retrieval using human queries, we train
   a map from the Shape Context to interpretable soft biometric quantities
   that can be reasoned about by humans. We show that a good linear
   correlation exists between Shape Context descriptors and soft biometrics
   quantities in the upper human torso and illustrate its application to
   retrieval in databases from human queries. Shape Context to biometrics
   maps are learned from virtual avatars rendered by computer graphics
   engines, to circumvent the need for time-consuming manual labelling of
   data sets. We obtained promising results of Shape Context based person
   re-identification and database retrieval from human compliant
   description of biometric traits, in both synthetic data and real
   imagery. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>DEC 15 2015</date><author>Nambiar, Athira
   Bernardino, Alexandre
   Nascimento, Jacinto</author></paper><paper><title>Performance Evaluation of OpenFOAM on Many-Core Architectures</title><abstract>In this article application of Open Source Field Operation and
   Manipulation (OpenFOAM) C++ libraries for solving engineering problems
   on many-core architectures is presented. Objective of this article is to
   present scalability of OpenFOAM on parallel platforms solving real
   engineering problems of fluid dynamics. Scalability test of OpenFOAM is
   performed using various hardware and different implementation of
   standard PCG and PBiCG Krylov iterative methods. Speed up of various
   implementations of linear solvers using GPU and MIC accelerators are
   presented in this paper. Numerical experiments of 3D lid-driven cavity
   flow for several cases with various number of cells are presented.</abstract><date>2015</date><author>Brzobohaty, Tomas
   Riha, Lubomir
   Karasek, Tomas
   Kozubek, Tomas</author></paper><paper><title>A guide to genome-wide association analysis and post-analytic
   interrogation</title><abstract>This tutorial is a learning resource that outlines the basic process and
   provides specific software tools for implementing a complete genome-wide
   association analysis. Approaches to post-analytic visualization and
   interrogation of potentially novel findings are also presented.
   Applications are illustrated using the free and open-source R
   statistical computing and graphics software environment, Bioconductor
   software for bioinformatics and the UCSC Genome Browser. Complete
   genome-wide association data on 1401 individuals across 861,473 typed
   single nucleotide polymorphisms from the PennCATH study of coronary
   artery disease are used for illustration. All data and code, as well as
   additional instructional resources, are publicly available through the
   Open Resources in Statistical Genomics project: . (c) 2015 The Authors.
   Statistics in Medicine Published by John Wiley &amp; Sons Ltd.</abstract><date>DEC 10 2015</date><author>Reed, Eric
   Nunez, Sara
   Kulp, David
   Qian, Jing
   Reilly, Muredach P.
   Foulkes, Andrea S.</author></paper><paper><title>Extracting Microfacet-based BRDF Parameters from Arbitrary Materials
   with Power Iterations</title><abstract>We introduce a novel fitting procedure that takes as input an arbitrary
   material, possibly anisotropic, and automatically converts it to a
   microfacet BRDF. Our algorithm is based on the property that the
   distribution of microfacets may be retrieved by solving an eigenvector
   problem that is built solely from backscattering samples. We show that
   the eigenvector associated to the largest eigenvalue is always the only
   solution to this problem, and compute it using the power iteration
   method. This approach is straightforward to implement, much faster to
   compute, and considerably more robust than solutions based on nonlinear
   optimizations. In addition, we provide simple conversion procedures of
   our fits into both Beckmann and GGX roughness parameters, and discuss
   the advantages of microfacet slope space to make our fits editable. We
   apply our method to measured materials from two large databases that
   include anisotropic materials, and demonstrate the benefits of spatially
   varying roughness on texture mapped geometric models.</abstract><date>JUL 2015</date><author>Dupuy, Jonathan
   Heitz, Eric
   Iehl, Jean-Claude
   Poulin, Pierre
   Ostromoukhov, Victor</author></paper><paper><title>Assessing the performance of four different categories of histological
   criteria in brain tumours grading by means of a computer-aided diagnosis
   image analysis system</title><abstract>Brain tumours are considered one of the most lethal and difficult to
   treat forms of cancer, with unknown aetiology and lack of any realistic
   screening. In this study, we examine, whether the combination of
   descriptive criteria, used by expert histopathologists in assessing
   histologic tissue samples, and quantitative image analysis features may
   improve the diagnostic accuracy of brain tumour grading. Data comprised
   61 cases of brain cancers (astrocytomas, oligodendrogliomas,
   meningiomas) collected from the archives of the University Hospital of
   Patras, Greece. Incorporating physician's descriptive criteria and image
   analysis's quantitative features into a discriminant function, a
   computer-aided diagnosis system was designed for discriminating
   low-grade from high-grade brain tumours. Physician's descriptive
   features, when solely used in the system, proved of high discrimination
   accuracy (93.4%). When verbal descriptive features were combined with
   quantitative image analysis features in the system, discrimination
   accuracy improved to 98.4%. The generalization of the proposed system to
   unseen data converged to an overall prediction accuracy of 86.7% +/-
   5.4%. Considering that histological grading affects treatment selection
   and diagnostic errors may be notable in clinical practice, the
   utilization of the proposed system may safeguard against diagnostic
   misinterpretations in every day clinical practice.</abstract><date>OCT 2015</date><author>Kostopoulos, S.
   Konstandinou, C.
   Sidiropoulos, K.
   Ravazoula, P.
   Kalatzis, I.
   Asvestas, P.
   Cavouras, D.
   Glotsos, D.</author></paper><paper><title>Analytical solutions for tree-like structure modelling using subdivision
   surfaces</title><abstract>We present a novel approach to efficiently modelling branch structures
   with high-quality meshes. Our approach has the following advantages.
   First, the limit surface can fit the target skeleton models as tightly
   as possible by reversely calculating the control vertices of subdivision
   surfaces. Second, high performance is achieved through our proposed
   analytical solutions and the parallel subdivision scheme on a graphics
   processing unit. Third, a smooth manifold quad-only mesh is produced
   from the adopted Catmull-Clark scheme. A number of examples are given to
   demonstrate applications of our approach in various branch structures,
   such as tree branches, animal torsos, and vasculatures. Copyright (c)
   2013 John Wiley &amp; Sons, Ltd.</abstract><date>JAN-FEB 2015</date><author>Zhu, Xiaoqiang
   Jin, Xiaogang
   You, Lihua</author></paper><paper><title>Particle-based fluids for viscous jet buckling</title><abstract>In this paper, we introduce a novel meshfree framework for animating
   free surface viscous liquids with jet buckling effects, such as coiling
   and folding. Our method is based on Smoothed Particle Hydrodynamics
   (SPH) fluids and allows more realistic and complex viscous behaviors
   than the previous SPH frameworks in computer animation literature. The
   viscous liquid is modeled by a non-Newtonian fluid flow and the variable
   viscosity under shear stress is achieved using a viscosity model known
   as Cross model. We demonstrate the efficiency and stability of our
   framework in a wide variety of animations, including scenarios with
   arbitrary geometries and high resolution of SPH particles. The
   interaction of the viscous liquid with complex solid obstacles is
   performed using boundary particles. Our framework is able to deal with
   different inlet velocity profiles and geometries of the injector, as
   well as moving inlet jet along trajectories given by cubic Hermite
   splines. Moreover, the simulation speed is significantly accelerated by
   using Computer Unified Device Architecture (CUDA) computing platform.
   (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>NOV 2015</date><author>de Souza Andrade, Luiz Fernando
   Sandim, Marcos
   Petronetto, Fabiano
   Pagliosa, Paulo
   Paiva, Afonso</author></paper><paper><title>Efficient localization and spectral estimation of an unknown number of
   ocean acoustic sources using a graphics processing unit</title><abstract>This paper develops a matched-field approach to localization and
   spectral estimation of an unknown number of ocean acoustic sources
   employing massively parallel implementation on a graphics processing
   unit (GPU) for real-time efficiency. A Bayesian formulation is developed
   in which the locations and complex spectra of multiple sources and noise
   variances are considered unknown random variables, and the Bayesian
   information criterion is minimized to estimate these parameters, as well
   as the number of sources present. Optimization is carried out using
   simulated annealing and includes steps that attempt to add/delete
   sources to/from the model. Closed-form maximum-likelihood (ML) solutions
   for source spectra and noise variances in terms of the source locations
   allow these parameters to be sampled implicitly, substantially reducing
   the dimensionality of the inversion. Source sampling, addition, and
   deletion are based on joint conditional probability distributions for
   source range and depth, which incorporate the ML spectral estimates.
   Computing these conditionals requires solving a very large number of
   systems of equations, which is carried out in parallel on a GPU,
   improving efficiency by 2 orders of magnitude. Simulated examples
   illustrate localizations and spectral estimation for a large number of
   sources (up to eight), and investigate mitigation of environmental
   mismatch via efficient multiple-frequency inversion. (C) 2015 Acoustical
   Society of America.</abstract><date>NOV 2015</date><author>Dosso, Stan E.
   Dettmer, Jan
   Wilmut, Michael J.</author></paper><paper><title>A New Thread Block Scheduling Technique for Improving the Performance of
   GPGPU Architecture</title><abstract>General-Purpose computing on Graphics Processing Units (GPGPUs) are
   becoming one of the most attractive computing platforms for handling
   high performance computing applications. This trend is more and more
   increasing especially with the help of new parallel programming models
   like CUDA and OpenCL, etc. The state-of-the-art GPGPU schedulers are
   likely to allocate as many as possible number of thread blocks into
   streaming multiprocessors. In fact, this is not always the optimal way
   to maximize the hardware utilization. This paper studies the drawbacks
   of the common-used round-robin scheduling at the thread block level.
   After that, we propose a new technique that can dynamically adjust the
   number of thread blocks to be assigned to streaming multiprocessors
   depending on the level of memory and interconnection network contention.
   Our simulation results on a 15-core GPGPU platform show that the
   proposed technique provides better performance over the baseline
   round-robin scheduling by around 6.3% on average.</abstract><date>2015</date><author>???
   ?????
   ???
   ???</author></paper><paper><title>BilKristal 4.0: A tool for crystal parameters extraction and defect
   quantification</title><abstract>In this paper, we present a revised version of BilKristal 3.0 tool.
   Raycast screenshot functionality is added to provide improved visual
   analysis. We added atomic distance analysis functionality to assess
   crystalline defects. We improved visualization capabilities by adding
   high level cut function definitions. Discovered bugs are fixed and small
   performance optimizations are made.New version program summaryProgram
   title: BilKristal 4.0Catalogue identifier: ADYU_v4_0Program summary URL:
   http://cpc.cs.qub.ac.uk/summaries/ADYU_v4_0.htmlProgram obtainable from:
   CPC Program Library, Queen's University, Belfast, N. IrelandLicensing
   provisions: Standard CPC licence,
   http://cpc.cs.qub.ac.uk/licence/licence.htmlNo. of lines in distributed
   program, including test data, etc.: 370130No. of bytes in distributed
   program, including test data, etc.: 8350850Distribution format:
   tar.gzProgramming language: C, C++, Microsoft.NET Framework 2.0 and
   OpenGL Libraries.Computer: Personal Computers with Windows operating
   system.Operating system: Windows XP or higher.RAM: 20-60
   Megabytes.Classification: 8.External routines: Microsoft.NET Framework
   2.0. For the visualization tool, graphics card driver should also
   support OpenGL.Catalogue identifier of previous version:
   ADYU_v3_0Journal reference of previous version: Comput. Phys. Comm. 187
   (2015) 266Does the new version supersede the previous version?:
   YesNature of problem: Determining the crystal structure parameters of a
   material is a very important issue in crystallography. Knowing the
   crystal structure parameters helps the understanding of the physical
   behavior of material. For complex structures, particularly for materials
   which also contain local symmetry as well as global symmetry, obtaining
   crystal parameters can be very hard.Solution method: The tool extracts
   crystal parameters such as primitive vectors, basis vectors and
   identifies the space group from atomic coordinates of crystal
   structures.Reasons for new version: Additional features, Performance
   optimizations, Minor bug corrections.Summary of revisions:Raycast
   screenshot functionality is added to the visualization tool. A
   raycasting algorithm similar to the one described in [4, 6] is used. The
   algorithm is multi-core parallelized as described in [5].Atomic distance
   analysis functionality is added. Tool can analyze the crystal structure
   and give statistical information of atom to atom distances.In the
   visualization tool, high level cut function support is added. Users can
   define cut functions, not just as cut-planes but with more complex
   functions as well.Automatic primitive vector and basis vector selection
   options are added. This way the system selects the simplest primitive
   and basis vector alternatives and continues without interrupting the
   user.In the visualization tool, an unused log file was being created and
   a redundant configuration file was being used. These issues are
   corrected.Dead-codes are removed to improve clarity.Restrictions:
   Assumptions are explained in [1, 2, 3]. However, none of them can be
   considered as a restriction to the complexity of the problem.Running
   time: The tool was able to process input files with more than a million
   atoms in less than 20 seconds on a PC with an Athlon quad-core CPU at
   3.2 GHz using the default parameter values. (C) 2015 Elsevier B.V. All
   rights reserved.</abstract><date>OCT 2015</date><author>Okuyan, Erkan
   Okuyan, Ceyda</author></paper><paper><title>Quasi-Static Antenna Design Algorithm image licensed by graphic stock A
   scientific approach to designing electrically small antennas</title><abstract>The quasi-static antenna design algorithm is a scientific approach to
   designing electrically small antennas. The radiation resistance is
   computed from the electrostatic dipole potential. The capacitance is
   computed from the electrostatic potential on the enclosed sphere. The
   Q-factor is calculated from the radiation resistance and capacitance.
   The general thick-disk-cap monopole, enclosed by a sphere, is modeled
   with electrostatic multipole basis functions on the disk. A sequence of
   solutions is computed with one to five electrostatic multipole basis
   functions. The sequence of solutions converges in shape and Q-factor.
   Chu's [1] theoretical limit QChu for an antenna enclosed by a sphere is
   used to calculate the Q-factor ratio (Q/QChu). At low frequencies, the
   Q-factor ratio for the thick-disk-cap monopole is 1.825. This is almost
   the same as the spherical-cap monopole Q-factor ratio of 1.75. The
   thick-disk-cap and spherical-cap monopole were modeled with Computer
   Simulation Technology (CST) Microwave Studio. At resonance, the
   thick-disk-cap monopole's Q-factor is 22.5. This is much smaller than
   the spherical-cap monopole's Q-factor of 46.1. Above the resonant
   frequency, the thick-disk-cap monopole had a 37% lower Q-factor.
   Stuart's equivalent circuit is fit to the CST impedance near the
   resonant frequency. The simplest circuit is the dipole eigenmode (a
   series LC and a resistor parallel to the L). The dipole eigenmode
   circuit gives the omega(2) radiation resistance. The CST radiation
   resistance includes a omega 4 term. A capacitor approximation of the
   higher-order eigenmodes explains the omega(4) term in the CST radiation
   resistance. The capacitor is parallel to the dipole eigenmodeThe
   quasi-static antenna design algorithm is a scientific approach to
   designing electrically small antennas. The radiation resistance is
   computed from the electrostatic dipole potential. The capacitance is
   computed from the electrostatic potential on the enclosed sphere. The
   Q-factor is calculated from the radiation resistance and capacitance.
   The general thick-disk-cap monopole, enclosed by a sphere, is modeled
   with electrostatic multipole basis functions on the disk. A sequence of
   solutions is computed with one to five electrostatic multipole basis
   functions. The sequence of solutions converges in shape and Q-factor.
   Chu's [1] theoretical limit QChu for an antenna enclosed by a sphere is
   used to calculate the Q-circuit. Stuart's equivalent circuit is used to
   compute the dc capacitance, inductance, effective height, Q-factor, and
   Q-factor ratio at low frequencies. The circuit model and CST impedance
   are almost identical. The agreement between the CST impedance and the dc
   quasi-static values is reasonable. The circuit model impedance is
   expressed in an easy-to-understand format.The quasi-static antenna
   design algorithm considers only the dipole eigenmode; the
   current-sharing factor between the dipole and higher eigenmodes
   introduces a small error in the radiation resistance and Q-factor.</abstract><date>OCT 2015</date><author>Jones, Thomas O., III</author></paper><paper><title>GPU accelerated solver for nonlinear reaction-diffusion systems.
   Application to the electrophysiology problem</title><abstract>Solving the electric activity of the heart possess a big challenge, not
   only because of the structural complexities inherent to the heart
   tissue, but also because of the complex electric behaviour of the
   cardiac cells. The multi-scale nature of the electrophysiology problem
   makes difficult its numerical solution, requiring temporal and spatial
   resolutions of 0.1 ms and 0.2 mm respectively for accurate simulations,
   leading to models with millions degrees of freedom that need to be
   solved for thousand time steps. Solution of this problem requires the
   use of algorithms with higher level of parallelism in multi-core
   platforms. In this regard the newer programmable graphic processing
   units (GPU) has become a valid alternative due to their tremendous
   computational horsepower. This paper presents results obtained with a
   novel electrophysiology simulation software entirely developed in
   Compute Unified Device Architecture (CUDA). The software implements
   fully explicit and semi-implicit solvers for the monodomain model, using
   operator splitting. Performance is compared with classical multi-core
   MPI based solvers operating on dedicated high-performance computer
   clusters. Results obtained with the GPU based solver show enormous
   potential for this technology with accelerations over 50x for
   three-dimensional problems. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Mena, Andres
   Ferrero, Jose M.
   Rodriguez Matas, Jose F.</author></paper><paper><title>The value of Retrospective and Concurrent Think Aloud in formative
   usability testing of a physician data query tool</title><abstract>Objective: To compare the performance of the Concurrent (CFA) and
   Retrospective (RTA) Think Aloud method and to assess their value in a
   formative usability evaluation of an Intensive Care Registry-physician
   data query tool designed to support ICU quality improvement
   processes.Methods: Sixteen representative intensive care physicians
   participated in the usability evaluation study. Subjects were allocated
   to either the CTA or RTA method by a matched randomized design. Each
   subject performed six usability-testing tasks of varying complexity in
   the query tool in a real-working context. Methods were compared with
   regard to number and type of problems detected. Verbal protocols of CTA
   and RTA were analyzed in depth to assess differences in verbal output.
   Standardized measures were applied to assess thoroughness in usability
   problem detection weighted per problem severity level and method overall
   effectiveness in detecting usability problems with regard to the time
   subjects spent per method.Results: The usability evaluation of the data
   query tool revealed a total of 43 unique usability problems that the
   intensive care physicians encountered. CTA detected unique usability
   problems with regard to graphics/symbols, navigation issues, error
   messages, and the organization of information on the query tool's
   screens. RTA detected unique issues concerning system match with
   subjects' language and applied terminology. The in-depth verbal protocol
   analysis of CTA provided information on intensive care physicians' query
   design strategies. Overall, CTA performed significantly better than RTA
   in detecting usability problems. CTA usability problem detection
   effectiveness was 0.80 vs. 0.62 (p &lt; 0.05) respectively, with an average
   difference of 42% less time spent per subject compared to RTA. In
   addition, CFA was more thorough in detecting usability problems of a
   moderate (0.85 vs. 0.7) and severe nature (0.71 vs. 0.57).Conclusion: In
   this study, the CIA is more effective in usability-problem detection and
   provided clarification of intensive care physician query design
   strategies to inform redesign of the query tool. However, CTA does not
   outperform RTA. The RTA additionally elucidated unique usability
   problems and new user requirements. Based on the results of this study,
   we recommend the use of CFA in formative usability evaluation studies of
   health information technology. However, we recommend further research on
   the application of RTA in usability studies with regard to user
   expertise and experience when focusing on user profile customized
   (re)design. (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>JUN 2015</date><author>Peute, Linda W. P.
   de Keizer, Nicolette F.
   Jaspers, Monique W. M.</author></paper><paper><title>Use of graphics processing units for automatic synthesis of programs</title><abstract>Genetic programming (GP) is an evolutionary method that allows computers
   to solve problems automatically. However, the computational power
   required for the evaluation of billions of programs imposes a serious
   limitation on the problem size. This work focuses on accelerating GP to
   support the synthesis of large problems. This is done by completely
   exploiting the highly parallel environment of graphics processing units
   (GPUs). Here, we propose a new quantum-inspired linear GP approach that
   implements all the GP steps in the GPU and provides the following: (1)
   significant performance improvements in the GP steps, (2) elimination of
   the overhead of copying the fitness results from the GPU to the CPU, and
   (3) incorporation of a new selection mechanism-to recognize the programs
   with the best evaluations. The proposed approach outperforms the
   previous approach for large-scale synthetic and real-world problems.
   Further, it provides a remarkable speedup over the CPU execution. (C)
   2015 Elsevier Ltd. All rights reserved.</abstract><date>AUG 2015</date><author>da Silva, Cleomar Pereira
   Dias, Douglas Mota
   Bentes, Cristiana
   Cavalcanti Pacheco, Marco Aurelio</author></paper><paper><title>EM Scattering From a Target Above a 1-D Randomly Rough Sea Surface Using
   GPU-Based Parallel FDTD</title><abstract>This letter presents the graphics processing unit (GPU)-based
   finite-difference time-domain (FDTD) algorithm to investigate the
   electromagnetic (EM) scattering from a target above a rough sea surface.
   The proposed method is validated by comparing the numerical results to
   those obtained through sequential FDTD execution on a central processing
   unit (CPU), as well as through method of moments (MOM). The comparison
   results show a significant improvement in computation efficiency for
   GPU-based FDTD versus the message passing interface (MPI)-based FDTD.
   Furthermore, our parallel implementation is employed to study the
   influences of wind speed and the tilt angle of the target on composite
   scattering from the target above a sea surface.</abstract><date>2015</date><author>Jia, Chungang
   Guo, Lixin
   Yang, Pengju</author></paper><paper><title>A fast GPU-based Monte Carlo simulation of proton transport with
   detailed modeling of nonelastic interactions</title><abstract>Purpose: Very fast Monte Carlo (MC) simulations of proton transport have
   been implemented recently on graphics processing units (GPUs). However,
   these MCs usually use simplified models for nonelastic proton-nucleus
   interactions. Our primary goal is to build a CPU-based proton transport
   MC with detailed modeling of elastic and nonelastic proton-nucleus
   collisions.Methods: Using the CUDA framework, the authors implemented
   CPU kernels for the following tasks: (1) simulation of beam spots from
   our possible scanning nozzle configurations, (2) proton propagation
   through CT geometry, taking into account nuclear elastic scattering,
   multiple scattering, and energy loss straggling, (3) modeling of the
   intranuclear cascade stage of nonelastic interactions when they occur,
   (4) simulation of nuclear evaporation, and (5) statistical error
   estimates on the dose. To validate our MC, the authors performed (1)
   secondary particle yield calculations in proton collisions with
   therapeutically relevant nuclei, (2) dose calculations in homogeneous
   phantoms, (3) recalculations of complex head and neck treatment plans
   from a commercially available treatment planning system, and compared
   with oEANT4.9.6p2/TOPAS.Results: Yields, energy, and angular
   distributions of secondaries from nonelastic collisions on various
   nuclei are in good agreement with the GEANT4.9.6p2 Bertini and Binary
   cascade models. The 3D-gamma pass rate at 2%-2 mm for treatment plan
   simulations is typically 98%. The net computational time on a NVIDIA
   GTX680 card, including all CPU CPU data transfers, is similar to 20 s
   for 1 x 10(7) proton histories.Conclusions: Our CPU-based MC is the
   first of its kind to include a detailed nuclear model to handle
   nonelastic interactions of protons with any nucleus. Dosimetric
   calculations are in very good agreement with oEANT4.9.6p2/TOPAS. Our MC
   is being integrated into a framework to perform fast routine clinical QA
   of pencil-beam based treatment plans, and is being used as the dose
   calculation engine in a clinically applicable MC-based IMPT treatment
   planning system. The detailed nuclear modeling will allow us to perform
   very fast linear energy transfer and neutron dose estimates on the CPU.
   (C) 2015 American Association of Physicists in Medicine.</abstract><date>JUN 2015</date><author>Tseung, H. Wan Chan
   Ma, J.
   Beltran, C.</author></paper><paper><title>A Parallelization Method for Neural Network Learning</title><abstract>Recently, the technology called General-Purpose computing on Graphics
   Processing Unit (GPGPU), which treats not only graphic processing but
   also general purpose calculation by using GPU, has been investigated
   because the GPU has higher performance than the CPU for the development
   of 3DCG or movie processing. GPU has dedicated circuits to draw
   graphics, so that it has the characteristic that the many simple
   arithmetic circuits are implemented. This characteristic has promise for
   applications not only to graphic processing but also to massive
   parallelism. In this research, we apply the technology to neural network
   learning, a form of intelligent signal processing. As conventional
   research, we proposed three methods of speeding up neural network
   learning. One of the methods, parallelization of pattern processing, has
   points that should be improved. In this paper, we report that the
   updating of the weight coefficients in the neurons is processed
   simultaneously by changing the order of the pattern calculations. The
   proposed calculation method is evaluated against test data sets. The
   results confirm that the proposed method converges similarly to the
   conventional method. We also propose an optimal implementation method
   for the GPU. This proposed method is found to be three to six times
   faster than the conventional method.</abstract><date>APR 2015</date><author>Tsuchida, Yuta
   Yoshioka, Michifumi</author></paper><paper><title>Multi-GPU-based Swendsen-Wang multi-cluster algorithm with reduced data
   traffic</title><abstract>The computational performance of multi-GPU applications can be degraded
   by the data communication between each GPU. To realize high-speed
   computation with multiple GPUs, we should minimize the cost of this data
   communication. In this paper, I propose a multiple GPU computing method
   for the Swendsen-Wang (SW) multi-cluster algorithm that reduces the data
   traffic between each GPU. I realize this reduction in data traffic by
   adjusting the connection information between each GPU in advance. The
   code is implemented on the large-scale open science TSUBAME 2.5
   supercomputer, and its performance is evaluated using a simulation of
   the three-dimensional Ising model at the critical temperature. The
   results show that the data communication between each GPU is reduced by
   90%, and the number of communications between each GPU decreases by
   about half. Using 512 GPUs, the computation time is 0.005 ns per spin
   update at the critical temperature for a total system size of N =
   4096(3). (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>OCT 2015</date><author>Komura, Yukihiro</author></paper><paper><title>ADMIRE: analysis and visualization of differential methylation in
   genomic regions using the Infinium HumanMethylation450 Assay</title><abstract>Background: DNA methylation at cytosine nucleotides constitutes
   epigenetic gene regulation impacting cellular development and a wide
   range of diseases. Cytosine bases of the DNA are converted to
   5-methylcytosine by the methyltransferase enzyme, acting as a reversible
   regulator of gene expression. Due to its outstanding importance in the
   epigenetic field, a number of lab techniques were developed to
   interrogate DNA methylation on a global range. Besides whole-genome
   bisulfite sequencing, the Infinium HumanMethylation450 Assay represents
   a versatile and cost-effective tool to investigate genome-wide changes
   of methylation patterns.Results: Analysis of DNA Methylation In genomic
   REgions (ADMIRE) is an open source, semi-automatic analysis pipeline and
   visualization tool for Infinium HumanMethylation450 Assays with a
   special focus on ease of use. It features flexible experimental
   settings, quality control, automatic filtering, normalization, multiple
   testing, and differential analyses on arbitrary genomic regions.
   Publication-ready graphics, genome browser tracks, and table outputs
   include summary data and statistics, permitting instant comparison of
   methylation profiles between sample groups and the exploration of
   methylation patterns along the whole genome. ADMIREs statistical
   approach permits simultaneous large-scale analyses of hundreds of assays
   with little impact on algorithm runtimes.Conclusions: The web-based
   version of ADMIRE provides a simple interface to researchers with
   limited programming skills, whereas the offline version is suitable for
   integration into custom pipelines. ADMIRE may be used via our freely
   available web service at https://bioinformatics.mpi-bn.mpg.de without
   any limitations concerning the size of a project. An offline version for
   local execution is available from our website or GitHub
   (https://github.molgen.mpg.de/loosolab/admire).</abstract><date>DEC 1 2015</date><author>Preussner, Jens
   Bayer, Julia
   Kuenne, Carsten
   Looso, Mario</author></paper><paper><title>Evaluation of Low-Contrast Detectability of Iterative Reconstruction
   across Multiple Institutions, CT Scanner Manufacturers, and Radiation
   Exposure Levels</title><abstract>Purpose: To compare image resolution from iterative reconstruction with
   resolution from filtered back projection for lowcontrast objects on
   phantom computed tomographic (CT) images across vendors and exposure
   levels.Materials and Methods: Randomized repeat scans of an American
   College of Radiology CT accreditation phantom (module 2, low contrast)
   were performed for multiple radiation exposures, vendors, and vendor
   iterative reconstruction algorithms. Eleven volunteers were presented
   with 900 images by using a custom-designed graphical user interface to
   perform a task created specifically for this reader study. Results were
   analyzed by using statistical graphics and analysis of variance.Results:
   Across three vendors (blinded as A, B, and C) and across three exposure
   levels, the mean correct classification rate was higher for iterative
   reconstruction than filtered back projection (P &lt; .01): 87.4% iterative
   reconstruction and 81.3% filtered back projection at 20 mGy, 70.3%
   iterative reconstruction and 63.9% filtered back projection at 12 mGy,
   and 61.0% iterative reconstruction and 56.4% filtered back projection at
   7.2 mGy. There was a significant difference in mean correct
   classification rate between vendor B and the other two vendors. Across
   all exposure levels, images obtained by using vendor B's scanner
   outperformed the other vendors, with a mean correct classification rate
   of 74.4%, while the mean correct classification rate for vendors A and C
   was 68.1% and 68.3%, respectively. Across all readers, the mean correct
   classification rate for iterative reconstruction (73.0%) was higher
   compared with the mean correct classification rate for filtered back
   projection (67.0%).Conclusion: The potential exists to reduce radiation
   dose without compromising low-contrast detectability by using iterative
   reconstruction instead of filtered back projection. There is substantial
   variability across vendor reconstruction algorithms. (C) RSNA, 2015</abstract><date>OCT 2015</date><author>Saiprasad, Ganesh
   Filliben, James
   Peskin, Adele
   Siegel, Eliot
   Chen, Joseph
   Trimble, Christopher
   Yang, Zhitong
   Christianson, Olav
   Samei, Ehsan
   Krupinski, Elizabeth
   Dima, Alden</author></paper><paper><title>An integrated environment model for a constructed wetland Hydrodynamics
   and transport processes</title><abstract>Constructed wetlands (CW) have become a popular technology for treating
   urban and agricultural stormwater runoff. In South Florida, Stormwater
   Treatment Areas (STAs) have been built to reduce phosphorus (P)
   concentrations in runoff from agriculture and other sources, including
   Lake Okeechobee discharges, prior to delivery to the Everglades
   Protection Area. The scale of this constructed wetland project is
   unprecedented in terms of size, cost, and scientific challenges.
   Models/tools are needed to provide detailed spatial and temporal
   information to optimize the P removal efficiency and to predict the
   dynamic response of STAs under a variety of management conditions. The
   Lake Okeechobee Environment Model (LOEM) developed for Lake Okeechobee
   has been enhanced to simulate hydrodynamics and transport processes in
   the wetland environment. The flow resistance caused by Submerged Aquatic
   Vegetation (SAV) and Emergent Aquatic Vegetation (EAV) is included in
   the LOEM-CW. The LOEM-CW is calibrated and validated with 6 years of
   measured data (2008-2013) at different locations in STA-314 Cells 3A and
   3B. Through graphic and statistical comparisons, it is shown that the
   model simulated stage, flow velocity, water temperature, and total
   suspended solid (TSS) in the study area reasonably well. The LOEM-CW is
   poised to serve as a powerful tool in wetland management and STA
   operation. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Jin, Kang-Ren
   Ji, Zhen-Gang</author></paper><paper><title>Shape-constrained level set segmentation for hybrid CPU-GPU computers</title><abstract>Due to its intrinsic advantages such as the ability to handle complex
   shapes, the level set method (LSM) has been widely applied to image
   segmentation. Nevertheless, the LSM is computationally expensive. In
   order to improve the performance of the traditional LSM both in terms of
   efficiency and effectiveness, we propose a novel algorithm based on the
   lattice Boltzmann method (LBM). Using local region statistics and prior
   shape, we design an effective and local speed function for the LSM, from
   which we deduce a shape prior based body force for LBM solver. An NVIDIA
   graphics processing units (GPU) is used to accelerate the method. Our
   introduced algorithm has several advantages. First, it is accurate even
   if there are some geometric transformations (rotation angle, scaling
   factor and translation vector) between the object to be segmented and
   the prior shape. Second, it is local and therefore suitable for
   massively parallel architectures. Third, the use of local region
   information allows it to deal with intensity inhomogeneities. Fourth,
   including shape prior allows the method to handle occlusion and noise.
   Fourth, the model is fast. Finally the algorithm can be used without
   shape prior by means of minor modification. Intensive experiments
   demonstrate, objectively and subjectively, the performance of the
   introduced framework. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>FEB 12 2016</date><author>Balla-Arabe, Souleymane
   Gao, Xinbo
   Ginhac, Dominique
   Yang, Fan</author></paper><paper><title>Performance and Scalability of the JCSDA Community Radiative Transfer
   Model (CRTM) on NVIDIA GPUs</title><abstract>An atmospheric radiative transfer model calculates radiative transfer of
   electromagnetic radiation through earth's atmosphere. The community
   radiative transfer model (CRTM) is a fast radiative transfer model for
   calculating the satellite infrared (IR) and microwave (MW) radiances of
   a given state of the Earth's atmosphere and its surface. The CRTM takes
   into account the radiance emission and absorption of various atmospheric
   gasses as well as the emission and the reflection of various surface
   types. Two different transmittance algorithms are currently available in
   the CRTM OPTRAN: optical depth in absorber space (ODAS) and optical
   depth in pressure space (ODPS). ODAS in the current CRTM allows two
   variable absorbers (water vapor and ozone). In this paper, we examine
   the feasibility of using graphics processing units (GPUs) to accelerate
   the CRTM with the ODAS transmittance model. Using commodity GPUs for
   accelerating CRTM means that the hardware costs of adding
   high-performance accelerators to computation hardware configuration are
   significantly reduced. Our results show that GPUs can provide
   significant speedup over conventional processors for the 8461-channel
   IASI sounder. In particular, a GPU on the dual-GPU NVIDIA GTX 590 card
   can provide a speedup 375x for the single-precision version of the CRTM
   ODAS compared to its single-threaded Fortran counterpart running on
   Intel i7 920 CPU, whereas the speedup for 1 CPU socket with respect to 1
   CPU core is only 6.3x. Furthermore, two NVIDIA GTX 590s provided
   speedups of 201x and 1367x for double precision and single precision
   versions of ODAS compared to single threaded Fortran code.</abstract><date>APR 2015</date><author>Mielikainen, Jarno
   Huang, Bormin
   Huang, Hung-Lung Allen
   Lee, Tsengdar</author></paper><paper><title>Collaborative Parallel Hybrid Metaheuristics on Graphics Processing Unit</title><abstract>Metaheuristics are nondeterministic optimization algorithms used to
   solve complex problems for which classic approaches are unsuitable.
   Despite their effectiveness, metaheuristics require considerable
   computational power and cannot easily be used in time critical
   applications. Fortunately, those algorithms are intrinsically parallel
   and have been implemented on shared memory systems and more recently on
   graphics processing units (GPUs). In this paper, we present highly
   effecient parallel implementations of the particle swarm optimization
   (PSO), the genetic algorithm (GA) and the simulated annealing (SA)
   algorithm on GPU using CUDA. Our approach exploits the parallelism at
   the solution level, follows an island model and allows for speedup up to
   346x for different benchmark functions. Most importantly, we also
   present a strategy that uses the generalized island model to integrate
   multiple metaheuristics into a parallel hybrid solution adapted to the
   GPU. Our proposed solution uses OpenMP to heavily exploit the concurrent
   kernel execution feature of recent NVIDIA GPUs, allowing for the
   parallel execution of the different metaheuristics in an asynchronous
   manner. Asynchronous hybrid metaheuristics has been developed for
   multicore CPU, but never for GPU. The speedup offered by the GPU is far
   superior and key to the optimization of solutions to complex engineering
   problems.</abstract><date>MAR 2015</date><author>Roberge, Vincent
   Tarbouchi, Mohammed
   Okou, Francis</author></paper><paper><title>Optimizing Deltoid Efficiency with Reverse Shoulder Arthroplasty Using a
   Novel Inset Center of Rotation Glenosphere Design.</title><abstract>INTRODUCTION: Paul Grammont's hemispherical gleno - sphere concept
   medializes the center of rotation (CoR) to the glenoid face to increase
   deltoid abductor moment arms and improve muscle efficiency. Reducing
   glenosphere thickness to less than half its spherical radius further
   medializes the CoR and offers the potential for even greater
   improvements in efficiency. To that end, this study quantifies deltoid
   abduc - tor moment arms for six different rTSA prostheses during
   scapular abduction from 0 degrees to 140 degrees.METHODS: A 3D computer model was
   developed in Uni - graphics to quantify deltoid moment arms during
   scapular abduction for the normal anatomic shoulder, the 36 mm Grammont
   Delta III (Depuy, Inc.), 36 mm BIO-RSA  (Tornier, Inc.), the 32 mm RSP 
   (DJO, Inc.), and the Equi - noxe  rTSA (Exactech, Inc.) with three
   different glenosphere geometries: 38 mm x 21 mm, 46 mm x 25 mm, and the
   novel 46 mm x 21 mm. Each muscle was simulated as three lines from
   origin to insertion as the arm was elevated; positional data was
   exported to Matlab where the abductor moment arms were calculated for
   the anterior, middle, and posterior deltoid from 0 degrees to 140 degrees humeral
   abduction in the scapular plane using a 1.8:1 scapular rhythm.RESULTS:
   The 46 mm x 21 mm glenosphere had the larg - est average abductor moment
   arms and also the largest efficiency for all three heads of the deltoid,
   having a 4.8% to 40.7% increase in the average deltoid efficiency
   relative to all other designs tested. The glenosphere design with the
   next most efficient deltoid was the 36 mm Delta III, which had the next
   most medialized CoR. The two least efficient designs were the BIO-RSA 
   and the DJO RSP  , which had the most lateral CoR.DISCUSSION: These
   results provide new biomechanical insights on the impact of glenosphere
   geometry on deltoid abductor moment arms and demonstrate that subtle
   changes in rTSA prosthesis design can result in dramatic improve -
   ments. Increasing glenosphere diameter while also decreas - ing
   thickness to be less than half its spherical radius may minimize the
   muscle forces required to perform activities of daily living. Clinical
   follow-up is necessary to demonstrate a reduction in complications
   related to joint over-loading and also demonstrate greater increases in
   range of motion for patients with weak musculature.</abstract><date>2015-Dec</date><author>Roche, Christopher P
   Hamilton, Matthew A
   Diep, Phong
   Wright, Thomas W
   Flurin, Pierre-Henr
   Zuckerman, Joseph D
   Routman, Howard D</author></paper><paper><title>A new optimization approach for mass-spring models parameterization</title><abstract>Mass-spring models (MSM) are frequently used to model deformable objects
   for computer graphics applications due to their simplicity and
   computational efficiency. However, the model parameters are not related
   to the constitutive laws of elastic material in an obvious way. The MSM
   parameters computation from a model based on continuum mechanics is a
   possibility to address this problem. Therefore, in this paper we propose
   a new method to derive MSM parameters using a data-driven strategy with
   a new objective function based on the model acceleration so that the MSM
   and the reference model behave similarly. The proposed methodology does
   not depend on reference model, mesh topology or static equilibrium
   configuration. We validate the methodology for deriving MSM systems
   using finite element method (FEM) and MSM itself as reference models.
   The obtained results are compared with related works. We also discuss
   its robustness against different discretizations and material
   properties. (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>SEP 2015</date><author>da Silva, Josildo Pereira
   Giraldi, Gilson A.
   Apolinario, Antonio L., Jr.</author></paper><paper><title>A parallel Fruchterman-Reingold algorithm optimized for fast
   visualization of large graphs and swarms of data</title><abstract>Graphs in computer science are widely used in social network analysis,
   computer networks, transportation networks, and many other areas. In
   general, they can visualize relationships between objects. However, fast
   drawing of graphs and other structures containing large numbers of data
   points with readable layouts is still a challenge. This paper describes
   a novel variant of the Fruchterman-Reingold graph layout algorithm which
   is adapted to GPU parallel architecture. A new approach based on space
   filling curves and a new way of repulsive forces computation on GPU are
   described. The paper contains both performance and quality tests of the
   new algorithm. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>FEB 2016</date><author>Gajdos, Petr
   Jezowicz, Tomas
   Uher, Vojtech
   Dohnalek, Pavel</author></paper><paper><title>Efficient ELM-Based Techniques for the Classification of Hyperspectral
   Remote Sensing Images on Commodity GPUs</title><abstract>Extreme learning machine (ELM) is an efficient learning algorithm that
   has been recently applied to hyperspectral image classification. In this
   paper, the first implementation of the ELM algorithm fully developed for
   graphical processing unit (GPU) is presented. ELM can be expressed in
   terms of matrix operations so as to take advantage of the single
   instruction multiple data (SIMD) computing paradigm of the GPU
   architecture. Additionally, several techniques like the use of
   ensembles, a spatial regularization algorithm, and a spectral-spatial
   classification scheme are applied and projected to GPU in order to
   improve the accuracy results of the ELM classifier. In the last case,
   the spatial processing is based on the segmentation of the hyperspectral
   image through a watershed transform. The experiments are performed on
   remote sensing data for land cover applications achieving competitive
   accuracy results compared to analogous support vector machine (SVM)
   strategies with significantly lower execution times. The best accuracy
   results are obtained with the spectral-spatial scheme based on applying
   watershed and a spatially regularized ELM.</abstract><date>JUN 2015</date><author>Lopez-Fandino, Javier
   Quesada-Barriuso, Pablo
   Heras, Dora B.
   Argueello, Francisco</author></paper><paper><title>High Performance GPU-Based Fourier Volume Rendering.</title><abstract>Fourier volume rendering (FVR) is a significant visualization technique
   that has been used widely in digital radiography. As a result of its
   ????(N (2)log?N) time complexity, it provides a faster alternative to
   spatial domain volume rendering algorithms that are ????(N (3))
   computationally complex. Relying on the Fourier projection-slice
   theorem, this technique operates on the spectral representation of a 3D
   volume instead of processing its spatial representation to generate
   attenuation-only projections that look like X-ray radiographs. Due to
   the rapid evolution of its underlying architecture, the graphics
   processing unit (GPU) became an attractive competent platform that can
   deliver giant computational raw power compared to the central processing
   unit (CPU) on a per-dollar-basis. The introduction of the compute
   unified device architecture (CUDA) technology enables
   embarrassingly-parallel algorithms to run efficiently on CUDA-capable
   GPU architectures. In this work, a high performance GPU-accelerated
   implementation of the FVR pipeline on CUDA-enabled GPUs is presented.
   This proposed implementation can achieve a speed-up of 117x compared to
   a single-threaded hybrid implementation that uses the CPU and GPU
   together by taking advantage of executing the rendering pipeline
   entirely on recent GPU architectures. </abstract><date>2015</date><author>Abdellah, Marwan
   Eldeib, Ayman
   Sharawi, Amr</author></paper><paper><title>Graphical processing unit-based parallelization of the Open Shortest
   Path First and Border Gateway Protocol routing protocols</title><abstract>Exponentially growing number of devices on Internet incurs an
   ever-increasing load on the network routers in executing network
   protocols. Parallel processing has recently become an unavoidable means
   to scale up the router performance. The research effort elaborated in
   this paper is focused on exploiting the modern trends of general-purpose
   computing on graphics processing unit computing in speeding up the
   execution of network protocols. An additional benefit is off-loading the
   CPU, which can now be fully dedicated to the packet processing and
   forwarding. To this end, the Shortest Path First algorithm in the Open
   Shortest Path First protocol and the choice of the best routes in the
   Border Gateway Protocol are parallelized for efficient execution on
   Compute Unified Device Architecture platform. An evaluation study was
   conducted on three different graphics processing units with
   representative network workload for a varying number of routes and
   devices. The obtained speedup results confirmed the viability and
   cost-effectiveness of such an approach. Copyright (C) 2014 John Wiley &amp;
   Sons, Ltd.</abstract><date>JAN 2015</date><author>Dundjerski, Dejan
   Tomasevic, Milo</author></paper><paper><title>Symptoms and Symptom Clusters Identified by Adolescents and Young Adults
   With Cancer Using a Symptom Heuristics App</title><abstract>Adolescents and young adults (AYAs) with cancer experience multiple
   distressing symptoms during treatment. Because the typical approach to
   symptom assessment does not easily reflect the symptom experience of
   individuals, alternative approaches to enhancing communication between
   the patient and provider are needed. We developed an iPad-based
   application that uses a heuristic approach to explore AYAs' cancer
   symptom experiences. In this mixed-methods descriptive study, 72 AYAs
   (13-29 years old) with cancer receiving myelosuppressive chemotherapy
   used the Computerized Symptom Capture Tool (C-SCAT) to create images of
   the symptoms and symptom clusters they experienced from a list of 30
   symptoms. They answered open-ended questions within the C-SCAT about the
   causes of their symptoms and symptom clusters. The images generated
   through the C-SCAT and accompanying free-text data were analyzed using
   descriptive, content, and visual analyses. Most participants (n=70)
   reported multiple symptoms (M=8.14). The most frequently reported
   symptoms were nausea (65.3%), feeling drowsy (55.6%), lack of appetite
   (55.6%), and lack of energy (55.6%). Forty-six grouped their symptoms
   into one or more clusters. The most common symptom cluster was
   nausea/eating problems/appetite problems. Nausea was most frequently
   named as the priority symptom in a cluster and as a cause of other
   symptoms. Although common threads were present in the symptoms
   experienced by AYAs, the graphic images revealed unique perspectives and
   a range of complexity of symptom relationships, clusters, and causes.
   Results highlight the need for a tailored approach to symptom management
   based on how the AYA with cancer perceives his or her symptom
   experience. (c) 2015 Wiley Periodicals, Inc.</abstract><date>DEC 2015</date><author>Ameringer, Suzanne
   Erickson, Jeanne M.
   Macpherson, Catherine Fiona
   Stegenga, Kristin
   Linder, Lauri A.</author></paper><paper><title>Adaptation of fluid model EULAG to graphics processing unit architecture</title><abstract>The goal of this study is to adapt the multiscale fluid solver EULerian
   or LAGrangian framewrok (EULAG) to future graphics processing units
   (GPU) platforms. The EULAG model has the proven record of successful
   applications, and excellent efficiency and scalability on conventional
   supercomputer architectures. Currently, the model is being implemented
   as the new dynamical core of the COSMO weather prediction framework.
   Within this study, two main modules of EULAG, namely the
   multidimensional positive definite advection transport algorithm
   (MPDATA) and the variational generalized conjugate residual, elliptic
   pressure solver Generalized Conjugate Residual (GCR) are analyzed and
   optimized. In this paper, a method is proposed, which ensures a
   comprehensive analysis of the resource consumption including registers,
   shared, and global memories. This method allows us to identify
   bottlenecks of the algorithm, including data transfers between host and
   global memory, global and shared memories, as well as GPU occupancy. We
   put the emphasis on providing a fixed memory access pattern, padding as
   well as organizing computation in the MPDATA algorithm. The testing and
   validation of the new GPU implementation have been carried out based on
   modeling decaying turbulence of a homogeneous incompressible fluid in a
   triply-periodic cube. Simulations performed using the standard version
   of EULAG and its new GPU implementation give similar solutions.
   Preliminary results show a promising increase in terms of computational
   efficiency. Copyright (c) 2014 John Wiley &amp; Sons, Ltd.</abstract><date>MAR 25 2015</date><author>Rojek, Krzysztof Andrzej
   Ciznicki, Milosz
   Rosa, Bogdan
   Kopta, Piotr
   Kulczewski, Michal
   Kurowski, Krzysztof
   Piotrowski, Zbigniew Pawel
   Szustak, Lukasz
   Wojcik, Damian Karol
   Wyrzykowski, Roman</author></paper><paper><title>Homogeneous nucleation and microstructure evolution in million-atom
   molecular dynamics simulation</title><abstract>Homogeneous nucleation from an undercooled iron melt is investigated by
   the statistical sampling of million-atom molecular dynamics (MD)
   simulations performed on a graphics processing unit (GPU). Fifty
   independent instances of isothermal MD calculations with one million
   atoms in a quasi-two-dimensional cell over a nanosecond reveal that the
   nucleation rate and the incubation time of nucleation as functions of
   temperature have characteristic shapes with a nose at the critical
   temperature. This indicates that thermally activated homogeneous
   nucleation occurs spontaneously in MD simulations without any inducing
   factor, whereas most previous studies have employed factors such as
   pressure, surface effect, and continuous cooling to induce nucleation.
   Moreover, further calculations over ten nanoseconds capture the
   microstructure evolution on the order of tens of nanometers from the
   atomistic viewpoint and the grain growth exponent is directly estimated.
   Our novel approach based on the concept of "melting pots in a
   supercomputer" is opening a new phase in computational metallurgy with
   the aid of rapid advances in computational environments.</abstract><date>AUG 27 2015</date><author>Shibuta, Yasushi
   Oguchi, Kanae
   Takaki, Tomohiro
   Ohno, Munekazu</author></paper><paper><title>u-CARE: user-friendly Comprehensive Antibiotic resistance Repository of
   Escherichia coli</title><abstract>Background and aims Despite medical advancements, Escherichia
   coli-associated infections remain a major public health concern and
   although an abundant information about E. coli and its antibiotic
   resistance mechanisms is available, no effective tool exists that
   integrates gene and genomic data in context to drug resistance, thus
   raising a need to develop a repository that facilitates integration and
   assimilation of factors governing drug resistance in E.
   coli.Descriptions User-friendly Comprehensive Antibiotic resistance
   Repository of Escherichia coli (u-CARE) is a manually curated catalogue
   of 52 antibiotics with reported resistance, 107 genes, transcription
   factors and single nucleotide polymorphism (SNPs) involved in multiple
   drug resistance of this pathogen. Each gene page provides detailed
   information about its resistance mechanisms, while antibiotic page
   consists of summary, chemical description and structural descriptors
   with links to external public databases like GO, CDD, DEG, Ecocyc, KEGG,
   Drug Bank, PubChem and UniProt. Moreover, the database integrates this
   reductive information to holistic data such as strain-specific and
   segment-specific pathogenic islands and operons. In addition, the
   database offers rich user interface for the visualisation and retrieval
   of information using various search criteria such as sequence, keyword,
   image and class search.Conclusions u-CARE is aimed to cater to the needs
   of researchers working in the field of antimicrobial drug resistance
   with minimal knowledge of bioinformatics. This database is also intended
   as a guide book to medical practitioners to avoid use of antibiotics
   against which resistance has already been reported in E. coli. The
   database is available from: http://www.e-bioinformatics.net/ucare</abstract><date>AUG 2015</date><author>Saha, Saurav B.
   Uttam, Vishwas
   Verma, Vivek</author></paper><paper><title>GPU phase-field lattice Boltzmann simulations of growth and motion of a
   binary alloy dendrite</title><abstract>A GPU code has been developed for a phase-field lattice Boltzmann (PFLB)
   method, which can simulate the dendritic growth with motion of solids in
   a dilute binary alloy melt. The GPU accelerated PFLB method has been
   implemented using CUDA C. The equiaxed dendritic growth in a shear flow
   and settling condition have been simulated by the developed GPU code. It
   has been confirmed that the PFLB simulations were efficiently
   accelerated by introducing the GPU computation. The characteristic
   dendrite morphologies which depend on the melt flow and the motion of
   the dendrite could also be confirmed by the simulations.</abstract><date>2015</date><author>Takaki, T.
   Rojas, R.
   Ohno, M.
   Shimokawabe, T.
   Aoki, T.</author></paper><paper><title>GLES: A Practical GPGPU Optimizing Compiler Using Data Sharing and
   Thread Coarsening</title><abstract>Writing optimized CUDA programs for General Purpose Graphics Processing
   Unit (GPGPU) is complicated and error-prone. Most of the former compiler
   optimization methods are impractical for many applications that contain
   divergent control flows, and they failed to fully exploit optimization
   opportunities in data sharing and thread coarsening. In this paper, we
   present GLES, an optimizing compiler for GPGPU programs. GLES proposes
   two optimization techniques based on divergence analysis. The first one
   is data sharing optimization for data reuse and bandwidth enhancement.
   The other one is thread granularity coarsening for reducing redundant
   instructions. Our experiments on 6 real-world programs show that GPGPU
   programs optimized by GLES achieve similar performance compared with
   manually tuned GPGPU programs. Furthermore, GLES is not only applicable
   to a much wider range of GPGPU programs than the state-of-art GPGPU
   optimizing compiler, but it also achieves higher or close performance on
   8 out of 9 benchmarks.</abstract><date>2015</date><author>Lin, Zhen
   Gao, Xiaopeng
   Wan, Han
   Jiang, Bo</author></paper><paper><title>An efficient solution for hazardous geophysical flows simulation using
   GPUs</title><abstract>The movement of poorly sorted material over steep areas constitutes a
   hazardous environmental problem. Computational tools help in the
   understanding and predictions of such landslides. The main drawback is
   the high computational effort required for obtaining accurate numerical
   solutions due to the high number of cells involved in the calculus. In
   order to overcome this problem, this work proposes the use of GPUs for
   decreasing significantly the CPU simulation time. The numerical scheme
   implemented in GPU is based on a finite volume scheme and it was
   validated in previous work with exact solutions and experimental data.
   The computational cost time obtained with the Graphical Hardware
   technology, GPU, is compared against Single-Core (sequential) and
   Multi-Core (parallel) CPU implementations. The GPU implementation allows
   to reduce the computational cost time in two orders of magnitude. (C)
   2015 Elsevier Ltd. All rights reserved.</abstract><date>MAY 2015</date><author>Lacasta, A.
   Juez, C.
   Murillo, J.
   Garcia-Navarro, P.</author></paper><paper><title>Acceleration of the Iterative Physical Optics Using Graphic Processing
   Unit</title><abstract>This paper shows the acceleration of iterative physical optics(IPO) for
   radar cross section(RCS) by using two techniques effectively. For the
   analysis of the multiple reflection in the cavity, IPO uses the near
   field method, unlike shooting and bouncing rays method which uses the
   geometric optics(GO). However, it is still far slower than physical
   optics(PO) and it is needed to accelerate the speed of IPO for practical
   purpose. In order to address this problem, graphic processing unit(GPU)
   can be applied to reduce calculation time and adaptive iterative
   physical optics-change rate(AIPO-CR) method is also applicable
   effectively to optimize iteration for acceleration of calculation.</abstract><date>2015</date><author>???
   ???</author></paper><paper><title>Visualization of multi-property landscapes for compound selection and
   optimization</title><abstract>Compound optimization generally requires considering multiple properties
   in concert and reaching a balance between them. Computationally, this
   process can be supported by multi-objective optimization methods that
   produce numerical solutions to an optimization task. Since a variety of
   comparable multi-property solutions are usually obtained further
   prioritization is required. However, the underlying multi-dimensional
   property spaces are typically complex and difficult to rationalize.
   Herein, an approach is introduced to visualize multi-property landscapes
   by adapting the concepts of star and parallel coordinates from computer
   graphics. The visualization method is designed to complement
   multi-objective compound optimization. We show that visualization makes
   it possible to further distinguish between numerically equivalent
   optimization solutions and helps to select drug-like compounds from
   multi-dimensional property spaces. The methodology is intuitive,
   applicable to a wide range of chemical optimization problems, and made
   freely available to the scientific community.</abstract><date>AUG 2015</date><author>de Leon, Antonio de la Vega
   Kayastha, Shilva
   Dimova, Dilyana
   Schultz, Thomas
   Bajorath, Juergen</author></paper><paper><title>Rapid calculation of gravity anomalies based on residual node densities
   and its GPU implementation</title><abstract>Gravity forward modeling is an important part of three dimensional
   physical property inversion; however, its time-consuming nature
   restricts its practical application. Several authors have made efforts
   to develop this procedure and have achieved some success. This paper
   proposes a new simple and fast approach for computing gravity anomalies
   at arbitrary points. In our method, the subsurface is divided into a 3D
   array of rectangular prismatic blocks, each with a constant density.
   Then the block density model is transformed into a residual node density
   model. Finally, gravity anomalies at survey points are calculated using
   the new residual node density-based forward formula. This method is
   implemented on a CPU-GPU heterogeneous architecture with the CUDA
   FORTRAN language and achieves more than four times acceleration compared
   to the classical method in the same computational environment and two
   orders of magnitude acceleration compared to one CPU processor on
   synthetic random density model tests. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>OCT 2015</date><author>Zhang Sheng
   Meng Xiaohong
   Chen Zhaoxi
   Zhou Junjie</author></paper><paper><title>Real-Time Three-Dimensional Cell Segmentation in Large-Scale Microscopy
   Data of Developing Embryos</title><abstract>d We present the Real-time Accurate Cell-shape Extractor (RACE), a
   high-throughput image analysis framework for automated three-dimensional
   cell segmentation in large-scale images. RACE is 55330 times faster and
   2-5 times more accurate than state-of-the-art methods. We demonstrate
   the generality of RACE by extracting cell-shape information from entire
   Drosophila, zebrafish, and mouse embryos imaged with confocal and
   light-sheet microscopes. Using RACE, we automatically reconstructed
   cellular-resolution tissue anisotropy maps across developing Drosophila
   embryos and quantified differences in cell-shape dynamics in wild-type
   and mutant embryos. We furthermore integrated RACE with our framework
   for automated cell lineaging and performed joint segmentation and cell
   tracking in entire Drosophila embryos. RACE processed these
   terabyte-sized datasets on a single computer within 1.4 days. RACE is
   easy to use, as it requires adjustment of only three parameters, takes
   full advantage of state-of-the-art multi-core processors and graphics
   cards, and is available as open-source software for Windows, Linux, and
   Mac OS.</abstract><date>JAN 25 2016</date><author>Stegmaier, Johannes
   Amat, Fernando
   Lemon, William C.
   McDole, Katie
   Wan, Yinan
   Teodoro, George
   Mikut, Ralf
   Keller, Philipp J.</author></paper><paper><title>Euler-Rodrigues formula variations, quaternion conjugation and intrinsic
   connections</title><abstract>This paper reviews the Euler-Rodrigues formula in the axis-angle
   representation of rotations, studies its variations and derivations in
   different mathematical forms as vectors, quaternions and Lie groups and
   investigates their intrinsic connections. The Euler-Rodrigues formula in
   the Taylor series expansion is presented and its use as an exponential
   map of Lie algebras is discussed particularly with a non-normalized
   vector. The connection between Euler-Rodrigues parameters and the
   Euler-Rodrigues formula is then demonstrated through quaternion
   conjugation and the equivalence between quaternion conjugation and an
   adjoint action of the Lie group is subsequently presented. The paper
   provides a rich reference for the Euler-Rodrigues formula, the
   variations and their connections and for their use in rigid body
   kinematics, dynamics and computer graphics. (C) 2015 The Author.
   Published by Elsevier Ltd.</abstract><date>OCT 2015</date><author>Dai, Jian S.</author></paper><paper><title>Psoralen derivatives as inhibitors of NF- interaction: the critical role
   of the furan ring</title><abstract>Simplified analogues of previously reported NF- interaction inhibitors,
   lacking the furan moiety, were synthesized and evaluated by performing
   experiments based on electrophoretic mobility shift assay (EMSA). The
   synthetic modifications led to simpler coumarin derivatives with lower
   activity allowing to better understand the minimal structural
   requirement for the binding to NF-kappa B.[GRAPHICS]</abstract><date>AUG 2015</date><author>Marzaro, Giovanni
   Lampronti, Ilaria
   Borgatti, Monica
   Manzini, Paolo
   Gambari, Roberto
   Chilin, Adriana</author></paper><paper><title>A Novel 3D Graphics DRAM Architecture for High-Performance and
   Low-Energy Memory Accesses</title><abstract>This paper presents a high-bandwidth 3D graphics DRAM architecture
   (3D-SGDRAM) with reduced access time and energy consumption. A novel 3D
   bank organization is employed with TSVs at subarray-level granularity to
   activate an optimal number of subarrays in lock-step to guarantee fast
   and low-energy memory access without significant area overhead. A new
   bitline interface enables access to only a selective group of bitlines
   in all active subarrays during a memory transaction, which greatly
   reduces row activation energy with optimal page size. Experimental
   results with CUDA benchmarks indicate that 3D-SGDRAM yields 57.5%,
   77.7%, and 45.2% improvements in power, latency, and energy-delay
   product (EDP) on average over state-of-the-art GDDR5 and GDDR5M
   solutions.</abstract><date>2015</date><author>Thakkar, Ishan G.
   Pasricha, Sudeep</author></paper><paper><title>Parallel resolution of the 3D Helmholtz equation based on multi-graphics
   processing unit clusters</title><abstract>The resolution of the 3D Helmholtz equation is required in the
   development of models related to a wide range of scientific and
   technological applications. For solving this equation in complex
   arithmetic, the biconjugate gradient (BCG) method is one of the most
   relevant solvers. However, this iterative method has a high
   computational cost because of the large sparse matrix and the vector
   operations involved. In this paper, a specific BCG method, adapted for
   the regularities of the Helmholtz equation is presented. This BCG is
   based on the implementation of a novel format (named Regular Format')
   that allows the storage of the large sparse matrix involved in the
   sparse matrix vector product in a compact form. The contribution of this
   work is twofold: (1) decreasing the memory requirements of the 3D
   Helmholtz equation using the Regular Format' and (2) speeding up the
   resolution of the equation using high performance computing resources. A
   hybrid Message Passing Interface (MPI)-graphics processing unit CUDA GPU
   parallelization that is capable of solving complex problems in short
   time has carried out (Fast-Helmholtz). Fast-Helmholtz combines
   optimizations at Message Passing Interface and GPU levels to reduce
   communications costs and to improve the exploitation of GPU
   architecture. This strategy makes it possible to extend the dimension of
   the Helmholtz problem to be solved, thanks to the relevant reduction of
   memory requirements and runtime. Copyright (c) 2014 John Wiley &amp; Sons,
   Ltd.</abstract><date>SEP 10 2015</date><author>Ortega, Gloria
   Lobera, Julia
   Garcia, Inmaculada
   Pilar Arroyo, M.
   Garzon, Ester M.</author></paper><paper><title>Improving the user experience of the rCUDA remote GPU virtualization
   framework</title><abstract>Graphics processing units (GPUs) are being increasingly embraced by the
   high-performance computing community as an effective way to reduce
   execution time by accelerating parts of their applications. remote CUDA
   (rCUDA) was recently introduced as a software solution to address the
   high acquisition costs and energy consumption of GPUs that constrain
   further adoption of this technology. Specifically, rCUDA is a middleware
   that allows a reduced number of GPUs to be transparently shared among
   the nodes in a cluster. Although the initial prototype versions of rCUDA
   demonstrated its functionality, they also revealed concerns with respect
   to usability, performance, and support for new CUDA features. In
   response, in this paper, we present a new rCUDA version that (1)
   improves usability by including a new component that allows an automatic
   transformation of any CUDA source code so that it conforms to the needs
   of the rCUDA framework, (2) consistently features low overhead when
   using remote GPUs thanks to an improved new communication architecture,
   and (3) supports multithreaded applications and CUDA libraries. As a
   result, for any CUDA-compatible program, rCUDA now allows the use of
   remote GPUs within a cluster with low overhead, so that a single
   application running in one node can use all GPUs available across the
   cluster, thereby extending the single-node capability of CUDA. Copyright
   (C) 2014 John Wiley &amp; Sons, Ltd.</abstract><date>SEP 25 2015</date><author>Reano, Carlos
   Silla, Federico
   Castello, Adrian
   Pena, Antonio J.
   Mayo, Rafael
   Quintana-Orti, Enrique S.
   Duato, Jose</author></paper><paper><title>GPUSCAN: GPU-Based Parallel Structural Clustering Algorithm for Networks</title><abstract>This paper presents a massively parallel implementation of a prominent
   network clustering algorithm, the structural clustering algorithm for
   networks (SCAN), on a graphical processing unit (GPU). SCAN is a fast
   and efficient clustering technique for finding hidden communities and
   isolating hubs/outliers within a network. However, for very large
   networks, it still takes considerable amount of time. With the
   introduction of massively parallel Compute Unified Device Architecture
   (CUDA) by Nvidia, applications properly employing GPUs are demonstrating
   high speed up. In current study, GPUSCAN, a CUDA based parallel
   implementation of SCAN, is presented. SCAN's computation steps have been
   carefully redesigned to run very efficiently on the GPU by transforming
   SCAN into a series of highly regular and independent concurrent
   operations. All intermediate data structures are created in the GPU to
   efficiently benefit from GPU's memory hierarchy. How these structures
   reformed and represented in the GPU memory hierarchy are illustrated.
   Now, through GPUSCAN, a large network or a batch of disjoint networks
   can be offloaded to the GPU for very fast and equivalent structural
   clustering. The performance of the GPU accelerated structural clustering
   has been shown to be much faster than the sequential CPU implementation.
   Both GPUSCAN and SCAN are tested on different size artificial and
   real-world networks. Results indicate that network becomes larger
   GPUSCAN significantly over performs SCAN. In tested datasets, speed-up
   of over 500-fold is achieved. For instance, calculating structural
   similarity and clustering of 5.5 million edges of the California road
   network in GPUSCAN is 513-fold faster than the serial version of SCAN.</abstract><date>DEC 2015</date><author>Stovall, Thomas Ryan
   Kockara, Sinan
   Avci, Recep</author></paper><paper><title>Computer graphics "Made in Germany" Darmstadt, the leading "Computer
   Graphics and Visual Computing Hub" in Europe: The way from 1975 to 2014</title><abstract>The paper reports on the 40 years of development of Computer Graphics
   and, more recently, Visual computing (VC) at the Technische Universitat
   Darmstadt in Germany, from its beginning in 1975 to the leading
   "Computer Graphics and Visual Computing Hub" in Europe as of 2014. This
   development is described along three axes. First, the institutional
   development and its rational to establish Computer Graphics as a
   discipline of Computer Science and as an enabling technology for
   developing our Knowledge Society, are described. Second, the scientific
   and technological impact based on the teaching activities and the large
   number of theses submitted in Darmstadt for the area during these 40
   years are addressed. Finally, the research road maps of the Computer
   Graphics and Visual Computing Hub in Darmstadt are presented relatively
   to the different stages of CG and VC research, relatively to a
   scientific view to the large number of projects implemented over these
   40 years and, finally, also relatively to the project results as seen
   from the media. In order to manage the quantity as well as the
   complexity of the information available, the description of these road
   maps is divided in four time periods: 1975-1984, 1985-1994,1995-2004 and
   2004-2015. The paper also gives the view of the authors on how they see
   the future of Computer Graphics and Visual Computing. At the end, the
   paper includes an extensive list of references for the reported content.
   (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>DEC 2015</date><author>Encarnacao, Jose L.
   Fellner, Dieter W.</author></paper><paper><title>Evaluating the Quality of Face Alignment without Ground Truth</title><abstract>The study of face alignment has been an area of intense research in
   computer vision, with its achievements widely used in computer graphics
   applications. The performance of various face alignment methods is often
   image-dependent or somewhat random because of their own strategy. This
   study aims to develop a method that can select an input image with good
   face alignment results from many results produced by a single method or
   multiple ones. The task is challenging because different face alignment
   results need to be evaluated without any ground truth. This study
   addresses this problem by designing a feasible feature extraction scheme
   to measure the quality of face alignment results. The feature is then
   used in various machine learning algorithms to rank different face
   alignment results. Our experiments show that our method is promising for
   ranking face alignment results and is able to pick good face alignment
   results, which can enhance the overall performance of a face alignment
   method with a random strategy. We demonstrate the usefulness of our
   ranking-enhanced face alignment algorithm in two practical applications:
   face cartoon stylization and digital face makeup.</abstract><date>OCT 2015</date><author>Sheng, Kekai
   Dong, Weiming
   Kong, Yan
   Mei, Xing
   Li, Jilin
   Wang, Chengjie
   Huang, Feiyue
   Hu, Bao-Gang</author></paper><paper><title>Speeding up the log-polar transform with inexpensive parallel hardware:
   graphics units and multi-core architectures</title><abstract>Log-polar imaging is a kind of foveal, biologically inspired visual
   representation with advantageous properties in practical applications in
   computer vision, robotics, and other fields. While the cheapest, most
   flexible, and most common approach to get log-polar images is to use
   software-based mappers, this solution entails a cost which prevents
   certain experiments or applications from being feasible. This may be the
   case in some real-time (robotic) applications and, in general, when the
   conversion cost is not affordable for the task at hand. To overcome this
   drawback and make log-polar imaging more generally available, parallel
   solutions with affordable modern multi-core architectures have been
   devised, implemented, and tested in this work. Experimental results
   reveal that speed-up factors as high as or higher than 10 or 20,
   depending on the configuration, are possible to get log-polar images
   from large gray-level or color cartesian images using commodity graphics
   processors. Remarkable speedups are also reported for current multi-core
   processors. This noteworthy performance allows visual tasks that would
   otherwise be unthinkable with sequential implementations to become
   feasible. Additionally, since three different approaches have been
   explored and compared in terms of several criteria, different
   cost-effective choices are advisable depending on different visual task
   requirements or hardware availability.</abstract><date>SEP 2015</date><author>Antonelli, Marco
   Igual, Francisco D.
   Ramos, Francisco
   Traver, V. Javier</author></paper><paper><title>A Fast Parallel Implementation of a PTAS for Fractional Packing and
   Covering Linear Programs</title><abstract>We present a parallel implementation of the randomized approximation
   algorithm for packing and covering linear programs presented by
   Koufogiannakis and Young (2007). Their approach builds on ideas of the
   sublinear time algorithm of Grigoriadis and Khachiyan's (Oper Res Lett
   18(2):53-58, 1995) and Garg and Konemann's (SIAM J Comput 37(2):630-652,
   2007) non-uniform-increment amortization scheme. With high probability
   it computes a feasible primal and dual solution whose costs are within a
   factor of of the optimal cost. In order to make their algorithm more
   parallelizable we also implemented a deterministic version of the
   algorithm, i.e. instead of updating a single random entry at each
   iteration we updated deterministically many entries at once. This slowed
   down a single iteration of the algorithm but allowed for larger
   step-sizes which lead to fewer iterations. We use NVIDIA's parallel
   computing architecture CUDA for the parallel environment. We report a
   speedup between one and two orders of magnitude over the times reported
   by Koufogiannakis and Young (2007).</abstract><date>OCT 2015</date><author>Jelic, Slobodan
   Laue, Soeren
   Matijevic, Domagoj
   Wijerama, Patrick</author></paper><paper><title>EFFECT OF DATA LAYOUT IN THE EVALUATION TIME OF NON-SEPARABLE FUNCTIONS
   ON GPU</title><abstract>CPUs are able to provide a tremendous computational power, but their
   optimal usage requires the optimization of memory access. The many
   threads available can mitigate the long memory access latencies, but
   this usually demands a reorganization of the data and algorithm to reach
   the performance peak. The addressed problem is to know which data layout
   produces a faster evaluation when dealing with population-based
   evolutionary algorithms optimizing non-separable functions. This
   knowledge will allow a more efficient design of evolutionary algorithms.
   Depending on the fitness function and the problem size, the most
   suitable layout can be implemented at the design phase of the algorithm,
   avoiding later costly code or data layout redesigns. In this paper,
   diverse non-separable functions, such as Rosenbrock and Rana functions,
   and data layouts are evaluated. The implemented layouts cover main
   techniques to maximize the performance: coalesced access to global
   memory, intensive use of on-chip memory: shared memory and registers,
   and variable reuse to minimize the global memory transactions.
   Conclusions about the optimum data layout related to the characteristics
   of the fitness function and the problem size are stated. Besides, the
   conclusions ease the decision-making process for future implementations
   of other non-separable functions.</abstract><date>2015</date><author>Cardenas-Montes, Miguel
   Vega-Rodriguez, Miguel A.</author></paper><paper><title>Label-free in vivo imaging of peripheral nerve by multispectral
   photoacoustic tomography</title><abstract>Unintentional surgical damage to nerves is mainly due to poor
   visualization of nerve tissue relative to adjacent structures.
   Multispectral photoacoustic tomography can provide chemical information
   with specificity and ultrasonic spatial resolution with centimeter
   imaging depth, making it a potential tool for noninvasive neural
   imaging. To implement this label-free imaging approach, a multispectral
   photoacoustic tomography platform was built. Imaging depth and spatial
   resolution were characterized. In vivo imaging of the femoral nerve that
   is 2 mm deep in a nude mouse was performed. Through multivariate curve
   resolution analysis, the femoral nerve was discriminated from the
   femoral artery and chemical maps of their spatial distributions were
   generated.[GRAPHICS]The femoral nerve was discriminated from the femoral
   artery by multivariate curve resolution analysis.</abstract><date>JAN 2016</date><author>Li, Rui
   Phillips, Evan
   Wang, Pu
   Goergen, Craig J.
   Cheng, Ji-Xin</author></paper><paper><title>Diffusion accessibility as a method for visualizing macromolecular
   surface geometry</title><abstract>Important three-dimensional spatial features such as depth and surface
   concavity can be difficult to convey clearly in the context of
   two-dimensional images. In the area of macromolecular visualization, the
   computer graphics technique of ray-tracing can be helpful, but further
   techniques for emphasizing surface concavity can give clearer
   perceptions of depth. The notion of diffusion accessibility is
   well-suited for emphasizing such features of macromolecular surfaces,
   but a method for calculating diffusion accessibility has not been made
   widely available. Here we make available a web-based platform that
   performs the necessary calculation by solving the Laplace equation for
   steady state diffusion, and produces scripts for visualization that
   emphasize surface depth by coloring according to diffusion
   accessibility. The URL is http://services.mbi.ucla.edu/DiffAcc/.</abstract><date>OCT 2015</date><author>Tsai, Yingssu
   Holton, Thomas
   Yeates, Todd O.</author></paper><paper><title>Computational Fluid Dynamics Computations Using a Preconditioned Krylov
   Solver on Graphical Processing Units</title><abstract>Graphical processing unit (GPU) computation in recent years has seen
   extensive growth due to advancement in both hardware and software stack.
   This has led to increase in the use of GPUs as accelerators across a
   broad spectrum of applications. This work deals with the use of general
   purpose GPUs for performing computational fluid dynamics (CFD)
   computations. The paper discusses strategies and findings on porting a
   large multifunctional CFD code to the GPU architecture. Within this
   framework, the most compute intensive segment of the software, the
   BiCGStab linear solver using additive Schwarz block preconditioners with
   point Jacobi iterative smoothing is optimized for the GPU platform using
   various techniques in CUDA Fortran. Representative turbulent channel and
   pipe flow are investigated for validation and benchmarking purposes.
   Both single and double precision calculations are highlighted. For a
   modest single block grid of 64 x 64 x 64, the turbulent channel flow
   computations showed a speedup of about eightfold in double precision and
   more than 13-fold for single precision on the NVIDIA Tesla GPU over a
   serial run on an Intel central processing unit (CPU). For the pipe flow
   consisting of 1.78 x 10(6) grid cells distributed over 36 mesh blocks,
   the gains were more modest at 4.5 and 6.5 for double and single
   precision, respectively.</abstract><date>JAN 2016</date><author>Amritkar, Amit
   Tafti, Danesh</author></paper><paper><title>Assessment of freeware programs for the reconstruction of tomography
   datasets obtained with a monochromatic synchrotron-based X-ray source</title><abstract>Synchrotron-based in-line phase-contrast computed tomography (PC-CT)
   allows soft tissue to be imaged with sub-gross resolution and has
   potential to be used as a diagnostic tool. The reconstruction and
   processing of in-line PC-CT datasets is a computationally demanding
   task; thus, an efficient and user-friendly software program is
   desirable. Four freeware programs (NRecon, PITRE, H-PITRE and Athabasca
   Recon) were compared for the availability of features such as dark- and
   flat-field calibration, beam power normalization, ring artifact removal,
   and alignment tools for optimizing image quality. An in-line PC-CT
   projection dataset (3751 projections, 180 degrees rotation, 10.13 mm x
   0.54 mm) was collected from a formalin-fixed canine prostate at the
   Biomedical Imaging and Therapy Bending Magnet (BMIT-BM) beamline of the
   Canadian Light Source. This dataset was processed with each of the four
   software programs and usability of the program was evaluated. Efficiency
   was assessed by how each program maximized computer processing power
   during computation. Athabasca Recon had the least-efficient memory
   usage, least user-friendly interface, and lacked a ring artifact removal
   feature. NRecon, PITRE and H-PITRE produced similar quality images, but
   the Athabasca Recon reconstruction suffered from the lack of a native
   ring remover algorithm. The 64-bit version of NRecon uses GPU (graphics
   processor unit) memory for accelerated processing and is userfriendly,
   but does not provide necessary parameters for in-line PC-CT data, such
   as dark- field and flat-field correction and beam power normalization.
   PITRE has many helpful features and tools, but lacks a comprehensive
   user manual and help section. H-PITRE is a condensed version of PITRE
   and maximizes computer memory for efficiency. To conclude, NRecon has
   fewer imaging processing tools than PITRE and H-PITRE, but is ideal for
   less experienced users due to a simple user interface. Based on the
   quality of reconstructed images, efficient use of computer memory and
   parameter availability, H-PITRE was the preferred of the four programs
   compared.</abstract><date>JUL 2015</date><author>Wolkowski, Bailey
   Snead, Elisabeth
   Wesolowski, Michal
   Singh, Jaswant
   Pettitt, Murray
   Chibbar, Rajni
   Melli, Seyedali
   Montgomery, James</author></paper><paper><title>Grow-Cut Based Automatic cDNA Microarray Image Segmentation</title><abstract>Complementary DNA (cDNA) microarray is a well-established tool for
   simultaneously studying the expression level of thousands of genes.
   Segmentation of microarray images is one of the main stages in a
   microarray experiment. However, it remains an arduous and challenging
   task due to the poor quality of images. Images suffer from noise,
   artifacts, and uneven background, while spots depicted on images can be
   poorly contrasted and deformed. In this paper, an original approach for
   the segmentation of cDNA microarray images is proposed. First, a
   preprocessing stage is applied in order to reduce the noise levels of
   the microarray image. Then, the grow-cut algorithm is applied separately
   to each spot location, employing an automated seed selection procedure,
   in order to locate the pixels belonging to spots. Application on
   datasets containing synthetic and real microarray images shows that the
   proposed algorithm performs better than other previously proposed
   methods. Moreover, in order to exploit the independence of the
   segmentation task for each separate spot location, both a multithreaded
   CPU and a graphics processing unit (GPU) implementation were evaluated.</abstract><date>JAN 2015</date><author>Katsigiannis, Stamos
   Zacharia, Eleni
   Maroulis, Dimitris</author></paper><paper><title>Multi-stage interactive genetic algorithm for collaborative product
   customization</title><abstract>Products are becoming increasingly more complex and intelligent, which
   requires users to participate in the design process in order to meet
   customer demands and enhance market competition. Interactive genetic
   algorithm (IGA) can effectively solve the optimization problem. However,
   the challenge still remains for IGA to ameliorate user fatigue and
   reduce the noise in the process of evolution. To address the issue, a
   multi-stage interactive genetic algorithm (MS-IGA) is proposed, which
   divides the large population of the traditional interactive genetic
   algorithm (TIGA) into several stages according to different functional
   requirements. The proposed MS-IGA is then applied to the car console
   conceptual design system, to better capture the knowledge of users'
   personalized requirements and accomplish the product design. This is
   especially important in the field of complex product configuration
   design, such as in cars, personal computers, smart phones and the like.
   Through the users' graphic interface, customers separately evaluate
   product design at every different stage of its evolution, which makes
   the proposed algorithm more directional than the TIGA. We also introduce
   genetic sense units, which represent different functional modules, in
   order to realize the customers' collaborative design. The extensive
   experimental results are provided to demonstrate that our proposed
   algorithm is correct and efficient according to the efficiency test,
   convergence analysis and fatigue test for application of the product
   design system, including car interior and other modular product. (C)
   2015 Elsevier B.V. All rights reserved.</abstract><date>JAN 15 2016</date><author>Dou, Runliang
   Zong, Chao
   Nan, Guofang</author></paper><paper><title>NFFinder: an online bioinformatics tool for searching similar
   transcriptomics experiments in the context of drug repositioning</title><abstract>Drug repositioning, using known drugs for treating conditions different
   from those the drug was originally designed to treat, is an important
   drug discovery tool that allows for a faster and cheaper development
   process by using drugs that are already approved or in an advanced trial
   stage for another purpose. This is especially relevant for orphan
   diseases because they affect too few people to make drug research de
   novo economically viable. In this paper we present NFFinder, a
   bioinformatics tool for identifying potential useful drugs in the
   context of orphan diseases. NFFinder uses transcriptomic data to find
   relationships between drugs, diseases and a phenotype of interest, as
   well as identifying experts having published on that domain. The
   application shows in a dashboard a series of graphics and tables
   designed to help researchers formulate repositioning hypotheses and
   identify potential biological relationships between drugs and diseases.
   NFFinder is freely available at http://nffinder.cnb.csic.es.</abstract><date>JUL 1 2015</date><author>Setoain, Javier
   Franch, Monica
   Martinez, Marta
   Tabas-Madrid, Daniel
   Sorzano, Carlos O. S.
   Bakker, Annette
   Gonzalez-Couto, Eduardo
   Elvira, Juan
   Pascual-Montano, Alberto</author></paper><paper><title>OpenACC programs of the Swendsen-Wang multi-cluster spin flip algorithm</title><abstract>We present sample OpenACC programs of the Swendsen-Wang multi-cluster
   spin flip algorithm. OpenACC is a directive-based programming model for
   accelerators without requiring modification to the underlying CPU code
   itself. In this paper, we deal with the classical spin models as with
   the sample CUDA programs (Komura and Okabe, 2014), that is,
   two-dimensional (2D) Ising model, three-dimensional (3D) Ising model, 2D
   Potts model, 3D Potts model, 2D XY model and 3D XY model. We explain the
   details of sample OpenACC programs and compare the performance of the
   present OpenACC implementations with that of the CUDA implementations
   for the 2D and 3D Ising models and the 2D and 3D XY models.Program
   summaryProgram title: SWspin_OpenACCCatalogue identifier:
   AEXU_v1_0Program summary URL:
   http://cpc.cs.qub.ac.uk/summaries/AEXU_v1_0.htmlProgram obtainable from:
   CPC Program Library, Queen's University, Belfast, N. IrelandLicensing
   provisions: Standard CPC licence,
   http://cpc.cs.qub.ac.uk/licence/licence.htmlNo. of lines in distributed
   program, including test data, etc.: 2898No. of bytes in distributed
   program, including test data, etc.: 9729Distribution format:
   tar.gzProgramming language: C, OpenACC.Computer: Any computer with an
   OpenACC-enabled accelerator (tested on NVIDIA GPU).Operating system: No
   limits (tested on Linux).RAM: About 1MiB for the parameters used in the
   sample programs.Classification: 23.Nature of problem: Monte Carlo
   simulation of classical spin systems. Ising model, q-state Potts model,
   and the classical XY model are treated for both two-dimensional and
   three-dimensional lattices.Solution method: Swendsen-Wang multi-cluster
   spin flip Monte Carlo method. The OpenACC implementation for the
   cluster-labeling is based on the work by Kalentev et al. [J. Parallel
   Distrib. Comput. 71 (2011) 615-620].Restrictions: The system size is
   limited depending on the memory of an accelerator.Running time: A few
   minutes per each program for the parameters used in the sample program.
   (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>DEC 2015</date><author>Komura, Yukihiro</author></paper><paper><title>Trichloroethylene degradation by persulphate with magnetite as a
   heterogeneous activator in aqueous solution</title><abstract>Iron oxide-magnetite (Fe3O4) as a heterogeneous activator to activate
   persulphate anions[GRAPHICS]for trichloroethylene (TCE) degradation was
   investigated in this study. The experimental results showed that TCE
   could be completely oxidized within 5 h by using 5 g L-1 magnetite and
   63 mM[GRAPHICS]indicating the effectiveness of the process for TCE
   removal. Various factors of the process, including.[GRAPHICS]and
   magnetite dosages, and initial solution pH, were evaluated, and TCE
   degradation fitted well to the pseudo-first-order kinetic model. The
   calculated kinetic rate constant was increased with
   increasing[GRAPHICS]and magnetite dosages, but it was independent of
   solution pH. In addition, the changes of magnetite morphology examined
   by scanning electron microscopy and X-ray powder diffraction,
   respectively, confirmed the slight corrosion with alpha-Fe2O3 coated on
   the magnetite surface. The probe compounds tests clearly identified the
   generation of the reactive oxygen species in the system. While the free
   radical quenching studies further demonstrated that[GRAPHICS]and center
   dot OH were the major radicals responsible for TCE degradation,
   whereas[GRAPHICS]contributed less in the system, and therefore the roles
   of reactive oxygen species on TCE degradation mechanisms were proposed
   accordingly. To our best knowledge, this is the first time the
   performance and mechanism of magnetite-activated persulphate oxidation
   for TCE degradation are reported. The findings of this study provided a
   new insight into the heterogeneous catalysis mechanism and showed a
   great potential for the practical application of this technique in in
   situ TCE-contaminated groundwater remediation.</abstract><date>JUN 3 2015</date><author>Ruan, Xiaoxin
   Gu, Xiaogang
   Lu, Shuguang
   Qiu, Zhaofu
   Sui, Qian</author></paper><paper><title>GPU-based acceleration of free energy calculations in solid state
   physics</title><abstract>Obtaining a thermodynamically accurate phase diagram through numerical
   calculations is a computationally expensive problem that is crucially
   important to understanding the complex phenomena of solid state physics,
   such as superconductivity. In this work we show how this type of
   analysis can be significantly accelerated through the use of modern
   GPUs. We illustrate this with a concrete example of free energy
   calculation in multi-band iron-based superconductors, known to exhibit a
   superconducting state with oscillating order parameter (OP). Our
   approach can also be used for classical BCS-type superconductors. With a
   customized algorithm and compiler tuning we are able to achieve a 19 x
   speedup compared to the CPU (119 x compared to a single CPU core),
   reducing calculation time from minutes to mere seconds, enabling the
   analysis of larger systems and the elimination of finite size
   effects.Program summaryProgram title: Free_EnergyCatalogue identifier:
   AEVX_v1_0Program summary URL:
   http://cpc.cs.qub.ac.uk/summaries/AEVX_v1_0.htmlProgram obtainable from:
   CPC Program Library, Queen's University, Belfast, N. IrelandLicensing
   provisions: GNU Lesser General Public License, version 3No. of lines in
   distributed program, including test data, etc.: 786No. of bytes in
   distributed program, including test data, etc.: 6304Distribution format:
   tar.gzProgramming language: Fortran, CUDA C.Computer: Any with a
   CUDA-compliant GPU.Operating system: No limits (tested on Linux).RAM:
   Typically tens of megabytes.Classification: 7, 6.5.Nature of problem:
   GPU-accelerated free energy calculations in multi-band iron-based
   superconductor models.Solution method: Parallel parameter space search
   for a global minimum of free energy.Unusual features: The same core
   algorithm is implemented in Fortran with OpenMP and OpenACC compiler
   annotations, as well as in CUDA C. The original Fortran implementation
   targets the CPU architecture, while the CUDA C version is hand-optimized
   for modern GPUs.Running time: Problem-dependent, up to several seconds
   for a single value of momentum and a linear lattice size on the order of
   10(3) (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>JUL 2015</date><author>Januszewski, Michal
   Ptok, Andrzej
   Crivelli, Dawid
   Gardas, Bartlomiej</author></paper><paper><title>3D Image Reconstructions and the Nyquist-Shannon Theorem</title><abstract>Fracture surfaces are occasionally modelled by Fourier's two-dimensional
   series that can be converted into digital 3D reliefs mapping the
   morphology of solid surfaces. Such digital replicas may suffer from
   various artefacts when processed inconveniently. Spatial aliasing is one
   of those artefacts that may devalue Fourier's replicas. According to the
   Nyquist-Shannon sampling theorem the spatial aliasing occurs when
   Fourier's frequencies exceed the Nyquist critical frequency. In the
   present paper it is shown that the Nyquist frequency is not the only
   critical limit determining aliasing artefacts but there are some other
   frequencies that intensify aliasing phenomena and form an infinite set
   of points at which numerical results abruptly and dramatically change
   their values. This unusual type of spatial aliasing is explored and some
   consequences for 3D computer reconstructions are presented.Graphical
   Abstract Fourier's replicas of scanned surfaces correctly reproduce all
   morphological features only if the number N of harmonic terms in the
   Fourier sum does not exceed a critical value derived from the Nyquist
   frequency of the scanned original. Incorporating harmonic terms of
   higher frequencies in the Fourier sum, an abrupt increase of aliasing
   effects occurs whenever the frequencies in the Fourier sum reach even
   multiples of the Nyquist frequency. Illustration N = 50 represents a
   correct reproduction whereas illustrations N = 228 and N = 440 show
   critical abrupt increases of aliasing artifacts.[GRAPHICS].</abstract><date>SEP 2015</date><author>Ficker, T.
   Martisek, D.</author></paper><paper><title>3D numerical simulations on GPUs of hyperthermia with nanoparticles by a
   nonlinear bioheat model</title><abstract>This paper deals with the numerical modeling of hyperthermia treatments
   by magnetic nanoparticles considering a 3D nonlinear Pennes' bioheat
   transfer model with a temperature-dependent blood perfusion in order to
   yield more accurate results. The tissue is modeled by considering skin,
   fat and muscle layers in addition to the tumor. The FDM in a
   heterogeneous medium is employed and the resulting system of nonlinear
   equations in the time domain is solved by a predictor multicorrector
   algorithm. Since the execution of the three-dimensional model requires a
   large amount of time, CUDA is used to speedup it. Experimental results
   showed that the parallelization with CUDA was very effective in
   improving performance, yielding gains up to 242 times when compared to
   the sequential execution time. (C) 2015 Elsevier B.V. All rights
   reserved.</abstract><date>MAR 15 2016</date><author>Reis, Ruy Freitas
   Loureiro, Felipe dos Santos
   Lobosco, Marcelo</author></paper><paper><title>Accelerating Smith-Waterman Alignment for Protein Database Search Using
   Frequency Distance Filtration Scheme Based on CPU-GPU Collaborative
   System.</title><abstract>The Smith-Waterman (SW) algorithm has been widely utilized for searching
   biological sequence databases in bioinformatics. Recently, several works
   have adopted the graphic card with Graphic Processing Units (GPUs) and
   their associated CUDA model to enhance the performance of SW
   computations. However, these works mainly focused on the protein
   database search by using the intertask parallelization technique, and
   only using the GPU capability to do the SW computations one by one.
   Hence, in this paper, we will propose an efficient SW alignment method,
   called CUDA-SWfr, for the protein database search by using the intratask
   parallelization technique based on a CPU-GPU collaborative system.
   Before doing the SW computations on GPU, a procedure is applied on CPU
   by using the frequency distance filtration scheme (FDFS) to eliminate
   the unnecessary alignments. The experimental results indicate that
   CUDA-SWfr runs 9.6 times and 96 times faster than the CPU-based SW
   method without and with FDFS, respectively. </abstract><date>2015</date><author>Liu, Yu
   Hong, Yang
   Lin, Chun-Yuan
   Hung, Che-Lun</author></paper><paper><title>GPU-based cluster-labeling algorithm without the use of conventional
   iteration: Application to the Swendsen-Wang multi-cluster spin flip
   algorithm</title><abstract>Cluster-labeling algorithms that use a single GPU can be roughly divided
   into direct and two-stage approaches. To date, both types use an
   iterative method to compare the labels of nearest-neighbor sites. In
   this paper, I present a GPU-based cluster-labeling algorithm that does
   not use conventional iteration. The proposed method is applicable to
   both direct algorithms and two-stage approaches. Under the proposed
   approach, only one comparison with the nearest-neighbor site is needed
   for a two-dimensional (2D) system, and just two comparisons are needed
   for three-dimensional (3D) systems. As an application of the new
   cluster-labeling algorithm, I consider the Swendsen-Wang (SW)
   multi-cluster spin flip algorithm. The performance of the proposed
   method is compared with that of other cluster-labeling algorithms for
   the SW multi-cluster spin flip problem using the 2D and 3D Ising models.
   As a result, the computation time of the new algorithm is shown to be
   40% faster than that of the previous algorithm for the 2D Ising model,
   and 20% faster than that of the previous algorithm for the 3D Ising
   model at the critical temperature. (C) 2015 Elsevier B.V. All rights
   reserved.</abstract><date>SEP 2015</date><author>Komura, Yukihiro</author></paper><paper><title>Point cloud modeling using the homogeneous transformation for
   non-cooperative pose estimation</title><abstract>A modeling process to simulate point cloud range data that a lidar
   (light detection and ranging) sensor produces is presented in this paper
   in order to support the development of non-cooperative pose (relative
   attitude and position) estimation approaches which will help improve
   proximity operation capabilities between two adjacent vehicles. The
   algorithms in the modeling process were based on the homogeneous
   transformation, which has been employed extensively in robotics and
   computer graphics, as well as in recently developed pose estimation
   algorithms. Using a flash lidar in a laboratory testing environment,
   point cloud data of a test article was simulated and compared against
   the "measured point cloud data. The simulated and measured data sets
   match closely, validating the modeling process. The modeling capability
   enables close examination of the characteristics of point cloud images
   of an object as it undergoes various translational and rotational
   motions. Relevant characteristics that will be crucial in
   non-cooperative pose estimation were identified such as shift,
   shadowing, perspective projection, jagged edges, and differential point
   cloud density. These characteristics will have to be considered in
   developing effective non-cooperative pose estimation algorithms. The
   modeling capability will allow extensive non-cooperative pose estimation
   performance simulations prior to field testing, saving development cost
   and providing performance metrics of the pose estimation concepts and
   algorithms under evaluation. The modeling process also provides "truth"
   pose of the test objects with respect to the sensor frame so that the
   pose estimation error can be quantified. Published by Elsevier Ltd. on
   behalf of IAA.</abstract><date>JUN-JUL 2015</date><author>Lim, Tae W.</author></paper><paper><title>Solving engineering models using hyperbolic matrix functions</title><abstract>In this paper a method for computing hyperbolic matrix functions based
   on Hermite matrix polynomial expansions is outlined. Hermite series
   truncation together with Paterson-Stockmeyer method allow to compute the
   hyperbolic matrix cosine efficiently. A theoretical estimate for the
   optimal value of its parameters is obtained. An efficient and
   highly-accurate Hermite algorithm and a MATLAB implementation have been
   developed. The MATLAB implementation has been compared with the MATLAB
   function funm on matrices of different dimensions, obtaining lower
   execution time and higher accuracy in most cases. To do this we used an
   NVIDIA Tesla K20 GPGPU card, the CUDA environment and MATLAB. With this
   implementation we get much better performance for large scale problems.
   (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>FEB 15 2016</date><author>Defez, Emilio
   Sastre, Jorge
   Ibanez, Javier
   Peinado, Jesus</author></paper><paper><title>Scalable high-dimensional dynamic stochastic economic modeling</title><abstract>We present a highly parallelizable and flexible computational method to
   solve high-dimensional stochastic dynamic economic models. Solving such
   models often requires the use of iterative methods, like time iteration
   or dynamic programming. By exploiting the generic iterative structure of
   this broad class of economic problems, we propose a parallelization
   scheme that favors hybrid massively parallel computer architectures.
   Within a parallel nonlinear time iteration framework, we interpolate
   policy functions partially on GPUs using an adaptive sparse grid
   algorithm with piecewise linear hierarchical basis functions. GPUs
   accelerate this part of the computation one order of magnitude thus
   reducing overall computation time by 50%. The developments in this paper
   include the use of a fully adaptive sparse grid algorithm and the use of
   a mixed MPI-Intel TBB-CUDA/Thrust implementation to improve the
   interprocess communication strategy on massively parallel architectures.
   Numerical experiments on "Piz Daint" (Cray XC30) at the Swiss National
   Supercomputing Centre show that high-dimensional international real
   business cycle models can be efficiently solved in parallel. To the best
   of our knowledge, this performance on a massively parallel petascale
   architecture for such nonlinear high-dimensional economic models has not
   been possible prior to present work. (C) 2015 Elsevier B.V. All rights
   reserved.</abstract><date>NOV 2015</date><author>Brumm, Johannes
   Mikushin, Dmitry
   Scheidegger, Simon
   Schenk, Olaf</author></paper><paper><title>PolyTop plus plus : an efficient alternative for serial and parallel
   topology optimization on CPUs &amp; GPUs</title><abstract>This paper presents the PolyTop++, an efficient and modular framework
   for parallel structural topology optimization using polygonal meshes. It
   consists of a C++ and CUDA (a parallel computing model for GPUs)
   alternative implementations of the PolyTop code by Talischi et al.
   (Struct Multidiscip Optim 45(3):329-357 2012b). PolyTop++ was designed
   to support both CPU and GPU parallel solutions. The software takes
   advantage of the C++ programming language and the CUDA model to design
   algorithms with efficient memory management, capable of solving
   large-scale problems, and uses its object-oriented flexibility in order
   to provide a modular scheme. We describe our implementation of different
   solvers for the finite element analysis, including both direct and
   iterative solvers, and an iterative 'matrix-free' solver; these were all
   implemented in serial and parallel modes, including a GPU version.
   Finally, we present numerical results for problems with about 40 million
   degrees of freedom both in 2D and 3D.</abstract><date>NOV 2015</date><author>Duarte, Leonardo S.
   Celes, Waldemar
   Pereira, Anderson
   Menezes, Ivan F. M.
   Paulino, Glaucio H.</author></paper><paper><title>Side-Channel Power Analysis of a GPU AES Implementation</title><abstract>Graphics Processing Units (GPUs) have been used to run a range of
   cryptographic algorithms. The main reason to choose a GPU is to
   accelerate the encryption/decryption speed. Since GPUs are mainly used
   for graphics rendering, and only recently have they become a
   fully-programmable parallel computing device, there has been little
   attention paid to their vulnerability to side-channel attacks.In this
   paper we present a study of side-channel vulnerability on a
   state-of-the-art graphics processor. To the best of our knowledge, this
   is the first work that attempts to extract the secret key of a block
   cipher implemented to run on a GPU. We present a side-channel power
   analysis methodology to extract all of the last round key bytes of a
   CUDA AES (Advanced Encryption Standard) implementation run on an NVIDIA
   TESLA GPU. We describe how we capture power traces and evaluate the
   power consumption of a GPU. We then construct an appropriate power model
   for the GPU. We propose effective methods to sample and process the GPU
   power traces so that we can recover the secret key of AES. Our results
   show that parallel computing hardware systems such as a GPU are highly
   vulnerable targets to power-based side-channel attacks, and need to be
   hardened against side-channel threats.</abstract><date>2015</date><author>Luo, Chao
   Fei, Yunsi
   Luo, Pei
   Mukherjee, Saoni
   Kaeli, David</author></paper><paper><title>Evaluating an Online Cognitive Training Platform for Older Adults: User
   Experience and Implementation Requirements</title><abstract></abstract><date>AUG 2015</date><author>Haesner, Marten
   Steinert, Anika
   Weichenberger, Markus</author></paper><paper><title>GPGPU optimized parallel implementation of AES using C plus plus AMP</title><abstract>Nowadays, the characterization of a computing system using attributes
   like "single core" is, for most applications, deprecated. Multiprocessor
   or multi-core platforms are, now, widespread and serve for solving more
   and more complex problems in shorter execution times. Video cards make
   no exception to this rule since, for the past years, they are based on
   powerful GPUs with high parallelism architectures and extremely fast
   memories. In addition, new development languages and platforms became
   available for the programmers. This way, the processing power of the GPU
   (Graphics Processing Unit) can now be used even for non-video or
   non-graphics applications that imply a serious amount of parallel
   processing. This paper presents a comparative study of AES algorithm
   implementation on CPU and two different GPGPU platforms. Similar studies
   involving GPGPUs are based on Nvidia's CUDA platform but this approach
   imposes a severe limitation over the application portability. In our
   approach a platform independent application was designed and implemented
   using C-H- AMP, the latest C++ extension oriented to parallel
   programming. Tests were conducted over two GPGPU platforms, one from
   NVidia and one from AMD and a multi-core CPU from Intel. Results show
   that cross-platform portability was achieved while the performances are
   similar or better as compared to similar studies.</abstract><date>JUN 2015</date><author>Munteanu, Gabriel
   Mocanu, Stefan
   Samu, Daniela</author></paper><paper><title>CUDA-quicksort: an improved GPU-based implementation of quicksort</title><abstract>Sorting is a very important task in computer science and becomes a
   critical operation for programs making heavy use of sorting algorithms.
   General-purpose computing has been successfully used on Graphics
   Processing Units (GPUs) to parallelize some sorting algorithms. Two
   GPU-based implementations of the quicksort were presented in literature:
   the GPU-quicksort, a compute-unified device architecture (CUDA)
   iterative implementation, and the CUDA dynamic parallel (CDP) quicksort,
   a recursive implementation provided by NVIDIA Corporation. We propose
   CUDA-quicksort an iterative GPU-based implementation of the sorting
   algorithm. CUDA-quicksort has been designed starting from GPU-quicksort.
   Unlike GPU-quicksort, it uses atomic primitives to perform inter-block
   communications while ensuring an optimized access to the GPU memory.
   Experiments performed on six sorting benchmark distributions show that
   CUDA-quicksort is up to four times faster than GPU-quicksort and up to
   three times faster than CDP-quicksort. An in-depth analysis of the
   performance between CUDA-quicksort and GPU-quicksort shows that the main
   improvement is related to the optimized GPU memory access rather than to
   the use of atomic primitives. Moreover, in order to assess the
   advantages of using the CUDA dynamic parallelism, we implemented a
   recursive version of the CUDA-quicksort. Experimental results show that
   CUDA-quicksort is faster than the CDP-quicksort provided by NVIDIA, with
   better performance achieved using the iterative implementation.
   Copyright (c) 2015 John Wiley &amp; Sons, Ltd.</abstract><date>JAN 2016</date><author>Manca, Emanuele
   Manconi, Andrea
   Orro, Alessandro
   Armano, Giuliano
   Milanesi, Luciano</author></paper><paper><title>Auto-tuned Krylov methods on cluster of graphics processing unit</title><abstract>Exascale computers are expected to have highly hierarchical
   architectures with nodes composed by multiple core processors (CPU;
   central processing unit) and accelerators (GPU; graphics processing
   unit). The different programming levels generate new difficult algorithm
   issues. In particular when solving extremely large linear systems, new
   programming paradigms of Krylov methods should be defined and evaluated
   with respect to modern state of the art of scientific methods. Iterative
   Krylov methods involve linear algebra operations such as dot product,
   norm, addition of vectors and sparse matrix-vector multiplication. These
   operations are computationally expensive for large size matrices. In
   this paper, we aim to focus on the best way to perform effectively these
   operations, in double precision, on GPU in order to make iterative
   Krylov methods more robust and therefore reduce the computing time. The
   performance of our algorithms is evaluated on several matrices arising
   from engineering problems. Numerical experiments illustrate the
   robustness and accuracy of our implementation compared to the existing
   libraries. We deal with different preconditioned Krylov methods:
   Conjugate Gradient for symmetric positive-definite matrices, and
   Generalized Conjugate Residual, Bi-Conjugate Gradient Conjugate
   Residual, transpose-free Quasi Minimal Residual, Stabilized BiConjugate
   Gradient and Stabilized BiConjugate Gradient (L) for the solution of
   sparse linear systems with non symmetric matrices. We consider and
   compare several sparse compressed formats, and propose a way to
   implement effectively Krylov methods on GPU and on multicore CPU.
   Finally, we give strategies to faster algorithms by auto-tuning the
   threading design, upon the problem characteristics and the hardware
   changes. As a conclusion, we propose and analyse hybrid sub-structuring
   methods that should pave the way to exascale hybrid methods.</abstract><date>JUN 3 2015</date><author>Magoules, Frederic
   Ahamed, Abal-Kassim Cheik
   Putanowicz, Roman</author></paper><paper><title>Design and implementation of a hybrid MPI-CUDA model for the
   Smith-Waterman algorithm</title><abstract>This paper provides a novel hybrid model for solving the multiple
   pair-wise sequence alignment problem combining message passing interface
   and CUDA, the parallel computing platform and programming model invented
   by NVIDIA. The proposed model targets homogeneous cluster nodes equipped
   with similar Graphical Processing Unit (GPU) cards. The model consists
   of the Master Node Dispatcher (MND) and the Worker GPU Nodes (WGN). The
   MND distributes the workload among the cluster working nodes and then
   aggregates the results. The WGN performs the multiple pair-wise sequence
   alignments using the Smith-Waterman algorithm. We also propose a
   modified implementation to the Smith-Waterman algorithm based on
   computing the alignment matrices row-wise. The experimental results
   demonstrate a considerable reduction in the running time by increasing
   the number of the working GPU nodes. The proposed model achieved a
   performance of about 12 Giga cell updates per second when we tested
   against the SWISS-PROT protein knowledge base running on four nodes.</abstract><date>2015</date><author>Khaled, Heba
   Faheem, Hossam El Deen Mostafa
   El Gohary, Rania</author></paper><paper><title>A versatile model for soft patchy particles with various patch
   arrangements</title><abstract>We propose a simple and general mesoscale soft patchy particle model,
   which can felicitously describe the deformable and surface-anisotropic
   characteristics of soft patchy particles. This model can be used in
   dynamics simulations to investigate the aggregation behavior and
   mechanism of various types of soft patchy particles with tunable number,
   size, direction, and geometrical arrangement of the patches. To improve
   the computational efficiency of this mesoscale model in dynamics
   simulations, we give the simulation algorithm that fits the compute
   unified device architecture (CUDA) framework of NVIDIA graphics
   processing units (GPUs). The validation of the model and the performance
   of the simulations using GPUs are demonstrated by simulating several
   benchmark systems of soft patchy particles with 1 to 4 patches in a
   regular geometrical arrangement. Because of its simplicity and
   computational efficiency, the soft patchy particle model will provide a
   powerful tool to investigate the aggregation behavior of soft patchy
   particles, such as patchy micelles, patchy microgels, and patchy
   dendrimers, over larger spatial and temporal scales.</abstract><date>2016</date><author>Li, Zhan-Wei
   Zhu, You-Liang
   Lu, Zhong-Yuan
   Sun, Zhao-Yan</author></paper><paper><title>Parallel cuda implementation of conflict detection for application to
   airspace deconfliction</title><abstract>Methods for maintaining separation between aircraft in the current
   airspace system rely heavily on human operators. A conflict is an event
   in which two or more aircraft experience a loss of minimum allowable
   separation. Interest has grown in developing more advanced automation
   tools to predict when a traffic conflict is going to occur and to assist
   in its resolution. The term air space deconfliction is used to describe
   the resolution of conflicts after they have been predicted or detected.
   Due to the computationally intensive character of conflict detection and
   airspace deconfliction, as well as their data parallel nature, they are
   naturally amenable to parallel processing. This work discusses a
   parallel implementation of a conflict detection algorithm for
   application to airspace deconfliction. It uses the NVIDIA Quadro FX 5800
   and the Tesla C1060 graphical processing units (GPUs) in conjunction
   with the Compute Unified Device Architecture (CUDA) hardware/software
   architecture. Details of the implementation are discussed, including the
   use of streams for asynchronous programming and the use of multiple
   GPUs. The performance of the parallel implementation is compared to that
   of an equivalent sequential version and shown to exhibit improvement in
   execution time. Recommendations are provided to further improve
   performance of the algorithm.</abstract><date>OCT 2015</date><author>Thompson, Elizabeth
   Clem, Nathan
   Peter, David A.
   Bryan, John
   Peterson, Barry I.
   Holbrook, Dave</author></paper><paper><title>Quest for a New Solver for EPANET 2</title><abstract>One of the best known hydraulic water distribution modeling toolkits
   that is most used by both researchers and practitioners is EPANET 2.
   Initially the authors aimed to speed up such simulations by utilizing
   modern multicore processors in the code implementation. The 30-year-old
   linear solver was replaced in steps by seven different modern multicore
   capable solvers. Subsequently, speedup tests were carried out with
   small-(up to 1.6 x 10(3) nodes), medium-(up to 6.3 x 10(4) nodes) and
   large-sized (up to 6.3 x 10(5) nodes) test cases. None of the tested
   solvers was found to perform faster than the original solver for
   networks with a real-world character, although two solvers showed a
   speedup on medium and large water distribution networks. This is an
   example that strategies to reduce computation time can produce promising
   results in a theoretic research environment, but fail in practical
   engineering applications. Likewise, this paper highlights again the
   importance of considering realistic test cases during the implementation
   phase. Further, the authors point out why the different tested
   strategies in this work have not succeeded. Although two other issues
   (implemented hash table and node reordering) could be identified for
   potential improvement of the code, it was concluded that the original
   solver is still the fastest for practical system configurations.</abstract><date>MAR 2016</date><author>Burger, Gregor
   Sitzenfrei, Robert
   Kleidorfer, Manfred
   Rauch, Wolfgang</author></paper><paper><title>A fast and energy-efficient Hamming decoder for software-defined radio
   using graphics processing units</title><abstract>The demand for scalable and fast error decoders has recently increased
   in software-defined radio-based communication systems. Hamming code,
   which is one of the promising error decoders, shows acceptable accuracy;
   however, the computational complexity of the decoder limits its use in
   real-time communication. To address this issue, this paper proposes a
   fully parallel implementation of the (7, 4) Hamming code on a graphics
   processing unit (GPU) by exploiting massive data-parallelism and
   increasing on-chip constant memory accesses. To further improve the
   performance of this proposed parallel approach, this paper explores the
   impact of different thread/block configurations and selects optimal
   thread/block configurations, which can occupy more hardware resources
   for performing parity checks, error detection and correction, and
   decoding of the received codeword. In addition, the proposed GPU-based
   Hamming decoder can provide significant scalability by supporting
   different message sizes, including 355,907 bytes, 2,959,475 bytes, and
   12,835,890 bytes. To verify the effectiveness of the GPU-based parallel
   Hamming decoder, this paper compares its performance with that of the
   multi-threading central processing unit (CPU) approach which is executed
   on an Intel multi-core processor. Experimental results indicate that the
   proposed GPU-based decoder operates at least 15.13 times faster and
   reduces the energy consumption by up to 913.17 % compared to the
   multi-threading CPU-based approach.</abstract><date>JUL 2015</date><author>Kim, Jaeyoung
   Kang, Myeongsu
   Islam, Md Shohidul
   Kim, Cheol-Hong
   Kim, Jong-Myon</author></paper><paper><title>DigitalVHI--a freeware open-source software application to capture the
   Voice Handicap Index and other questionnaire data in various languages.</title><abstract>In this short report we introduce DigitalVHI, a free open-source
   software application for obtaining Voice Handicap Index (VHI) and other
   questionnaire data, which can be put on a computer in clinics and used
   in clinical practice. The software can simplify performing clinical
   studies since it makes the VHI scores directly available for analysis in
   a digital form. It can be downloaded from
   http://www.christian-herbst.org/DigitalVHI/. </abstract><date>2015-Jul</date><author>Herbst, Christian T
   Oh, Jinook
   Vydrova, Jitka
   Svec, Jan G</author></paper><paper><title>Adaptive kinetic-fluid solvers for heterogeneous computing architectures</title><abstract>We show feasibility and benefits of porting an adaptive multi-scale
   kinetic-fluid code to CPU-GPU systems. Challenges are due to the
   irregular data access for adaptive Cartesian mesh, vast difference of
   computational cost between kinetic and fluid cells, and desire to evenly
   load all CPUs and GPUs during grid adaptation and algorithm refinement.
   Our Unified Flow Solver (UFS) combines Adaptive Mesh Refinement (AMR)
   with automatic cell-by-cell selection of kinetic or fluid solvers based
   on continuum breakdown criteria. Using GPUs enables hybrid simulations
   of mixed rarefied-continuum flows with a million of Boltzmann cells each
   having a 24 x 24 x 24 velocity mesh. We describe the implementation of
   CUDA kernels for three modules in UFS: the direct Boltzmann solver using
   the discrete velocity method (DVM), the Direct Simulation Monte Carlo
   (DSMC) solver, and a mesoscopic solver based on the Lattice Boltzmann
   Method (LBM), all using adaptive Cartesian mesh. Double digit speedups
   on single GPU and good scaling for multi-GPUs have been demonstrated.
   (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>DEC 15 2015</date><author>Zabelok, Sergey
   Arslanbekov, Robert
   Kolobov, Vladimir</author></paper><paper><title>Image Segmentation With Cage Active Contours</title><abstract>In this paper, we present a framework for image segmentation based on
   parametrized active contours. The evolving contour is parametrized
   according to a reduced set of control points that form a closed polygon
   and have a clear visual interpretation. The parametrization, called mean
   value coordinates, stems from the techniques used in computer graphics
   to animate virtual models. Our framework allows to easily formulate
   region-based energies to segment an image. In particular, we present
   three different local region-based energy terms: 1) the mean model; 2)
   the Gaussian model; 3) and the histogram model. We show the behavior of
   our method on synthetic and real images and compare the performance with
   state-of-the-art level set methods.</abstract><date>DEC 2015</date><author>Garrido, Lluis
   Guerrieri, Marite
   Igual, Laura</author></paper><paper><title>Accelerating Lattice Boltzmann Applications with OpenACC</title><abstract>An increasingly large number of HPC systems rely on heterogeneous
   architectures combining traditional multi-core CPUs with power efficient
   accelerators. Designing efficient applications for these systems has
   been troublesome in the past as accelerators could usually be programmed
   only using specific programming languages - such as CUDA - threatening
   maintainability, portability and correctness. Several new programming
   environments try to tackle this problem; among them OpenACC offers a
   high-level approach based on directives. In OpenACC, one annotates
   existing C, C++ or Fortran codes with compiler directive clauses to mark
   program regions to offload and run on accelerators and to identify
   available parallelism. This approach directly addresses code
   portability, leaving to compilers the support of each different
   accelerator, but one has to carefully assess the relative costs of
   potentially portable approach versus computing efficiency. In this paper
   we address precisely this issue, using as a test-bench a massively
   parallel Lattice Boltzmann code. We implement and optimize this
   multi-node code using OpenACC and OpenMPI. We also compare performance
   with that of the same algorithm written in CUDA, OpenCL and C for GPUs,
   Xeon-Phi and traditional multi-core CPUs, and characterize through an
   accurate time model its scaling behavior on a large cluster of GPUs.</abstract><date>2015</date><author>Calore, Enrico
   Kraus, Jiri
   Schifano, Sebastiano Fabio
   Tripiccione, Raffaele</author></paper><paper><title>Standardization of a Graphic Symbol System as an Alternative
   Communication Tool for Turkish</title><abstract>Graphic symbols are commonly used across countries in order to support
   individuals with communicative deficiency. The literature review
   revealed the absence of such a system for Turkish socio-cultural
   context. In this study, the aim was to develop a symbol system
   appropriate for the Turkish socio-cultural context. The process began
   with studies designed to delineate the scope of the proposed system.
   Firstly, a dictionary was formed, founded on the literature. In the
   following stage, a visual design form was developed with a view to
   identifying visual representations of these words. Using this form, 106
   participants were asked for their opinion. The data collected were
   examined, and common traits of the graphic(s) indicated for each word
   were identified so that alternative graphic(s) could be prepared
   accordingly. From one to five visual representations were identified for
   each word and the corresponding graphic symbols were drawn in electronic
   media. An e-measure was developed in order to find out whether these
   graphics were sufficient to represent the corresponding objects,
   concepts, or situations. The scale was sent to participant groups across
   Turkey to obtain the opinions of individuals from divergent age,
   culture, and educational backgrounds. A total of 1,099 participants were
   asked for their opinion. This resulted in a new system consisting of
   standard graphic symbol(s) for 843 words and seven forms of affix
   structures appropriate for writing Turkish using graphic symbols.</abstract><date>JAN 2016</date><author>Karal, Yasemin
   Karal, Hasan
   Silbir, Lokman
   Altun, Taner</author></paper><paper><title>GPU Accelerated Digital Volume Correlation</title><abstract>A sub-voxel digital volume correlation (DVC) method combining the 3D
   inverse compositional Gauss-Newton (ICGN) algorithm with the 3D fast
   Fourier transform-based cross correlation (FFT-CC) algorithm is proposed
   to eliminate path-dependence in current iterative DVC methods caused by
   the initial guess transfer scheme. The proposed path-independent DVC
   method is implemented on NVIDIA compute unified device architecture
   (CUDA) for GPU devices. Powered by parallel computing technology, the
   proposed DVC method achieves a significant improvement in computation
   speed on a common desktop computer equipped with a low-end graphics card
   containing 1536 CUDA cores, i.e., up to 23.3 times faster than the
   sequential implementation and 3.7 times faster than the multithreaded
   implementation of the same DVC method running on a 6-core CPU. This
   speedup, which has no compromise with resolution, accuracy and
   precision, benefits from the coarse-grained parallelism that the points
   of interest (POIs) are processed simultaneously and also from the
   fine-grained parallelism that the calculation at each POI is performed
   with multiple threads in GPU. The experimental study demonstrates the
   superiority of the GPU-based parallel computing for acceleration of DVC
   over the multi-core CPU-based one, in particular on a PC level computer.</abstract><date>FEB 2016</date><author>Wang, T.
   Jiang, Z.
   Kemao, Q.
   Lin, F.
   Soon, S. H.</author></paper><paper><title>Cell lineage visualisation</title><abstract>Cell lineages describe the developmental history of Cell populations and
   are produced by combining time-lapse imaging and image processing.
   Biomedical researchers study Cell lineages to understand fundamental
   processes such as Cell differentiation and the pharmacodynamic action of
   anticancer agents. Yet, the interpretation of Cell lineages is hindered
   by their complexity and insufficient capacity for visual analysis. We
   present a novel approach for interactive visualisation of Cell lineages.
   Based on an understanding of Cellular biology and live-Cell imaging
   methodology, we identify three requirements: multimodality (Cell
   lineages combine spatial, temporal, and other properties), symmetry
   (related to lineage branching structure), and synchrony (related to
   temporal alignment of Cellular events). We address these by combining
   visual summaries of the spatiotemporal behaviour of an arbitrary number
   of lineages, including variation from average behaviour, with node-link
   representations that emphasise the presence or absence of symmetry and
   synchrony. We illustrate the merit of our approach by presenting a
   real-world case study where the cytotoxic action of the anticancer drug
   topotecan was determined.</abstract><date>JUN 2015</date><author>Pretorius, A. J.
   Khan, I. A.
   Errington, R. J.</author></paper><paper><title>A time-energy performance analysis of Map Reduce on heterogeneous
   systems with GPUs</title><abstract>Motivated by the explosion of Big Data analytics, performance
   improvements in low-power (wimpy) systems and the increasing energy
   efficiency of CPUs, this paper presents a time-energy performance
   analysis of MapReduce on heterogeneous systems with GPUs. We evaluate
   the time and energy performance of three MapReduce applications with
   diverse resource demands on a Hadoop-CUDA framework. As executing these
   applications on heterogeneous systems with GPUs is challenging, we
   introduce a novel lazy processing technique which requires no
   modifications to the underlying Hadoop framework. To analyze the impact
   of heterogeneity, we compare the heterogeneous CPU+GPU with the
   homogeneous CPU-only execution across three systems with diverse
   characteristics, (i) a traditional high-performance (brawny) Intel i7
   system hosting a discrete 640-core Nvidia GPU of the latest Maxwell
   generation, (ii) a wimpy platform consisting of a quad-core ARM
   Cortex-A9 hosting the same discrete Maxwell CPU, and (iii) a wimpy
   platform integrating four ARM Cortex-A15 cores and 192 Nvidia Kepler CPU
   cores on the same chip. These systems encompass both intra-node
   heterogeneity with discrete CPUs and intra-chip heterogeneity with
   integrated CPUs. Our measurement-based performance analysis highlights
   the following results. For compute-intensive workloads, the brawny
   heterogeneous system achieves speedups of up to 2.3 and reduces the
   energy usage by almost half compared to the brawny homogeneous system.
   As expected, for applications where data transfers dominate the
   execution time, heterogeneity exhibits worse time-energy performance
   compared to homogeneous systems. For such applications, the
   heterogeneous wimpy A9 system with discrete GPU uses around 14 times the
   energy of homogeneous A9 system due to both system resource imbalances
   and high power overhead of the discrete CPU. However, comparing among
   heterogeneous systems, the wimpy A15 with integrated CPU uses the lowest
   energy across all workloads. This allows us to establish an execution
   time equivalence ratio between a single brawny node and multiple wimpy
   nodes. Based on this equivalence ratio, the wimpy nodes exhibit energy
   savings of two-thirds while maintaining the same execution time. This
   result advocates the potential usage of heterogeneous wimpy systems with
   integrated CPUs for Big Data analytics. (C) 2015 Elsevier B.V. All
   rights reserved.</abstract><date>SEP 2015</date><author>Loghin, Dumitrel
   Ramapantulu, Lavanya
   Barbu, Oana
   Teo, Yong Meng</author></paper><paper><title>Massively Parallel GPU Design of Automatic Target Generation Process in
   Hyperspectral Imagery</title><abstract>A popular algorithm for hyperspectral image interpretation is the
   automatic target generation process (ATGP). ATGP creates a set of
   targets from image data in an unsupervised fashion without prior
   knowledge. It can be used to search a specific target in unknown scenes
   and when a target's size is smaller than a single pixel. Its application
   has been demonstrated in many fields including geology, agriculture, and
   intelligence. However, the algorithm requires long time to process due
   to the massive amount of data. To expedite the process, the graphics
   processing units (GPUs) are an attractive alternative in comparison with
   traditional CPU architectures. In this paper, we propose a GPU-based
   massively parallel version of ATGP, which provides real-time performance
   for the first time in the literature. The HYDICE image data (307 * 307
   pixels and 210 spectral bands) are used for benchmark. Our optimization
   efforts on the GPU-based ATGP algorithm using one NVIDIA Tesla K20 GPU
   with I/O transfer can achieve a speedup of 362x with respect to its
   single-threaded CPU counterpart. We also tested the algorithm on
   Airborne Visible/InfraRed Imaging Spectrometer (AVIRIS) WTC dataset (512
   * 614 * 224 of 224 bands) and Cuprite dataset (35 * 350 * 188 of 188
   bands), the speedup was 416x and 320x, respectively, when the target
   number was 15.</abstract><date>JUN 2015</date><author>Li, Xiaojie
   Huang, Bormin
   Zhao, Kai</author></paper><paper><title>Highly optimized simulations on single- and multi-GPU systems of the 3D
   Ising spin glass model</title><abstract>We present a highly optimized implementation of a Monte Carlo (MC)
   simulator for the three-dimensional Ising spin-glass model with bimodal
   disorder, i.e., the 3D Edwards-Anderson model running on CUDA enabled
   GPUs. Multi-GPU systems exchange data by means of the Message Passing
   Interface (MPI). The chosen MC dynamics is the classic Metropolis one,
   which is purely dissipative, since the aim was the study of the critical
   off-equilibrium relaxation of the system. We focused on the following
   issues: (i) the implementation of efficient memory access patterns for
   nearest neighbours in a cubic stencil and for lagged-Fibonacci-like
   pseudo-Random Numbers Generators (PRNGs); (ii) a novel implementation of
   the asynchronous multispin-coding Metropolis MC step allowing to store
   one spin per bit and (iii) a multi-GPU version based on a combination of
   MPI and CUDA streams. Cubic stencils and PRNGs are two subjects of very
   general interest because of their widespread use in many simulation
   codes. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Lulli, M.
   Bernaschi, M.
   Parisi, G.</author></paper><paper><title>Accelerating decomposition of light field video for compressive
   multi-layer display</title><abstract>Compressive light field display based on multi-layer LCDs is becoming a
   popular solution for 3D display. Decomposing light field into layer
   images is the most challenging task. Iterative algorithm is an effective
   solver for this high-dimensional decomposition problem. Existing
   algorithms, however, iterate from random initial values. As such,
   significant computation time is required due to the deviation between
   random initial estimate and target values. Real-time 3D display at video
   rate is difficult based on existing algorithms. In this paper, we
   present a new algorithm to provide better initial values and accelerate
   decomposition of light field video. We utilize internal coherence of
   single light field frame to transfer the ignorance-to-target to a much
   lower resolution level. In addition, we explored external coherence for
   further accelerating light field video and achieved 5.91 times speed
   improvement. We built a prototype and developed parallel algorithm based
   on CUDA. (C) 2015 Optical Society of America</abstract><date>DEC 28 2015</date><author>Cao, Xuan
   Geng, Zheng
   Li, Tuotuo
   Zhang, Mei
   Zhang, Zhaoxing</author></paper><paper><title>A Six-Year Longitudinal Evaluation of the DICOM GSDF Conformance
   Stability of LCD Monitors</title><abstract></abstract><date>JUN 2015</date><author>McKenney, S.
   Bevins, N.
   Olariu, E.
   Flynn, M.</author></paper><paper><title>Parallel programing templates for remote sensing image processing on GPU
   architectures: design and implementation</title><abstract>Remote sensing image processing is characterized with features of
   massive data processing, intensive computation, and complex processing
   algorithms. These characteristics make the rapid processing of remote
   sensing images very difficult and inefficient. The rapid development of
   general-purpose graphic process unit (GPGPU) computing technology has
   resulted in continuous improvement in GPU computing performance. Its
   strong floating point calculating capability, high intensive
   computation, small volume, and excellent performance-cost ratio provide
   an effective solution to the problems faced in remote sensing image
   processing. However, current usage of GPU in remote sensing image
   processing applications has been limited to specific parallel algorithms
   and their optimization of processes, rather than formed well-established
   models and methods. This has introduced serious problems to the
   development of remote sensing image processing algorithms on GPU
   architectures. For example, GPU parallel strategies and algorithms are
   highly coupled and non-reusable. The processing system is closely
   associated with the GPU hardware so that programming for remote sensing
   algorithms on GPU is nothing but easy. In this paper, we attempt to
   explore a reusable GPU-based remote sensing image parallel processing
   model and to establish a set of parallel programming templates, which
   provides programmers with a more simple and effective way for
   programming parallel remote sensing image processing algorithms.</abstract><date>JAN 2016</date><author>Ma, Yan
   Chen, Lajiao
   Liu, Peng
   Lu, Ke</author></paper><paper><title>Intuitive and Efficient Camera Control with the Toric Space</title><abstract>A large range of computer graphics applications such as data
   visualization or virtual movie production require users to position and
   move viewpoints in 3D scenes to effectively convey visual information or
   tell stories. The desired viewpoints and camera paths are required to
   satisfy a number of visual properties (e.g. size, vantage angle,
   visibility, and on-screen position of targets). Yet, existing camera
   manipulation tools only provide limited interaction methods and
   automated techniques remain computationally expensive.In this work, we
   introduce the Toric space, a novel and compact representation for
   intuitive and efficient virtual camera control. We first show how visual
   properties are expressed in this Toric space and propose an efficient
   interval-based search technique for automated viewpoint computation. We
   then derive a novel screen-space manipulation technique that provides
   intuitive and real-time control of visual properties. Finally, we
   propose an effective viewpoint interpolation technique which ensures the
   continuity of visual properties along the generated paths. The proposed
   approach (i) performs better than existing automated viewpoint
   computation techniques in terms of speed and precision, (ii) provides a
   screen-space manipulation tool that is more efficient than classical
   manipulators and easier to use for beginners, and (iii) enables the
   creation of complex camera motions such as long takes in a very short
   time and in a controllable way. As a result, the approach should quickly
   find its place in a number of applications that require interactive or
   automated camera control such as 3D modelers, navigation tools or 3D
   games.</abstract><date>AUG 2015</date><author>Lino, Christophe
   Christie, Marc</author></paper><paper><title>Impact of data layouts on the efficiency of GPU-accelerated IDW
   interpolation.</title><abstract>This paper focuses on evaluating the impact of different data layouts on
   the computational efficiency of GPU-accelerated Inverse Distance
   Weighting (IDW) interpolation algorithm. First we redesign and improve
   our previous GPU implementation that was performed by exploiting the
   feature of CUDA dynamic parallelism (CDP). Then we implement three
   versions of GPU implementations, i.e., the naive version, the tiled
   version, and the improved CDP version, based upon five data layouts,
   including the Structure of Arrays (SoA), the Array of Structures (AoS),
   the Array of aligned Structures (AoaS), the Structure of Arrays of
   aligned Structures (SoAoS), and the Hybrid layout. We also carry out
   several groups of experimental tests to evaluate the impact.
   Experimental results show that: the layouts AoS and AoaS achieve better
   performance than the layout SoA for both the naive version and tiled
   version, while the layout SoA is the best choice for the improved CDP
   version. We also observe that: for the two combined data layouts (the
   SoAoS and the Hybrid), there are no notable performance gains when
   compared to other three basic layouts. We recommend that: in practical
   applications, the layout AoaS is the best choice since the tiled version
   is the fastest one among three versions. The source code of all
   implementations are publicly available. </abstract><date>2016</date><author>Mei, Gang
   Tian, Hong</author></paper><paper><title>Accurate calculation of computer-generated holograms using
   angular-spectrum layer-oriented method</title><abstract>Fast calculation and correct depth cue are crucial issues in the
   calculation of computer-generated hologram (CGH) for high quality
   three-dimensional (3-D) display. An angular-spectrum based algorithm for
   layer-oriented CGH is proposed. Angular spectra from each layer are
   synthesized as a layer-corresponded sub-hologram based on the fast
   Fourier transform without paraxial approximation. The proposed method
   can avoid the huge computational cost of the point-oriented method and
   yield accurate predictions of the whole diffracted field compared with
   other layer-oriented methods. CGHs of versatile formats of 3-D digital
   scenes, including computed tomography and 3-D digital models, are
   demonstrated with precise depth performance and advanced image quality.
   (C) 2015 Optical Society of America</abstract><date>OCT 5 2015</date><author>Zhao, Yan
   Cao, Liangcai
   Zhang, Hao
   Kong, Dezhao
   Jin, Guofan</author></paper><paper><title>A Microscopic Optically Tracking Navigation System That Uses
   High-resolution 3D Computer Graphics</title><abstract>Three-dimensional (3D) computer graphics (CG) are useful for
   preoperative planning of neurosurgical operations. However, application
   of 3D CG to intraoperative navigation is not widespread because existing
   commercial operative navigation systems do not show 3D CG in sufficient
   detail. We have developed a microscopic optically tracking navigation
   system that uses high-resolution 3D CG. This article presents the
   technical details of our microscopic optically tracking navigation
   system. Our navigation system consists of three components: the
   operative microscope, registration, and the image display system. An
   optical tracker was attached to the microscope to monitor the position
   and attitude of the microscope in real time; point-pair registration was
   used to register the operation room coordinate system, and the image
   coordinate system; and the image display system showed the 3D CG image
   in the field-of-view of the microscope. Ten neurosurgeons (seven males,
   two females; mean age 32.9 years) participated in an experiment to
   assess the accuracy of this system using a phantom model. Accuracy of
   our system was compared with the commercial system. The 3D CG provided
   by the navigation system coincided well with the operative scene under
   the microscope. Target registration error for our system was 2.9 +/- 1.9
   mm. Our navigation system provides a clear image of the operation
   position and the surrounding structures. Systems like this may reduce
   intraoperative complications.</abstract><date>AUG 2015</date><author>Yoshino, Masanori
   Saito, Toki
   Kin, Taichi
   Nakagawa, Daichi
   Nakatomi, Hirofumi
   Oyama, Hiroshi
   Saito, Nobuhito</author></paper><paper><title>Multi-scale Visualization of Molecular Architecture Using Real-Time
   Ambient Occlusion in Sculptor</title><abstract>The modeling of large biomolecular assemblies relies on an efficient
   rendering of their hierarchical architecture across a wide range of
   spatial level of detail. We describe a paradigm shift currently under
   way in computer graphics towards the use of more realistic global
   illumination models, and we apply the so-called ambient occlusion
   approach to our opensource multi-scale modeling program, Sculptor. While
   there are many other higher quality global illumination approaches going
   all the way up to full GPU-accelerated ray tracing, they do not provide
   size-specificity of the features they shade. Ambient occlusion is an
   aspect of global lighting that offers great visual benefits and powerful
   user customization. By estimating how other molecular shape features
   affect the reception of light at some surface point, it effectively
   simulates indirect shadowing. This effect occurs between molecular
   surfaces that are close to each other, or in pockets such as protein or
   ligand binding sites. By adding ambient occlusion, large macromolecular
   systems look much more natural, and the perception of characteristic
   surface features is strongly enhanced. In this work, we present a
   realtime implementation of screen space ambient occlusion that delivers
   realistic cues about tunable spatial scale characteristics of
   macromolecular architecture. Heretofore, the visualization of large
   biomolecular systems, comprising e.g. hundreds of thousands of atoms or
   Mega-Dalton size electron microscopy maps, did not take into account the
   length scales of interest or the spatial resolution of the data. Our
   approach has been uniquely customized with shading that is tuned for
   pockets and cavities of a user-defined size, making it useful for
   visualizing molecular features at multiple scales of interest. This is a
   feature that none of the conventional ambient occlusion approaches
   provide. Actual Sculptor screen shots illustrate how our implementation
   supports the size-dependent rendering of molecular surface features.</abstract><date>OCT 2015</date><author>Wahle, Manuel
   Wriggers, Willy</author></paper><paper><title>Digitalized accurate modeling of SPCB with multi-spiral surface based on
   CPC algorithm</title><abstract>The main methods of the existing multi-spiral surface geometry modeling
   include spatial analytic geometry algorithms, graphical method,
   interpolation and approximation algorithms. However, there are some
   shortcomings in these modeling methods, such as large amount of
   calculation, complex process, visible errors, and so on. The above
   methods have, to some extent, restricted the design and manufacture of
   the premium and high-precision products with spiral surface
   considerably. This paper introduces the concepts of the spatially
   parallel coupling with multi-spiral surface and spatially parallel
   coupling body. The typical geometry and topological features of each
   spiral surface forming the multi-spiral surface body are determined, by
   using the extraction principle of datum point cluster, the algorithm of
   coupling point cluster by removing singular point, and the "spatially
   parallel coupling" principle based on the non-uniform B-spline for each
   spiral surface. The orientation and quantitative relationships of datum
   point cluster and coupling point cluster in Euclidean space are
   determined accurately and in digital description and expression,
   coupling coalescence of the surfaces with multi-coupling point clusters
   under the Pro/E environment. The digitally accurate modeling of
   spatially parallel coupling body with multi-spiral surface is realized.
   The smooth and fairing processing is done to the three-blade end-milling
   cutter's end section area by applying the principle of spatially
   parallel coupling with multi-spiral surface, and the alternative entity
   model is processed in the four axis machining center after the end mill
   is disposed. And the algorithm is verified and then applied effectively
   to the transition area among the multi-spiral surface. The proposed
   model and algorithms may be used in design and manufacture of the
   multi-spiral surface body products, as well as in solving essentially
   the problems of considerable modeling errors in computer graphics and
   engineering in multi-spiral surface's connection available with
   approximate methods or graphical methods.</abstract><date>SEP 2015</date><author>Huang Yanhua
   Gu Lizhi</author></paper><paper><title>A curvilinear stochastic-FDTD algorithm for 3-D EMC problems with media
   uncertainties</title><abstract>Purpose - Stochastic uncertainties in material parameters have a
   significant impact on the analysis of real-world electromagnetic
   compatibility (EMC) problems. Conventional approaches via the
   Monte-Carlo scheme attempt to provide viable solutions, yet at the
   expense of prohibitively elongated simulations and system overhead, due
   to the large amount of statistical implementations. The purpose of this
   paper is to introduce a 3-D stochastic finite-difference time-domain
   (S-FDTD) technique for the accurate modelling of generalised EMC
   applications with highly random media properties, while concurrently
   offering fast and economical single-run
   realisations.Design/methodology/approach - The proposed method
   establishes the concept of covariant/contravariant metrics for robust
   tessellations of arbitrarily curved structures and derives the mean
   value and standard deviation of the generated fields in a single-run.
   Also, the critical case of geometrical and physical uncertainties is
   handled via an optimal parameterisation, which locally reforms the
   curvilinear grid. In order to pursue extra speed efficiency, code
   implementation is conducted through contemporary graphics processor
   units and parallel programming.Findings - The curvilinear S-FDTD
   algorithm is proven very precise and stable, compared to existing
   multiple-realisation approaches, in the analysis of
   statistically-varying problems. Moreover, its generalised formulation
   allows the effective treatment of realistic structures with arbitrarily
   curved geometries, unlike staircase schemes. Finally, the GPU-based
   enhancements accomplish notably accelerated simulations that may exceed
   the level of 120 times. Conclusively, the featured technique can
   successfully attain highly accurate results with very limited system
   requirements.Originality/value - Development of a generalised
   curvilinear S-FDTD methodology, based on a covariant/contravariant
   algorithm. Incorporation of the important geometric/physical
   uncertainties through a locally adaptive curved mesh. Speed advancement
   via modern GPU and CUDA programming which leads to reliable estimations,
   even for abrupt statistical media parameter fluctuations.</abstract><date>2015</date><author>Pyrialakos, Georgios
   Papadimopoulos, Athanasios
   Zygiridis, Theodoros
   Kantartzis, Nikolaos
   Tsiboukis, Theodoros</author></paper><paper><title>An Approach to Utilize FMEA for Autonomous Vehicles to Forecast Decision
   Outcome</title><abstract>Every autonomous vehicle has an analytic framework which monitors the
   decision making of the vehicle to keep it safe. By tweaking the FMEA
   (Failure Mode Effect Analysis) framework and applying this to the
   decision system will make significant increase in the quality of the
   decisions, especially in series of decision and its overall outcome.
   This will avoid collisions and better quality of decision. The proposed
   methodology uses this approach to identify the risks associated with the
   best alternative selected. The FMEA requires to be running at real time.
   It has to keep its previous experiences in hand to do quick/split time
   decision making. This paper considers a case study of FMEA framework
   applied to autonomous driving vehicles to support decision making. It
   shows a significant increase in the performance in the execution of FMEA
   over GPU. It also brings out a comparison of CUDA to TPL and sequential
   execution.</abstract><date>2015</date><author>Khaiyum, Samitha
   Pal, Bishwajit
   Kumaraswamy, Y. S.</author></paper><paper><title>Nonlinear Dynamic Analysis Efficiency by Using a GPU Parallelization</title><abstract>A graphics processing unit (GPU) parallelization approach was
   implemented to improve the efficiency of nonlinear dynamic analysis. The
   GPU parallelization approach speeded up the computation of implicit time
   integration and reduced total calculation time. In addition, a parallel
   equations solver is introduced to solve the equation system. Numerical
   examples of reinforced concrete (RC) frames were used to investigate the
   parallel computing speedup of the GPU parallelization approach. An
   implementation of these RC frame models for fiber beam-column elements
   was presented. The parallel finite element program is developed to
   provide parallel execution on personal computer (PC) with different
   CUDA-capable GPUs. The different number of degrees of freedom from low
   to high was adopted in the numerical examples. Detailed tests on
   accuracy, runtime, and speedup are conducted on different GPUs. The
   nonlinear dynamic response using the GPU parallelization program was in
   good agreement with that obtained by ABAQUS. Numerical studies indicate
   that compared with original sequential approach, the GPU parallelization
   program achieves a 22 times speedups of the solving equation system and
   improves the overall efficiency of time integration by up to 94%.</abstract><date>NOV 17 2015</date><author>Li, Hong-yu
   Teng, Jun
   Li, Zuo-hua
   Zhang, Lu</author></paper><paper><title>The VideoMob Interactive Art Installation Connecting Strangers through
   Inclusive Digital Crowds</title><abstract>VideoMob is an interactive video platform and an artwork that enables
   strangers visiting different installation locations to interact across
   time and space through a computer interface that detects their presence,
   video-records their actions while automatically removing the video
   background through computer vision, and co-situates visitors as part of
   the same digital environment. Through the combination of individual user
   videos to form a digital crowd, strangers are connected through the
   graphic display. Our work is inspired by the way distant people can
   interact with each other through technology and influenced by artists
   working in the realm of interactive art. We deployed VideoMob in a
   variety of settings, locations, and contexts to observe hundreds of
   visitors' reactions. By analyzing behavioral data collected through
   depth cameras from our 1,068 recordings across eight venues, we studied
   how participants behave when given the opportunity to record their own
   video portrait into the artwork. We report the specific activity
   performed in front of the camera and the influences that existing crowds
   impose on new participants. Our analysis informs the integration of a
   series of possible novel interaction paradigms based on real-time
   analysis of the visitors' behavior through specific computer vision and
   machine learning techniques that have the potential to increase the
   engagement of the artwork's visitors and to impact user experience.</abstract><date>JUL 2015</date><author>Grenader, Emily
   Rodrigues, Danilo Gasques
   Nos, Fernando
   Weibel, Nadir</author></paper><paper><title>Improved CUDA programs for GPU computing of Swendsen-Wang multi-cluster
   spin flip algorithm: 2D and 3D Ising, Potts, and XY models</title><abstract>We present new versions of sample CUDA programs for the GPU computing of
   the Swendsen-Wang multi-cluster spin flip algorithm. In this update, we
   add the method of GPU-based cluster-labeling algorithm without the use
   of conventional iteration (Komura, 2015) to those programs. For
   high-precision calculations, we also add a random-number generator in
   the cuRAND library. Moreover, we fix several bugs and remove the extra
   usage of shared memory in the kernel functions.</abstract><date>MAR 2016</date><author>Komura, Yukihiro
   Okabe, Yutaka</author></paper><paper><title>GPU-based parallel clustered differential pulse code modulation</title><abstract>Hyperspectral remote sensing technology is widely used in marine remote
   sensing, geological exploration, atmospheric and environmental remote
   sensing. Owing to the rapid development of hyperspectral remote sensing
   technology, resolution of hyperspectral image has got a huge boost. Thus
   data size of hyperspectral image is becoming larger. In order to reduce
   their saving and transmission cost, lossless compression for
   hyperspectral image has become an important research topic.In recent
   years, large numbers of algorithms have been proposed to reduce the
   redundancy between different spectra. Among of them, the most classical
   and expansible algorithm is the Clustered Differential Pulse Code
   Modulation (C-DPCM) algorithm. This algorithm contains three parts:
   first clusters all spectral lines, then trains linear predictors for
   each band. Secondly, use these predictors to predict pixels, and get the
   residual image by subtraction between original image and predicted
   image. Finally, encode the residual image. However, the process of
   calculating predictors is time-costing. In order to improve the
   processing speed, we propose a parallel C-DPCM based on CUDA (Compute
   Unified Device Architecture) with GPU.Recently, general-purpose
   computing based on GPUs has been greatly developed. The capacity of GPU
   improves rapidly by increasing the number of processing units and
   storage control units. CUDA is a parallel computing platform and
   programming model created by NVIDIA. It gives developers direct access
   to the virtual instruction set and memory of the parallel computational
   elements in GPUs.Our core idea is to achieve the calculation of
   predictors in parallel. By respectively adopting global memory, shared
   memory and register memory, we finally get a decent speedup.</abstract><date>2015</date><author>Wu, Jiaji
   Li, Wenze
   Kong, Wanqiu</author></paper><paper><title>CU-Simulator: A Parallel Scalable Simulation Platform for Radio Channel
   in Wireless Sensor Networks</title><abstract>Due to the computational intensive nature, the current available WSN
   simulators, which are based on the traditional CPU computing
   architecture, cannot run in a linear scalability. In this paper, we
   propose and set up CU-Simulator, a parallel radio channel simulator to
   enhance the performance for simulating data packet transmission in WSNs
   using NVIDIA's CUDA-enabled GPU parallel computing architecture. First,
   the node positions are simulated on GPU. Second, we propose an efficient
   data structure for acceleration, called CUDA-quad-trees, residing in the
   fast on-chip memory of GPU, to organize sensor nodes in such a manner
   that the detection of possible transmitters is facilitated. Third, a
   CUDA parallel radio channel simulating engine is established.
   Experimental results show that CU-Simulator has a super-linear
   scalability and greatly outperforms a CPU implementation with up to
   452.07-times speedup on an HP Z800 workstation with a NVIDIA Tesla C2070
   card and an Intel Xeon Core-quad CPU.</abstract><date>2015</date><author>Jian, Liheng
   Liu, Ying
   Yi, Weidong</author></paper><paper><title>Stochastic Soft Shadow Mapping</title><abstract>In this paper, we extend the concept of pre-filtered shadow mapping to
   stochastic rasterization, enabling real-time rendering of soft shadows
   from planar area lights. Most existing soft shadow mapping methods lose
   important visibility information by relying on pinhole renderings from
   an area light source, providing plausible results only for small light
   sources. Since we sample the entire 4D shadow light field
   stochastically, we are able to closely approximate shadows of large area
   lights as well. In order to efficiently reconstruct smooth shadows from
   this sparse data, we exploit the analogy of soft shadow computation to
   rendering defocus blur, and introduce a multiplane pre-filtering
   algorithm. We demonstrate how existing pre-filterable approximations of
   the visibility function, such as variance shadow mapping, can be
   extended to four dimensions within our framework.</abstract><date>JUL 2015</date><author>Liktor, G.
   Spassov, S.
   Mueckl, G.
   Dachsbacher, C.</author></paper><paper><title>ChIPseeker: an R/Bioconductor package for ChIP peak annotation,
   comparison and visualization</title><abstract>ChIPseeker is an R package for annotating ChIP-seq data analysis. It
   supports annotating ChIP peaks and provides functions to visualize ChIP
   peaks coverage over chromosomes and profiles of peaks binding to TSS
   regions. Comparison of ChIP peak profiles and annotation are also
   supported. Moreover, it supports evaluating significant overlap among
   ChIP-seq datasets. Currently, ChIPseeker contains 15 000 bed file
   information from GEO database. These datasets can be downloaded and
   compare with user's own data to explore significant overlap datasets for
   inferring co-regulation or transcription factor complex for further
   investigation.</abstract><date>JUL 15 2015</date><author>Yu, Guangchuang
   Wang, Li-Gen
   He, Qing-Yu</author></paper><paper><title>Effects of multispectral excitation on the sensitivity of molecular
   optoacoustic imaging</title><abstract>Molecular optoacoustic (photoacoustic) imaging typically relies on the
   spectral identification of absorption signatures from molecules of
   interest. To achieve this, two or more excitation wavelengths are
   employed to sequentially illuminate tissue. Due to depth-related
   spectral dependencies and detection related effects, the multispectral
   optoacoustic tomography (MSOT) spectral unmixing problem presents a
   complex non-linear inversion operation. So far, different studies have
   showcased the spectral capacity of optoacoustic imaging, without however
   relating the performance achieved to the number of wavelengths employed.
   Overall, the dependence of the sensitivity and accuracy of optoacoustic
   imaging as a function of the number of illumination wavelengths has not
   been so far comprehensively studied. In this paper we study the impact
   of the number of excitation wavelengths employed on the sensitivity and
   accuracy achieved by molecular optoacoustic tomography. We present a
   quantitative analysis, based on synthetic MSOT datasets and observe a
   trend of sensitivity increase for up to 20 wavelengths. Importantly we
   quantify this relation and demonstrate an up to an order of magnitude
   sensitivity increase of multi-wavelength illumination vs. single or dual
   wavelength optoacoustic imaging. Examples from experimental animal
   studies are finally utilized to support the findings.[GRAPHICS]In vivo
   MSOT imaging of a mouse brain bearing a tumor that is expressing a
   near-infrared fluorescent protein. (a) Monochromatic optoacoustic
   imaging at the peak excitation wavelength of the fluorescent protein.
   (b) Overlay of the detected bio-distribution of the protein (red
   pseudocolor) on the monochromatic optoacoustic image. (c) Ex vivo
   validation by means of cryoslicing fluorescence imaging.</abstract><date>AUG 2015</date><author>Tzoumas, Stratis
   Nunes, Antonio
   Deliolanis, Nikolaos C.
   Ntziachristos, Vasilis</author></paper><paper><title>PhyloGene server for identification and visualization of co-evolving
   proteins using normalized phylogenetic profiles</title><abstract>Proteins that function in the same pathways, protein complexes or the
   same environmental conditions can show similar patterns of sequence
   conservation across phylogenetic clades. In species that no longer
   require a specific protein complex or pathway, these proteins, as a
   group, tend to be lost or diverge. Analysis of the similarity in
   patterns of sequence conservation across a large set of eukaryotes can
   predict functional associations between different proteins, identify new
   pathway members and reveal the function of previously uncharacterized
   proteins. We used normalized phylogenetic profiling to predict protein
   function and identify new pathway members and disease genes. The
   phylogenetic profiles of tens of thousands conserved proteins in the
   human, mouse, Caenorhabditis elegans and Drosophila genomes can be
   queried on our new web server, PhyloGene. PhyloGene provides intuitive
   and user-friendly platform to query the patterns of conservation across
   86 animal, fungal, plant and protist genomes. A protein query can be
   submitted either by selecting the name from whole-genome protein sets of
   the intensively studied species or by entering a protein sequence. The
   graphic output shows the profile of sequence conservation for the query
   and the most similar phylogenetic profiles for the proteins in the
   genome of choice. The user can also download this output in numerical
   form.</abstract><date>JUL 1 2015</date><author>Sadreyev, Ilyas R.
   Ji, Fei
   Cohen, Emiliano
   Ruvkun, Gary
   Tabach, Yuval</author></paper><paper><title>Scalability of 3D deterministic particle transport on the Intel MIC
   architecture</title><abstract>The key to large-scale parallel solutions of deterministic particle
   transport problem is single-node computation performance. Hence,
   single-node computation is often parallelized on multi-core or many-core
   computer architectures. However, the number of on-chip cores grows
   quickly with the scale-down of feature size in semiconductor technology.
   In this paper, we present a scalability investigation of one energy
   group time-independent deterministic discrete ordinates neutron
   transport in 3D Cartesian geometry (Sweep3D) on Intel's Many Integrated
   Core (MIC) architecture, which can provide up to 62 cores with four
   hardware threads per core now and will own up to 72 in the future. The
   parallel programming model, OpenMP, and vector intrinsic functions are
   used to exploit thread parallelism and vector parallelism for the
   discrete ordinates method, respectively. The results on a 57-core MIC
   coprocessor show that the implementation of Sweep3D on MIC has good
   scalability in performance. In addition, the application of the Roofline
   model to assess the implementation and performance comparison between
   MIC and Tesla K20C Graphics Processing Unit (GPU) are also reported.</abstract><date>OCT 2015</date><author>Wang Qing-Lin
   Liu Jie
   Gong Chun-Ye
   Xing Zuo-Cheng</author></paper><paper><title>Wanderer, an interactive viewer to explore DNA methylation and gene
   expression data in human cancer</title><abstract>Background: The Cancer Genome Atlas (TCGA) offers a multilayered view of
   genomics and epigenomics data of many human cancer types. However, the
   retrieval of expression and methylation data from TCGA is a cumbersome
   and time-consuming task.Results: Wanderer is an intuitive Web tool
   allowing real time access and visualization of gene expression and DNA
   methylation profiles from TCGA. Given a gene query and selection of a
   TCGA dataset (e.g., colon adenocarcinomas), the Web resource provides
   the expression profile, at the single exon level, and the DNA
   methylation levels of HumanMethylation450 BeadChip loci inside or in the
   vicinity of the queried gene. Graphic and table outputs include
   individual and summary data as well as statistical tests, allowing the
   comparison of tumor and normal profiles and the exploration along the
   genomic locus and across tumor collections.Conclusions: Wanderer offers
   a simple interface to straightforward access to TCGA data, amenable to
   experimentalists and clinicians without bioinformatics expertise.
   Wanderer may be accessed at http://maplab.cat/wanderer.</abstract><date>JUN 23 2015</date><author>Diez-Villanueva, Anna
   Mallona, Izaskun
   Peinado, Miguel A.</author></paper><paper><title>An instance selection method for large datasets based on Markov
   Geometric Diffusion</title><abstract>Given the growing amount of data produced from within different areas of
   knowledge, data mining methods currently have to face challenging
   datasets with greater numbers of instances and attributes. However, the
   processing capacity of data mining algorithms is struggling under this
   growth. One alternative for tackling the problem is to perform instance
   selection on the data in order to reduce its size, as a preprocessing
   step for data mining algorithms.This study presents e-MGD, a method for
   instance selection as an extension of the Markov Geometric Diffusion
   method, which is a linear complexity method used in computer graphics
   for the simplification of triangular meshes. The original method was
   extended so that it was capable of reducing datasets commonly found in
   the field of data mining. For this purpose, two essential points of
   adjustment were required. Firstly, it was necessary to build a geometric
   structure from the data and secondly, to adjust the method so that it
   could deal with types of attributes encountered within these datasets.
   These adjustments however, did not influence the complexity of the final
   e-MGD, since it remained linear, which enabled it to be applied to
   datasets with a greater number of instances and features. One distinct
   characteristic of the proposed extension is that it focuses on
   preserving dataset information rather than improving classification
   accuracy, as in the case of most instance selection methods.In order to
   assess the performance of the method, we compared it with a number of
   classical and contemporary instance selection methods using medium to
   large datasets, plus a further set of very large datasets. The results
   demonstrated a good performance in terms of classification accuracy when
   compared to results from other methods, indicating that the e-MGD is a
   good alternative for instance selection. (C) 2015 Elsevier B.V. All
   rights reserved.</abstract><date>JAN 2016</date><author>Silva, Duilio A. N. S.
   Souza, Leandro C.
   Motta, Gustavo H. M. B.</author></paper><paper><title>Ocular Defect Rehabilitation Using Photography and Digital Imaging: A
   Clinical Report</title><abstract>Ocular disorders occasionally necessitate surgical intervention that may
   lead to eye defects. The primary objective in restoring and
   rehabilitating such defects with an ocular prosthesis is to enable
   patients to cope better with associated psychological stress and to
   return to their accustomed lifestyle. A series of detailed steps for
   custom-made ocular prosthesis fabrication using the advantages of
   digital photography to replace the conventional oil paint and monopoly
   iris painting technique are presented in this article. In the present
   case, a digital photograph of the patient's iris was captured using a
   digital camera and manipulated on a computer using graphic software to
   produce a replica of the natural iris. The described technique reduces
   treatment time, increases simplicity, and permits the patient's natural
   iris to be replicated without the need for iris painting and special
   artistic skills.</abstract><date>AUG 2015</date><author>Buzayan, Muaiyed M.
   Ariffin, Yusnidar T.
   Yunus, Norsiah
   Mahmood, Wan Adida Azina Binti</author></paper><paper><title>Parallel three-dimensional numerical simulation of rotating detonation
   engine on graphics processing units</title><abstract>Rotating detonation engine is a new concept engine driven by detonation
   combustion for aerospace vehicles. In this paper, three-dimensional
   numerical simulation was used to investigate the combustion process and
   kinetic properties of reactive flow within the combustion chamber. To
   utilize fine grid in simulation, it requires a significant amount of CPU
   Time to depict this combustion. With the help of NVIDIA's CUDA, the
   computation was discretized and distributed to multiple graphics
   processing units, making the computation time reasonable and tolerable.
   The present study showed both the way to map our model into CUDA
   programming and the performance of acceleration, explicating the details
   of CUDA programming for different types of data. The result verified
   that CUDA was an efficient method for detonation simulation. (C) 2014
   Elsevier Ltd. All rights reserved.</abstract><date>MAR 30 2015</date><author>Liu Meng
   Zhang Shuang
   Wang Jianping
   Chen Yifeng</author></paper><paper><title>From Geocentrism to Allocentrism: Teaching the Phases of the Moon in a
   Digital Full-Dome Planetarium</title><abstract>An increasing number of planetariums worldwide are turning digital,
   using ultra-fast computers, powerful graphic cards, and high-resolution
   video projectors to create highly realistic astronomical imagery in real
   time. This modern technology makes it so that the audience can observe
   astronomical phenomena from a geocentric as well as an allocentric
   perspective (the view from space). While the dome creates a sense of
   immersion, the digital planetarium introduces a new way to teach
   astronomy, especially for topics that are inherently three-dimensional
   and where seeing the phenomenon from different points of view is
   essential. Like a virtual-reality environment, an immersive digital
   planetarium helps learners create a more scientifically accurate
   visualization of astronomical phenomena. In this study, a digital
   planetarium was used to teach the phases of the Moon to children aged 12
   to 14. To fully grasp the lunar phases, one must imagine the spherical
   Moon (as perceived from space), revolving around the Earth while being
   illuminated by the Sun, and then reconcile this view with the geocentric
   perspective. Digital planetariums allow learners to have both an
   allocentric and a geocentric perspective on the lunar phases. Using a
   Design experiment approach, we tested an educational scenario in which
   the lunar phases were taught in an allocentric digital planetarium.
   Based on qualitative data collected before, during, and after the
   planetarium intervention, we were able to demonstrate that five out of
   six participants had a better understanding of the lunar phases after
   the planetarium session.</abstract><date>FEB 2016</date><author>Chastenay, Pierre</author></paper><paper><title>A GPU OpenCL based cross-platform Monte Carlo dose calculation engine
   (goMC)</title><abstract>Monte Carlo (MC) simulation has been recognized as the most accurate
   dose calculation method for radiotherapy. However, the extremely long
   computation time impedes its clinical application. Recently, a lot of
   effort has been made to realize fast MC dose calculation on graphic
   processing units (GPUs). However, most of the GPU-based MC dose engines
   have been developed under NVidia's CUDA environment. This limits the
   code portability to other platforms, hindering the introduction of
   GPU-based MC simulations to clinical practice. The objective of this
   paper is to develop a GPU OpenCL based cross-platform MC dose engine
   named goMC with coupled photon-electron simulation for external photon
   and electron radiotherapy in the MeV energy range. Compared to our
   previously developed GPU-based MC code named gDPM (Jia et al 2012 Phys.
   Med. Biol. 57 7783-97), goMC has two major differences. First, it was
   developed under the OpenCL environment for high code portability and
   hence could be run not only on different GPU cards but also on CPU
   platforms. Second, we adopted the electron transport model used in
   EGSnrc MC package and PENELOPE's random hinge method in our new dose
   engine, instead of the dose planning method employed in gDPM. Dose
   distributions were calculated for a 15 MeV electron beam and a 6 MV
   photon beam in a homogenous water phantom, a water-bone-lung-water slab
   phantom and a half-slab phantom. Satisfactory agreement between the two
   MC dose engines goMC and gDPM was observed in all cases. The average
   dose differences in the regions that received a dose higher than 10% of
   the maximum dose were 0.48-0.53% for the electron beam cases and
   0.15-0.17% for the photon beam cases. In terms of efficiency, goMC was
   similar to 4-16% slower than gDPM when running on the same NVidia TITAN
   card for all the cases we tested, due to both the different electron
   transport models and the different development environments. The code
   portability of our new dose engine goMC was validated by successfully
   running it on a variety of different computing devices including an
   NVidia GPU card, two AMD GPU cards and an Intel CPU processor.
   Computational efficiency among these platforms was compared.</abstract><date>OCT 7 2015</date><author>Tian, Zhen
   Shi, Feng
   Folkerts, Michael
   Qin, Nan
   Jiang, Steve B.
   Jia, Xun</author></paper><paper><title>gPGA: GPU Accelerated Population Genetics Analyses</title><abstract>BackgroundThe isolation with migration (IM) model is important for
   studies in population genetics and phylogeography. IM program applies
   the IM model to genetic data drawn from a pair of closely related
   populations or species based on Markov chain Monte Carlo (MCMC)
   simulations of gene genealogies. But computational burden of IM program
   has placed limits on its application.MethodologyWith strong
   computational power, Graphics Processing Unit (GPU) has been widely used
   in many fields. In this article, we present an effective implementation
   of IM program on one GPU based on Compute Unified Device Architecture
   (CUDA), which we call gPGA.ConclusionsCompared with IM program, gPGA can
   achieve up to 52.30X speedup on one GPU. The evaluation results
   demonstrate that it allows datasets to be analyzed effectively and
   rapidly for research on divergence population genetics.</abstract><date>AUG 6 2015</date><author>Zhou, Chunbao
   Lang, Xianyu
   Wang, Yangang
   Zhu, Chaodong</author></paper><paper><title>Real-time moving object detection algorithm on high-resolution videos
   using GPUs</title><abstract>Modern imaging sensors with higher megapixel resolution and frame rates
   are being increasingly used for wide-area video surveillance (VS). This
   has produced an accelerated demand for high-performance implementation
   of VS algorithms for real-time processing of high-resolution videos. The
   emergence of multi-core architectures and graphics processing units
   (GPUs) provides energy and cost-efficient platform to meet the real-time
   processing needs by extracting data level parallelism in such
   algorithms. However, the potential benefits of these architectures can
   only be realized by developing fine-grained parallelization strategies
   and algorithm innovation. This paper describes parallel implementation
   of video object detection algorithms like Gaussians mixture model (GMM)
   for background modelling, morphological operations for post-processing
   and connected component labelling (CCL) for blob labelling. Novel
   parallelization strategies and fine-grained optimization techniques are
   described for fully exploiting the computational capacity of CUDA cores
   on GPUs. Experimental results show parallel GPU implementation achieves
   significant speedups of similar to 250x for binary morphology, similar
   to 15x for GMM and similar to 2x for CCL when compared to sequential
   implementation running on Intel Xeon processor, resulting in processing
   of 22.3 frames per second for HD videos.</abstract><date>JAN 2016</date><author>Kumar, Praveen
   Singhal, Ayush
   Mehta, Sanyam
   Mittal, Ankush</author></paper><paper><title>2D lid-driven cavity flow simulation using GPU-CUDA with a high-order
   finite difference scheme</title><abstract>The high parallelism and low cost of the graphic processing units (GPUs)
   have attracted the interest of scientists and engineers requiring high
   computational power with a modest investment. This work explores the use
   of a GPU in the solution of the 2D lid-driven cavity flow problem using
   the pressure-velocity formulation for Reynolds numbers up to and turning
   to a 4th order finite difference scheme for spatial discretization.
   Results showed good agreement with those reported in the literature. The
   solver was implemented in both the CPU and the GPU in order to compare
   their performance, whereupon the latter was seventy times faster.</abstract><date>JUL 2015</date><author>Franco, Ediguer E.
   Barrera, Helver M.
   Lain, Santiago</author></paper><paper><title>Bayesian State-Space Modelling on High-Performance Hardware Using LibBi</title><abstract>LibBi is a software package for state space modelling and Bayesian
   inference on modern computer hardware, including multi-core central
   processing units, many-core graphics processing units, and
   distributed-memory clusters of such devices. The software parses a
   domain-specific language for model specification, then optimizes,
   generates, compiles and runs code for the given model, inference method
   and hardware platform. In presenting the software, this work serves as
   an introduction to state space models and the specialized methods
   developed for Bayesian inference with them. The focus is on sequential
   Monte Carlo (SMC) methods such as the particle filter for state
   estimation, and the particle Markov chain Monte Carlo and SMC2 methods
   for parameter estimation. All are well-suited to current computer
   hardware. Two examples are given and developed throughout, one a linear
   three-element windkessel model of the human arterial system, the other a
   nonlinear Lorenz '96 model. These are specified in the prescribed
   modelling language, and LibBi demonstrated by performing inference with
   them. Empirical results are presented, including a performance
   comparison of the software with different hardware configurations.</abstract><date>OCT 2015</date><author>Murray, Lawrence M.</author></paper><paper><title>GPU Compute Unified Device Architecture (CUDA)-based Parallelization of
   the RRTMG Shortwave Rapid Radiative Transfer Model</title><abstract>Radiative transfer of electromagnetic radiation through a planetary
   atmosphere is computed using an atmospheric radiative transfer model
   (RTM). One RTM is the rapid RTM (RRTM), which calculates both longwave
   and shortwave atmospheric radiative fluxes and heating rates. Broadband
   radiative transfer code for general circulation model (GCM)
   applications, rapid RTM for global (RRTMG), is based on the singlecolumn
   reference code, RRTM. The focus of this paper is on the RRTMG shortwave
   (RRTMG_SW) model. Due to its accuracy, RRTMG_SW has been implemented
   operationally in many weather forecast and climatemodels. In this paper,
   we examine the feasibility of using graphics processing units (GPUs) to
   accelerate the RRTMG_SW for a massive amount of atmospheric profiles. In
   recent years, GPUs have emerged as a low-cost, low-power, and a very
   high-performance alternative to conventional central processing units
   (CPUs). GPUs can provide a substantial improvement in RRTMG speed by
   supporting the parallel computation of large numbers of independent
   radiative calculations in separate atmospheric profiles. A
   GPU-compatible version of RRTMG was implemented and thorough testing was
   performed to ensure that the original level of accuracy is retained. Our
   results show that GPUs can provide significant speedup over conventional
   CPUs. In particular, Nvidia's Tesla K40 GPU card can provide a speedup
   of 202x compared to its single-threaded Fortran counterpart running on
   Intel Xeon E5-2603 CPU, whereas the speedup for four CPU cores, on one
   CPU socket, with respect to 1 CPU core is 5.6x.</abstract><date>FEB 2016</date><author>Mielikainen, Jarno
   Price, Erik
   Huang, Bormin
   Huang, Hung-Lung Allen
   Lee, Tsengdar</author></paper><paper><title>Falcon: A Graph Manipulation Language for Heterogeneous Systems</title><abstract>Graph algorithms have been shown to possess enough parallelism to keep
   several computing resources busy-even hundreds of cores on a GPU.
   Unfortunately, tuning their implementation for efficient execution on a
   particular hardware configuration of heterogeneous systems consisting of
   multicore CPUs and GPUs is challenging, time consuming, and error prone.
   To address these issues, we propose a domain-specific language (DSL),
   Falcon, for implementing graph algorithms that (i) abstracts the
   hardware, (ii) provides constructs to write explicitly parallel programs
   at a higher level, and (iii) can work with general algorithms that may
   change the graph structure (morph algorithms). We illustrate the usage
   of our DSL to implement local computation algorithms (that do not change
   the graph structure) and morph algorithms such as Delaunay mesh
   refinement, survey propagation, and dynamic SSSP on GPU and multicore
   CPUs. Using a set of benchmark graphs, we illustrate that the generated
   code performs close to the state-of-the-art hand-tuned implementations.</abstract><date>JAN 2016</date><author>Unnikrishnan, C.
   Nasre, Rupesh
   Srikant, Y. N.</author></paper><paper><title>The DynDom3D Webserver for the Analysis of Domain Movements in
   Multimeric Proteins</title><abstract>DynDom3D is a program for the analysis of domain movements in multimeric
   proteins. Its inputs are two structure files that indicate a possible
   domain movement, but the onus has been on the user to process the files
   so that there is the necessary one-to-one equivalence between atoms in
   the two atom lists. This is often a prohibitive task to carry out
   manually, which has limited the application of DynDom3D. Here we report
   on a webserver with a preprocessor that automatically creates an
   equivalence between atoms using sequence alignment methods. The
   processed structure files are passed to DynDom3D and the results are
   presented on a webpage that includes molecular graphics for easy
   visualization.</abstract><date>JAN 1 2016</date><author>Girdlestone, Christopher
   Hayward, Steven</author></paper><paper><title>iTagPlot: an accurate computation and interactive drawing tool for tag
   density plot</title><abstract>Motivation: Tag density plots are very important to intuitively reveal
   biological phenomena from capture-based sequencing data by visualizing
   the normalized read depth in a region.Results: We have developed
   iTagPlot to compute tag density across functional features in parallel
   using multicores and a grid engine and to interactively explore it in a
   graphical user interface. It allows us to stratify features by defining
   groups based on biological function and measurement, summary statistics
   and unsupervised clustering.</abstract><date>JUL 15 2015</date><author>Kim, Sung-Hwan
   Ezenwoye, Onyeka
   Cho, Hwan-Gue
   Robertson, Keith D.
   Choi, Jeong-Hyeon</author></paper><paper><title>Layered holographic stereogram based on inverse Fresnel diffraction</title><abstract>We propose an efficient algorithm using layered holographic stereogram
   for three-dimensional (3D) computer-generated holograms. The hologram is
   spatially partitioned into multiple holographic elements (hogels) to
   provide the occlusion effect and motion parallax by use of multiple
   viewpoint rendering. Each hogel is calculated with inverse Fresnel
   diffraction by slicing the viewing frustum according to the depth image.
   The sliced layers can provide accurate depth cues for reconstruction
   since the geometric information of the 3D scene is faithfully matched.
   The algorithm is compatible with computer graphics rendering techniques
   and robust for holograms with different parameters. When the hogel size
   equals 1 mm, the signal-to-noise ratio of the diffraction calculation is
   above 39 dB with a propagation distance longer than 10 mm. Numerical
   simulations and optical experiments have demonstrated that the proposed
   method can reconstruct quality 3D images with reduced computational
   load. (C) 2016 Optical Society of America</abstract><date>JAN 20 2016</date><author>Zhang, Hao
   Zhao, Yan
   Cao, Liangcai
   Jin, Guofan</author></paper><paper><title>A non-rigid point matching method with local topology preservation for
   accurate bladder dose summation in high dose rate cervical brachytherapy</title><abstract>GEC-ESTRO guidelines for high dose rate cervical brachytherapy advocate
   the reporting of the D2cc (the minimum dose received by the maximally
   exposed 2cc volume) to organs at risk. Due to large interfractional
   organ motion, reporting of accurate cumulative D2cc over a
   multifractional course is a non-trivial task requiring deformable image
   registration and deformable dose summation. To efficiently and
   accurately describe the point-to-point correspondence of the bladder
   wall over all treatment fractions while preserving local topologies, we
   propose a novel graphic processing unit (GPU)-based non-rigid point
   matching algorithm. This is achieved by introducing local anatomic
   information into the iterative update of correspondence matrix
   computation in the 'thin plate splines-robust point matching' (TPS-RPM)
   scheme. The performance of the GPU-based TPS-RPM with local topology
   preservation algorithm (TPS-RPM-LTP) was evaluated using four
   numerically simulated synthetic bladders having known deformations, a
   custom-made porcine bladder phantom embedded with twenty one fiducial
   markers, and 29 fractional computed tomography (CT) images from seven
   cervical cancer patients. Results show that TPS-RPM-LTP achieved
   excellent geometric accuracy with landmark residual distance error (RDE)
   of 0.7 +/- 0.3 mm for the numerical synthetic data with different scales
   of bladder deformation and structure complexity, and 3.7 +/- 1.8 mm and
   1.6 +/- 0.8 mm for the porcine bladder phantom with large and small
   deformation, respectively. The RDE accuracy of the urethral orifice
   landmarks in patient bladders was 3.7 +/- 2.1 mm. When compared to the
   original TPS-RPM, the TPS-RPMLTP improved landmark matching by reducing
   landmark RDE by 50 +/- 19%, 37 +/- 11% and 28 +/- 11% for the synthetic,
   porcine phantom and the patient bladders, respectively. This was
   achieved with a computational time of less than 15s in all cases with
   GPU acceleration. The efficiency and accuracy shown with the TPS-RPM-LTP
   indicate that it is a practical and promising tool for bladder dose
   summation in adaptive cervical cancer brachytherapy.</abstract><date>FEB 7 2016</date><author>Chen, Haibin
   Zhong, Zichun
   Liao, Yuliang
   Pompos, Arnold
   Hrycushko, Brian
   Albuquerque, Kevin
   Zhen, Xin
   Zhou, Linghong
   Gu, Xuejun</author></paper><paper><title>Correcting Illumina data</title><abstract>Next-generation sequencing technologies revolutionized the ways in which
   genetic information is obtained and have opened the door for many
   essential applications in biomedical sciences. Hundreds of gigabytes of
   data are being produced, and all applications are affected by the errors
   in the data. Many programs have been designed to correct these errors,
   most of them targeting the data produced by the dominant technology of
   Illumina. We present a thorough comparison of these programs. Both HiSeq
   and MiSeq types of Illumina data are analyzed, and correcting
   performance is evaluated as the gain in depth and breadth of coverage,
   as given by correct reads and k-mers. Time and memory requirements,
   scalability and parallelism are considered as well. Practical guidelines
   are provided for the effective use of these tools. We also evaluate the
   efficiency of the current state-of-the-art programs for correcting
   Illumina data and provide research directions for further improvement.</abstract><date>JUL 2015</date><author>Molnar, Michael
   Ilie, Lucian</author></paper><paper><title>Real-time fMRI processing with physiological noise correction -
   Comparison with off-line analysis</title><abstract>Background: While applications of real-time functional magnetic
   resonance imaging (rtfMRI) are growing rapidly, there are still
   limitations in real-time data processing compared to off-line
   analysis.New methods: We developed a proof-of-concept real-time fMRI
   processing (rtfMRIp) system utilizing a personal computer (PC) with a
   dedicated graphic processing unit (GPU) to demonstrate that it is now
   possible to perform intensive whole-brain fMRI data processing in
   real-time. The rtfMRIp performs slice-timing correction, motion
   correction, spatial smoothing, signal scaling, and general linear model
   (GLM) analysis with multiple noise regressors including physiological
   noise modeled with cardiac (RETROICOR) and respiration volume per time
   (RVT).Results: The whole-brain data analysis with more than 100,000
   voxels and more than 250 volumes is completed in less than 300 ms, much
   faster than the time required to acquire the fMRI volume. Real-time
   processing implementation cannot be identical to off-line analysis when
   time-course information is used, such as in slice-timing correction,
   signal scaling, and GLM. We verified that reduced slice-timing
   correction for real-time analysis had comparable output with off-line
   analysis. The real-time GLM analysis, however, showed over-fitting when
   the number of sampled volumes was small.Comparison with existing
   methods: Our system implemented real-time RETROICOR and RVT
   physiological noise corrections for the first time and it is capable of
   processing these steps on all available data at a given time, without
   need for recursive algorithms.Conclusions: Comprehensive data processing
   in rtfMRI is possible with a PC, while the number of samples should be
   considered in real-time GLM. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>DEC 30 2015</date><author>Misaki, Masaya
   Barzigar, Nafise
   Zotev, Vadim
   Phillips, Raquel
   Cheng, Samuel
   Bodurka, Jerzy</author></paper><paper><title>Efficient Execution of Multiple CUDA Applications Using Transparent
   Suspend, Resume and Migration</title><abstract>GPUs are now one of the mainstream high-performance processors,
   embodying rich sets of computational as well as bandwidth resources.
   However, an individual GPU application typically does not exploit the
   resources on a GPU in its entirety, and thus concurrent execution of
   multiple applications may be advantageous in terms of total execution
   time and energy, by multiplexing on less utilized resources. Although
   modern GPU features such as Hyper-Q allow such a concurrent execution,
   it is at the risk of causing device memory shortage, and thus crashing
   the application or even the entire node. Our Mobile CUDA realizes safe,
   concurrent execution of multiple, unmodified CUDA applications using a
   transparent checkpointing approach, and achieves both improved
   throughput and energy savings for a mix of applications exhibiting
   different GPU resource requirements on multiple GPUs. Performance
   evaluation using the Rodinia benchmark suite shows that Mobile CUDA
   reduces total execution time by 18.4% and total energy by 5.5% on mixed
   workloads.</abstract><date>2015</date><author>Suzuki, Taichiro
   Nukada, Akira
   Matsuoka, Satoshi</author></paper><paper><title>Integrative Simulation Environment for Conceptual Structural Analysis</title><abstract>This paper introduces the integrative simulation environment (ISE)
   methodology and a proof-of-concept implementation, designed as modular
   and extensible approach for interactive structural simulation. The ISE
   methodology allows users to sketch, model, simulate, visualize, and
   augment data based on an immediate-mode paradigm where sketches are
   automatically turned into models, which are immediately passed to a
   simulation engine to be analyzed in order to have the results passed
   back to a visualizer, operating within the same display context that the
   initial sketch and model were created in. In other words, data can be
   acquired, defined, delivered, transformed, computed, visualized, and
   augmented throughout the entire integrative simulation process in a
   real-time loop, providing the user with an intuitive and interactive
   modeling interface. ISE as such provides a mechanism to conceptually
   study cause and effect relationships of design decisions, while allowing
   for real-time, context specific, digital content overlay onto as-built
   structures via an augmented reality technique. (C) 2014 American Society
   of Civil Engineers.</abstract><date>JUL 2015</date><author>Ge, Li
   Kuester, Falko</author></paper><paper><title>Real-time implementation of remotely sensed hyperspectral image unmixing
   on GPUs</title><abstract>Spectral unmixing is one of the most popular techniques to analyze
   remotely sensed hyperspectral images. It generally comprises three
   stages: (1) reduction of the dimensionality of the original image to a
   proper subspace; (2) automatic identification of pure spectral
   signatures (called endmembers); and (3) estimation of the fractional
   abundance of each endmember in each pixel of the scene. The spectral
   unmixing process allows sub-pixel analysis of hyperspectral images, but
   can be computationally expensive due to the high dimensionality of the
   data. In this paper, we develop the first real-time implementation of a
   full spectral unmixing chain in commodity graphics processing units
   (GPUs). These hardware accelerators offer a source of computational
   power that is very appealing in hyperspectral remote sensing
   applications, mainly due to their low cost and adaptivity to on-board
   processing scenarios. The implementation has been developed using the
   compute device unified architecture (CUDA) and tested on an NVidia (TM)
   GTX 580 GPU, achieving real-time unmixing performance in two different
   case studies: (1) characterization of thermal hot spots in hyperspectral
   images collected by NASA's Airborne Visible Infra-red Imaging
   Spectrometer (AVIRIS) during the terrorist attack to the World Trade
   Center complex in New York City, and (2) sub-pixel mapping of minerals
   in AVIRIS hyperspectral data collected over the Cuprite mining district
   in Nevada.</abstract><date>SEP 2015</date><author>Sanchez, Sergio
   Ramalho, Rui
   Sousa, Leonel
   Plaza, Antonio</author></paper><paper><title>GPU-assisted real-time three dimensional shape measurement by
   speckle-embedded fringe</title><abstract>This paper presents a novel two-frame method of fringe projection for
   real-time, accurate and unambiguous three-dimensional shape measurement.
   One of the used frames is a speckle pattern and the other one is a
   composite image which is fused by that speckle image and sinusoidal
   fringes. The sinusoidal component is used to retrieve the wrapped phase
   map. The frame of the speckle is employed to remove the phase ambiguity
   for the reconstruction of the absolute depth. Compared with traditional
   multi-frequency phase-shifting methods, the proposed scheme is of much
   lower sensitivity to movements as the result of the reduced number of
   used patterns. Moreover, its measuring precision is very close to that
   of the phase-shifting method, which indicates the method is of high
   accuracy. To process data in real time, a CUDA-enabled Graphics
   Processing Unit (GPU) is introduced to accelerate the computations of
   phase and depth. With our system, measurements can be performed at 21
   frames per second with a resolution of 307K points per frame.</abstract><date>2015</date><author>Feng, Shijie
   Chen, Qian
   Zuo, Chao</author></paper><paper><title>EFFICIENT AND ACCURATE PARALLEL INVERSION OF THE GAMMA DISTRIBUTION</title><abstract>A method for parallel inversion of the gamma distribution is described.
   This is very desirable for random number generation in Monte Carlo
   simulations where gamma variates are required. Let a be a fixed but
   arbitrary positive real number. Explicitly, given a list of uniformly
   distributed random numbers our algorithm applies the quantile function
   (inverse CDF) of the gamma distribution with shape parameter a to each
   element. The result is, therefore, a list of random numbers distributed
   according to the said distribution. The output of our algorithm has
   accuracy close to a choice of single-or double-precision machine
   epsilon. Inversion of the gamma distribution is traditionally
   accomplished using some form of root finding. This is known to be
   computationally expensive. Our algorithm departs from this paradigm by
   using an initialization phase to construct, on the fly, a piecewise
   Chebyshev polynomial approximation to a transformation function, which
   can be evaluated very quickly during variate generation. The Chebyshev
   polynomials are high order, for good accuracy, and generated via
   recurrence relations derived from nonlinear second order ODEs. A novelty
   of our approach is that the same change of variable is applied to each
   uniform random number before evaluating the transformation function.
   This is particularly amenable to implementation on SIMD architectures,
   whose performance is sensitive to frequently diverging execution flows
   due to conditional statements (branch divergence). We show the
   performance of a CUDA GPU implementation of our algorithm (called
   Quantus) is within an order of magnitude of the time to compute the
   normal quantile function.</abstract><date>2015</date><author>Luu, Thomas</author></paper><paper><title>A Shooting and Bouncing Ray (SBR) Modeling Framework Involving
   Dielectrics and Perfect Conductors</title><abstract>A parallel computing architecture based on NVIDIA's compute unified
   device architecture (CUDA) for the modeling of electromagnetic wave
   propagation by means of the high-frequency approximation method,
   shooting and bouncing rays (SBRs), is introduced and evaluated. The
   algorithm provides a reliable treatment of problems involving bulky and
   nonperfectly conducting materials. The general case of wave propagation
   through arbitrary dielectric materials also considering evanescent waves
   is presented in a concise way and a simplified treatment of the
   practically important case of thin dielectric layers over perfectly
   electrically conducting (PEC) bodies is considered. The resulting hybrid
   SBR approach is capable of dealing with PEC and the various dielectric
   object cases in a unified manner. A wide variety of scattering problems
   is considered, and scattered field predictions are compared to exact
   method of moments (MoM) and finite-element method (FEM) results. In
   particular, the benefits of the SBR within bulky dielectrics are
   highlighted.</abstract><date>AUG 2015</date><author>Brem, Robert
   Eibert, Thomas F.</author></paper><paper><title>GPU acceleration of amplitude-preserved Q compensation prestack time
   migration</title><abstract>Amplitude-preserved Q compensation prestack time migration (Q migration)
   is a new method that evolved from prestack Kirchhoff time migration
   (PKTM). Five algorithms are developed for Q migration on graphics
   processing units (GPUs). First, the principle of Q migration is briefly
   introduced. Second, one parallel strategy, namely, imaging domain
   parallel strategy, is proposed to accelerate Q migration on a single GPU
   by developing GPU algorithm. Results show that the imaging domain
   parallel strategy with the corresponding algorithm is superior to the
   CPU algorithm in several aspects, i.e., faster computing speed, shorter
   computing time, and higher computational efficiency. Third, based on the
   imaging domain parallel strategy, two division methods, namely, seismic
   data division method and velocity data division method, are presented to
   optimize the performance of Q migration on multi-GPUs and four
   algorithms are implemented by using Message Passing Interface (MPI)+
   Compute Unified Device Architecture (CUDA) and multi-thread + CUDA. An
   optimal algorithm is determined by comparing the performance of four
   algorithms. Results demonstrate that the optimal algorithm has the
   shortest computing time, which is 3.85 times shorter than that of a
   single GPU when four GPUs are all involved in computation and 300 times
   shorter than that of a 4-core central processing unit (CPU). Finally, a
   parallel computing framework on GPU cluster is established, which
   consists of imaging domain parallel strategy, seismic data division
   method and MPI+CUDA. This framework is suitable for all prestack time
   migration (PSTM) methods and has a short computing time and high speedup
   ratio on GPU cluster. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>SEP 2015</date><author>Han, Fei
   Sun, Sam Z.</author></paper><paper><title>Anomaly detection with low magnetic flux: A fluxgate sensor network
   application</title><abstract>Recent studies on remote detection methods were mostly for improving
   variables like sensing distance, sensitivity and power consumption.
   Especially using anisotropic magneto-resistive sensors with low power
   consumption and high sensitivity for detecting subsurface magnetic
   materials became very popular in last decades. In our study, for
   detecting subsurface materials, we have used fluxgate sensor network for
   having even higher sensitivity and also minimizing the power consumption
   by detecting the changing rates of horizontal component of earth's
   magnetic flux which is assumed to be very low. We have constituted a
   magnetic measurement system which comprises a detector system, which has
   a mechanism enables sensors to move in 3-D space, a data acquisition
   module for processing and sending all sensor information, and a computer
   for running the magnetic flux data evaluation and recording software.
   Using this system, tests are carried out to detect anomalies on
   horizontal component of earth's magnetic flux which is created by
   different subsurface materials with known magnetic, chemical and
   geometric properties. The harmonics of horizontal component of earth's
   magnetic flux in scanned area are analyzed by the help of DSP Lock-In
   amplifier and the amplitudes of high variation harmonics are shown as
   computer graphics. Using the graphic information, the upside surface
   geometry of subsurface material is defined. For identifying the magnetic
   anomalies, we have used the scale-invariant feature transform
   (SIFT)-binary robust invariant scalable keypoints (BRISKs) as keypoint
   and descriptor. We used an algorithm for matching the newly scanned
   image to the closest image in database which is constituted of mines and
   possible other metal objects like cans, etc. Results show that, if the
   proposed detection system is used instead of metal detectors which
   cannot distinguish mines from other metal materials and alert for every
   type of metal with different geometries, it can be said that miss alarm
   count, work force and time can be decreased dramatically. In this paper,
   mostly the setup of the system is described and in Appendix A some
   experimental outputs of the system for different geometries of metal
   samples are given. And also for comparing the results of the proposed
   system, additional experiments are carried out with a different type of
   sensor chip, namely KMZ51, and also given in Appendix A. (C) 2015
   Elsevier Ltd. All rights reserved.</abstract><date>MAR 2016</date><author>Ege, Yavuz
   Coramik, Mustafa
   Kabadayi, Murat
   Citak, Hakan
   Kalender, Osman
   Yuruklu, Emrah
   Kurt, Unal
   Nazlibilek, Sedat</author></paper><paper><title>Fast Methods for Spherical Linear Interpolation in Minkowski Space</title><abstract>Spherical linear interpolation in Minkowski space has got a number of
   important applications in computer graphics, physics and kinematics.
   Spherical linear interpolation in Minkowski space involves the
   computation of trigonometric functions, which are computationally
   expensive. The computation will be fast since the implementation does
   not need to evaluate any trigonometric functions in the inner loop.
   Furthermore, no renormalization is necessary and therefore it is a true
   spherical interpolation in Minkowski space. We propose that incremental
   Slerp in Minkowski space. In this paper we demonstrate four different
   methods for incremental Slerp in Minkowski space.</abstract><date>DEC 2015</date><author>Ghadami, Raheleh
   Rahebi, Javad
   Yayly, Yusuf</author></paper><paper><title>Description, molecular phylogeny, and natural history of a new
   kleptoparasitic species of gelechiid moth (Lepidoptera) associated with
   Melastomataceae galls in Brazil</title><abstract>The male, female, pupa and larva of a new species of Gelechiidae
   (Lepidoptera), Locharcha opportuna Moreira and Becker, are described and
   illustrated with the aid of optical and scanning electron microscopy. A
   preliminary analysis of mitochondrial DNA sequences including members of
   related lineages is also provided. The immature stages are associated
   with galls induced by a species of Palaeomystella Fletcher (Lepidoptera:
   Momphidae) on Tibouchina sellowiana (Cham.) Cogn. (Melastomataceae),
   endemic to the Atlantic Rainforest. Larvae are kleptoparasitic, usurping
   the gall internal space and thereafter feeding on the internal tissues.
   By determining the variation in population density of both species and
   following gall development individually throughout ontogeny under field
   conditions, we demonstrated that the kleptoparasite completes its life
   cycle inside galls induced by Palaeomystella, where pupation occurs. The
   variation in seasonal abundance of the kleptoparasite is tied to that of
   the cecidogenous species, with their corresponding peaks in density
   occurring sequentially.[GRAPHICS]</abstract><date>AUG 26 2015</date><author>Luz, Fernando A.
   Goncalves, Gislene L.
   Moreira, Gilson R. P.
   Becker, Vitor O.</author></paper><paper><title>The Ettention software package</title><abstract>We present a novel software package for the problem "reconstruction from
   projections" in electron microscopy. The Ettention framework consists of
   a set of modular building-blocks for tomographic reconstruction
   algorithms. The well-known block iterative reconstruction method based
   on Kaczmarz algorithm is implemented using these building-blocks,
   including adaptations specific to electron tomography. Ettention
   simultaneously features (1) a modular, object-oriented software design,
   (2) optimized access to high-performance computing (HPC) platforms such
   as graphic processing units (GPU) or many-core architectures like Xeon
   Phi, and (3) accessibility to microscopy end-users via integration in
   the IMOD package and eTomo user interface. We also provide developers
   with a clean and well-structured application programming interface (API)
   that allows for extending the software easily and thus makes it an ideal
   platform for algorithmic research while hiding most of the technical
   details of high-performance computing. (c) 2015 Elsevier B.V. All rights
   reserved.</abstract><date>FEB 2016</date><author>Dahmen, Tim
   Marsalek, Lukas
   Marniok, Nico
   Turonova, Beata
   Bogachev, Sviatoslav
   Trampert, Patrick
   Nickels, Stefan
   Slusallek, Philipp</author></paper><paper><title>Natural convection with an array of solid obstacles in an enclosure by
   lattice Boltzmann method on a CUDA computation platform</title><abstract>Lattice Boltzmann method is implemented for conjugate heat transfer of a
   square cavity with solid obstacles. Natural convection is considered in
   the fluid region while conduction is assumed in the solid obstacles. By
   adjusting the velocity field and thermal diffusivities, only one single
   temperature distribution function is needed to simulate the convection
   in the fluid region and the heat conduction in the solid obstacles
   region. As a result, matching boundary conditions at the interface are
   automatically satisfied. The conjugate heat transfer in a rectangular
   enclosure of natural convection with solid obstacles is considered next.
   Streamlines, temperature distributions are presented for different
   Rayleigh numbers, thermal diffusivity ratios and number of solid blocks.
   The parallel characteristic of LBM is well fitted to the parallel
   hardware of graphic processor units (GPU) using a CUDA platform.
   Implementation of conjugate heat transfer LBM algorithm on GPU is
   presented next. It is found that GPU can accelerate the computation by a
   factor up to 20 as compared to the non-parallel CPU code. It is
   demonstrated that lattice Boltzmann Method is an effective approach to
   simulate conjugate heat transfer with solid obstacles. Published by
   Elsevier Ltd.</abstract><date>FEB 2016</date><author>Ren, Qinlong
   Chan, Cho Lik</author></paper><paper><title>A Fully GPU-Based Ray-Driven Backprojector via a Ray-Culling Scheme with
   Voxel-Level Parallelization for Cone-Beam CT Reconstruction</title><abstract>A ray-driven backprojector is based on ray-tracing, which computes the
   length of the intersection between the ray paths and each voxel to be
   reconstructed. To reduce the computational burden caused by these
   exhaustive intersection tests, we propose a fully graphics processing
   unit (GPU)-based ray-driven backprojector in conjunction with a
   ray-culling scheme that enables straightforward parallelization without
   compromising the high computing performance of a GPU. The purpose of the
   ray-culling scheme is to reduce the number of ray-voxel intersection
   tests by excluding rays irrelevant to a specific voxel computation. This
   rejection step is based on an axis-aligned bounding box (AABB) enclosing
   a region of voxel projection, where eight vertices of each voxel are
   projected onto the detector plane. The range of the rectangular-shaped
   AABB is determined by min/max operations on the coordinates in the
   region. Using the indices of pixels inside the AABB, the rays passing
   through the voxel can be identified and the voxel is weighted as the
   length of intersection between the voxel and the ray. This procedure
   makes it possible to reflect voxel-level parallelization, allowing an
   independent calculation at each voxel, which is feasible for a GPU
   implementation. To eliminate redundant calculations during ray-culling,
   a shared-memory optimization is applied to exploit the GPU memory
   hierarchy. In experimental results using real measurement data with
   phantoms, the proposed GPU-based ray-culling scheme reconstructed a
   volume of resolution 280x280x176 in 77 seconds from 680 projections of
   resolution 1024x768 , which is 26 times and 7.5 times faster than
   standard CPU-based and GPU-based ray-driven backprojectors,
   respectively. Qualitative and quantitative analyses showed that the
   ray-driven backprojector provides high-quality reconstruction images
   when compared with those generated by the Feldkamp-Davis-Kress algorithm
   using a pixel-driven backprojector, with an average of 2.5 times higher
   contrast-to-noise ratio, 1.04 times higher universal quality index, and
   1.39 times higher normalized mutual information.</abstract><date>DEC 2015</date><author>Park, Hyeong-Gyu
   Shin, Yeong-Gil
   Lee, Ho</author></paper><paper><title>Computationally Efficient Implementation of a Hamming Code Decoder Using
   Graphics Processing Unit</title><abstract>This paper presents a computationally efficient implementation of a
   Hamming code decoder on a graphics processing unit (GPU) to support
   real-time software-defined radio, which is a software alternative for
   realizing wireless communication. The Hamming code algorithm is
   challenging to parallelize effectively on a GPU because it works on
   sparsely located data items with several conditional statements, leading
   to non-coalesced, long latency, global memory access, and huge thread
   divergence. To address these issues, we propose an optimized
   implementation of the Hamming code on the GPU to exploit the higher
   parallelism inherent in the algorithm. Experimental results using a
   compute unified device architecture (CUDA)-enabled NVIDIA GeForce GTX
   560, including 335 cores, revealed that the proposed approach achieved a
   99x speedup versus the equivalent CPU-based implementation.</abstract><date>APR 2015</date><author>Islam, Md Shohidul
   Kim, Cheol-Hong
   Kim, Jong-Myon</author></paper><paper><title>Box, cable and smartphone: a simple laparoscopic trainer.</title><abstract>BACKGROUND: Laparoscopic surgery requires different abilities to open
   surgery, and is challenging to learn within the confines of the
   operating theatre. With the development of laparoscopic surgery in
   modern surgery, the importance in improving these skills is becoming an
   increasing focus of surgical training programmes.CONTEXT: The assembly
   of the laparoscopic trainer and exercises was performed at the
   University of Sydney Clinical School located at Hornsby Hospital in
   Sydney, Australia. The objective was to design and construct a new
   concept smartphone box laparoscopic trainer that is affordable and
   replicable, and to demonstrate its usefulness in practising laparoscopic
   techniques to improve skills outside of the operating
   theatre.INNOVATION: The trainer was constructed using a personal
   smartphone, cardboard box, video graphics array (VGA) adaptor, VGA cable
   and a computer screen. Laparoscopic instruments and materials used for
   simulated task exercises were obtained from the operating theatre.
   Simulated demonstrations of simple laparoscopic tasks included suture
   handling, instrument knot-tying and anastomotic suture
   techniques.IMPLICATIONS: The smartphone box trainer is inexpensive
   (approximately $60) and took less than 20minutes to build. The cost was
   almost entirely for the VGA adaptor. The box trainer was light, portable
   and easily transported to any setting that provided a computer screen.
   It is an inexpensive, easy-to-assemble, replicable model that benefits
   from the advanced technology of personal smartphones, and can be easily
   accessed as a useful tool in learning and improving laparoscopic
   techniques. Laparoscopic surgery requires different abilities to open
   surgery.</abstract><date>2015-Dec</date><author>Lee, Migie
   Savage, Jason
   Dias, Maxwell
   Bergersen, Philip
   Winter, Matthew</author></paper><paper><title>Introduction of a nomogram for predicting adverse pregnancy outcomes
   based on maternal serum markers in the quad screen test</title><abstract>The aim of this study was to develop a nomogram that can calculate a
   total score, derived from each serum marker in the quad screen test, for
   systematically predicting adverse pregnancy outcomes (APOs).We
   retrospectively reviewed 3684 singleton pregnant women who underwent a
   quad screen test and gave birth at a single medical centre from January
   2005 to December 2010. The serum marker data from the quad screen test
   and pregnancy outcomes were used to construct logistic regression models
   for predicting the risks of APOs. APO was defined as the presence of at
   least one of the following: preeclampsia, preterm delivery before 34
   weeks of gestation, small for gestational age, foetal loss, and foetal
   demise. A graphic nomogram was generated to represent the scoring model
   using the regression coefficient of each serum marker.A nomogram for the
   prediction of APOs using each serum marker in the quad test was
   developed based on the logistic regression analysis. The positive
   predictive values for the subsequent development of an APO were ascended
   stepwise as the calculated score increases. The area under the receiver
   operating characteristic curve of this score for the prediction of APO
   was 0.596 (95 % confidence interval 0.569-0.623).We here introduced a
   nomogram for stratifying the risk of APOs in patients with abnormal
   serum markers in the quad screen test. Although the validity of the
   nomogram is too weak to be used in clinical routine, but it may provide
   additional information for practitioners counselling pregnant women and
   for predicting APOs.</abstract><date>SEP 2015</date><author>An, Jung-Joo
   Ji, Hyun-Young
   You, Ji Yeon
   Woo, Sook-Young
   Choi, Suk-Joo
   Oh, Soo-young
   Roh, Cheong-Rae
   Kim, Jong-Hwa</author></paper><paper><title>Visual Analytics for the Exploration of Tumor Tissue Characterization</title><abstract>Tumors are heterogeneous tissues consisting of multiple regions with
   distinct characteristics. Characterization of these intra-tumor regions
   can improve patient diagnosis and enable a better targeted treatment.
   Ideally, tissue characterization could be performed non-invasively,
   using medical imaging data, to derive per voxel a number of features,
   indicative of tissue properties. However, the high dimensionality and
   complexity of this imaging-derived feature space is prohibiting for easy
   exploration and analysis - especially when clinical researchers require
   to associate observations from the feature space to other reference
   data, e.g., features derived from histopathological data. Currently, the
   exploratory approach used in clinical research consists of juxtaposing
   these data, visually comparing them and mentally reconstructing their
   relationships. This is a time consuming and tedious process, from which
   it is difficult to obtain the required insight. We propose a visual tool
   for: (1) easy exploration and visual analysis of the feature space of
   imaging-derived tissue characteristics and (2) knowledge discovery and
   hypothesis generation and confirmation, with respect to reference data
   used in clinical research. We employ, as central view, a 2D embedding of
   the imaging-derived features. Multiple linked interactive views provide
   functionality for the exploration and analysis of the local structure of
   the feature space, enabling linking to patient anatomy and clinical
   reference data. We performed an initial evaluation with ten clinical
   researchers. All participants agreed that, unlike current practice, the
   proposed visual tool enables them to identify, explore and analyze
   heterogeneous intra-tumor regions and particularly, to generate and
   confirm hypotheses, with respect to clinical reference data.</abstract><date>JUN 2015</date><author>Raidou, R. G.
   van der Heide, U. A.
   Dinh, C. V.
   Ghobadi, G.
   Kallehauge, J. F.
   Breeuwer, M.
   Vilanova, A.</author></paper><paper><title>Particle filtering on GPU architectures for manufacturing applications</title><abstract>Particle filters are nonlinear estimators that can be used to detect
   anomalies in manufacturing processes. Although promising, their high
   computational cost often prevents their implementation in real-time
   applications. Recently, the introduction of graphics processing units
   (GPUs) has enabled the acceleration of computationally intensive
   processes with their massive parallel capabilities. This article
   presents the acceleration of the particle filter and the auxiliary
   particle filter, two of the most important particle methods, on a GPU
   using NVIDIA CUDA technology. This is illustrated via simulation for a
   remelting process where the accelerated algorithms return accurate
   estimates while still being two orders of magnitude faster than the
   physical process even for calculations that involve millions of
   particles. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>AUG 2015</date><author>Lopez, Felipe
   Zhang, Lixun
   Mok, Aloysius
   Beaman, Joseph</author></paper><paper><title>Directive-Based Compilers for GPUs</title><abstract>General Purpose Graphics Computing Units can be effectively used for
   enhancing the performance of many contemporary scientific applications.
   However, programming GPUs using machine-specific notations like CUDA or
   OpenCL can be complex and time consuming. In addition, the resulting
   programs are typically fine-tuned for a particular target device. A
   promising alternative is to program in a conventional and
   machine-independent notation extended with directives and use compilers
   to generate GPU code automatically. These compilers enable portability
   and increase programmer productivity and, if effective, would not impose
   much penalty on performance.This paper evaluates two such compilers, PGI
   and Cray. We first identify a collection of standard transformations
   that these compilers can apply. Then, we propose a sequence of manual
   transformations that programmers can apply to enable the generation of
   efficient GPU kernels. Lastly, using the Rodinia Benchmark suite, we
   compare the performance of the code generated by the PGI and Cray
   compilers with that of code written in CUDA. Our evaluation shows that
   the code produced by the PGI and Cray compilers can perform well. For 6
   of the 15 benchmarks that we evaluated, the compiler generated code
   achieved over 85% of the performance of a hand-tuned CUDA version.</abstract><date>2015</date><author>Ghike, Swapnil
   Gran, Ruben
   Garzaran, Maria J.
   Padua, David A.</author></paper><paper><title>Parallel GPU architecture framework for the WRF Single Moment 6-class
   microphysics scheme</title><abstract>An Earth-observing remote sensing instrument is used to collect
   information about the physical environment within its
   instantaneous-field-of-view and is often placed aboard a suborbital or
   satellite platform for maximal spatial coverage. Remote sensing
   inversion techniques can extract valuable meteorological parameters that
   are subsequently passed through weather models for research and
   forecasting. One of the several microphysics packages for clouds and
   precipitation in weather models is WRF Single Moment 6-class (WSM6)
   scheme, and it is now widely used. With the advancement in Graphics
   Processing Units (GPUs), implementation of a fast and parallel WSM6
   scheme is achievable. This paper describes a massively parallel GPU
   design of the WSM6 scheme. The performance is compared to a CPU
   implementation running on Intel Xeon E5-2603 at 1.8 GHz. Our
   implementation shows a speedup of 216 x using a single NVIDIA K40 GPU as
   compared to its CPU counterpart running on one CPU core, whereas the
   speedup for one CPU socket (4 cores) with respect to one CPU core is
   only 3.7 x. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>OCT 2015</date><author>Huang, Melin
   Huang, Bormin
   Gu, Lingjia
   Huang, H. -L. Allen
   Goldberg, Mitchell D.</author></paper><paper><title>Trends in Continuity and Interpolation for Computer Graphics</title><abstract></abstract><date>NOV-DEC 2015</date><author>Gonzalez Garcia, Francisco</author></paper><paper><title>Enhancing the quality of reconstructed 3D objects by using point
   clusters</title><abstract>A novel algorithm for constructing computer-generated holograms using
   point clusters is presented. This method exploits the precalculated
   triangular meshes used in previous research and can reconstruct less
   noisy 3D objects. In addition, the high-speed property of a
   ferroelectric liquid crystal spatial light modulator is utilized to
   enhance the reconstruction quality. All 3D holograms generated in this
   paper are based on Fresnel propagation; thus, the Fresnel plane is
   treated as a vital element in producing the hologram. A GeForce GTX 770
   graphics card with 2 GB memory was used to achieve parallel high-speed
   hologram generation. (C) 2015 Optical Society of America</abstract><date>JUN 20 2015</date><author>Yang, Fan
   Kaczorowski, Andrzej
   Wilkinson, Tim D.</author></paper><paper><title>OpenACC acceleration of an unstructured CFD solver based on a
   reconstructed discontinuous Galerkin method for compressible flows</title><abstract>An OpenACC directive-based graphics processing unit (GPU) parallel
   scheme is presented for solving the compressible Navier-Stokes equations
   on 3D hybrid unstructured grids with a third-order reconstructed
   discontinuous Galerkin method. The developed scheme requires the minimum
   code intrusion and algorithm alteration for upgrading a legacy solver
   with the GPU computing capability at very little extra effort in
   programming, which leads to a unified and portable code development
   strategy. A face coloring algorithm is adopted to eliminate the memory
   contention because of the threading of internal and boundary face
   integrals. A number of flow problems are presented to verify the
   implementation of the developed scheme. Timing measurements were
   obtained by running the resulting GPU code on one Nvidia Tesla K20c GPU
   card (Nvidia Corporation, Santa Clara, CA, USA) and compared with those
   obtained by running the equivalent Message Passing Interface (MPI)
   parallel CPU code on a compute node (consisting of two AMD Opteron 6128
   eight-core CPUs (Advanced Micro Devices, Inc., Sunnyvale, CA, USA)).
   Speedup factors of up to 24x and 1.6x for the GPU code were achieved
   with respect to one and 16 CPU cores, respectively. The numerical
   results indicate that this OpenACC-based parallel scheme is an effective
   and extensible approach to port unstructured high-order CFD solvers to
   GPU computing. Copyright (c) 2015John Wiley &amp; Sons, Ltd.</abstract><date>MAY 30 2015</date><author>Xia, Yidong
   Lou, Jialin
   Luo, Hong
   Edwards, Jack
   Mueller, Frank</author></paper><paper><title>A web-based application to inform consumers about the products based on
   corn and soybeans sold in Romania</title><abstract></abstract><date>AUG 20 2015</date><author>Dorottya, Domokos Alice</author></paper><paper><title>Bounded Distortion Harmonic Mappings in the Plane</title><abstract>We present a framework for the computation of harmonic and conformal
   mappings in the plane with mathematical guarantees that the computed
   mappings are C-infinity, locally injective and satisfy strict bounds on
   the conformal and isometric distortion. Such mappings are very desirable
   in many computer graphics and geometry processing applications.We
   establish the sufficient and necessary conditions for a harmonic planar
   mapping to have bounded distortion. Our key observation is that these
   conditions relate solely to the boundary behavior of the mapping. This
   leads to an efficient and accurate algorithm that supports handle-based
   interactive shape-and-image deformation and is demonstrated to
   outperform other state-of-the-art methods.</abstract><date>AUG 2015</date><author>Chen, Renjie
   Weber, Ofir</author></paper><paper><title>A Scalable Formal Debugging Approach with Auto-Correction Capability
   Based on Static Slicing and Dynamic Ranking for RTL Datapath Designs</title><abstract>By increasing the complexity of digital systems, verification and
   debugging of such systems have become a major problem and economic
   issue. Although many computer aided design (CAD) solutions have been
   suggested to enhance efficiency of existing debugging approaches, they
   are still suffering from lack of providing a small set of potential
   error locations and also automatic correction mechanisms. On the other
   hand, the ever-growing usage of digital signal processing (DSP),
   computer graphics and embedded systems applications that can be modeled
   as polynomial computations in their datapath designs, necessitate an
   effective method to deal with their verification, debugging and
   correction. In this paper, we introduce a formal debugging approach
   based on static slicing and dynamic ranking methods to derive a reduced
   ordered set of potential error locations. In addition, to speed up
   finding true errors in the presence of multiple design errors, error
   candidates are sorted in decreasing order of their probability of being
   an error. After that, a mutation-based technique is employed to
   automatically correct bugs even in the case of multiple bugs. In order
   to evaluate the effectiveness of our approach, we have applied it to
   several industrial designs. The experimental results show that the
   proposed technique enables us to locate and correct even multiple bugs
   with high confidence in a short run time even for complex designs of up
   to several thousand lines of RTL code.</abstract><date>JUN 2015</date><author>Alizadeh, Bijan
   Behnam, Payman
   Sadeghi-Kohan, Somayeh</author></paper><paper><title>Spatio-Temporal Prediction Based Algorithm for Parallel Improvement of
   HEVC</title><abstract>The next generation high efficiency video coding (HEVC) standard
   achieves high performance by extending the encoding block to 64 x 64.
   There are some parallel tools to improve the efficiency for encoder and
   decoder. However, owing to the dependence of the current prediction
   block and surrounding block, parallel processing at CU level and Sub-CU
   level are hard to achieve. In this paper, focusing on the spatial motion
   vector prediction (SMVP) and temporal motion vector prediction (TMVP),
   parallel improvement for spatio-temporal prediction algorithms are
   presented, which can remove the dependency between prediction coding
   units and neighboring coding units. Using this proposal, it is
   convenient to process motion estimation in parallel, which is suitable
   for different parallel platforms such as multi-core platform, compute
   unified device architecture (CUDA) and so on. The simulation experiment
   results demonstrate that based on HM12.0 test model for different test
   sequences, the proposed algorithm can improve the advanced motion vector
   prediction with only 0.01% BD-rate increase that result is better than
   previous work, and the BDPSNR is almost the same as the HEVC reference
   software.</abstract><date>NOV 2015</date><author>Jiang, Xiantao
   Song, Tian
   Shimamoto, Takashi
   Shi, Wen
   Wang, Lisheng</author></paper><paper><title>Comparative Analyses of Universal Extraction Buffers for Assay of Stress
   Related Biochemical and Physiological Parameters</title><abstract>Comparative efficiency of three extraction solutions, including the
   universal sodium phosphate buffer (USPB), the Tris-HCl buffer (UTHB),
   and the specific buffers, were compared for assays of soluble protein,
   free proline, superoxide radical ([GRAPHICS]), hydrogen peroxide (H2O2),
   and the antioxidant enzymes such as superoxide dismutase (SOD), catalase
   (CAT), guaiacol peroxidase (POD), ascorbate peroxidase (APX),
   glutathione peroxidase (GPX), and glutathione reductase (GR) in Populus
   deltoide. Significant differences for protein extraction were detected
   via sodium dodecyl sulfate polyacrylamide gel electrophoresis (SDS-PAGE)
   and two-dimensional electrophoresis (2-DE). Between the two universal
   extraction buffers, the USPB showed higher efficiency for extraction of
   soluble protein, CAT, GR,[GRAPHICS], GPX, SOD, and free proline, while
   the UTHB had higher efficiency for extraction of APX, POD, and H2O2.
   When compared with the specific buffers, the USPB showed higher
   extraction efficiency for measurement of soluble protein, CAT, GR,
   and[GRAPHICS], parallel extraction efficiency for GPX, SOD, free
   proline, and H2O2, and lower extraction efficiency for APX and POD,
   whereas the UTHB had higher extraction efficiency for measurement of POD
   and H2O2. Further comparisons proved that 100mM USPB buffer showed the
   highest extraction efficiencies. These results indicated that USPB would
   be suitable and efficient for extraction of soluble protein, CAT, GR,
   GPX, SOD, H2O2,[GRAPHICS], and free proline.</abstract><date>OCT 3 2015</date><author>Han, Chunyu
   Chan, Zhulong
   Yang, Fan</author></paper><paper><title>Complete description of the skull and mandible of the giant mustelid
   Eomellivora piveteaui Ozansoy, 1965 (Mammalia, Carnivora, Mustelidae),
   from Batallones (MN10), late Miocene (Madrid, Spain)</title><abstract>We describe cranial, mandibular, and dental remains of five individuals
   of the giant mustelid Eomellivora piveteaui Ozansoy, 1965, from the late
   Miocene (MN10) site of Cerro de los Batallones (Madrid, Spain)-the first
   complete cranial remains recorded for this species and the most complete
   remains of the genus. This new sample enables a review of the systematic
   status of Eomellivora, leading us to accept as valid the species E.
   piveteaui Ozansoy, 1965, E. wimani Zdansky, 1924, E. ursogulo (Orlov,
   1948), and E. hungarica Kretzoi, 1942. Our phylogenetic hypothesis
   indicates that Eomellivora is the sister taxon of the extant Mellivora
   capensis and E. piveteaui had a common ancestor within the crown group
   E. wimani-E. ursogulo. Eomellivora piveteaui was specialized for a more
   hypercarnivorous diet than the largest extant terrestrial mustelids,
   although it also had some derived bone-crushing adaptations. Eomellivora
   piveteaui had an active predatory role in the late Miocene carnivore
   faunas, exploiting both small and relatively large prey.SUPPLEMENTAL
   DATA-Supplemental materials are available for this article for free
   at[GRAPHICS]</abstract><date>JUL 4 2015</date><author>Valenciano, Alberto
   Abella, Juan
   Sanisidro, Oscar
   Hartstone-Rose, Adam
   Angeles Alvarez-Sierra, Maria
   Morales, Jorge</author></paper><paper><title>IBiSS, a versatile and interactive tool for integrated sequence and 3D
   structure analysis of large macromolecular complexes</title><abstract>Motivation: In the past few years, an increasing number of crystal and
   cryo electron microscopy (cryo-EM) structures of large macromolecular
   complexes, such as the ribosome or the RNA polymerase, have become
   available from various species. These multi-subunit complexes can be
   difficult to analyze at the level of amino acid sequence in combination
   with the 3D structural organization of the complex. Therefore, novel
   tools for simultaneous analysis of structure and sequence information of
   complex assemblies are required to better understand the basis of
   molecular mechanisms and their functional implications.Results: Here, we
   present a web-based tool, Integrative Biology of Sequences and
   Structures (IBiSS), which is designed for interactively displaying 3D
   structures and selected sequences of subunits from large macromolecular
   complexes thus allowing simultaneous structure-sequence analysis such as
   conserved residues involved in catalysis or protein-protein interfaces.
   This tool comprises a Graphic User Interface and uses a rapid-access
   internal database, containing the relevant pre-aligned multiple
   sequences across all species available and 3D structural information.
   These annotations are automatically retrieved and updated from UniProt
   and crystallographic and cryo-EM data available in the Protein Data Bank
   (PDB) and Electron Microscopy Data Bank (EMDB).</abstract><date>OCT 15 2015</date><author>Beinsteiner, Brice
   Michalon, Jonathan
   Klaholz, Bruno P.</author></paper><paper><title>Parallelized TSM-RT Method for the Fast RCS Prediction of the 3-D
   Large-Scale Sea Surface by CUDA</title><abstract>In this paper, a parallelized two-scale model-ray tracing (TSM-RT)
   method is developed for the fast radar cross-section prediction of the
   3-D large-scale sea surface by the compute unified device architecture
   (CUDA). The TSM-RT method is an accelerated RT method, where the sea
   surface is constructed into a great deal of large triangles, which
   consist of many small triangles; this is called the two-scale model. For
   the TSM-RT method, the large triangles are utilized to determine the
   asymptotical ray paths, while the small triangles are utilized to
   calculate the scattered far fields. In addition, the CUDA of NVIDIA
   takes advantage of the graphics processing units (GPUs) for parallel
   computing, and greatly improves the speed of computation. The scattering
   field of each ray in the TSM-RT method is calculated independently.
   Therefore, a parallelization concept, which is based on the CUDA
   technology, is presented to improve the computational efficiency. The
   asynchronous transfer and shared memory are used to further improve the
   speedup factors. Our method is validated by comparing the numerical
   results with those obtained using a CPU. At the same time, significant
   speedup factors are achieved compared with the CPU-based TSM-RT.</abstract><date>OCT 2015</date><author>Meng, Xiao
   Guo, Li-xin
   Fan, Tian-qi</author></paper><paper><title>Chlorophyll content retrieval from hyperspectral remote sensing imagery.</title><abstract>Chlorophyll content is the essential parameter in the photosynthetic
   process determining leaf spectral variation in visible bands. Therefore,
   the accurate estimation of the forest canopy chlorophyll content is a
   significant foundation in assessing forest growth and stress affected by
   diseases. Hyperspectral remote sensing with high spatial resolution can
   be used for estimating chlorophyll content. In this study, the
   chlorophyll content was retrieved step by step using Hyperion imagery.
   Firstly, the spectral curve of the leaf was analyzed, 25 spectral
   characteristic parameters were identified through the correlation
   coefficient matrix, and a leaf chlorophyll content inversion model was
   established using a stepwise regression method. Secondly, the pixel
   reflectance was converted into leaf reflectance by a geometrical-optical
   model (4-scale). The three most important parameters of reflectance
   conversion, including the multiple scattering factor (M 0 ), and the
   probability of viewing the sunlit tree crown (P T ) and the background
   (P G ), were estimated by leaf area index (LAI), respectively. The
   results indicated that M 0 , P T , and P G could be described as a
   logarithmic function of LAI, with all R (2) values above 0.9. Finally,
   leaf chlorophyll content was retrieved with RMSE=7.3574mug/cm(2), and
   canopy chlorophyll content per unit ground surface area was estimated
   based on leaf chlorophyll content and LAI. Chlorophyll content mapping
   can be useful for the assessment of forest growth stage and diseases. </abstract><date>2015-Jul</date><author>Yang, Xiguang
   Yu, Ying
   Fan, Wenyi</author></paper><paper><title>Multipurpose prevention technologies for sexual and reproductive health:
   mapping global needs for introduction of new preventive products</title><abstract>Objectives: Worldwide, Women face sexual and reproductive health (SRH)
   risks including unintended pregnancy and sexually transmitted infections
   (STIs) including HIV. Multipurpose prevention technologies (MPTs)
   combine protection against two or more SRH risks into one product. Male
   and female condoms are the only currently available MPT products, but
   several other forms of MPTs are in development We examined the global
   distribution of selected SRH issues to determine where various risks
   have the greatest geographical overlap.Study design: We examined four
   indicators relevant to MPTs in development: HIV prevalence, herpes
   simplex virus type 2 prevalence (HSV-2), human papillomavirus prevalence
   (HPV) and the proportion of women with unmet need for modern
   contraception. Using ArcGIS Desktop, we mapped these indicators
   individually and in combination on choropleth and graduated symbol maps.
   We conducted a principal components analysis to reduce data and enable
   visual mapping of all four indicators on one graphic to identify
   overlap.Results: Our findings document the greatest overlapping risks in
   Sub-Saharan Africa, and we specify countries in greatest need by
   specific MPT indication.Conclusions: These results can inform strategic
   planning for MPT introduction, market segmentation and demand
   generation; data limitations also highlight the need for improved
   (non-HIV) STI surveillance globally.Implications: MPTs are products in
   development with the potential to empower women to prevent two or more
   SRH risks. Geographic analysis of overlapping SRH risks demonstrates
   particularly high need in Sub-Saharan Africa. This study can help to
   inform strategic planning for MPT introduction, market segmentation and
   demand generation. (C) 2016 The Authors. Published by Elsevier Inc.</abstract><date>JAN 2016</date><author>Schelar, Erin
   Polis, Chelsea B.
   Essam, Timothy
   Looker, Katharine J.
   Bruni, Laia
   Chrisman, Cara J.
   Manning, Judy</author></paper><paper><title>3D Printing of Protein Models in an Undergraduate Laboratory: Leucine
   Zippers</title><abstract>An upper-division undergraduate laboratory experiment is described that
   explores the structure/function relationship of protein domains, namely
   leucine zippers, through a molecular graphics computer program and
   physical models fabricated by 3D printing. By generating solvent
   accessible surfaces and color-coding hydrophobic, basic, and acidic
   amino acid residues, students are able to visualize noncovalent
   interactions that are important in protein folding and protein protein
   interactions.</abstract><date>DEC 2015</date><author>Meyer, Scott C.</author></paper><paper><title>3D gravity interface inversion constrained by a few points and its GPU
   acceleration</title><abstract>We present a fast procedure for the interface inversion of density
   contrast on the platform of GPU. Firstly, based on the iterative method
   in spatial domain, we introduce soft constraints by some few control
   points, making the inversion results closer to reality. In this problem,
   we assume that the density contrast decays with depth according to a
   parabolic law. Secondly, in order to meet the interface inversion for a
   wide range large-scale gravity data, we develop GPU program of the 3D
   interface inversion of basement relief, and give the corresponding
   optimization strategies and the speedup tests for the calculation
   results. We demonstrate the applicability and efficacy of this technique
   by inverting gravity anomalies caused by a synthetic model of a density
   interface and a set of real field data. (C) 2015 Elsevier Ltd. All
   rights reserved.</abstract><date>NOV 2015</date><author>Chen, Zhaoxi
   Meng, Xiaohong
   Zhang, Sheng</author></paper><paper><title>A model-driven blocking strategy for load balanced sparse matrix-vector
   multiplication on GPUs</title><abstract>Sparse Matrix-Vector multiplication (SpMV) is one of the key operations
   in linear algebra. Overcoming thread divergence, load imbalance and
   un-coalesced and indirect memory access due to sparsity and irregularity
   are challenges to optimizing SpMV on GPUs.In this paper we present a new
   Blocked Row-Column (BRC) storage format with a two-dimensional blocking
   mechanism that addresses these challenges effectively. It reduces thread
   divergence by reordering and blocking rows of the input matrix with
   nearly equal number of non-zero elements onto the same execution units
   (i.e., warps). BRC improves load balance by partitioning rows into
   blocks with a constant number of non-zeros such that different warps
   perform the same amount of work. We also present an approach to optimize
   BRC performance by judicious selection of block size based on sparsity
   characteristics of the matrix.A CUDA implementation of BRC outperforms
   NVIDIA CUSP and cuSPARSE libraries and other stateof-the-art SpMV
   formats on a range of unstructured sparse matrices from multiple
   application domains. The BRC format has been integrated with PETSc,
   enabling its use in PETSc's solvers. Furthermore, when partitioning the
   input matrix, BRC achieves near linear speedup on multiple GPUs. (C)
   2014 Elsevier Inc. All rights reserved.</abstract><date>FEB 2015</date><author>Ashari, Arash
   Sedaghati, Naser
   Eisenlohr, John
   Sadayappan, P.</author></paper><paper><title>Performance modeling and analysis of heterogeneous lattice Boltzmann
   simulations on CPU-GPU clusters</title><abstract>Computational fluid dynamic simulations are in general very compute
   intensive. Only by parallel simulations on modern supercomputers the
   computational demands of complex simulation tasks can be satisfied.
   Facing these computational demands GPUs offer high performance, as they
   provide the high floating point performance and memory to processor chip
   bandwidth. To successfully utilize GPU clusters for the daily business
   of a large community, usable software frameworks must be established on
   these clusters. The development of such software frameworks is only
   feasible with maintainable software designs that consider performance as
   a design objective right from the start. For this work we extend the
   software design concepts to achieve more efficient and highly scalable
   multi-GPU parallelization within our software framework waLBerla for
   multi-physics simulations centered around the lattice Boltzmann method.
   Our software designs now also support a pure-MPI and a hybrid
   parallelization approach capable of heterogeneous simulations using CPUs
   and GPUs in parallel. For the first time weak and strong scaling
   performance results obtained on the Tsubame 2.0 cluster for more than
   1000 GPUs are presented using waLBerla. With the help of a new
   communication model the parallel efficiency of our implementation is
   investigated and analyzed in a detailed and structured performance
   analysis. The suitability of the waLBerla framework for production runs
   on large GPU clusters is demonstrated. As one possible application we
   show results of strong scaling experiments for flows through a porous
   medium. (C) 2015 Published by Elsevier B.V.</abstract><date>JUL 2015</date><author>Feichtinger, Christian
   Habich, Johannes
   Koestler, Harald
   Ruede, Ulrich
   Aoki, Takayuki</author></paper><paper><title>Solving the Resource Constrained Project Scheduling Problem using the
   parallel Tabu Search designed for the CUDA platform</title><abstract>The Resource Constrained Project Scheduling Problem, which is considered
   to be difficult to tackle even for small instances, is a well-known
   scheduling problem in the operations research domain. To solve the
   problem we have proposed a parallel Tabu Search algorithm to find high
   quality solutions in a reasonable time. We show that our parallel Tabu
   Search algorithm for graphics cards (GPUs) outperforms other existing
   Tabu Search approaches in terms of quality of solutions and the number
   of evaluated schedules per second. Moreover, the algorithm for graphics
   cards is about 10.5/42.7 times faster (J90 benchmark instances) than the
   optimized parallel/sequential algorithm for the Central Processing Unit
   (CPU). The same quality of solutions is achieved up to 5.4/22 times
   faster in comparison to the parallel/sequential CPU algorithm
   respectively. The advantages of the GPU version arise from the
   sophisticated data-structures and their suitable placement in the device
   memory, tailor-made methods, and last but not least the effective
   communication scheme. (C) 2014 Elsevier Inc. All rights reserved.</abstract><date>MAR 2015</date><author>Bukata, Libor
   Sucha, Premysl
   Hanzalek, Zdenek</author></paper><paper><title>SNSMIL, a real-time single molecule identification and localization
   algorithm for super-resolution fluorescence microscopy</title><abstract>Single molecule localization based super-resolution fluorescence
   microscopy offers significantly higher spatial resolution than predicted
   by Abbe's resolution limit for far field optical microscopy. Such
   super-resolution images are reconstructed from wide-field or total
   internal reflection single molecule fluorescence recordings.
   Discrimination between emission of single fluorescent molecules and
   background noise fluctuations remains a great challenge in current data
   analysis. Here we present a real-time, and robust single molecule
   identification and localization algorithm, SNSMIL (Shot Noise based
   Single Molecule Identification and Localization). This algorithm is
   based on the intrinsic nature of noise, i.e., its Poisson or shot noise
   characteristics and a new identification criterion, Q(SNSMIL), is
   defined. SNSMIL improves the identification accuracy of single
   fluorescent molecules in experimental or simulated datasets with high
   and inhomogeneous background. The implementation of SNSMIL relies on a
   graphics processing unit (GPU), making real-time analysis feasible as
   shown for real experimental and simulated datasets.</abstract><date>JUN 22 2015</date><author>Tang, Yunqing
   Dai, Luru
   Zhang, Xiaoming
   Li, Junbai
   Hendriks, Johnny
   Fan, Xiaoming
   Gruteser, Nadine
   Meisenberg, Annika
   Baumann, Arnd
   Katranidis, Alexandros
   Gensch, Thomas</author></paper><paper><title>GPU acceleration for digitally reconstructed radiographs using bindless
   texture objects and CUDA/OpenGL interoperability.</title><abstract>This paper features an advanced implementation of the X-ray rendering
   algorithm that harnesses the giant computing power of the current
   commodity graphics processors to accelerate the generation of high
   resolution digitally reconstructed radiographs (DRRs). The presented
   pipeline exploits the latest features of NVIDIA Graphics Processing Unit
   (GPU) architectures, mainly bindless texture objects and dynamic
   parallelism. The rendering throughput is substantially improved by
   exploiting the interoperability mechanisms between CUDA and OpenGL. The
   benchmarks of our optimized rendering pipeline reflect its capability of
   generating DRRs with resolutions of 2048(2) and 4096(2) at interactive
   and semi interactive frame-rates using an NVIDIA GeForce 970 GTX device.
   </abstract><date>2015-Aug</date><author>Abdellah, Marwan
   Eldeib, Ayman
   Owis, Mohamed I</author></paper><paper><title>Multi-layer Lattice Model for Real-Time Dynamic Character Deformation</title><abstract>Due to the recent advancement of computer graphics hardware and software
   algorithms, deformable characters have become more and more popular in
   real-time applications such as computer games. While there are mature
   techniques to generate primary deformation from skeletal movement,
   simulating realistic and stable secondary deformation such as jiggling
   of fats remains challenging. On one hand, traditional volumetric
   approaches such as the finite element method require higher
   computational cost and are infeasible for limited hardware such as game
   consoles. On the other hand, while shape matching based simulations can
   produce plausible deformation in real-time, they suffer from a stiffness
   problem in which particles either show unrealistic deformation due to
   high gains, or cannot catch up with the body movement. In this paper, we
   propose a unified multi-layer lattice model to simulate the primary and
   secondary deformation of skeleton-driven characters. The core idea is to
   voxelize the input character mesh into multiple anatomical layers
   including the bone, muscle, fat and skin. Primary deformation is applied
   on the bone voxels with lattice-based skinning. The movement of these
   voxels is propagated to other voxel layers using lattice shape matching
   simulation, creating a natural secondary deformation. Our multi-layer
   lattice framework can produce simulation quality comparable to those
   from other volumetric approaches with a significantly smaller
   computational cost. It is best to be applied in real-time applications
   such as console games or interactive animation creation.</abstract><date>OCT 2015</date><author>Iwamoto, Naoya
   Shum, Hubert P. H.
   Yang, Longzhi
   Morishima, Shigeo</author></paper><paper><title>Manifold Next Event Estimation</title><abstract>We present manifold next event estimation (MNEE), a specialised
   technique for Monte Carlo light transport simulation to render
   refractive caustics by connecting surfaces to light sources (next event
   estimation) across transmissive interfaces. We employ correlated
   sampling by means of a perturbation strategy to explore all half vectors
   in the case of rough transmission while remaining outside of the context
   of Markov chain Monte Carlo, improving temporal stability. MNEE builds
   on differential geometry and manifold walks. It is very lightweight in
   its memory requirements, as it does not use light caching methods such
   as photon maps or importance sampling records. The method integrates
   seamlessly with existing Monte Carlo estimators via multiple importance
   sampling.</abstract><date>JUL 2015</date><author>Hanika, Johannes
   Droske, Marc
   Fascione, Luca</author></paper><paper><title>Optimized parallel implementation of face detection based on GPU
   component</title><abstract>Face detection is an important aspect for various domains such as:
   biometrics, video surveillance and human computer interaction. Generally
   a generic face processing system includes a face detection, or
   recognition step, as well as tracking and rendering phase. In this
   paper, we develop a real-time and robust face detection implementation
   based on GPU component. Face detection is performed by adapting the
   Viola and Jones algorithm. We have developed and designed optimized
   several parallel implementations of these algorithms based on graphics
   processors GPU using CUDA (Compute Unified Device Architecture)
   description.First, we implemented the Viola and Jones algorithm in the
   basic CPU version. The basic application is widened to GPU version using
   CUDA technology, and freeing CPU to perform other tasks. Then, the face
   detection algorithm has been optimized for the GPU using a grid topology
   and shared memory. These programs are compared and the results are
   presented. Finally, to improve the quality of face detection a second
   proposition was performed by the implementation of WaldBoost algorithm.
   (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>AUG 2015</date><author>Chouchene, Marwa
   Sayadi, Fatma Ezahra
   Bahri, Haythem
   Dubois, Julien
   Miteran, Johel
   Atri, Mohamed</author></paper><paper><title>SAT Charts New Territory</title><abstract></abstract><date>SEP 2015</date><author>Nuwer, Rachel</author></paper><paper><title>Large-scale genome-wide association studies on a GPU cluster using a
   CUDA-accelerated PGAS programming model</title><abstract>Detecting epistasis, such as 2-SNP interactions, in genome-wide
   association studies (GWAS) is an important but time consuming operation.
   Consequently, GPUs have already been used to accelerate these studies,
   reducing the runtime for moderately-sized datasets to less than 1 hour.
   However, single-GPU approaches cannot perform large-scale GWAS in
   reasonable time. In this work we present multiEpistSearch, a tool to
   detect epistasis that works on GPU clusters. While CUDA is used for
   parallelization within each GPU, the workload distribution among GPUs is
   performed with Unified Parallel C++ (UPC++), a novel extension of C++
   that follows the Partitioned Global Address Space (PGAS) model.
   multiEpistSearch is able to analyze large-scale datasets with 5 million
   SNPs from 10,000 individuals in less than 3 hours using 24 NVIDIA GTX
   Titans.</abstract><date>NOV 2015</date><author>Gonzalez-Dominguez, Jorge
   Kaessens, Jan Christian
   Wienbrandt, Lars
   Schmidt, Bertil</author></paper><paper><title>Exploiting the Potential of GPUs for Modular Multiplication in ECC</title><abstract>In traditional multiple precision large integer multiplication
   algorithm, the required number of additions approximates the number of
   multiplications needed. In some platforms, the great number of add
   instructions will occupy about half of computing latency in the overall
   implementation. In this paper, we propose a multiplication algorithm
   using separated multiply-add-with-carry instruction supported by NVIDIA
   GPUs. In the algorithm, we reorder the computational sequence, in which
   nearly all additions and carry flags handling can be combined with the
   multiplication instructions. The number of add instructions needed
   decreases from O(n(2)) in prevailing schoolbook algorithm to O(n). Our
   resulting 256-bit modular multiplication and modular square over
   Mersenne prime respectively achieve 3.3837 billion and 5.9928 billion
   operations per second and reach 96% of GPU hardware limitation. An
   elliptic curve point multiplication implementation using our algorithm
   achieves 43.6% speedup compared to the existing fastest work.</abstract><date>2015</date><author>Zheng, Fangyu
   Pan, Wuqiong
   Lin, Jingqiang
   Jing, Jiwu
   Zhao, Yuan</author></paper><paper><title>THE ROLE OF COMPUTER ANIMATION IN TEACHING TECHNICAL SUBJECTS</title><abstract>Computer animation has a positive effect on memorizing knowledge by
   students. Used in the process of teaching of technical subjects, it is
   conductive to the development of mind. Animation allows to familiarize
   the students with the schemes of solving technical problems and shows
   the mode of operation of machinery and equipment. In the technique,
   animations are used, inter alia, in the processes of designing,
   engineering calculations, visualisation and monitoring technological
   processes and visualisation of assembly processes. The article discusses
   the role of computer animation in the teaching process and the examples
   of applications using computer animation and supporting the teaching
   process of technical subjects. Selected examples of technical processes
   in both computer-aided design and manufacturing programs as well as in
   graphics and animation programs are presented.</abstract><date>DEC 2015</date><author>Dziedzic, Krzysztof
   Barszcz, Marcin
   Pasnikowska-Lukaszuk, Magdalena
   Jankowska, Agnieszka</author></paper><paper><title>Fast fingerprint identification using GPUs</title><abstract>Fingerprints are widely used in a variety of biometric identification
   systems. The fingerprint matching process is a processing step whose
   computational requirements limit the size of the fingerprint database
   that can be dealt with.Fingerprint matching algorithms based on minutiae
   are one of the most relevant families of biometric identification
   techniques. The scalability of these models is determined not only by
   the number of fingerprints but also the number of minutiae per
   fingerprint Therefore, processing millions of fingerprints per second
   requires being able to process hundreds of millions of minutiae per
   second.In this paper we present a new design of the minutiae based
   fingerprint matching algorithm presented by Jiang et al. specifically
   created for GPU based massively parallel architectures. The parallel
   design allows speed-up ratios of up to 15 with one GPU compared to
   multi-threaded CPU implementations, and up to 54 using several GPUs in
   parallel and fingerprint processing rates of between 300,000 and
   1,500,000 fingerprints per second. (c) 2015 Elsevier Inc. All rights
   reserved.</abstract><date>APR 20 2015</date><author>Lastra, Miguel
   Carabano, Jesus
   Gutierrez, Pablo D.
   Benitez, Jose M.
   Herrera, F.</author></paper><paper><title>Evaluation of the 3-D finite difference implementation of the acoustic
   diffusion equation model on massively parallel architectures</title><abstract>The diffusion equation model is a popular tool in room acoustics
   modeling. The 3-D Finite Difference (3D-FD) implementation predicts the
   energy decay function and the sound pressure level in closed
   environments. This simulation is computationally expensive, as it
   depends on the resolution used to model the room. With such high
   computational requirements, a high-level programming language (e.g.,
   Matlab) cannot deal with real life scenario simulations. Thus, it
   becomes mandatory to use our computational resources more efficiently.
   Manycore architectures, such as NVIDIA GPUs or Intel Xeon Phi offer new
   opportunities to enhance scientific computations, increasing the
   performance per watt, but shifting to a different programming model.
   This paper shows the roadmap to use massively parallel architectures in
   a 3D-FD simulation. We evaluate the latest generation of NVIDIA and
   Intel architectures. Our experimental results reveal that NVIDIA
   architectures outperform by a wide margin the Intel Xeon Phi
   co-processor while dissipating approximately 50W less (25%) for
   large-scale input problems. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>AUG 2015</date><author>Hernandez, Mario
   Imbernon, Baldomero
   Navarro, Juan M.
   Garcia, Jose M.
   Cebrian, Juan M.
   Cecilia, Jose M.</author></paper><paper><title>Mesh segmentation based on curvatures using the GPU</title><abstract>This paper proposes an efficient algorithm for decomposition of a 3D
   arbitrary triangular mesh into surface patches. Our method is based on
   the discrete curvatures for an accurate partitioning criterion and
   presents a fast clustering scheme of vertices using quick shift
   algorithm. It was implemented on the GPU (Graphics Processing Unit)
   because it is common for object geometry to exist in graphic memory so
   that more computational work is done directly on the graphic device. The
   proposed method results in fast estimation of curvatures and high
   quality of mesh segmentation. Also we applied it to NPR drawing of 3D
   meshes.</abstract><date>MAY 2015</date><author>Lee, Jung
   Kim, Seokhun
   Kim, Sun-Jeong</author></paper><paper><title>Rational cubic clipping with linear complexity for computing roots of
   polynomials</title><abstract>Many problems in computer aided geometric design and computer graphics
   can be turned into a root-finding problem of polynomial equations. Among
   various clipping methods, the ones based on the Bernstein-Bezier form
   have good numerical stability. One of such clipping methods is the
   k-clipping method, where k = 2, 3 and often called a cubic clipping
   method when k = 3. It utilizes 0(n(2)) time to find two polynomials of
   degree k bounding the given polynomial f(t) of degree n, and achieves a
   convergence rate of k + 1 for a single root. The roots of the bounding
   polynomials of degree k are then used for bounding the roots of f(t).
   This paper presents a rational cubic clipping method for finding two
   bounding cubics within 0(n) time, which can achieve a higher convergence
   rate 5 than that of 4 of the previous cubic clipping method. When the
   bounding cubics are obtained, the remaining operations are the same as
   those of previous cubic clipping method. Numerical examples show the
   efficiency and the convergence rate of the new method. (C) 2015 Elsevier
   Inc. All rights reserved.</abstract><date>JAN 15 2016</date><author>Chen, Xiao-diao
   Ma, Weiyin</author></paper><paper><title>SHEsisPCA: A GPU-Based Software to Correct for Population Stratification
   that Efficiently Accelerates the Process for Handling Genome-Wide
   Datasets</title><abstract>Population stratification is a problem in genetic association studies
   because it is likely to highlight loci that underlie the population
   structure rather than disease-related loci. At present, principal
   component analysis (PCA) has been proven to be an effective way to
   correct for population stratification. However, the conventional PCA
   algorithm is time-consuming when dealing with large datasets. We
   developed a Graphic processing unit (GPU)-based PCA software named
   SHEsisPCA (http://analysis.bio-x.cn/SHEsisMain.htm) that is highly
   parallel with a highest speedup greater than 100 compared with its CPU
   version. A cluster algorithm based on X-means was also implemented as a
   way to detect population subgroups and to obtain matched cases and
   controls in order to reduce the genomic inflation and increase the
   power. A study of both simulated and real datasets showed that SHEsisPCA
   ran at an extremely high speed while the accuracy was hardly reduced.
   Therefore, SHEsisPCA can help correct for population stratification much
   more efficiently than the conventional CPU-based algorithms.</abstract><date>AUG 20 2015</date><author>Shen, Jiawei
   Li, Zhiqiang
   Shi, Yongyong</author></paper><paper><title>Object tracking mask-based NLUT on GPUs for real-time generation of
   holographic videos of three-dimensional scenes</title><abstract>A new object tracking mask-based novel-look-up-table (OTM-NLUT) method
   is proposed and implemented on graphics-processing-units (GPUs) for
   real-time generation of holographic videos of three-dimensional (3-D)
   scenes. Since the proposed method is designed to be matched with
   software and memory structures of the GPU, the number of
   computeunified-device-architecture (CUDA) kernel function calls and the
   computergenerated hologram (CGH) buffer size of the proposed method have
   been significantly reduced. It therefore results in a great increase of
   the computational speed of the proposed method and enables real-time
   generation of CGH patterns of 3-D scenes. Experimental results show that
   the proposed method can generate 31.1 frames of Fresnel CGH patterns
   with 1,920 x 1,080 pixels per second, on average, for three test 3-D
   video scenarios with 12,666 object points on three GPU boards of NVIDIA
   GTX TITAN, and confirm the feasibility of the proposed method in the
   practical application of electro-holographic 3-D displays. (C) 2015
   Optical Society of America</abstract><date>FEB 9 2015</date><author>Kwon, M. -W.
   Kim, S. -C.
   Yoon, S. -E.
   Ho, Y. -S.
   Kim, E. -S.</author></paper><paper><title>A recommended workflow methodology in the creation of an educational and
   training application incorporating a digital reconstruction of the
   cerebral ventricular system and cerebrospinal fluid circulation to aid
   anatomical understanding</title><abstract>Background: The use of computer-aided learning in education can be
   advantageous, especially when interactive three-dimensional (3D) models
   are used to aid learning of complex 3D structures. The anatomy of the
   ventricular system of the brain is difficult to fully understand as it
   is seldom seen in 3D, as is the flow of cerebrospinal fluid (CSF). This
   article outlines a workflow for the creation of an interactive training
   tool for the cerebral ventricular system, an educationally challenging
   area of anatomy. This outline is based on the use of widely available
   computer software packages.Methods: Using MR images of the cerebral
   ventricular system and several widely available commercial and free
   software packages, the techniques of 3D modelling, texturing, sculpting,
   image editing and animations were combined to create a workflow in the
   creation of an interactive educational and training tool. This was
   focussed on cerebral ventricular system anatomy, and the flow of
   cerebrospinal fluid.Results: We have successfully created a robust
   methodology by using key software packages in the creation of an
   interactive education and training tool. This has resulted in an
   application being developed which details the anatomy of the ventricular
   system, and flow of cerebrospinal fluid using an anatomically accurate
   3D model. In addition to this, our established workflow pattern
   presented here also shows how tutorials, animations and self-assessment
   tools can also be embedded into the training application.Conclusions:
   Through our creation of an established workflow in the generation of
   educational and training material for demonstrating cerebral ventricular
   anatomy and flow of cerebrospinal fluid, it has enormous potential to be
   adopted into student training in this field. With the digital age
   advancing rapidly, this has the potential to be used as an innovative
   tool alongside other methodologies for the training of future healthcare
   practitioners and scientists. This workflow could be used in the
   creation of other tools, which could be developed for use not only on
   desktop and laptop computers but also smartphones, tablets and fully
   immersive stereoscopic environments. It also could form the basis on
   which to build surgical simulations enhanced with haptic interaction.</abstract><date>OCT 19 2015</date><author>Manson, Amy
   Poyade, Matthieu
   Rea, Paul</author></paper><paper><title>Performance evaluation of CUDA programming for 5-axis machining
   multi-scale simulation</title><abstract>5-Axis milling simulations in CAM software are mainly used to detect
   collisions between the tool and the part. They are very limited in terms
   of surface topography investigations to validate machining strategies as
   well as machining parameters such as chordal deviation, scallop height
   and tool feed. Z-buffer or N-buffer machining simulations provide more
   precise simulations but require long computation time, especially when
   using realistic cutting tools models including cutting edges geometry.
   Thus, the aim of this paper is to evaluate Nvidia CUDA architecture to
   speed-up Z-buffer or N-buffer machining simulations. Several strategies
   for parallel computing are investigated and compared to single-threaded
   and multi-threaded CPU, relatively to the complexity of the simulation.
   Simulations are conducted with two different configurations including
   Nvidia Quadro 4000 and Geforce GTX 560 graphic cards. (C) 2015 Elsevier
   B.V. All rights reserved.</abstract><date>AUG 2015</date><author>Abecassis, Felix
   Lavernhe, Sylvain
   Tournier, Christophe
   Boucard, Pierre-Alain</author></paper><paper><title>Evaluation of the Risk of Grade 3 Oral and Pharyngeal Dysphagia Using
   Atlas-Based Method and Multivariate Analyses of Individual Patient Dose
   Distributions</title><abstract>Purpose: The study aimed to apply the atlas of complication incidence
   (ACI) method to patients receiving radical treatment for head and neck
   squamous cell carcinomas (HNSCC), to generate constraints based on
   dose-volume histograms (DVHs), and to identify clinical and dosimetric
   parameters that predict the risk of grade 3 oral mucositis (g3OM) and
   pharyngeal dysphagia (g3PD).Methods and Materials: Oral and pharyngeal
   mucosal DVHs were generated for 253 patients who received radiation (RT)
   or chemoradiation (CRT). They were used to produce ACI for g3OM and
   g3PD. Multivariate analysis (MVA) of the effect of dosimetry, clinical,
   and patient-related variables was performed using logistic regression
   and bootstrapping. Receiver operating curve (ROC) analysis was also
   performed, and the Youden index was used to find volume constraints that
   discriminated between volumes that predicted for toxicity.Results: We
   derived statistically significant dose-volume constraints for g3OM over
   the range v28 to v70. Only 3 statistically significant constraints were
   derived for g3PD v67, v68, and v69. On MVA, mean dose to the oral mucosa
   predicted for g3OM and concomitant chemotherapy and mean dose to the
   inferior constrictor (IC) predicted for g3PD.Conclusions: We have used
   the ACI method to evaluate incidences of g3OM and g3PD and ROC analysis
   to generate constraints to predict g3OM and g3PD derived from entire
   individual patient DVHs. On MVA, the strongest predictors were radiation
   dose (for g3OM) and concomitant chemotherapy (for g3PD). Crown Copyright
   (C) 2015 Published by Elsevier Inc. All rights reserved.</abstract><date>NOV 1 2015</date><author>Otter, Sophie
   Schick, Ulrike
   Gulliford, Sarah
   Lal, Punita
   Franceschini, Davide
   Newbold, Katie
   Nutting, Christopher
   Harrington, Kevin
   Bhide, Shreerang</author></paper><paper><title>soilphysics: An R package for calculating soil water availability to
   plants by different soil physical indices</title><abstract>Soil available water is an important factor for plant growth. It has
   been estimated by different soil physical indices, such as the least
   limiting water range (LLWR), integral water capacity (IWC) and integral
   energy (E-I). Moreover, salinity is an important limitation for soil
   water availability to plants. Despite the advances in the quantification
   of LLWR, IWC and E-I, a comprehensive description of the computational
   methods, including data management, curve fitting procedures and
   graphing techniques, is still lacking. The salinity effect on these
   quantities has still not been implemented in a computer package. In this
   paper, we present an R package soilphysics and its implementations to
   determine LLWR, IWC and E-I. We described the theory behind each
   implementation, illustrated the functionalities and validated the
   outcomes of soilphysics with other software packages for LLWR, IWC and
   E-I calculations (an Excels (R) algorithm and SAWCal). The salinity
   effect on soil available water was also employed in the package. The
   outcomes are basically the same as other software available, with small
   differences (&lt;4%). The package soilphysics takes advantage of all the
   power of R for dealing with extensive algorithms and for building
   high-quality graphics. It is currently available from the CRAN website
   (http://cran.r-project.org/web/packages/soilphysics/index.html). (c)
   2015 Elsevier B.V. All rights reserved.</abstract><date>JAN 2016</date><author>de Lima, R. P.
   da Silva, A. R.
   da Silva, A. P.
   Leao, T. P.
   Mosaddeghi, M. R.</author></paper><paper><title>Graphical Aids to the Estimation and Discrimination of Uncertain
   Numerical Data</title><abstract>This research investigates the performance of graphical dot arrays
   designed to make discrimination of relative numerosity as effortless as
   possible at the same time as making absolute (quantitative) numerosity
   estimation as effortful as possible. Comparing regular, random, and
   hybrid (randomized regular) configurations of dots, the results indicate
   that both random and hybrid configurations reduce absolute numerosity
   estimation precision, when compared with regular dots arrays. However,
   discrimination of relative numerosity is significantly more accurate for
   hybrid dot arrays than for random dot arrays. Similarly, human subjects
   report significantly lower levels of subjective confidence in judgments
   when using hybrid dot configurations as compared with regular
   configurations; and significantly higher levels of subjective confidence
   as compared with random configurations. These results indicate that data
   graphics based on the hybrid, randomized-regular configurations of dots
   are well-suited to applications that require decisions to be based on
   numerical data in which the absolute quantities are less certain than
   the relative values. Examples of such applications include
   decision-making based on the outputs of empirically-based mathematical
   models, such as health-related policy decisions using data from
   predictive epidemiological models.</abstract><date>OCT 27 2015</date><author>Jeong, Myeong-Hun
   Duckham, Matt
   Bleisch, Susanne</author></paper><paper><title>GPU Accelerated Flow Computation by the Streamfunction-velocity (psi-nu)
   Formulation</title><abstract>In this work, we present an optimization strategy for implementing the
   BiCGStah iterative solver on graphic processing units (GPU) for
   computing incompressible viscous flows governed by the unsteady
   Navier-Stokes (N-S) equations on a CUDA platform. A recently developed
   psi-nu formulation is used to discretize the biharmonic form of the N-S
   equation and we obtain remarkable speed up of 40 times on finer grids
   for the lid-driven square cavity flow. The GPU implementation enabled us
   to compute the flow in extremely finer grids and very small scales were
   resolved with remarkable accuracy.</abstract><date>2015</date><author>Kalita, Jiten C.
   Upadhyaya, Parikshit
   Gupta, Murli M.</author></paper><paper><title>Detecting symmetries in polynomial Bezier curves</title><abstract>In a recent article, Alcazar (2014) presents algorithms for detecting
   central and mirror symmetries in planar polynomial curves, expressed
   with proper parameterization in the monomial (i.e., power) basis.
   However, for practical purposes in Computer Graphics and CAGD, the usual
   choice is the Bernstein-Bezier representation, because of its superior
   numerical and geometric characteristics. We point out that, in this form
   and for properly parameterized curves, detecting symmetry amounts to
   simply checking that the Bezier points exhibit pairwise symmetry. This
   result is a direct consequence of well-known properties of the Bezier
   representation, namely its symmetry, affine invariance, and uniqueness
   for proper parameterizations. Detecting the existence of a symmetric
   segment in a Bezier curve also amounts to a simple task, by analysing
   the last non-vanishing derivatives. Finally, these results carry over in
   a straightforward manner to symmetries in Euclidean space. (C) 2015
   Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Sanchez-Reyes, J.</author></paper><paper><title>Structural diversity of the epigenetics pocketome</title><abstract>Protein families involved in chromatin-templated events are emerging as
   novel target classes in oncology and other disease areas. The ability to
   discover selective inhibitors against chromatin factors depends on the
   presence of structural features that are unique to the targeted sites.
   To evaluate challenges and opportunities toward the development of
   selective inhibitors, we calculated all pair wise structural distances
   between 575 structures from the protein databank representing 163 unique
   binding pockets found in protein domains that write, read or erase
   post-translational modifications on histones, DNA, and RNA. We find that
   the structural similarity of binding sites does not always follow the
   sequence similarity of protein domains. Our analysis reveals increased
   risks of activity across target-class for compounds competing with the
   cofactor of protein arginine methyltransferases, lysine
   acetyltransferases, and sirtuins, while exploiting the conformational
   plasticity of a protein target is a path toward selective inhibition.
   The structural diversity landscape of the epigenetics pocketome can be
   explored via an open-access graphic user interface at . Proteins 2015;
   83:1316-1326. (c) 2015 Wiley Periodicals, Inc.</abstract><date>JUL 2015</date><author>Cabaye, Alexandre
   Nguyen, Kong T.
   Liu, Lihua
   Pande, Vineet
   Schapira, Matthieu</author></paper><paper><title>Parallelizing a Matched Pair of Ray-Tracing Projector and Backprojector
   for Iterative Cone-Beam CT Reconstruction</title><abstract>Iterative reconstruction methods in X-ray CT can provide better image
   quality than analytical methods but their applications in practice are
   still limited due to computationally expensive calculations of repeated
   projection and backprojection operations. In the past decade,
   GPU-accelerated methods have been successfully used to reduce the
   computation time for projection and backprojection. However, it has been
   of a difficult problem to overcome a trade-off between the accuracy of
   reconstructed images and the efficiency of parallel computations. For
   example, when the size of the voxel in the reconstructed volume is
   larger than that of the detector bin, the use of the conventional
   unmatched projector-backprojector pair can lower the accuracy of
   reconstructed images due to the error caused by the mismatch between the
   projector and backprojector. In this paper, we propose a new
   GPU-accelerated scheme for the most widely used ray- tracing method
   (RTM) to perform projection and backprojection operations. Unlike the
   previous works that accelerate the computation of backprojection by
   using approximations, our method does not use any approximations for
   parallelizing the projection and backprojection operations. Since our
   method is exact, the results are as accurate as those obtained from the
   nonaccelerated method. We apply our method to iterative reconstruction
   for dental cone-beam CT systems and test its performance using both the
   simulated data using a 3-D digital phantom and the real data acquired
   from an offset flat-panel X-ray CT system. Our experimental results show
   that, the proposed method achieves a substantially high acceleration
   rate while retaining the accuracy of the RTM for both projection and
   backprojection.</abstract><date>FEB 2015</date><author>Van-Giang Nguyen
   Lee, Soo-Jin</author></paper><paper><title>Fast phase processing in off-axis holography by CUDA including parallel
   phase unwrapping.</title><abstract>We present parallel processing implementation for rapid extraction of
   the quantitative phase maps from off-axis holograms on the Graphics
   Processing Unit (GPU) of the computer using computer unified device
   architecture (CUDA) programming. To obtain efficient implementation, we
   parallelized both the wrapped phase map extraction algorithm and the
   two-dimensional phase unwrapping algorithm. In contrast to previous
   implementations, we utilized unweighted least squares phase unwrapping
   algorithm that better suits parallelism. We compared the proposed
   algorithm run times on the CPU and the GPU of the computer for various
   sizes of off-axis holograms. Using the GPU implementation, we extracted
   the unwrapped phase maps from the recorded off-axis holograms at 35
   frames per second (fps) for 4 mega pixel holograms, and at 129 fps for 1
   mega pixel holograms, which presents the fastest processing framerates
   obtained so far, to the best of our knowledge. We then used common-path
   off-axis interferometric imaging to quantitatively capture the phase
   maps of a micro-organism with rapid flagellum movements. </abstract><date>2016-Feb-22</date><author>Backoach, Ohad
   Kariv, Saar
   Girshovitz, Pinhas
   Shaked, Natan T</author></paper><paper><title>Distributed evolutionary algorithms and their models: A survey of the
   state-of-the-art</title><abstract>The increasing complexity of real-world optimization problems raises new
   challenges to evolutionary computation. Responding to these challenges,
   distributed evolutionary computation has received considerable attention
   over the past decade. This article provides a comprehensive survey of
   the state-of-the-art distributed evolutionary algorithms and models,
   which have been classified into two groups according to their task
   division mechanism. Population-distributed models are presented with
   master-slave, island, cellular, hierarchical, and pool architectures,
   which parallelize an evolution task at population, individual, or
   operation levels. Dimension-distributed models include coevolution and
   multi-agent models, which focus on dimension reduction. Insights into
   the models, such as synchronization, homogeneity, communication,
   topology, speedup, advantages and disadvantages are also presented and
   discussed. The study of these models helps guide future development of
   different and/or improved algorithms. Also highlighted are recent
   hotspots in this area, including the cloud and MapReduce-based
   implementations, GPU and CUDA-based implementations, distributed
   evolutionary multiobjective optimization, and real-world applications.
   Further, a number of future research directions have been discussed,
   with a conclusion that the development of distributed evolutionary
   computation will continue to flourish. (C) 2015 Elsevier B.V. All rights
   reserved.</abstract><date>SEP 2015</date><author>Gong, Yue-Jiao
   Chen, Wei-Neng
   Zhan, Zhi-Hui
   Zhang, Jun
   Li, Yun
   Zhang, Qingfu
   Li, Jing-Jing</author></paper><paper><title>A Survey of Algorithmic Shapes</title><abstract>In the context of computer-aided design, computer graphics and geometry
   processing, the idea of generative modeling is to allow the generation
   of highly complex objects based on a set of formal construction rules.
   Using these construction rules, a shape is described by a sequence of
   processing steps, rather than just by the result of all applied
   operations: shape design becomes rule design. Due to its very general
   nature, this approach can be applied to any domain and to any shape
   representation that provides a set of generating functions. The aim of
   this survey is to give an overview of the concepts and techniques of
   procedural and generative modeling, as well as their applications with a
   special focus on archeology and architecture.</abstract><date>OCT 2015</date><author>Krispel, Ulrich
   Schinko, Christoph
   Ullrich, Torsten</author></paper><paper><title>Towards a parallel component in a GPU-CUDA environment: a case study
   with the L-BFGS Harwell routine</title><abstract>Modern graphics processing units (GPUs) have been at the leading edge of
   increasing parallelism over the last 10 years. This fact has encouraged
   the use of GPUs in a broader range of applications, where developers are
   required to lever age this technology with new programming models which
   ease the task of writing programs to run efficiently on GPUs. In this
   paper, we discuss the main guidelines to assist the developer when
   porting sequential scientific code on modern GPUs. These guidelines were
   carried out by porting the L-BFGS, the (Limited memory-) BFGS algorithm
   for large scale optimization, available as Harwell routine VA15. The
   specific interest in the L-BFGS algorithm arises from the fact that this
   is the computational module with the longest running time of a
   Oceanographic Data Assimilation application software, on which some of
   the authors are working.</abstract><date>JAN 2 2015</date><author>D'Amore, L.
   Laccetti, G.
   Romano, D.
   Scotti, G.
   Murli, A.</author></paper><paper><title>An efficient FPGA architecture for integer nth root computation</title><abstract>In embedded computing, it is common to find applications such as signal
   processing, image processing, computer graphics or data compression that
   might benefit from hardware implementation for the computation of
   integer roots of order[GRAPHICS]. However, the scientific literature
   lacks architectural designs that implement such operations for different
   values of N, using a low amount of resources. This article presents a
   parameterisable field programmable gate array (FPGA) architecture for an
   efficient Nth root calculator that uses only adders/subtractors
   and[GRAPHICS]location memory elements. The architecture was tested for
   different values of[GRAPHICS], using 64-bit number representation. The
   results show a consumption up to 10% of the logical resources of a
   Xilinx XC6SLX45-CSG324C device, depending on the value of N. The
   hardware implementation improved the performance of its corresponding
   software implementations in one order of magnitude. The architecture
   performance varies from several thousands to seven millions of root
   operations per second.</abstract><date>OCT 3 2015</date><author>Rangel-Valdez, Nelson
   Hugo Barron-Zambrano, Jose
   Torres-Huitzil, Cesar
   Torres-Jimenez, Jose</author></paper><paper><title>Development of fast patient position verification software using 2D-3D
   image registration and its clinical experience</title><abstract>To improve treatment workflow, we developed a graphic processing unit
   (GPU)-based patient positional verification software application and
   integrated it into carbon-ion scanning beam treatment. Here, we
   evaluated the basic performance of the software. The algorithm provides
   2D/3D registration matching using CT and orthogonal X-ray flat panel
   detector (FPD) images. The participants were 53 patients with tumors of
   the head and neck, prostate or lung receiving carbon-ion beam treatment.
   2D/3D-ITchi-Gime (ITG) calculation accuracy was evaluated in terms of
   computation time and registration accuracy. Registration calculation was
   determined using the similarity measurement metrics gradient difference
   (GD), normalized mutual information (NMI), zero-mean normalized
   cross-correlation (ZNCC), and their combination. Registration accuracy
   was dependent on the particular metric used. Representative examples
   were determined to have target registration error (TRE) = 0.45 +/- 0.23
   mm and angular error (AE) = 0.35 +/- 0.18 degrees with ZNCC + GD for a
   head and neck tumor; TRE = 0.12 +/- 0.07 mm and AE = 0.16 +/- 0.07
   degrees with ZNCC for a pelvic tumor; and TRE = 1.19 +/- 0.78 mm and AE
   = 0.83 +/- 0.61 degrees with ZNCC for lung tumor. Calculation time was
   less than 7.26 s.The new registration software has been successfully
   installed and implemented in our treatment process. We expect that it
   will improve both treatment workflow and treatment accuracy.</abstract><date>SEP 2015</date><author>Mori, Shinichiro
   Kumagai, Motoki
   Miki, Kentaro
   Fukuhara, Riki
   Haneishi, Hideaki</author></paper><paper><title>Q&amp;A: The dinosaur doctor.</title><abstract></abstract><date>2015-Jun-4</date><author>Horner, Jack
   Hoffman, Jascha</author></paper><paper><title>Design and Test Based on Stream Processor Programmable Cluster
   Architecture</title><abstract>As the representative of common programmable stream processor, the
   performance of GPU develops rapidly, which has broken Moore's law which
   is obeyed by CPU. GPU applies the performance of programmability and
   functional expansibility to support complicated computation and process,
   and the feature has been acknowledged in the industry. The paper deeply
   researches programmable model CUDA based on NVIDIA GPU, and analyzes
   CUDA technique. And the paper applies MPI+CUDA hybrid programming model
   and common stream processor to load the model, which not only separates
   the control of stream processor cluster from computation, and optimizes
   multi-data stream processing strategy, but also promotes the performance
   of stream processor cluster system compared with traditional x86 cluster
   system.</abstract><date>2015</date><author>Xu Wen
   Wu Dongyan
   Qi Lijun
   Wang Mingge
   Xiao Jingxin</author></paper><paper><title>MULTEM: A new multislice program to perform accurate and fast electron
   diffraction and imaging simulations using Graphics Processing Units with
   CUDA</title><abstract>The main features and the GPU implementation of the MULTEM program are
   presented and described. This new program performs accurate and fast
   multislice simulations by including higher order expansion of the
   multislice solution of the high energy Schrodinger equation, the correct
   subslicing of the three-dimensional potential and top-bottom surfaces.
   The program implements different kinds of simulation for CTEM, STEM, ED,
   PED, CBED, ADF-TEM and ABF-HC with proper treatment of the spatial and
   temporal incoherences. The multislice approach described here treats the
   specimen as amorphous material which allows a straightforward
   implementation of the frozen phonon approximation. The generalized
   transmission function for each slice is calculated when is needed and
   then discarded. This allows us to perform large simulations that can
   include millions of atoms and keep the computer memory requirements to a
   reasonable level. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>SEP 2015</date><author>Lobato, I.
   Van Dyck, D.</author></paper><paper><title>Robust polyhedral Minkowski sums with GPU implementation</title><abstract>We present a Minkowski sum algorithm for polyhedra based on convolution.
   We develop robust CPU and GPU implementations, using our ACP strategy to
   eliminate degeneracy and to enforce a user-specified backward error
   bound. We test the programs on 45 inputs with an error bound of 10(-8).
   The CPU program outperforms prior work, including non-robust programs.
   The GPU program using 2688 CUDA cores exhibits a median speedup factor
   of 36, which increases to 68 on the 6 hardest tests. For example, it
   computes a Minkowski sum with a million features in 20 seconds. (C) 2015
   Elsevier Ltd. All rights reserved.</abstract><date>OCT 2015</date><author>Kyung, Min-Ho
   Sacks, Elisha
   Milenkovic, Victor</author></paper><paper><title>Aurore: new software for neutron reflectivity data analysis</title><abstract>Aurore is a free software application based on MATLAB scripts designed
   for the graphical analysis, inspection and simulation of neutron
   reflectivity data. Its architecture, combined with graphics and other
   advantages of the MATLAB environment, should allow continued development
   of this software and inclusion of new features and analysis methods. The
   development of the software was driven by the necessity for a
   non-commercial open-source application for the analysis of neutron
   reflectivity data. Aurore provides a robust and reliable method for
   evaluation of parameter uncertainty, a feature almost absent in similar
   software applications. In the present paper the main functionalities of
   the software are presented, together with a comprehensive description of
   the modeling approaches available at the moment. The code is released
   under a Creative Commons Attribution Non-Commercial License V2.0. The
   software application can be downloaded at
   http://aurorenr.sourceforge.net/.</abstract><date>FEB 2016</date><author>Gerelli, Yuri</author></paper><paper><title>Accelerating Multiple Compound Comparison Using LINGO-Based
   Load-Balancing Strategies on Multi-GPUs</title><abstract>Compound comparison is an important task for the computational
   chemistry. By the comparison results, potential inhibitors can be found
   and then used for the pharmacy experiments. The time complexity of a
   pairwise compound comparison is O(n(2)), where n is the maximal length
   of compounds. In general, the length of compounds is tens to hundreds,
   and the computation time is small. However, more and more compounds have
   been synthesized and extracted now, even more than tens of millions.
   Therefore, it still will be time-consuming when comparing with a large
   amount of compounds (seen as a multiple compound comparison problem,
   abbreviated to MCC). The intrinsic time complexity of MCC problem is
   O(k(2)n(2)) with k compounds of maximal length n. In this paper, we
   propose a GPU-based algorithm for MCC problem, called CUDA-MCC, on
   single -and multi-GPUs. Four LINGO-based load-balancing strategies are
   considered in CUDA-MCC in order to accelerate the computation speed
   among thread blocks on GPUs. CUDA-MCC was implemented by C+OpenMP+CUDA.
   CUDA-MCC achieved 45 times and 391 times faster than its CPU version on
   a single NVIDIA Tesla K20mGPU card and a dual-NVIDIA Tesla K20m GPU
   card, respectively, under the experimental results.</abstract><date>2015</date><author>Lin, Chun-Yuan
   Wang, Chung-Hung
   Hung, Che-Lun
   Lin, Yu-Shiang</author></paper><paper><title>Spherical Fibonacci Mapping</title><abstract>Spherical Fibonacci point sets yield nearly uniform point distributions
   on the unit sphere S-2 subset of R-3. The forward generation of these
   point sets has been widely researched and is easy to implement, such
   that they have been used in various applications.Unfortunately, the lack
   of an efficient mapping from points on the unit sphere to their closest
   spherical Fibonacci point set neighbors rendered them impractical for a
   wide range of applications, especially in computer graphics. Therefore,
   we introduce an inverse mapping from points on the unit sphere which
   yields the nearest neighbor in an arbitrarily sized spherical Fibonacci
   point set in constant time, without requiring any precomputations or
   table lookups.We show how to implement this inverse mapping on GPUs
   while addressing arising floating point precision problems. Further, we
   demonstrate the use of this mapping and its variants, and show how to
   apply it to fast unit vector quantization. Finally, we illustrate the
   means by which to modify this inverse mapping for texture mapping with
   smooth filter kernels and showcase its use in the field of procedural
   modeling.</abstract><date>NOV 2015</date><author>Keinert, Benjamin
   Innmann, Matthias
   Saenger, Michael
   Stamminger, Marc</author></paper><paper><title>GRAPHICS PROCESSING UNIT COMPUTATIONS FOR FINITE ELEMENT OPTIMIZATION: A
   REVIEW AND SOME ISSUES TO BE ADDRESSED</title><abstract>In finite element optimization, the computational load is a limiting
   issue. Parallelization has been the preferred route to overcome this
   problem but was again limited by the cost of computers and the number of
   processors available. The graphics processing unit (GPU) on a PC
   provides a means of implementing the massive computations on numerous
   parallel threads cheaply on PCs. The purpose of this is to review finite
   element matrix equation solution on the GPU and point out areas where
   further investigation is warranted. Our intention is to direct
   computational research and computer architecture development so that we
   may use the GPU better for more effective computational parallelization
   in finite element field computation.</abstract><date>JUL-SEP 2015</date><author>Sivasuthan, Sivamayam
   Karthik, Victor U.
   Mathialakan, Thavappiragasam
   Rawashdeh, Mohammad R.
   Jayakumar, Paramsothy
   Thyagarajan, Ravi S.
   Hoole, S. Ratnajeevan. H.</author></paper><paper><title>Sub-second pencil beam dose calculation on GPU for adaptive proton
   therapy</title><abstract>Although proton therapy delivered using scanned pencil beams has the
   potential to produce better dose conformity than conventional
   radiotherapy, the created dose distributions are more sensitive to
   anatomical changes and patient motion. Therefore, the introduction of
   adaptive treatment techniques where the dose can be monitored as it is
   being delivered is highly desirable. We present a GPU-based dose
   calculation engine relying on the widely used pencil beam algorithm,
   developed for on-line dose calculation. The calculation engine was
   implemented from scratch, with each step of the algorithm parallelized
   and adapted to run efficiently on the GPU architecture. To ensure fast
   calculation, it employs several application-specific modifications and
   simplifications, and a fast scatter-based implementation of the
   computationally expensive kernel superposition step. The calculation
   time for a skull base treatment plan using two beam directions was 0.22s
   on an Nvidia Tesla K40 GPU, whereas a test case of a cubic target in
   water from the literature took 0.14s to calculate. The accuracy of the
   patient dose distributions was assessed by calculating the gamma-index
   with respect to a gold standard Monte Carlo simulation. The passing
   rates were 99.2% and 96.7%, respectively, for the 3%/3 mm and 2%/2 mm
   criteria, matching those produced by a clinical treatment planning
   system.</abstract><date>JUN 21 2015</date><author>da Silva, Joakim
   Ansorge, Richard
   Jena, Rajesh</author></paper><paper><title>Large-scale parallelization based on CPU and GPU cluster for
   cosmological fluid simulations</title><abstract>We present our parallel implementation for large-scale cosmological
   simulations of 3D supersonic fluids based on CPU and GPU clusters. Our
   developments are based on a CPU code named WIGEON. It is shown that,
   compared to the original sequential Fortran code, a speedup of 19-31
   (depending on the specific GPU card) can be achieved on single GPU.
   Furthermore, our results show that the pure MPI parallelization scales
   very well up to 10 thousand CPU cores. In addition, a hybrid CPU/GPU
   parallelization scheme is introduced and a detailed analysis of the
   speedup and the scaling on the different number of CPU/GPU units are
   presented (up to 256 GPU cards due to computing resource limitation).
   Our high scalability and speedup rely on the domain decomposition
   approach, optimization of the algorithm and a series of techniques to
   optimize the CUDA implementation, especially in the memory access
   pattern on CPU. We believe this hybrid MPI + CUDA code can be an
   excellent candidate for 10 Peta-scale computing and beyond. (C) 2014
   Elsevier Ltd. All rights reserved.</abstract><date>MAR 30 2015</date><author>Meng, Chen
   Wang, Long
   Cao, Zongyan
   Feng, Long-long
   Zhu, Weishan</author></paper><paper><title>Multi-GPU Accelerated Admittance Method for High-Resolution Human
   Exposure Evaluation</title><abstract>Objective: A multi-graphics processing unit (GPU) accelerated admittance
   method solver is presented for solving the induced electric field in
   high-resolution anatomical models of human body when exposed to external
   low-frequency magnetic fields. Methods: In the solver, the anatomical
   model is discretized as a three-dimensional network of admittances. The
   conjugate orthogonal conjugate gradient (COCG) iterative algorithm is
   employed to take advantage of the symmetric property of the
   complex-valued linear system of equations. Compared against the widely
   used bi-conjugate gradient stabilized method, the COCG algorithm can
   reduce the solving time by 3.5 times and reduce the storage requirement
   by about 40%. The iterative algorithm is then accelerated further by
   using multiple NVIDIA GPUs. The computations and data transfers between
   GPUs are overlapped in time by using asynchronous concurrent execution
   design. The communication overhead is well hidden so that the
   acceleration is nearly linear with the number of GPU cards. Results:
   Numerical examples show that our GPU implementation running on four
   NVIDIA Tesla K20c cards can reach 90 times faster than the CPU
   implementation running on eight CPU cores (two Intel Xeon E5-2603
   processors). Conclusion: The implemented solver is able to solve large
   dimensional problems efficiently. A whole adult body discretized in 1-mm
   resolution can be solved in just several minutes. Significance: The high
   efficiency achieved makes it practical to investigate human exposure
   involving a large number of cases with a high resolution that meets the
   requirements of international dosimetry guidelines.</abstract><date>DEC 2015</date><author>Xiong, Zubiao
   Feng, Shi
   Kautz, Richard
   Chandra, Sandeep
   Altunyurt, Nevin
   Chen, Ji</author></paper><paper><title>A Hybrid CPU/GPU Pattern-Matching Algorithm for Deep Packet Inspection</title><abstract>The large quantities of data now being transferred via high-speed
   networks have made deep packet inspection indispensable for security
   purposes. Scalable and low-cost signature-based network intrusion
   detection systems have been developed for deep packet inspection for
   various software platforms. Traditional approaches that only involve
   central processing units (CPUs) are now considered inadequate in terms
   of inspection speed. Graphic processing units (GPUs) have superior
   parallel processing power, but transmission bottlenecks can reduce
   optimal GPU efficiency. In this paper we describe our proposal for a
   hybrid CPU/GPU pattern-matching algorithm (HPMA) that divides and
   distributes the packet-inspecting workload between a CPU and GPU. All
   packets are initially inspected by the CPU and filtered using a simple
   pre-filtering algorithm, and packets that might contain malicious
   content are sent to the GPU for further inspection. Test results
   indicate that in terms of random payload traffic, the matching speed of
   our proposed algorithm was 3.4 times and 2.7 times faster than those of
   the AC-CPU and AC-GPU algorithms, respectively. Further, HPMA achieved
   higher energy efficiency than the other tested algorithms.</abstract><date>OCT 5 2015</date><author>Lee, Chun-Liang
   Lin, Yi-Shan
   Chen, Yaw-Chung</author></paper><paper><title>PSO Efficient Implementation on GPUs Using Low Latency Memory</title><abstract>This paper proposes an efficient implementation for the Particle Swarm
   Optimization (PSO) algorithm using the shared memory available in the
   Graphic Processing Units (GPU) of CUDA (Compute Unified Device
   Architecture) platforms. In our proposal each dimension of each particle
   is mapped as a thread. The threads are executed in parallel within a GPU
   block. Since the GPU blocks present a maximum number of allowed parallel
   threads, we propose to use multiple sub-swarms. Each sub-swarm is
   executed in a GPU block aiming at maximizing data alignments and
   avoiding instructions bifurcations. We also propose two communication
   mechanisms and two topologies in order to allow the sub-swarm to
   exchange information and collaborate by using the GPU global memory. The
   results for 8 sub-swarms, each one with 32 particles and 32 dimensions,
   show speedups up to 100 and 5 times when compared to the serial
   implementation and PSO start-of-art implementation for CUDA,
   respectively. Our proposal allows one to deploy PSO algorithms in
   continuous optimization problems, which present many input variables.
   This type of problem is very common in engineering.</abstract><date>MAY 2015</date><author>Silva, E. H. M.
   Bastos Filho, C. J. A.</author></paper><paper><title>NBODY6++GPU: ready for the gravitational million-body problem</title><abstract>Accurate direct N-body simulations help to obtain detailed information
   about the dynamical evolution of star clusters. They also enable
   comparisons with analytical models and Fokker-Planck or Monte Carlo
   methods. NBODY6 is a well-known direct N-body code for star clusters,
   and NBODY6++ is the extended version designed for large particle number
   simulations by supercomputers. We present NBODY6++ GPU, an optimized
   version of NBODY6++ with hybrid parallelization methods (MPI, GPU,
   OpenMP, and AVX/SSE) to accelerate large direct N-body simulations, and
   in particular to solve the million-body problem. We discuss the new
   features of the NBODY6++ GPU code, benchmarks, as well as the first
   results from a simulation of a realistic globular cluster initially
   containing a million particles. For million-body simulations, NBODY6++
   GPU is 400-2000 times faster than NBODY6 with 320 CPU cores and 32
   NVIDIA K20X GPUs. With this computing cluster specification, the
   simulations of million-body globular clusters including 5 per cent
   primordial binaries require about an hour per half-mass crossing time.</abstract><date>JUL 11 2015</date><author>Wang, Long
   Spurzem, Rainer
   Aarseth, Sverre
   Nitadori, Keigo
   Berczik, Peter
   Kouwenhoven, M. B. N.
   Naab, Thorsten</author></paper><paper><title>Synthesis, antiproliferative activity, and molecular docking studies of
   curcumin analogues bearing pyrazole ring</title><abstract>Several curcumin analogues bearing pyrazole were synthesized and
   characterized by IR, NMR, and mass spectral data. There were four tested
   compounds among 11 synthesized compounds, which were evaluated for
   antiproliferative activity and showed significant activity in both
   one-dose and five-dose assays. The antiproliferative effects were tested
   on a panel of 60 cell lines, according to the National Cancer Institute
   screening protocol. The most active compounds among the series were
   3,5-bis(4-hydroxy-3-methylstyryl)-1H-pyrazole-1-carboxamide (3k) which
   showed mean percent growth inhibition of 116.09 in one-dose assay at 10
   A mu M, and GI(50) values were ranging between 0.0912 and 2.36 A mu M in
   five-dose assay. The best results were recorded on the leukaemia cell
   lines with value ranging from 0.0912 to 0.365 A mu M. All the tested
   compounds showed broad-spectrum antiproliferative activity over
   different cancer cell lines. When compared with the standard drug
   paclitaxel, the compound 3k showed superior activity on nearly 42 cell
   lines. The molecular docking study was performed to explore the binding
   interaction of these curcumin analogues with the active site of EGFR
   tyrosine kinase (EGFR-TK). The hydroxyl group of both phenyl rings was
   important for the rein-geminated hydrogen bonding by either side chain
   or backbone with the active site of EGFR-TK.Four curcumin analogues were
   evaluated for their antiproliferative activity and showed promising
   results. The molecular docking studies showed that all the compounds
   (3a-k) were well accommodated in the EGFR tyrosine kinase.[GRAPHICS].</abstract><date>DEC 2015</date><author>Ahsan, Mohamed Jawed
   Choudhary, Kavita
   Jadav, Surender Singh
   Yasmin, Sabina
   Ansari, Md. Yousuf
   Sreenivasulu, Reddymasu</author></paper><paper><title>GPU Computing in Bayesian Inference of Realized Stochastic Volatility
   Model</title><abstract>The realized stochastic volatility (RSV) model that utilizes the
   realized volatility as additional information has been proposed to infer
   volatility of financial time series. We consider the Bayesian inference
   of the RSV model by the Hybrid Monte Carlo (HMC) algorithm. The HMC
   algorithm can be parallelized and thus performed on the GPU for speedup.
   The GPU code is developed with CUDA Fortran. We compare the
   computational time in performing the HMC algorithm on GPU (GTX 760) and
   CPU (Intel i7-4770 3.4GHz) and find that the GPU can be up to 17 times
   faster than the CPU. We also code the program with OpenACC and find that
   appropriate coding can achieve the similar speedup with CUDA Fortran.</abstract><date>2015</date><author>Takaishi, Tetsuya</author></paper><paper><title>High-quality tree structures modelling using local convolution surface
   approximation</title><abstract>In this paper, we propose a local convolution surface approximation
   approach for quickly modelling tree structures with pleasing visual
   effect. Using our proposed local convolution surface approximation, we
   present a tree modelling scheme to create the structure of a tree with a
   single high-quality quad-only mesh. Through combining the strengths of
   the convolution surfaces, subdivision surfaces and GPU, our tree
   modelling approach achieves high efficiency and good mesh quality. With
   our method, we first extract the line skeletons of given tree models by
   contracting the meshes with the Laplace operator. Then we approximate
   the original tree mesh with a convolution surface based on the extracted
   skeletons. Next, we tessellate the tree trunks represented by
   convolution surfaces into quad-only subdivision surfaces with good edge
   flow along the skeletal directions. We implement the most time-consuming
   subdivision and convolution approximation on the GPU with CUDA, and
   demonstrate applications of our proposed approach in branch editing and
   tree composition.</abstract><date>JAN 2015</date><author>Zhu, Xiaoqiang
   Jin, Xiaogang
   You, Lihua</author></paper><paper><title>Advanced mathematical on-line analysis in nuclear experiments. Usage of
   parallel computing CUDA routines in standard root analysis</title><abstract>Compute Unified Device Architecture (CUDA) is a parallel computing
   platform developed by Nvidia for increase speed of graphics by usage of
   parallel mode for processes calculation. The success of this solution
   has opened technology General-Purpose Graphic Processor Units (GPGPUs)
   for applications not coupled with graphics. The GPGPUs system can be
   applying as effective tool for reducing huge number of data for pulse
   shape analysis measures, by on-line recalculation or by very quick
   system of compression. The simplified structure of CUDA system and model
   of programming based on example Nvidia GForce GTX580 card are presented
   by our poster contribution in stand-alone version and as ROOT
   application.</abstract><date>2015</date><author>Grzeszczuk, A.
   Kowalski, S.</author></paper><paper><title>Parallel Numerical Simulations of Three-Dimensional Electromagnetic
   Radiation with MPI-CUDA Paradigms</title><abstract>Using parallel computation can enhance the performance of numerical
   simulation of electromagnetic radiation and get great runtime reduction.
   We simulate the electromagnetic radiation calculation based on the
   multicore CPU and GPU Parallel Architecture Clusters by using MPI-OpenMP
   and MPI-CUDA hybrid parallel algorithm. This is an effective solution
   comparing to the traditional finite-difference time-domain method which
   has a shortage in the calculation of the electromagnetic radiation on
   the problem of inadequate large data space and time. What is more, we
   use regional segmentation, subregional data communications,
   consolidation, and other methods to improve procedures nested
   parallelism and finally verify the correctness of the calculation
   results. Studying these two hybrid models of parallel algorithms run on
   the high-performance cluster computer, we draw the conclusion that both
   models are suitable for large-scale numerical calculations, and MPI-CUDA
   hybrid model can achieve higher speedup.</abstract><date>2015</date><author>He, Bing
   Tang, Long
   Xie, Jiang
   Wang, XiaoWei
   Song, AnPing</author></paper><paper><title>Computer Modeling and Simulation of Fruit Sunscald</title><abstract>Although the simulation of many kinds of natural phenomena has been
   studied in the field of computer simulation and graphics, the
   reproduction of natural fruit diseases processes has not received much
   attention. Sunscald is the representative of physiological diseases.
   This paper presented a novel computer modeling and simulation method for
   fruit sunscald. We mainly focus on the morphology change of fruit
   appearance affected by sunscald disease under the condition of water
   loss. An improved mass spring model is proposed, by combining cell
   turgor pressure with mass spring. We adopt this physical deformation
   model for dynamic simulation of fruit sunscald disease. We calculate the
   cell turgor pressure variation due to water loss and thereby get the
   displacement changing of every mass particle in our simulation system.
   Finally, the software of Maya is adopted to render the deform model to
   render the simulation result. Experiments demonstrate the effectiveness
   of our method.</abstract><date>JUL 2015</date><author>Liu, Shiguang
   Fan, Dongfang</author></paper><paper><title>Applying and exploring a new modeling approach of functional
   connectivity regarding ecological network: A case study on the dynamic
   lines of space syntax</title><abstract>The construction of ecological network or other continuous habitat is
   essential for urban eco-system; however, to quantify the heterogeneous
   functional connectivity for eco-network is academically attractive and
   challenging. The dynamic lines of space syntax, tenable to simulate
   perception and navigation flows in network-configured human settlements,
   is introduced to inspire idea and approach to modeling connectivity in
   eco-network, while the classical graphic notions and variables are
   assumed functional to new relationship between other species and
   eco-network. After mapping continuous functional components among land
   layout into free space and then into dynamic lines that influence
   bio-flows, the paper conducts an exploration on functional connectivity
   of Singapore's green network. Conclusions involves the distribution
   heterogeneity of basic variables, Connectivity, Control, Mean Depth and
   Integration, demonstrating each meaning for functional connectivity in
   the network, with a step-wise Integration further comparing the
   connectivity patterns under different behavioral ranges. Moreover, a
   scale robustness determined via linear regression between Integration
   and Connectivity reveals network functionality as behavioral scale
   varying. The analogical modeling of space syntax raised in this paper is
   adaptive and instructive, particularly if original essential traits
   remain valid between the substituted species and space, because several
   principles and characteristics of conventional connectivity models can
   be logically inherited, while the graphic notions of dynamic line shares
   unique advantages. (C) 2014 Elsevier B.V. All rights reserved.</abstract><date>DEC 24 2015</date><author>Yang Tianxiang
   Jing Dong
   Wang Shoubing</author></paper><paper><title>Graphics processing unit-based alignment of protein interaction networks</title><abstract>Network alignment is an important bridge to understanding human
   protein-protein interactions (PPIs) and functions through model
   organisms. However, the underlying subgraph isomorphism problem
   complicates and increases the time required to align protein interaction
   networks (PINs). Parallel computing technology is an effective solution
   to the challenge of aligning large-scale networks via sequential
   computing. In this study, the typical Hungarian-Greedy Algorithm (HGA)
   is used as an example for PIN alignment. The authors propose a HGA with
   2-nearest neighbours (HGA-2N) and implement its graphics processing unit
   (GPU) acceleration. Numerical experiments demonstrate that HGA-2N can
   find alignments that are close to those found by HGA while dramatically
   reducing computing time. The GPU implementation of HGA-2N optimises the
   parallel pattern, computing mode and storage mode and it improves the
   computing time ratio between the CPU and GPU compared with HGA when
   large-scale networks are considered. By using HGA-2N in GPUs, conserved
   PPIs can be observed, and potential PPIs can be predicted. Among the
   predictions based on 25 common Gene Ontology terms, 42.8% can be found
   in the Human Protein Reference Database. Furthermore, a new method of
   reconstructing phylogenetic trees is introduced, which shows the same
   relationships among five herpes viruses that are obtained using other
   methods.</abstract><date>AUG 2015</date><author>Xie, Jiang
   Zhou, Zhonghua
   Ma, Jin
   Xiang, Chaojuan
   Nie, Qing
   Zhang, Wu</author></paper><paper><title>Affective surfing in the visualized interface of a digital library for
   children</title><abstract>The uncertainty children experience when searching for information
   influences their information seeking behavior by stimulating curiosity
   or hindering their search efforts. This study explored the interactions
   and the usability of various search interfaces, and the enjoyment or
   uncertainty experienced by children when using them. Structural Equation
   Modeling was used to determine whether children feel uncertainty or a
   sense of control when using virtual game-like interfaces to search for
   information associated with entertainment or as a means to satisfy an
   assigned learning task. We then analyzed the weight relationships among
   three latent variables (information needs, interface media, and
   affective state) using statistical (path) analysis. Our results indicate
   that children prefer using a retrieval interface with situated
   affordance to satisfy entertainment-related information needs, as
   opposed to searching for information to solve specific problems.
   Furthermore, their perceptions of text and graphic icons determined the
   degree to which they experienced a sense of uncertainty or control. When
   searching for entertainment-related information, they were better able
   to deal with uncertainty and sought greater control in their search
   interface, compared to when they were searching for information related
   to assigned tasks. According to their information needs, children may
   regard a game-like interface as a toy or a tool for learning. The
   results of this study can serve as reference for the future development
   of information search interfaces aimed at arousing the interest of
   children. The use of virtual game-like interfaces to guide the IS
   behavior of children warrants further study. (C) 2015 Elsevier Ltd. All
   rights reserved.</abstract><date>JUL 2015</date><author>Wu, Ko-Chiu</author></paper><paper><title>MIA: Mutual Information Analyzer, a graphic user interface program that
   calculates entropy, vertical and horizontal mutual information of
   molecular sequence sets</title><abstract>Background: Short and long range correlations in biological sequences
   are central in genomic studies of covariation. These correlations can be
   studied using mutual information because it measures the amount of
   information one random variable contains about the other. Here we
   present MIA (Mutual Information Analyzer) a user friendly graphic
   interface pipeline that calculates spectra of vertical entropy (VH),
   vertical mutual information (VMI) and horizontal mutual information
   (HMI), since currently there is no user friendly integrated platform
   that in a single package perform all these calculations. MIA also
   calculates Jensen-Shannon Divergence (JSD) between pair of different
   species spectra, herein called informational distances. Thus, the
   resulting distance matrices can be presented by distance histograms and
   informational dendrograms, giving support to discrimination of closely
   related species.Results: In order to test MIA we analyzed sequences from
   Drosophila Adh locus, because the taxonomy and evolutionary patterns of
   different Drosophila species are well established and the gene Adh is
   extensively studied. The search retrieved 959 sequences of 291 species.
   From the total, 450 sequences of 17 species were selected. With this
   dataset MIA performed all tasks in less than three hours: gathering,
   storing and aligning fasta files; calculating VH, VMI and HMI spectra;
   and calculating JSD between pair of different species spectra. For each
   task MIA saved tables and graphics in the local disk, easily accessible
   for future analysis.Conclusions: Our tests revealed that the
   "informational model free" spectra may represent species signatures.
   Since JSD applied to Horizontal Mutual Information spectra resulted in
   statistically significant distances between species, we could calculate
   respective hierarchical clusters, herein called Informational
   Dendrograms (ID). When compared to phylogenetic trees all Informational
   Dendrograms presented similar taxonomy and species clusterization.</abstract><date>DEC 10 2015</date><author>Lichtenstein, Flavio
   Antoneli, Fernando, Jr.
   Briones, Marcelo R. S.</author></paper><paper><title>Doppler Time-of-Flight Imaging</title><abstract>Over the last few years, depth cameras have become increasingly popular
   for a range of applications, including human-computer interaction and
   gaming, augmented reality, machine vision, and medical imaging. Many of
   the commercially-available devices use the time-of-flight principle,
   where active illumination is temporally coded and analyzed in the camera
   to estimate a per-pixel depth map of the scene. In this paper, we
   propose a fundamentally new imaging modality for all time-of-flight
   (ToF) cameras: per-pixel radial velocity measurement. The proposed
   technique exploits the Doppler effect of objects in motion, which shifts
   the temporal illumination frequency before it reaches the camera. Using
   carefully coded illumination and modulation frequencies of the ToF
   camera, object velocities directly map to measured pixel intensities. We
   show that a slight modification of our imaging system allows for color,
   depth, and velocity information to be captured simultaneously. Combining
   the optical flow computed on the RGB frames with the measured metric
   radial velocity allows us to further estimate the full 3D metric
   velocity field of the scene. The proposed technique has applications in
   many computer graphics and vision problems, for example motion tracking,
   segmentation, recognition, and motion deblurring.</abstract><date>AUG 2015</date><author>Heide, Felix
   Heidrich, Wolfgang
   Hullin, Matthias
   Wetzstein, Gordon</author></paper><paper><title>Parallel multi-level 2D-DWT on CUDA GPUs and its application in ring
   artifact removal</title><abstract>This paper presented two schemes of parallel 2D discrete wavelet
   transform (DWT) on Compute Unified Device Architecture graphics
   processing units. For the first scheme, the image and filter are
   transformed to spectral domain by using Fast Fourier Transformation
   (FFT), multiplied and then transformed back to space domain by using
   inverse FFT. For the second scheme, the image pixels are convolved
   directly with filters. Because there is no data relevance, the
   convolution for data points on different positions could be executed
   concurrently. To reduce data transfer, the boundary extension and
   down-sampling are processed during data loading stage, and transposing
   is completed implicitly during data storage. A similar skill is adopted
   when parallelizing inverse 2D DWT. To further speed up the data access,
   the filter coefficients are stored in the constant memory. We have
   parallelized the 2D DWT for dozens of wavelet types and achieved a
   speedup factor of over 380 times compared with that of its CPU version.
   We applied the parallel 2D DWT in a ring artifact removal procedure; the
   executing speed was accelerated near 200 times compared with its CPU
   version. The experimental results showed that the proposed parallel 2D
   DWT on graphics processing units can significantly improve the
   performance for a wide variety of wavelet types and is promising for
   various applications. Copyright (c) 2015 John Wiley &amp; Sons, Ltd.</abstract><date>DEC 10 2015</date><author>Zhu, Leqing
   Zhou, Yadong
   Zhang, Daxing
   Wang, Dadong
   Wang, Huiyan
   Wang, Xun</author></paper><paper><title>GSWABE: faster GPU-accelerated sequence alignment with optimal alignment
   retrieval for short DNA sequences</title><abstract>In this paper, we present GSWABE, a graphics processing unit
   (GPU)-accelerated pairwise sequence alignment algorithm for a collection
   of short DNA sequences. This algorithm supports all-to-all pairwise
   global, semi-global and local alignment, and retrieves optimal
   alignments on Compute Unified Device Architecture (CUDA)-enabled GPUs.
   All of the three alignment types are based on dynamic programming and
   share almost the same computational pattern. Thus, we have investigated
   a general tile-based approach to facilitating fast alignment by deeply
   exploring the powerful compute capability of CUDA-enabled GPUs. The
   performance of GSWABE has been evaluated on a Kepler-based Tesla K40 GPU
   using a variety of short DNA sequence datasets. The results show that
   our algorithm can yield a performance of up to 59.1 billions cell
   updates per second (GCUPS), 58.5 GCUPS and 50.3 GCUPS for global,
   semi-global and local alignment, respectively. Furthermore, on the same
   system GSWABE runs up to 156.0 times faster than the Streaming SIMD
   Extensions (SSE)-based SSW library and up to 102.4 times faster than the
   CUDA-based MSA-CUDA (the first stage) in terms of local alignment.
   Compared with the CUDA-based gpu-pairAlign, GSWABE demonstrates stable
   and consistent speedups with a maximum speedup of 11.2, 10.7, and 10.6
   for global, semi-global, and local alignment, respectively. Copyright
   (c) 2014 John Wiley &amp; Sons, Ltd.</abstract><date>MAR 25 2015</date><author>Liu, Yongchao
   Schmidt, Bertil</author></paper><paper><title>DEM simulation of reverse faulting through sands with the aid of GPU
   computing</title><abstract>This paper presents the results of analyses of a Discrete Element
   Methodology (DEM) developed to study reverse faulting through sandy
   layers, considering the inherent ability of DEM to model strain
   localization phenomena in granular soils; the paper emphasizes on
   engineering significances of the results. An efficient DEM code is
   implemented on Graphic Processor Unit (GPU) using CUDA architecture in
   order to reduce significantly the computation effort of present DEM
   simulations. The results of sensitivity analyses show that the average
   inclinations of the reverse fault ruptures increase by increasing both
   fault dip angle and soil ductility, due to the variations of the maximum
   principle strain directions and dilation angles along the fault
   ruptures. This causes that the corresponding fault ruptures outcrop the
   ground surface at nearer distances from the projection of the fault
   trace in the bedrock on the ground surface. In addition, the results
   indicate that the fault ruptures in denser sand create higher gradients
   on the ground surface at outcropping locations compared with looser
   sand, causing more serious damage to adjacent structures. Moreover, the
   minimum bedrock displacement required for the rupture to reach the
   ground surface is evaluated adopting an internal energy criterion, being
   proportional to the soil ductility. In addition, the results show that
   the limits of distortion zones on the ground surface shift generally
   toward the hanging wall by increasing the fault dip angle. (C) 2015
   Elsevier Ltd. All rights reserved.</abstract><date>MAY 2015</date><author>Hazeghian, Mohammad
   Soroush, Abbas</author></paper><paper><title>Visualization of multi-property landscapes for compound selection and
   optimization.</title><abstract>Compound optimization generally requires considering multiple properties
   in concert and reaching a balance between them. Computationally, this
   process can be supported by multi-objective optimization methods that
   produce numerical solutions to an optimization task. Since a variety of
   comparable multi-property solutions are usually obtained further
   prioritization is required. However, the underlying multi-dimensional
   property spaces are typically complex and difficult to rationalize.
   Herein, an approach is introduced to visualize multi-property landscapes
   by adapting the concepts of star and parallel coordinates from computer
   graphics. The visualization method is designed to complement
   multi-objective compound optimization. We show that visualization makes
   it possible to further distinguish between numerically equivalent
   optimization solutions and helps to select drug-like compounds from
   multi-dimensional property spaces. The methodology is intuitive,
   applicable to a wide range of chemical optimization problems, and made
   freely available to the scientific community. </abstract><date>2015-Aug</date><author>de la Vega de Leon, Antonio
   Kayastha, Shilva
   Dimova, Dilyana
   Schultz, Thomas
   Bajorath, Jurgen</author></paper><paper><title>More Efficient Virtual Shadow Maps for Many Lights</title><abstract>Recently, several algorithms have been introduced that enable real-time
   performance for many lights in applications such as games. In this
   paper, we explore the use of hardware-supported virtual cube-map shadows
   to efficiently implement high-quality shadows from hundreds of light
   sources in real time and within a bounded memory footprint. In addition,
   we explore the utility of ray tracing for shadows from many lights and
   present a hybrid algorithm combining ray tracing with cube maps to
   exploit their respective strengths. Our solution supports real-time
   performance with hundreds of lights in fully dynamic high-detail scenes.</abstract><date>JUN 2015</date><author>Olsson, Ola
   Billeter, Markus
   Sintorn, Erik
   Kampe, Viktor
   Assarsson, Ulf</author></paper><paper><title>Parallelizing Branch-and-Bound on GPUs for Optimization of Multiproduct
   Batch Plants</title><abstract>Parallel implementation of the Branch-and-Bound (B&amp;B) technique for
   optimization problems is a promising approach to accelerating their
   solution, but it remains challenging on Graphics Processing Units (GPUs)
   due to B&amp;B's irregular data structures and poor
   computation/communication ratio. The contributions of this paper are as
   follows: (1) we develop two basic implementations (iterative and
   recursive) of B&amp;B on systems with GPUs for a practical application
   scenario optimal design of multi-product batch plants; (2) we propose
   and implement several optimizations of our CUDA code using both
   algorithmic techniques of reducing branch divergence and GPU-specific
   properties of the memory hierarchy; and (3) we evaluate our
   implementations and optimizations on a modern GPU-based system and we
   report our experimental results.</abstract><date>2015</date><author>Borisenko, Andrey
   Haidl, Michael
   Gorlatch, Sergei</author></paper><paper><title>CUDA-Powered CTBE Algorithm for Zero-Latency Data Warehouse</title><abstract>The systems dedicated for Zero-Latency Data Warehouses must meet the
   growing requirements for the most up-to-date data. The currently used
   sequential algorithms are not suited to deal with the pressure on
   receiving the freshest data. The one-module architecture implemented in
   current solutions, limits the development opportunities and increases
   the risk of critical system failure. In this paper we propose a new,
   innovative, multi-modular system that is based on parallel Choose
   Transaction by Election (CTBE) algorithm. Additionally we utilize the
   CUDA architecture to boost system efficiency, using computing power of
   multi-core graphic processors. The aim of this paper is to highlight
   pros and cons of such a solution. Performed tests and results show the
   potential and capabilities of the multi-modular system, using CUDA
   architecture.</abstract><date>2015</date><author>Gorawski, Marcin
   Lis, Damian
   Gorawska, Anna</author></paper><paper><title>MASH Suite Pro: A Comprehensive Software Tool for Top-Down Proteomics</title><abstract>Top-down mass spectrometry (MS)-based proteomics is arguably a
   disruptive technology for the comprehensive analysis of all proteoforms
   arising from genetic variation, alternative splicing, and
   posttranslational modifications (PTMs). However, the complexity of
   top-down high-resolution mass spectra presents a significant challenge
   for data analysis. In contrast to the well-developed software packages
   available for data analysis in bottom-up proteomics, the data analysis
   tools in top-down proteomics remain underdeveloped. Moreover, despite
   recent efforts to develop algorithms and tools for the deconvolution of
   top-down high-resolution mass spectra and the identification of proteins
   from complex mixtures, a multifunctional software platform, which allows
   for the identification, quantitation, and characterization of
   proteoforms with visual validation, is still lacking. Herein, we have
   developed MASH Suite Pro, a comprehensive software tool for top-down
   proteomics with multifaceted functionality. MASH Suite Pro is capable of
   processing high-resolution MS and tandem MS (MS/MS) data using two
   deconvolution algorithms to optimize protein identification results. In
   addition, MASH Suite Pro allows for the characterization of PTMs and
   sequence variations, as well as the relative quantitation of multiple
   proteoforms in different experimental conditions. The program also
   provides visualization components for validation and correction of the
   computational outputs. Furthermore, MASH Suite Pro facilitates data
   reporting and presentation via direct output of the graphics. Thus, MASH
   Suite Pro significantly simplifies and speeds up the interpretation of
   high-resolution top-down proteomics data by integrating tools for
   protein identification, quantitation, characterization, and visual
   validation into a customizable and user-friendly interface. We envision
   that MASH Suite Pro will play an integral role in advancing the
   burgeoning field of topdown proteomics.</abstract><date>FEB 2016</date><author>Cai, Wenxuan
   Guner, Huseyin
   Gregorich, Zachery R.
   Chen, Albert J.
   Ayaz-Guner, Serife
   Peng, Ying
   Valeja, Santosh G.
   Liu, Xiaowen
   Ge, Ying</author></paper><paper><title>A hybrid parallel cellular automata model for urban growth simulation
   over GPU/CPU heterogeneous architectures</title><abstract>As an important spatiotemporal simulation approach and an effective tool
   for developing and examining spatial optimization strategies (e.g., land
   allocation and planning), geospatial cellular automata (CA) models often
   require multiple data layers and consist of complicated algorithms in
   order to deal with the complex dynamic processes of interest and the
   intricate relationships and interactions between the processes and their
   driving factors. Also, massive amount of data may be used in CA
   simulations as high-resolution geospatial and non-spatial data are
   widely available. Thus, geospatial CA models can be both computationally
   intensive and data intensive, demanding extensive length of computing
   time and vast memory space. Based on a hybrid parallelism that combines
   processes with discrete memory and threads with global memory, we
   developed a parallel geospatial CA model for urban growth simulation
   over the heterogeneous computer architecture composed of multiple
   central processing units (CPUs) and graphics processing units (GPUs).
   Experiments with the datasets of California showed that the overall
   computing time for a 50-year simulation dropped from 13,647seconds on a
   single CPU to 32seconds using 64 GPU/CPU nodes. We conclude that the
   hybrid parallelism of geospatial CA over the emerging heterogeneous
   computer architectures provides scalable solutions to enabling complex
   simulations and optimizations with massive amount of data that were
   previously infeasible, sometimes impossible, using individual computing
   approaches.</abstract><date>MAR 3 2016</date><author>Guan, Qingfeng
   Shi, Xuan
   Huang, Miaoqing
   Lai, Chenggang</author></paper><paper><title>Real-time multiview human pose tracking using graphics processing
   unit-accelerated particle swarm optimization</title><abstract>This paper describes how to achieve real-time tracking of 3D human
   motion using multiview images and graphics processing unit
   (GPU)-accelerated particle swarm optimization. The tracking involves
   configuring the 3D human model in the pose described by each particle
   and then rasterizing it in each 2D plane. The Compute Unified Device
   Architecture threads rasterize the columns of the triangles and perform
   the summing of the fitness values of pixels belonging to the processed
   columns. Such a parallel particle swarm optimization (PSO) exhibits the
   level of parallelism that allows us to effectively utilize the GPU
   resources. Image acquisition and image processing are multithreaded and
   run on CPU in parallel with PSO-based searching for the best pose. Owing
   to such task decomposition, the tracking of the full human body can be
   performed at rates of 12 frames per second. For a PSO consisting of 1000
   particles and executing 10 iterations, the GPU achieves an average
   speedup of 12 over the CPU. Using marker-less motion capture system
   consisting of four calibrated and synchronized cameras, the efficiency
   comparisons were conducted on four CPU cores and four GTX GPUs on two
   cards. Copyright (c) 2014 John Wiley &amp; Sons, Ltd.</abstract><date>APR 25 2015</date><author>Rymut, Boguslaw
   Kwolek, Bogdan</author></paper><paper><title>Computer simulation and image guidance for individualised dynamic spinal
   stabilization</title><abstract>Dynamic implants for the human spine are used to re-establish regular
   segmental motion. However, the results have often been unsatisfactory
   and complications such as screw loosening are common. Individualisation
   of appliances and precision implantation are needed to improve the
   outcome of this procedure. Computer simulation, virtual implant
   optimisation and image guidance were used to improve the technique.A
   human lumbar spine computer model was developed using multi-body
   simulation software. The model simulates spinal motion under load and
   degenerative changes. After virtual degeneration of a L4/5 segment,
   virtual pedicle screw-based implants were introduced. The implants'
   positions and properties were iteratively optimised. The resulting
   implant positions were used as operative plan for image guidance and
   finally implemented in a physical spine model.In the simulation, the
   introduction and optimisation of virtually designed dynamic implants
   could partly compensate for the effects of virtual lumbar segment
   degeneration. The optimised operative plan was exported to two different
   image-guidance systems for transfer to a physical spine
   model.Three-dimensional computer graphic simulation is a feasible means
   to develop operative plans for dynamic spinal stabilization. These
   operative plans can be transferred to commercially available
   image-guidance systems for use in implantation of physical implants in a
   spine model. This concept has important potential in the design of
   operative plans and implants for individualised dynamic spine
   stabilization surgery.</abstract><date>AUG 2015</date><author>Kantelhardt, S. R.
   Hausen, U.
   Kosterhon, M.
   Amr, A. N.
   Gruber, K.
   Giese, A.</author></paper><paper><title>Statistical simulation of SAR variability with geometric and tissue
   property changes by using the unscented transform</title><abstract>PurposeThe local specific absorption rate (SAR) is critical to the
   safety of radio frequency transmit coils. A statistical simulation
   approach is introduced to address the local SAR variability related to
   tissue property and geometric variations.MethodsThe local SAR is modeled
   as the output of a nonlinear transformation with factors that may affect
   its value being treated as random input variables. Instead of using the
   Monte Carlo method with a large number of sample points, the unscented
   transform is applied with a small set of deterministic sample points. A
   sensitivity analysis is further performed to determine the significance
   of each input variable. Electromagnetic simulations are carried out by
   the finite-difference time-domain method implemented on graphic
   processing unit.ResultsThe local SAR variability of a 7 Tesla square
   loop coil for spine imaging and a 16-element brain imaging array as the
   result of tissue property and geometric changes were examined
   respectively. SAR limits were determined based on their means and
   standard deviations.ConclusionThe proposed approach is efficient and
   general for the study of local SAR variability. Magn Reson Med
   73:2357-2362, 2015. (c) 2014 Wiley Periodicals, Inc.</abstract><date>JUN 2015</date><author>Shao, Yu
   Zeng, Peng
   Wang, Shumin</author></paper><paper><title>Optimizing of Real-time Multi-scale Display of DICOM Image based on CUDA</title><abstract>The multi-scale display of digital imaging and communications in
   Medicine (DICOM) is the important step of detecting the lesions and
   making the radiotherapy plan. It is achieved by the dynamic regulation
   of the multi-scale display which also is called window operation to
   observe the organ tissues with different densities. The traditional way
   utilizing the linear function fails to be consistent with the
   characteristics of the human eye and loses image details. In this paper,
   nonlinear multi-scale display method is utilized to extend the gray
   level of the region of interest. In order to solve the contradiction
   between the long computing time of the nonlinear function and the real
   time of the dynamic window operation, the parallel generation algorithm
   of the images is implemented on the CUDA platform. The experiment
   results have shown that the optimization method has reached more
   elaborate display and at least an order of magnitude faster than
   traditional algorithms.</abstract><date>2015</date><author>Wang, Yangping
   Wang, Yangping
   Jiang, Peizhao
   Shi, Yue</author></paper><paper><title>High dynamic range imaging pipeline on the GPU</title><abstract>Use of high dynamic range (HDR) images and video in image processing and
   computer graphics applications is rapidly gaining popularity. However,
   creating and displaying high resolution HDR content on CPUs is a time
   consuming task. Although some previous work focused on real-time tone
   mapping, implementation of a full HDR imaging (HDRI) pipeline on the GPU
   has not been detailed. In this article we aim to fill this gap by
   providing a detailed description of how the HDRI pipeline, from HDR
   image assembly to tone mapping, can be implemented exclusively on the
   GPU. We also explain the trade-offs that need to be made for improving
   efficiency and show timing comparisons for CPU versus GPU
   implementations of the HDRI pipeline.</abstract><date>JUN 2015</date><author>Akyuz, Ahmet Oguz</author></paper><paper><title>High accuracy digital image correlation powered by GPU-based parallel
   computing</title><abstract>A sub-pixel digital image correlation (DIC) method with a
   path-independent displacement tracking strategy has been implemented on
   NVIDIA compute unified device architecture (CUDA) for graphics
   processing unit (GPU) devices. Powered by parallel computing technology,
   this parallel DIC (paDIC) method, combining an inverse compositional
   Gauss-Newton (IC-GN) algorithm for sub-pixel registration with a fast
   Fourier transform-based cross correlation (FFT-CC) algorithm for
   integer-pixel initial guess estimation, achieves a superior computation
   efficiency over the DIC method purely running on CPU. In the experiments
   using simulated and real speckle images, the paDIC reaches a computation
   speed of 1.66 x 10(5) POI/s (points of interest per second) and 1.13 x
   10(5) POI/s respectively, 57-76 times faster than its sequential
   counterpart, without the sacrifice of accuracy and precision. To the
   best of our knowledge, it is the fastest computation speed of a
   sub-pixel DIC method reported heretofore. (C) 2015 Elsevier Ltd. All
   rights reserved.</abstract><date>JUN 2015</date><author>Zhang, Lingqi
   Wang, Tianyi
   Jiang, Zhenyu
   Kemao, Qian
   Liu, Yiping
   Liu, Zejia
   Tang, Liqun
   Dong, Shoubin</author></paper><paper><title>Reciprocal Drag-and-Drop</title><abstract>Drag-and-drop has become ubiquitous, both on desktop computers and
   touch-sensitive surfaces. It is used to move and edit the geometry of
   elements in graphics editors, to adjust parameters using controllers
   such as sliders, or to manage views (e.g., moving and resizing windows,
   panning maps). Reverting changes made via a drag-and-drop usually
   entails performing the reciprocal drag-and-drop action. This can be
   costly as users have to remember the previous position of the object and
   put it back precisely. We introduce the DND-1 model that handles all
   past locations of graphical objects. We redesign the Dwell-and-Spring
   widget to interact with this history, and explain how applications can
   implement DND-1 to enable users to perform reciprocal drag-and-drop to
   any past location for both individual objects and groups of objects. We
   report on two user studies, whose results show that users understand
   DND-1, and that Dwell-and-Spring enables them to interact with this
   model effectively.</abstract><date>DEC 2015</date><author>Appert, Caroline
   Chapuis, Olivier
   Pietriga, Emmanuel
   Lobo, Maria-Jesus</author></paper><paper><title>Beyond the Bezel: Coin-Op Arcade Video Game Cabinets as Design History</title><abstract>This article studies the surfaces and shapes of coin-op arcade video
   game machine cabinets produced within the USA between 1971 and 1979 to
   demonstrate how design played a constitutive role in defining 'the
   game'. Such a focus highlights how the cabinet designs of Nutting
   Associate's Computer Space (1971) and Atari's Pong (1972) differentiated
   the nascent medium from presiding forms of public amusement, namely
   pinball and electromechanical games, to expand the market for coin-op
   machines. In addition, restricting the article's interest to a period
   before the arcade video game 'craze' hit full swing with its major
   'stars' on the near horizon and with the design paradigms of older games
   still prevalent, provides a look into machines for which cabinets played
   a much larger role in 'filling in the gaps' when color monitors and
   multicolored graphics were not standard. My disassembly of Boot Hill
   (Midway, 1977) and Warrior (Vectorbeam, 1979) offers concrete examples
   of the cabinet's role in shaping the game when the modified televisions
   behind the bezel still radiated in black and white. The article closes
   by drawing a brief parallel between disciplinary debates that have
   shaped Design History with those that are currently forming the emergent
   area of Game History.</abstract><date>NOV 2015</date><author>Guins, Raiford</author></paper><paper><title>A Photometric Sampling Strategy for Reflectance Characterization and
   Transference</title><abstract>Rendering 3D models with real world reflectance properties is an open
   research problem with significant applications in the field of computer
   graphics and image understanding. In this paper, our interest is in the
   characterization and transference of appearance from a source object
   onto a target 3D shape. To this end, a three-step strategy is proposed.
   In the first step, reflectance is sampled by rotating a light source in
   concentric circles around the source object. Singular value
   decomposition is then used for describing, in a pixel-wise manner,
   appearance features such as color, texture, and specular regions. The
   second step introduces a Markov random field transference method based
   on surface normal correspondence between the source object and a
   synthetic sphere. The aim of this step is to generate a sphere whose
   appearance emulates that of the source material. In the third step,
   final transference of properties is performed from the surface normals
   of the generated sphere to the surface normals of the target 3D model.
   Experimental evaluation validates the suitability of the proposed
   strategy for transferring appearance from a variety of materials between
   diverse shapes. </abstract><date>2015-06</date><author>Castela?n, Mario
   Cruz-Pe?rez, Elier
   Torres-Me?ndez, Luz Abril</author></paper><paper><title>Electromagnetic metamaterial simulations using a GPU-accelerated FDTD
   method</title><abstract>Metamaterials composed of artificial subwavelength structures exhibit
   extraordinary properties that cannot be found in nature. Designing
   artificial structures having exceptional properties plays a pivotal role
   in current metamaterial research. We present a new numerical simulation
   scheme for metamaterial research. The scheme is based on a graphic
   processing unit (GPU)-accelerated finite-difference time-domain (FDTD)
   method. The FDTD computation can be significantly accelerated when GPUs
   are used instead of only central processing units (CPUs). We explain how
   the fast FDTD simulation of large-scale metamaterials can be achieved
   through communication optimization in a heterogeneous CPU/GPU-based
   computer cluster. Our method also includes various advanced FDTD
   techniques: the non-uniform grid technique, the
   total-field/scattered-field (TFSF) technique, the auxiliary field
   technique for dispersive materials, the running discrete Fourier
   transform, and the complex structure setting. We demonstrate the power
   of our new FDTD simulation scheme by simulating the negative refraction
   of light in a coaxial waveguide metamaterial.</abstract><date>DEC 2015</date><author>Seok, Myung-Su
   Lee, Min-Gon
   Yoo, SeokJae
   Park, Q-Han</author></paper><paper><title>An integrated environmental model for a surface flow constructed
   wetland: Water quality processes</title><abstract>It is a challenge to apply coupled hydrodynamic, sediment, water
   quality, submerged aquatic vegetation (SAV), and emergent aquatic
   vegetation (EAV) models to the studies of constructed wetlands (CWs). So
   far, there are few published studies on CWs that included comprehensive
   SAV, EAV and nutrient cycling processes, had detailed model-data
   comparisons for multiple years, and were applied to support CWs
   management. Stormwater Treatment Areas (STAs) in south Florida are CWs
   that have been built to reduce phosphorus (P) concentrations from
   agricultural drainage waters and Lake Okeechobee discharges. The scale
   of this CWs project is unprecedented in terms of size, cost, and
   scientific challenges. The STA management needs models/tools to provide
   detailed spatial and temporal information to optimize the P removal
   efficiency and to predict the dynamic response of STAs under a variety
   of management conditions. Based on the Lake Okeechobee Environment Model
   (LOEM) developed in the past 15 years, the LOEM-CW water quality model
   is developed for simulating water quality processes in CWs. The coupled
   interactions of SAV and EAV with P variables are included in the LOEM-CW
   to ensure that the P cycling processes are represented realistically.
   The LOEM-CW is calibrated and verified with 6 years of measured data
   (2008-2013) at 6 locations in STA-314 Cells 3A13B. Through graphic and
   statistical comparisons, it is shown that the model simulated P,
   nitrogen, and dissolved oxygen processes in the STA well. The model
   results illustrate that increasing water depth increases the retention
   time and decreases effluent TP concentration. By adjusting the existing
   outflow rates, the STA can be more efficient in removing P. It has been
   shown that the LOEM-CW can serve as a useful tool in wetland management.
   (C) 2015 Published by Elsevier B.V.</abstract><date>JAN 2016</date><author>Ji, Zhen-Gang
   Jin, Kang-Ren</author></paper><paper><title>GPU-based MapReduce for large-scale near-duplicate video retrieval</title><abstract>With the exponential growth of multimedia data, people are overwhelmed
   with massive amount of online videos, of which Near-Duplicate Videos
   (NDVs) occupy a large portion. In this paper, we present a novel
   framework for NDV retrieval, which explores the parallel power of two
   promising techniques: Graphics Processing Unit (GPU) and MapReduce. With
   the power of the proposed framework, various key algorithms in the field
   of computer vision, such as K-Means clustering, bag of features,
   inverted file index with hamming embedding and weak geometric
   consistency, are applied to NDV retrieval. Experimental results on the
   benchmark CC_WEB_VIDEO NDV dataset demonstrate that the proposed
   framework can significantly speed up processing huge amounts of video
   repositories.</abstract><date>DEC 2015</date><author>Wang, Hanli
   Zhu, Fengkuangtian
   Xiao, Bo
   Wang, Lei
   Jiang, Yu-Gang</author></paper><paper><title>Mobile ultrafast ultrasound imaging system based on smartphone and
   tablet devices</title><abstract>Mobile and cost effective ultrasound devices are used in point of care
   scenarios or the drama room. To reduce the costs of such devices we
   already presented the possibilities of consumer devices like the Apple
   iPad for full signal processing of raw data for ultrasound image
   generation. Emerging technologies like ultrafast ultrasound imaging
   result in new algorithms for example for shearwave elastography or
   vector velocity imaging but also enable the creation of a full image
   with only one excitation/reception event based on plane wave imaging.
   This way acquisition times and power consumption of ultrasound imaging
   can be reduced for low power mobile devices based on consumer
   electronics realizing the transition from FPGA or ASIC based beamforming
   into more flexible software beamforming. This is usually performed on a
   GPU utilizing massive parallel processing (like CUDA or OpenCL) but with
   the development of modern processors (A7, A8 and A8X) for its
   smartphones and tablets Apple introduced parallel GPU hardware and the
   framework "Metal" for advanced graphics and general purpose GPU
   processing for the iOS platform. We use it for medical signal
   reconstruction in the mobile plane wave beamforming and imaging on
   ultrasound channel data sets measured with our research systems "DiPhAS"
   in ultrafast imaging mode. We were able to integrate the beamforming
   reconstruction into our mobile ultrasound processing application on the
   iOS platform. The next step into realizing a mobile, fully software
   based ultrasound system was made. The beamforming can be performed at up
   to 62 Hz at reasonable image quality on iPad Air 2 hardware providing
   real time imaging including the post-processing of beamformed data into
   images (envelope detection and scan conversion).</abstract><date>2015</date><author>Hewener, Holger
   Tretbar, Steffen</author></paper><paper><title>Gctf: Real-time CTF determination and correction</title><abstract>Accurate estimation of the contrast transfer function (CTF) is critical
   for a near-atomic resolution cryo electron microscopy (cryoEM)
   reconstruction. Here, a GPU-accelerated computer program, Gctf, for
   accurate and robust, real-time CTF determination is presented. The main
   target of Gctf is to maximize the cross-correlation of a simulated CTF
   with the logarithmic amplitude spectra (LAS) of observed micrographs
   after background subtraction. Novel approaches in Gctf improve both
   speed and accuracy. In addition to GPU acceleration (e.g. 10-50x), a
   fast '1-dimensional search plus 2-dimensional refinement (1S2R)'
   procedure further speeds up Gctf. Based on the global CTF determination,
   the local defocus for each particle and for single frames of movies is
   accurately refined, which improves CTF parameters of all particles for
   subsequent image processing. Novel diagnosis method using equiphase
   averaging (EPA) and self-consistency verification procedures have also
   been implemented in the program for practical use, especially for aims
   of near-atomic reconstruction. Gctf is an independent program and the
   outputs can be easily imported into other cryoEM software such as Relion
   (Scheres, 2012) and Frealign (Grigorieff, 2007). The results from
   several representative datasets are shown and discussed in this paper.
   (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>JAN 2016</date><author>Zhang, Kai</author></paper><paper><title>Acceleration of High Angular Momentum Electron Repulsion Integrals and
   Integral Derivatives on Graphics Processing Units</title><abstract>We present an efficient implementation of ab initio self-consistent
   field (SCF) energy and gradient calculations that run on Compute Unified
   Device Architecture (CUDA) enabled graphical processing units (GPUs)
   using recurrence relations. We first discuss the machine-generated code
   that calculates the electron-repulsion integrals (ERIs) for different
   ERI types. Next we describe the porting of the SCF gradient calculation
   to GPUs, which results in an acceleration of the computation of the
   first-order derivative of the ERIs. However, only s, p, and d ERIs and s
   and p derivatives could be executed simultaneously on GPUs using the
   current version of CUDA and generation of NVidia GPUs using a previously
   described algorithm [Miao and Merz J. Chem. Theory Comput. 2013, 9,
   965-976.]. Hence, we developed an algorithm to compute f type ERIs and d
   type ERI derivatives on GPUs. Our benchmarks shows the performance GPU
   enable ERI and ERI derivative computation yielded speedups of 10-18
   times relative to traditional CPU execution. An accuracy analysis using
   double-precision calculations demonstrates that the overall accuracy is
   satisfactory for most applications.</abstract><date>APR 2015</date><author>Miao, Yipu
   Merz, Kenneth M., Jr.</author></paper><paper><title>The Relative Odds of Progressing by Structural and Functional Tests in
   Glaucoma</title><abstract></abstract><date>JUN 2015</date><author>Marvasti, Amir
   Zangwill, Linda M.
   Abe, Ricardo Y.
   Diniz-Filho, Alberto
   Gracitelli, Carolina
   Weinreb, Robert N.
   Girkin, Christopher A.
   Liebmann, Jeffrey M.
   Medeiros, Felipe A.</author></paper><paper><title>PR-STM: Priority Rule Based Software Transactions for the GPU</title><abstract>In this paper we describe an implementation of a software transactional
   memory library for the GPU written in CUDA. We describe the
   implementation of our transaction mechanism which features both
   tentative and regular locking along with a contention management policy
   based on a simple, yet effective, static priority rule called Priority
   Rule Software Transactional Memory (PR-STM). We demonstrate competitive
   performance results in comparison with existing STMs for both the GPU
   and CPU. While GPU comparisons have been studied, to the best of our
   knowledge we are the first to provide results comparing GPU based STMs
   with a CPU based STM.</abstract><date>2015</date><author>Shen, Qi
   Sharp, Craig
   Blewitt, William
   Ushaw, Gary
   Morgan, Graham</author></paper><paper><title>Escher: A Web Application for Building, Sharing, and Embedding Data-Rich
   Visualizations of Biological Pathways</title><abstract>Escher is a web application for visualizing data on biological pathways.
   Three key features make Escher a uniquely effective tool for pathway
   visualization. First, users can rapidly design new pathway maps. Escher
   provides pathway suggestions based on user data and genome-scale models,
   so users can draw pathways in a semi-automated way. Second, users can
   visualize data related to genes or proteins on the associated reactions
   and pathways, using rules that define which enzymes catalyze each
   reaction. Thus, users can identify trends in common genomic data types
   (e.g. RNA-Seq, proteomics, ChIP)-in conjunction with metabolite-and
   reaction-oriented data types (e.g. metabolomics, fluxomics). Third,
   Escher harnesses the strengths of web technologies (SVG, D3, developer
   tools) so that visualizations can be rapidly adapted, extended, shared,
   and embedded. This paper provides examples of each of these features and
   explains how the development approach used for Escher can be used to
   guide the development of future visualization tools.</abstract><date>AUG 2015</date><author>King, Zachary A.
   Draeger, Andreas
   Ebrahim, Ali
   Sonnenschein, Nikolaus
   Lewis, Nathan E.
   Palsson, Bernhard O.</author></paper><paper><title>Software for estimation of second order effective material properties of
   porous samples with geometrical and physical nonlinearity accounted for</title><abstract>A method and an algorithm for numerical estimation of effective
   mechanical properties of porous materials are presented. The effective
   properties are sought in the form of the nonlinear relation between the
   second Piola-Kirchhoff stress tensor and the Green strain tensor for
   anisotropic materials with second-order nonlinearities accounted for.
   The effective characteristics of test models are computed by means of a
   CAE Fidesys program module based on the proposed algorithm. The
   effective material properties as functions of porosity are examined. The
   finite element mesh that contained more than a million of elements was
   used while performing stress analysis of a specimen. To reduce computing
   time, assembly and solution of the global equation system was done in
   parallel using CUDA technology. The computations were carried out on
   NVIDIA Tesla C2050 graphics processors. Our results show that accounting
   for nonlinear effects is essential for correct estimation of effective
   properties of porous materials. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>AUG 2015</date><author>Vershinin, A. V.
   Levin, V. A.
   Zingerman, K. M.
   Sboychakov, A. M.
   Yakovlev, M. Ya.</author></paper><paper><title>Parallel Implementation of Polarimetric Synthetic Aperture Radar Data
   Processing for Unsupervised Classification Using the Complex Wishart
   Classifier</title><abstract>This work investigates the parallel implementation of target
   decomposition and unsupervised classification algorithms for
   polarimetric synthetic aperture radar (POLSAR) data processing. The
   algorithms are implemented using two different parallel programming
   models: 1) clusters of CPUs, using message passing interface (MPI), and
   2) commodity graphic processing units (GPUs), using the compute device
   unified architecture (CUDA). POLSAR data processing generally involves a
   large amount of computations as the full polarimetric information needs
   to be decomposed and analyzed. Our experiments reveal that GPU
   architectures provide a good framework for massive parallelization of
   POLSAR data processing. For instance, it is found that a single GPU can
   be more efficient than a cluster of 128 nodes with speedups of more than
   100x in comparison with the single processor times. The proposed
   implementation makes the best use of low-level features in the GPU
   architecture such as shared memories, while also providing coalesced
   accesses to memory in order to achieve maximum performance.</abstract><date>NOV 2015</date><author>Sanchez, Sergio
   Marpu, Prashanth R.
   Plaza, Antonio
   Paz-Gallardo, Abel</author></paper><paper><title>Real-time HD image distortion correction in heterogeneous parallel
   computing systems using efficient memory access patterns</title><abstract>High-definition video is becoming a standard in clinical endoscopy.
   State-of-the-art systems for medical endoscopy provide 1080p video
   streams at 60 Hz. For such high resolutions and frame rates, the
   real-time execution of image-processing tasks is far from trivial,
   requiring careful algorithm design and development. In this article, we
   propose a fully functional software-based solution for correcting the
   radial distortion (RD) of HD video that runs in real time in a personal
   computer (PC) equipped with a conventional graphics processing unit
   (GPU) and a video acquisition card. Our system acquires the video feed
   directly from the digital output of the endoscopic camera control unit,
   warps each frame using a heterogeneous parallel computing architecture,
   and outputs the result back to the display. Although we target the
   particular problem of correcting geometric distortion in medical
   endoscopy, the concepts and framework herein described can be extended
   to other image-processing tasks with hard real-time requirements. We
   show that a heterogeneous approach, as well as efficient memory access
   patterns in the GPU, improve the performance of this highly memory-bound
   algorithm, leading to frame rates above 250 fps.</abstract><date>JAN 2016</date><author>Melo, Rui
   Falcao, Gabriel
   Barreto, Joao P.</author></paper><paper><title>Graphic Warning Labels Elicit Affective and Thoughtful Responses from
   Smokers: Results of a Randomized Clinical Trial</title><abstract>ObjectiveObservational research suggests that placing graphic images on
   cigarette warning labels can reduce smoking rates, but field studies
   lack experimental control. Our primary objective was to determine the
   psychological processes set in motion by naturalistic exposure to
   graphic vs. text-only warnings in a randomized clinical trial involving
   exposure to modified cigarette packs over a 4-week period. Theories of
   graphic-warning impact were tested by examining affect toward smoking,
   credibility of warning information, risk perceptions, quit intentions,
   warning label memory, and smoking risk knowledge.MethodsAdults who
   smoked between 5 and 40 cigarettes daily (N = 293; mean age = 33.7), did
   not have a contra-indicated medical condition, and did not intend to
   quit were recruited from Philadelphia, PA and Columbus, OH. Smokers were
   randomly assigned to receive their own brand of cigarettes for four
   weeks in one of three warning conditions: text only, graphic images plus
   text, or graphic images with elaborated text.ResultsData from 244
   participants who completed the trial were analyzed in
   structural-equation models. The presence of graphic images (compared to
   text-only) caused more negative affect toward smoking, a process that
   indirectly influenced risk perceptions and quit intentions (e.g.,
   image-&gt;negative affect-&gt;risk perception-&gt;quit intention). Negative
   affect from graphic images also enhanced warning credibility including
   through increased scrutiny of the warnings, a process that also
   indirectly affected risk perceptions and quit intentions (e.g.,
   image-&gt;negative affect-&gt;risk scrutiny-&gt;warning credibility-&gt;risk
   perception-&gt;quit intention). Unexpectedly, elaborated text reduced
   warning credibility. Finally, graphic warnings increased
   warning-information recall and indirectly increased smoking-risk
   knowledge at the end of the trial and one month later.ConclusionsIn the
   first naturalistic clinical trial conducted, graphic warning labels are
   more effective than text-only warnings in encouraging smokers to
   consider quitting and in educating them about smoking's risks. Negative
   affective reactions to smoking, thinking about risks, and perceptions of
   credibility are mediators of their impact.</abstract><date>DEC 16 2015</date><author>Evans, Abigail T.
   Peters, Ellen
   Strasser, Andrew A.
   Emery, Lydia F.
   Sheerin, Kaitlin M.
   Romer, Daniel</author></paper><paper><title>A hybrid construction of a decision tree for multimedia contents</title><abstract>The growing availability of large amounts of multimedia contents in
   science and industry have made data mining applications such as data
   classification highly demanding. The contribution of this paper is
   two-fold. First, we propose an approach for constructing a decision tree
   based classification model for multimedia contents. Second, in order to
   speed up the performance of the proposed model, we propose a hybrid
   CPU-GPU approach for construction of decision tree on Graphic Processing
   Unit (GPU). Our approach not only accelerates the construction of
   decision tree via GPU computing, but also does so by considering the
   power and energy consumption of the GPU. Through the experiments, we
   demonstrate that the proposed hybrid CPU-GPU approach outperforms
   CPU-based sequential implementation by several times.</abstract><date>OCT 2015</date><author>Nasridinov, Aziz
   Ihm, Sun-Young
   Park, Young-Ho</author></paper><paper><title>Fast Knowledge Discovery in Time Series with GPGPU on Genetic
   Programming</title><abstract>We tackle the problem of knowledge discovery in time series data using
   genetic programming and GPGPUs. Using genetic programming, various
   precursor patterns that have certain attractive qualities are evolved to
   predict the events of interest. Unfortunately, evolving a set of diverse
   patterns typically takes huge execution time, sometimes longer than one
   month for this case. In this paper, we address this problem by proposing
   a parallel GP framework using GPGPUs, particularly in the context of big
   financial data. By maximally exploiting the structure of the nVidia
   GPGPU platform on stock market time series data, we were able see more
   than 250-fold reduction in the running time.</abstract><date>2015</date><author>Ha, Sungjoo
   Moon, Byung-Ro</author></paper><paper><title>Physically Meaningful Rendering using Tristimulus Colours</title><abstract>In photorealistic image synthesis the radiative transfer equation is
   often not solved by simulating every wavelength of light, but instead by
   computing tristimulus transport, for instance using sRGB primaries as a
   basis. This choice is convenient, because input texture data is usually
   stored in RGB colour spaces. However, there are problems with this
   approach which are often overlooked or ignored. By comparing to spectral
   reference renderings, we show how rendering in tristimulus colour spaces
   introduces colour shifts in indirect light, violation of energy
   conservation, and unexpected behaviour in participating media.
   Furthermore, we introduce a fast method to compute spectra from almost
   any given XYZ input colour. It creates spectra that match the input
   colour precisely. Additionally, like in natural reflectance spectra,
   their energy is smoothly distributed over wide wavelength bands. This
   method is both useful to upsample RGB input data when spectral transport
   is used and as an intermediate step for corrected tristimulus-based
   transport. Finally, we show how energy conservation can be enforced in
   RGB by mapping colours to valid reflectances.</abstract><date>JUL 2015</date><author>Meng, Johannes
   Simon, Florian
   Hanika, Johannes
   Dachsbacher, Carsten</author></paper><paper><title>Screening of a library of 4-aryl/heteroaryl-4H-fused pyrans for xanthine
   oxidase inhibition: synthesis, biological evaluation and docking studies</title><abstract>A series of 4-aryl/heteroaryl-4H-fused pyrans was synthesized via
   multicomponent reaction in a microwave synthesizer. All the pyrans were
   evaluated for in vitro xanthine oxidase inhibition. Structure-activity
   relationship was also established. Among the series of 108 compounds,
   Compound 5n was the most potent displaying remarkable inhibition against
   the enzyme with an IC50 value of 0.59 mu M. Enzyme kinetic study was
   carried out for the compound 5n to determine the type of inhibition. The
   study revealed that the compound 5n was a mixed-type inhibitor.
   Molecular modelling studies were also performed to figure out the
   interactions of both the enantiomers of 5n with the amino acid residues
   of the enzyme.[GRAPHICS].</abstract><date>AUG 2015</date><author>Kaur, Ramandeep
   Naaz, Fatima
   Sharma, Sahil
   Mehndiratta, Samir
   Gupta, Manish Kumar
   Bedi, Preet Mohinder Singh
   Nepali, Kunal</author></paper><paper><title>Distributed multi-scale muscle simulation in a hybrid MPI-CUDA
   computational environment</title><abstract>We present Mexie, an extensible and scalable software solution for
   distributed multi-scale muscle simulations in a hybrid MPI-CUDA
   environment. Since muscle contraction relies on the integration of
   physical and biochemical properties across multiple length and time
   scales, these models are highly processor and memory intensive. Existing
   parallelization efforts for accelerating multi-scale muscle simulations
   imply the usage of expensive large-scale computational resources, which
   produces overwhelming costs for the everyday practical application of
   such models. In order to improve the computational speed within a
   reasonable budget, we introduce the concept of distributed calculations
   of multi-scale muscle models in a mixed CPU-GPU environment. The concept
   is applied to a two-scale muscle model, in which a finite element macro
   model is coupled with the microscopic Huxley kinetics model. Finite
   element calculations of a continuum macroscopic model take place
   strictly on the CPU, while numerical solutions of the partial
   differential equations of Huxley's cross-bridge kinetics are calculated
   on both CPUs and GPUs. We present a modular architecture of the
   solution, along with an internal organization and a specific load
   balancer that is aware of memory boundaries in such a heterogeneous
   environment. Solution was verified on both benchmark and real-world
   examples, showing high utilization of involved processing units,
   ensuring high scalability. Speed-up results show a boost of two orders
   of magnitude over any previously reported distributed multi-scale muscle
   models. This major improvement in computational feasibility of
   multi-scale muscle models paves the way for new discoveries in the field
   of muscle modeling and future clinical applications.</abstract><date>JAN 2016</date><author>Ivanovic, Milos
   Stojanovic, Boban
   Kaplarevic-Malisic, Ana
   Gilbert, Richard
   Mijailovich, Srboljub</author></paper><paper><title>A direct tridiagonal solver based on Givens rotations for GPU
   architectures</title><abstract>g-Spike, a parallel algorithm for solving general nonsymmetric
   tridiagonal systems for the GPU, and its CUDA implementation are
   described. The solver is based on the Spike framework, applying Givens
   rotations and QR factorization without pivoting. It also implements a
   low-rank modification strategy to compute the Spike DS decomposition
   even when the partitioning defines singular submatrices along the
   diagonal. The method is also used to solve the reduced system resulting
   from the Spike partitioning. Numerical experiments with problems of high
   order indicate that g-Spike is competitive in runtime with existing GPU
   methods, and can provide acceptable results when other methods cannot be
   applied or fail. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Venetis, I. E.
   Kouris, A.
   Sobczyk, A.
   Gallopoulos, E.
   Sameh, A. H.</author></paper><paper><title>Key-layered normal distributions transform for point cloud registration</title><abstract>A new scan matching algorithm is proposed using the concept of key
   layers. In the conventional multi-layered normal distributions transform
   (MLNDT), the number of layers and iterations per layer are fixed and
   mismatches in point clouds occur due to the limited number of optimising
   iterations per layer. Moreover, the accuracy of registration is low and
   the number of layers is heuristically determined in MLNDT. The proposed
   key-layered normal distributions transform (KLNDT) works well with both
   enhanced success rate and accuracy. It is also possible for KLNDT to
   register in higher layers than the traditional MLNDT.</abstract><date>NOV 19 2015</date><author>Hong, Hyunki
   Lee, B. H.</author></paper><paper><title>GPU parallelization of unstructured/hybrid grid ALE multigrid unsteady
   solver for moving body problems</title><abstract>Graphics Processing Units (GPUs) are currently being used to accelerate
   Computational Fluid Dynamics (CFD) codes and many GPU based CFD solvers
   have already showed that GPUs have great capabilities in numerical
   simulations. The development and validation of an unstructured/hybrid
   grid ALE (Arbitrary Lagrangian-Eulerian) multigrid unsteady solver on
   the GPU for moving body problems are present. This GPU based unsteady
   solver consists of two modules. A BICGSTAB based mesh deformation module
   which updates the mesh nodes to new positions as the interfaces of
   bodies move, and a geometrical multigrid (GMG) RANS (Reynolds-Averaged
   Navier-Stokes) solver which manages the flow computation using the dual
   time stepping approach. The GPU implementation and optimization for this
   two modules are discussed, respectively. Both 2D and 3D validation cases
   are carried out to analyze the accuracy and efficiency of this solver.
   The GPU results obtained agree well with the experimental measurements,
   as well as the numerical solutions presented by other researchers. A
   mixed MPI-CUDA implementation makes the unsteady flow solver capable of
   running on a multi-GPU cluster that consists of a set of GPUs and CPUs.
   Both the strong and weak scalability of the code are investigated on a
   heterogeneous computing platform. (C) 2014 Elsevier Ltd. All rights
   reserved.</abstract><date>MAR 30 2015</date><author>Ma, Wenpeng
   Lu, Zhonghua
   Zhang, Jian</author></paper><paper><title>GPU Accelerated Discontinuous Galerkin Methods for Shallow Water
   Equations</title><abstract>We discuss the development, verification, and performance of a GPU
   accelerated discontinuous Galerkin method for the solutions of two
   dimensional nonlinear shallow water equations. The shallow water
   equations are hyperbolic partial differential equations and are widely
   used in the simulation of tsunami wave propagations. Our algorithms are
   tailored to take advantage of the single instruction multiple data
   (SIMD) architecture of graphic processing units. The time integration is
   accelerated by local time stepping based on amulti-rate Adams-Bashforth
   scheme. A total variational bounded limiter is adopted for nonlinear
   stability of the numerical scheme. This limiter is coupled with a mass
   and momentum conserving positivity preserving limiter for the special
   treatment of a dry or partially wet element in the triangulation.
   Accuracy, robustness and performance are demonstrated with the aid of
   test cases. Furthermore, we developed a unified multi-threading model
   OCCA. The kernels expressed in OCCA model can be cross-compiled with
   multi-threading models OpenCL, CUDA, and OpenMP. We compare the
   performance of the OCCA kernels when cross-compiled with these models.</abstract><date>JUL 2015</date><author>Gandham, Rajesh
   Medina, David
   Warburton, Timothy</author></paper><paper><title>Dynamic response of a frame-foundation-soil system: a coupled BEM-FEM
   procedure and a GPU implementation</title><abstract>Graphics processing units have experienced an increasing demand for
   general-purpose computer applications such as engineering, science,
   finance, among other areas. This study uses such devices to analyze the
   performance of a code designed to evaluate the dynamic response of
   frame-foundation-soil system. In the present article, a direct version
   of the boundary element method (BEM) is applied to synthesize the 3D
   dynamic compliance matrix of a rigid and massless foundation interacting
   with unbounded soil profiles. The foundation compliance matrix is
   coupled to a frame, modeled by the finite element method (FEM) leading
   to the dynamic response of a coupled frame-foundation-soil system. The
   direct version of the 3D boundary element method is built based on the
   stationary fundamental solution of an elastic full-space. Viscoelastic
   effects are incorporated by means of the elastic-viscoelastic
   correspondence principle. The article describes the methodology applied
   to couple rigid bodies with the BEM mesh. The soil-foundation
   interaction result is given in terms of a dynamic compliance matrix for
   a rigid and massless foundation. The frame structural analysis is based
   on FEM where bar and beam elements are coupled. This strategy allows
   performing frequency domain analysis of such structural systems. The
   performance analysis is done by comparing two codes: one was exclusively
   developed in C language and the other one in CUDA C (Compute Unified
   Device Architecture). The level of speedup was achieved by implementing
   some functions in the frame model, i.e., on the FEM based code.</abstract><date>JUL 2015</date><author>Carrion, Ronaldo
   Mesquita, Euclides
   Ansoni, Jonas Laerte</author></paper><paper><title>Comparative visualization of genotype-Phenotype relationships</title><abstract></abstract><date>AUG 2015</date><author>Yaikhom, Gagarine
   Morgan, Hugh
   Sneddon, Duncan
   Retha, Ahmad
   Atienza-Herrero, Julian
   Blake, Andrew
   Brown, James
   Di Fenza, Armida
   Fiegel, Tanja
   Horner, Neil
   Ring, Natalie
   Santos, Luis
   Westerberg, Henrik
   Brown, Steve D. M.
   Mallon, Ann-Marie</author></paper><paper><title>An investigation into inter- and intragenomic variations of graphic
   genomic signatures</title><abstract>Background: Motivated by the general need to identify and classify
   species based on molecular evidence, genome comparisons have been
   proposed that are based on measuring mostly Euclidean distances between
   Chaos Game Representation (CGR) patterns of genomic DNA
   sequences.Results: We provide, on an extensive dataset and using several
   different distances, confirmation of the hypothesis that CGR patterns
   are preserved along a genomic DNA sequence, and are different for DNA
   sequences originating from genomes of different species. This finding
   lends support to the theory that CGRs of genomic sequences can act as
   graphic genomic signatures. In particular, we compare the CGR patterns
   of over five hundred different 150,000 bp genomic sequences spanning one
   complete chromosome from each of six organisms, representing all
   kingdoms of life: H. sapiens (Animalia; chromosome 21), S. cerevisiae
   (Fungi; chromosome 4), A. thaliana (Plantae; chromosome 1), P.
   falciparum (Protista; chromosome 14), E. coli (Bacteria - full genome),
   and P. furiosus (Archaea - full genome). To maximize the diversity
   within each species, we also analyze the interrelationships within a set
   of over five hundred 150,000 bp genomic sequences sampled from the
   entire aforementioned genomes. Lastly, we provide some preliminary
   evidence of this method's ability to classify genomic DNA sequences at
   lower taxonomic levels by comparing sequences sampled from the entire
   genome of H. sapiens (class Mammalia, order Primates) and of M. musculus
   (class Mammalia, order Rodentia), for a total length of approximately
   174 million basepairs analyzed. We compute pairwise distances between
   CGRs of these genomic sequences using six different distances, and
   construct Molecular Distance Maps, which visualize all sequences as
   points in a two-dimensional or three-dimensional space, to
   simultaneously display their interrelationships.Conclusion: Our analysis
   confirms, for this dataset, that CGR patterns of DNA sequences from the
   same genome are in general quantitatively similar, while being different
   for DNA sequences from genomes of different species. Our assessment of
   the performance of the six distances analyzed uses three different
   quality measures and suggests that several distances outperform the
   Euclidean distance, which has so far been almost exclusively used for
   such studies.</abstract><date>AUG 7 2015</date><author>Karamichalis, Rallis
   Kari, Lila
   Konstantinidis, Stavros
   Kopecki, Steffen</author></paper><paper><title>SPARSE: Seed Point Auto-Generation for Random Walks Segmentation
   Enhancement in medical inhomogeneous targets delineation of
   morphological MR and CT images.</title><abstract>In medical image processing, robust segmentation of inhomogeneous
   targets is a challenging problem. Because of the complexity and
   diversity in medical images, the commonly used semiautomatic
   segmentation algorithms usually fail in the segmentation of
   inhomogeneous objects. In this study, we propose a novel algorithm
   imbedded with a seed point autogeneration for random walks segmentation
   enhancement, namely SPARSE, for better segmentation of inhomogeneous
   objects. With a few user-labeled points, SPARSE is able to generate
   extended seed points by estimating the probability of each voxel with
   respect to the labels. The random walks algorithm is then applied upon
   the extended seed points to achieve improved segmentation result. SPARSE
   is implemented under the compute unified device architecture (CUDA)
   programming environment on graphic processing unit (GPU) hardware
   platform. Quantitative evaluations are performed using clinical
   homogeneous and inhomogeneous cases. It is found that the SPARSE can
   greatly decrease the sensitiveness to initial seed points in terms of
   location and quantity, as well as the freedom of selecting parameters in
   edge weighting function. The evaluation results of SPARSE also
   demonstrate substantial improvements in accuracy and robustness to
   inhomogeneous target segmentation over the original random walks
   algorithm. </abstract><date>2015 Mar 08</date><author>Chen, Haibin
   Zhen, Xin
   Gu, Xuejun
   Yan, Hao
   Cervino, Laura
   Xiao, Yang
   Zhou, Linghong</author></paper><paper><title>Video segmentation with L-0 gradient minimization</title><abstract>Video segmentation is an important preprocessing step for many computer
   vision and graphics tasks. Its main goal is to group the voxels in the
   video volume with similar appearance and motion into spatio-temporally
   consistent supervoxels. In this paper, we formulate video segmentation
   as an L-0 gradient minimization problem, so that the spatio-temporal
   coherence can be effectively enforced through a gradient sparsity
   pursuit way. In our method, the appearance and motion descriptor space
   is first built for over-segmented image patches of each video frame.
   Then the L-0 gradient minimization is performed in the descriptor space,
   for both spatial and temporal dimensions. To solve the non-convex L-0
   norm minimization problem, we extend the fused coordinate descent
   algorithm from 2D image grids to 3D video volume. We conduct
   quantitative evaluation of our method in a public video segmentation
   benchmark LIBSVX. The experimental results demonstrate our superior
   performance to state-of-the-arts in segmentation accuracy and
   undersegmentation error, and comparable performance in boundary recall
   and explained variation. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>FEB 2016</date><author>Cheng, Xuan
   Feng, Yuanli
   Zeng, Ming
   Liu, Xinguo</author></paper><paper><title>Chinese herbal medicine network and core treatments for allergic skin
   diseases: Implications from a nationwide database</title><abstract>Ethno-pharmacological relevance: Chinese herbal medicine (CHM) is
   commonly used to treat skin diseases, but CHM prescription patterns are
   difficult to understand due to their complexity and interconnections.
   This study aimed to demonstrate CHM core treatments and network for
   treatment of allergic skin diseases by analyzing a nationwide
   prescription database.Materials and methods: All CHM prescriptions made
   for atopic dermatitis (with age limitation &lt;= 12 years) and urticaria
   for the entire year of 2011 were included. Association rule mining (ARM)
   combined with social network analysis (SNA) were used to analyze CHM
   prescriptions and explore the CHM prescription pattern and
   network.Results: A total of 27,350 and 97,188 prescriptions for atopic
   dermatitis and urticaria, respectively, were analyzed. Xiao-Feng-San
   (XFS) was the most commonly used CHM (32% of prescriptions for atopic
   dermatitis and 47.4% for urticaria) and was the core treatment for both
   diseases. Moreover, 42 and 82 important CHM-CHM combinations were
   identified to establish the CHM network, and XFS with Dictamnus
   dasycarpus Turcz was the most prevalent (6.4% for atopic dermatitis and
   9.1% for urticaria). Traditional Chinese Medicine heat syndrome was most
   prevalent cause. Extensive anti-inflammation, anti-allergy,
   anti-oxidation, and anti-bacterial effects were also found among the
   CHMs.Conclusions: Network analysis on CHM prescriptions provides graphic
   and comprehensive illustrations regarding CHM treatment for atopic
   dermatitis and urticaria. The CHM network analysis of prescriptions is
   essential to realize the CHM treatments and to select suitable
   candidates for clinical use or further studies. (C) 2015 Elsevier
   Ireland Ltd. All rights reserved.</abstract><date>JUN 20 2015</date><author>Chen, Hsing-Yu
   Lin, Yi-Hsuan
   Huang, Jen-Wu
   Chen, Yu-Chun</author></paper><paper><title>Analyses of pupils' polygonal shape drawing strategy with respect to
   handwriting performance</title><abstract>Polygonal shape drawing tasks are commonly used in psychological,
   clinical and standard handwriting tests to evaluate children's
   development. Early detection of physical/mental disorders within
   subjects therefore requires objective analysis of the drawing tasks.
   This analysis would help to identify specific rehabilitation needs and
   accurate detection of disorders. Herein, the aim is to determine the
   correlation between the performance of polygonal shape drawing and
   levels in handwriting performance. In the reported experimentation two
   groups of participants aged between 6 and 7 were studied. The first
   group was identified by educational experts as being below-average
   writers within their age group whilst the second group was age-matched
   controls of average and above. Subjects were required to draw an
   isosceles triangle within a novel computer-based framework founded on a
   pen-based graphic tablet capture device. Subsequently, a sequential
   feature vector containing performance values relating to the order in
   which they drew the triangle was extracted from tablet data and compared
   against one another when presented in constructional strategy models.
   Statistical analyses and automated classification were applied to
   sequences to infer handwriting level based on the triangle drawing
   strategy. From our experiments drawing strategies showed significant
   differences in drawing end-point position, number of strokes used, and
   the frequency of particular drawing strategies amongst average and
   below-average handwriting groups. Additionally, a support vector machine
   classifier was used to detect group membership based on the triangle
   drawing strategy. From this exemplar polygonal shape drawing study it is
   revealed that there are details in children's drawing strategy which
   considerably differs in grouping based on handwriting performance.</abstract><date>AUG 2015</date><author>Tabatabaey-Mashadi, Narges
   Sudirman, Rubita
   Guest, Richard M.
   Khalid, Puspa Inayat</author></paper><paper><title>GPU-Accelerated Parallel Coevolutionary Algorithm for Parameters
   Identification and Temperature Monitoring in Permanent Magnet
   Synchronous Machines</title><abstract>A hierarchical fast parallel co-evolutionary immune particle swarm
   optimization (PS()) algorithm, accelerated by graphics processing unit
   (CPU) technique (C-PCIPS0), is proposed for multiparameter
   identification and temperature monitoring of permanent magnet
   synchronous machines (PMSM). It is composed of two levels and is
   developed based on compute unified device architecture (CUDA). In
   G-PCIPSO, the antibodies (Abs) of higher level memory are selected from
   the lower level swarms and improved by immune clonal-selection operator.
   The search information exchanges between swarms using the memory-based
   sharing mechanism. Moreover, an immune vaccine-enhanced operator is
   proposed to lead the Pbests particles to unexplored areas. Optimized
   parallel implementations of C-PCIPSO algorithm is developed on CPU using
   CUDA, which significantly speeds up the search process. Finally, the
   proposed algorithm is applied to multiple parameters identification and
   temperature monitoring of PMSM. It can track parameter variation and
   achieve temperature monitoring online effectively. Compared with a
   CPU-based serial execution, the computational efficiency is greatly
   enhanced by CPU-accelerated parallel computing technique.</abstract><date>OCT 2015</date><author>Liu, Zhao-Hua
   Li, Xiao-Hua
   Wu, Liang-Hong
   Zhou, Shao-Wu
   Liu, Kan</author></paper><paper><title>Improved L-0 Gradient Minimization with L-1 Fidelity for Image Smoothing</title><abstract>Edge-preserving image smoothing is one of the fundamental tasks in the
   field of computer graphics and computer vision. Recently, L-0 gradient
   minimization (LGM) has been proposed for this purpose. In contrast to
   the total variation (TV) model which employs the L-1 norm of the image
   gradient, the LGM model adopts the L-0 norm and yields much better
   results for the piecewise constant image. However, as an improvement of
   the total variation (TV) model, the LGM model also suffers, even more
   seriously, from the staircasing effect and is not robust to noise. In
   order to overcome these drawbacks, in this paper, we propose an
   improvement of the LGM model by prefiltering the image gradient and
   employing the L-1 fidelity. The proposed improved LGM (ILGM) behaves
   robustly to noise and overcomes the staircasing artifact effectively.
   Experimental results show that the ILGM is promising as compared with
   the existing methods.</abstract><date>SEP 18 2015</date><author>Pang, Xueshun
   Zhang, Suqi
   Gu, Junhua
   Li, Lingling
   Liu, Boying
   Wang, Huaibin</author></paper><paper><title>GPU Acceleration for Simulating Massively Parallel Many-Core Platforms</title><abstract>Emerging massively parallel architectures such as a general-purpose
   processor plus many-core programmable accelerators are creating an
   increasing demand for novel methods to perform their architectural
   simulation. Most state-of-the-art simulation technologies are
   exceedingly slow and the need to model full system many-core
   architectures adds further to the complexity issues. This paper presents
   a fast, scalable and parallel simulator, which uses a novel methodology
   to accelerate the simulation of a many-core coprocessor using GPU
   platforms. The main idea is to use. The target architecture of the
   associated. Simulation of many target nodes is mapped to the many
   hardware-threads available on highly parallel GPU platforms. This paper
   presents a novel methodology to accelerate the simulation of many-core
   coprocessors using GPU platforms. We demonstrate the challenges,
   feasibility and benefits of our idea to use heterogeneous system (CPU
   and GPU) to simulate future architecture of many-core heterogeneous
   platforms. The target architecture selected to evaluate our methodology
   consists of an ARM general purpose CPU coupled with many-core
   coprocessor with thousands of simple in-order cores connected in a tile
   network. This work presents optimization techniques used to parallelize
   the simulation specifically for acceleration on GPUs. We partition the
   full system simulation between CPU and GPU, where the target general
   purpose CPU is simulated on the host CPU, whereas the many-core
   coprocessor is simulated on the NVIDIA Tesla 2070 GPU platform. Our
   experiments show performance of up to 50 MIPS when simulating the entire
   heterogeneous chip, and high scalability with increasing cores on
   coprocessor.</abstract><date>MAY 2015</date><author>Raghav, Shivani
   Ruggiero, Martino
   Marongiu, Andrea
   Pinto, Christian
   Atienza, David
   Benini, Luca</author></paper><paper><title>Eye gaze technology: a South African perspective.</title><abstract>PURPOSE: Based on the bioecological model by Bronfenbrenner, this paper
   will provide a broad perspective on factors that need to be taken into
   account in order to facilitate communication and participation in
   preliterate children making use of electronic Augmentative and
   Alternative Communication (AAC) systems accessed through eye
   gaze.METHOD: Two case studies of children who have been provided with
   the technology described are presented. The case studies were analysed
   using the four nested systems of the ecology as a framework to describe
   not only the environment, but also the processes and interactions
   between the persons and their context.RESULTS: Risk and opportunity
   factors are evident at all levels of the ecology.CONCLUSIONS: While a
   good fit between the person and the technology is an essential starting
   point, additional factors pertaining to the partner, the immediate
   environment as well as meso-, exo- and macrosystemic issues (such as
   societal attitudes and funding sources) have a significant influence on
   benefits derived. In resource-limited environments, the lack of support
   at more distal levels of the ecology (meso-, exo- and marosystemic
   levels) seems to be a factor that differentiates these environments from
   more resourced ones. Implications for Rehabilitation Within
   resource-limited environments lack of support from wider ecological
   systems pose a risk to the implementation of eye gaze technology.
   Attempts to improve collaboration between all role players could provide
   the opportunity for the establishment of an integrated plan for
   intervention and set the stage for information sharing and multiskilling
   between role players. Intervention should not only be aimed at
   addressing the needs of the individual client and their family, but also
   focus on building community capacity that could provide support to
   others.</abstract><date>2015-Jul</date><author>van Niekerk, Karin
   Tonsing, Kerstin</author></paper><paper><title>The effect of misclassification error on risk estimation in case-control
   studies.</title><abstract>INTRODUCTION: In epidemiological studies, misclassification error,
   especially differential misclassification, has serious
   implications.OBJECTIVE: To illustrate how differential misclassification
   error (DME) and non-differential misclassification error (NDME) occur in
   a case-control design and to describe the trends in DME and
   NDME.METHODS: Different sensitivity levels, specificity levels,
   prevalence rates and odds ratios were simulated. Interaction graphics
   were constructed to study bias in the different settings, and the effect
   of the different factors on bias was described using linear
   models.RESULTS: One hundred per cent of the biases caused by NDME were
   negative. DME biased the association positively more often than it did
   negatively (70 versus 30%), increasing or decreasing the OR estimate
   towards the null hypothesis.CONCLUSIONS: The effect of the sensitivity
   and specificity in classifying exposure, the prevalence of exposure in
   controls and true OR differed between positive and negative biases. The
   use of valid exposure classification instruments with high sensitivity
   and high specificity is recommended to mitigate this type of bias.</abstract><date>2015 Apr-Jun</date><author>Baena, Armando
   Garces-Palacio, Isabel Cristina
   Grisales, Hugo</author></paper><paper><title>Benchmark test of accelerated multi-slice simulation by GPGPU</title><abstract>A fast multi-slice image simulation by parallelized computation using a
   graphics processing unit (CPU) has been developed. The image simulation
   contains multiple sets of computing steps, such as Fourier transform and
   pixel-to-pixel operation. The efficiency of CPU varies depending on the
   type of calculation. In the effective case of utilizing CPU, the
   calculation speed is conducted hundreds of times faster than a central
   processing unit (CPU). The benchmark test of parallelized multi-slice
   was performed, and the results of contents, such as TEM imaging, STEM
   imaging and CBD calculation are reported. Some features of the
   simulation software are also introduced. (C) 2015 Elsevier B.V. All
   rights reserved,</abstract><date>NOV 2015</date><author>Hosokawa, Fumio
   Shinkawa, Takao
   Arai, Yoshihiro
   Sannomiya, Takumi</author></paper><paper><title>Optimization of Lightweight Encryption Algorithm (LEA) using Threads and
   Shared Memory of GPU</title><abstract>As big-data and cloud security technologies become popular, many
   researchers have recently been conducted on faster and lighter
   encryption. As a result, National Security Research Institute developed
   LEA which is lightweight and fast block cipher. To date, there have been
   various studies on lightweight encryption algorithm (LEA) for speeding
   up using GPU rather than conventional CPU. However, it is rather
   difficult to explore any guideline how to manipulate the GPU for the
   efficient usage of the LEA. Therefore, we introduce a guideline which
   explains how to implement and design the optimal LEA using GPU.</abstract><date>2015</date><author>???
   ???</author></paper><paper><title>Systematic Fusion of CUDA Kernels for Iterative Sparse Linear System
   Solvers</title><abstract>We introduce a systematic analysis in order to fuse CUDA kernels arising
   in efficient iterative methods for the solution of sparse linear
   systems. Our procedure characterizes the input and output vectors of
   these methods, combining this information together with a dependency
   analysis, in order to decide which kernels to merge. The experiments on
   a recent NVIDIA "Kepler" GPU report significant gains, especially in
   energy consumption, for the fused implementations derived from the
   application of the methodology to three of the most popular Krylov
   subspace solvers with/without preconditioning.</abstract><date>2015</date><author>Aliaga, Jose I.
   Perez, Joaquin
   Quintana-Orti, Enrique S.</author></paper><paper><title>Computational Intelligence-based Entertaining Level Generation for
   Platform Games</title><abstract>With computers becoming ubiquitous and high resolution graphics reaching
   the next level, computer games have become a major source of
   entertainment. It has been a tedious task for game developers to measure
   the entertainment value of the computer games. The entertainment value
   of a game does depend upon the genre of the game in addition to the game
   contents. In this paper, we propose a set of entertainment metrics for
   the platform genre of games. The set of entertainment metrics is
   proposed based upon certain theories on entertainment in computer games.
   To test the metrics, we use an evolutionary algorithm for automated
   generation of game rules which are entertaining. The proposed approach
   starts with an initial set of randomly generated games and, based upon
   the proposed metrics as an objective function, guides the evolutionary
   process. The results produced are counterchecked against the
   entertainment criteria of humans by conducting a human user survey and a
   controller learning ability experiment. The proposed metrics and the
   evolutionary process of generating games can be employed by any platform
   game for the purpose of automatic generation of interesting games
   provided an initial search space is given.</abstract><date>NOV 2 2015</date><author>Halim, Zahid
   Baig, Abdul Rauf
   Abbas, Ghulam</author></paper><paper><title>A Review of Eye Gaze in Virtual Agents, Social Robotics and HCI:
   Behaviour Generation, User Interaction and Perception</title><abstract>A person's emotions and state of mind are apparent in their face and
   eyes. As a Latin proverb states: 'The face is the portrait of the mind;
   the eyes, its informers'. This presents a significant challenge for
   Computer Graphics researchers who generate artificial entities that aim
   to replicate the movement and appearance of the human eye, which is so
   important in human-human interactions. This review article provides an
   overview of the efforts made on tackling this demanding task. As with
   many topics in computer graphics, a cross-disciplinary approach is
   required to fully understand the workings of the eye in the transmission
   of information to the user. We begin with a discussion of the movement
   of the eyeballs, eyelids and the head from a physiological perspective
   and how these movements can be modelled, rendered and animated in
   computer graphics applications. Furthermore, we present recent research
   from psychology and sociology that seeks to understand higher level
   behaviours, such as attention and eye gaze, during the expression of
   emotion or during conversation. We discuss how these findings are
   synthesized in computer graphics and can be utilized in the domains of
   Human-Robot Interaction and Human-Computer Interaction for allowing
   humans to interact with virtual agents and other artificial entities. We
   conclude with a summary of guidelines for animating the eye and head
   from the perspective of a character animator.</abstract><date>SEP 2015</date><author>Ruhland, K.
   Peters, C. E.
   Andrist, S.
   Badler, J. B.
   Badler, N. I.
   Gleicher, M.
   Mutlu, B.
   McDonnell, R.</author></paper><paper><title>A GPU-implemented physics-based haptic simulator of tooth drilling</title><abstract>Background Restorative dentistry simulation is one of the most
   challenging applications involving virtual reality and haptics. This
   paper presents a haptics-based tooth drilling simulator for dental
   education.Methods Unlike the existing methods, the force model is based
   on physical properties which consider the geometrical model of the tool.
   In order to provide uniform force feedback from tooth layers, a new
   approach is suggested in which the physical properties of each tooth
   voxel are subsequently used in calculating the feedback force. We
   implement a hashing algorithm for collision detection due to its reduced
   time complexity. The haptics algorithm has been implemented on a
   graphics processing unit using the CUDA toolbox.Results In parallel
   processing, the speed of haptic loop execution is increased almost 8
   times.Conclusion The proposed idea for force calculation leads to a
   uniform sensation of force. An important feature of the designed system
   is the capability to run in a real-time fashion. Copyright (C) 2015 John
   Wiley &amp; Sons, Ltd.</abstract><date>DEC 2015</date><author>Razavi, M.
   Talebi, H. A.
   Zareinejad, M.
   Dehghan, M. R.</author></paper><paper><title>Runtime Analysis of GPU-Based Stereo Matching</title><abstract>This paper elaborates on the possibility to leverage the highly parallel
   nature of GPUs to implement more efficient stereo matching algorithms.
   Different algorithms have been implemented and compared on the CPU and
   the GPU in order to show the speedup gained by moving the computation to
   the graphics card. The results were evaluated for accuracy using the
   test available on the Middlebury website for stereo vision. An
   assessment of the runtime performance was done by a script which
   examined the runtime behaviour of the individual steps of the stereo
   matching algorithm.</abstract><date>NOV 2015</date><author>Zentner, Christian
   Liu, Yan</author></paper><paper><title>Blood pressure increases with body size in mammals</title><abstract>In a recent technical comment regarding our analysis of the scaling of
   blood pressure with body mass in mammals (White and Seymour 2014),
   Packard (2015) argues that the trends in our graphs do not accurately
   reflect the relationship between the original variables, and that
   neither the graphics nor the accompanying statistical analyses provide
   strong support for the conclusions from the study, namely that larger
   mammals have higher arterial blood pressures. Here we take the
   opportunity to respond to these criticisms.</abstract><date>DEC 2015</date><author>White, Craig R.
   Seymour, Roger S.</author></paper><paper><title>Runtime and Architecture Support for Efficient Data Exchange in
   Multi-Accelerator Applications</title><abstract>Heterogeneous parallel computing applications often process large data
   sets that require multiple GPUs to jointly meet their needs for physical
   memory capacity and compute throughput. However, the lack of high-level
   abstractions in previous heterogeneous parallel programming models force
   programmers to resort to multiple code versions, complex data copy steps
   and synchronization schemes when exchanging data between multiple GPU
   devices, which results in high software development cost, poor
   maintainability, and even poor performance. This paper describes the HPE
   runtime system, and the associated architecture support, which enables a
   simple, efficient programming interface for exchanging data between
   multiple GPUs through either interconnects or cross-node network
   interfaces. The runtime and architecture support presented in this paper
   can also be used to support other types of accelerators. We show that
   the simplified programming interface reduces programming complexity. The
   research presented in this paper started in 2009. It has been
   implemented and tested extensively in several generations of HPE runtime
   systems as well as adopted into the NVIDIA GPU hardware and drivers for
   CUDA 4.0 and beyond since 2011. The availability of real hardware that
   support key HPE features gives rise to a rare opportunity for studying
   the effectiveness of the hardware support by running important
   benchmarks on real runtime and hardware. Experimental results show that
   in a exemplar heterogeneous system, peer DMA and double-buffering,
   pinned buffers, and software techniques can improve the
   inter-accelerator data communication bandwidth by 2 x. They can also
   improve the execution speed by 1.6 x for a 3D finite difference, 2.5 x
   for 1D FFT, and 1.6 x for merge sort, all measured on real hardware. The
   proposed architecture support enables the HPE runtime to transparently
   deploy these optimizations under simple portable user code, allowing
   system designers to freely employ devices of different capabilities. We
   further argue that simple interfaces such as HPE are needed for most
   applications to benefit from advanced hardware features in practice.</abstract><date>MAY 2015</date><author>Cabezas, Javier
   Gelado, Isaac
   Stone, John E.
   Navarro, Nacho
   Kirk, David B.
   Hwu, Wen-mei</author></paper><paper><title>Annealing evolutionary parallel algorithm analysis of optimization
   arrangement on mistuned blades with non-linear friction</title><abstract>This paper sets up a lumped parameter model of engine bladed disk system
   when considering the nonlinear friction damping based on mistuned
   parameters which is obtained from the blade modal experiment. A bladed
   arrangement optimization method, namely annealing evolutionary algorithm
   with tabu list is presented which combines the local search ability of
   SA (simulated annealing) and the global searching ability of GA (genetic
   algorithm) introducing tabu list as the search memory list. Parallel
   TAEA (tabu annealing evolutionary algorithm) is presented based on CUDA
   (Compute Unified Device Architecture) combining GPU (Graphics Processing
   Unit) and its performance is analyzed. The results show that
   optimization based on CUDA framework can improve computing speed. At the
   same time using optimization results can reduce the amplitude of forced
   vibration response of bladed disk system and make it in the range of
   allowable engineering.</abstract><date>DEC 2015</date><author>Yuan, Huiqun
   Zhao, Tianyu
   Yang, Wenjun
   Pan, Honggang</author></paper><paper><title>A Scalable Massively Parallel Motion and Disparity Estimation Scheme for
   Multiview Video Coding</title><abstract>Multiview video coding (MVC) has recently received considerable
   attention. It is proposed as an extension of H.264/Advanced Video Coding
   (AVC) standard for multiple video source compression. To resolve the
   extremely high computational complexity of MVC (and in fact other AVC
   techniques), suitable parallel algorithms need to be developed that are
   amenable to implementation on low-cost massively parallel architecture,
   platforms that have found a common place due to recent advances in the
   parallel computer architecture. The high complexity of MVC is due to its
   prediction structure, where motion estimation (ME) between the frames
   and disparity estimation (DE) between the views contribute to more than
   99% of overall complexity of the coder. This paper presents the
   development and implementation of a scalable massively parallel fast
   search algorithm to significantly reduce the computational cost of ME/DE
   over the current best available full block matching, and suboptimal fast
   search algorithms. The proposed massively parallel fast search algorithm
   (DZfast), when evaluated over eight views, outperforms the existing full
   search and fast search MVC algorithms by a factor of up to 245.8 and
   8.4, respectively. This speedup comes at no or minute loss in
   rate-distortion performance.</abstract><date>FEB 2016</date><author>Jiang, Caoyang
   Nooshabadi, Saeid</author></paper><paper><title>Multi-agent simulation on multiple GPUs</title><abstract>Multi-agent simulation is widely used in many areas including biology,
   economic, political, and environmental science to study complex systems.
   Unfortunately, it is computationally expensive. In this paper, we shall
   explore the implementation of a general multi-agent simulation system on
   a system with multiple GPUs acting as accelerators. In particular, we
   have ported the popular Java multi-agent simulation framework MASON to a
   nVidia CUDA-based multi-GPU setting. We evaluated our implementation
   over different numbers and types of nVidia GPUs. For our evaluation, we
   ported three models in the original version of MASON. On the well-known
   Boids model, we achieved a speedup of 187x. Using a fictional model, we
   showed that speedup of up to 468x is possible. In the paper, we shall
   also describe the detailed internals of our system, and the various
   issues we encountered and how they were solved. (C) 2015 Elsevier B.V.
   All rights reserved.</abstract><date>SEP 2015</date><author>Ho, N. M.
   Thoai, N.
   Wong, W. F.</author></paper><paper><title>PteroTerra: a searchable pterosaur database web application that
   interfaces with Google Earth</title><abstract>Recently, paleontologists have begun using Internet databases and Google
   Earth((R)) as new tools to share data with the scientific community. The
   newly developed web application PteroTerra ([GRAPHICS]), implemented
   using the Ruby on Rails((R)) web framework, is a specimen-based
   pterosaur database that interfaces with Google Earth((R)). This database
   currently catalogues over 1300 pterosaur specimens from all over the
   world and includes information about each specimen such as taxon name,
   classification, geologic age, geographic location of discovery, geologic
   formation, rock type, paleoenvironment, articulation, wingspan, proposed
   diet and housing institution. The application allows users to search for
   specimens based on keywords and to create groups of pterosaurs based on
   shared characteristics. Groups can then be downloaded as a.kml file,
   which can be automatically uploaded into Google Earth((R)) in order to
   study geographic patterns of pterosaur specimens based on any criteria
   of interest. The use and continuous updating of PteroTerra will provide
   pterosaurologists and other paleontologists with a central location for
   storing and obtaining information about particular pterosaur specimens,
   as well as a way for researchers to observe pterosaur patterns on a
   worldwide scale. The principles behind this program can easily be
   expanded to other fields of study.</abstract><date>AUG 18 2015</date><author>McLain, Matthew A.
   Chase, Brad
   Bryant, Eric</author></paper><paper><title>A GPU Implementation of an Explicit Compact FDTD Algorithm with a
   Digital Impedance Filter for Room Acoustics Applications</title><abstract>In recent years, computational engineering has undergone great changes
   due to the development of the graphics processing unit (GPU) technology.
   For example, in room acoustics, the wave-based methods, that formerly
   were considered too expensive for 3-D impulse response simulations, are
   now chosen to exploit the parallel nature of GPU devices considerably
   reducing the execution time of the simulations. There exist
   contributions related to this topic that have explored the performance
   of different GPU algorithms; however, the computational analysis of a
   general explicit model that incorporates algorithms with different
   neighboring orders and a general frequency dependent impedance boundary
   model has not been properly developed. In this paper, we present a GPU
   implementation of a complete room acoustic model based on a family of
   explicit finite-difference time-domain (FDTD) algorithms. We first
   develop a strategy for implementing a frequency independent (FI)
   impedance model which is free from thread divergences and then, we
   extend the model adding a digital impedance filter (DIF) boundary
   subroutine able to compute the acoustic pressure of different nodes such
   as corners or edges without an additional performance penalty. Both
   implementations are validated and deeply analyzed by performing
   different 3-D numerical experiments. Finally, we define a performance
   metric which is able to objectively measure the computing throughput of
   a FDTD implementation using a simple number. The robustness of this
   metric allows us to compare algorithms even if these have been run in
   different GPU cards or have been formulated with other explicit models.</abstract><date>AUG 2015</date><author>Spa, Carlos
   Rey, Anton
   Hernandez, Erwin</author></paper><paper><title>An Iterative Process for Developing and Evaluating a Computer-Based
   Prostate Cancer Decision Aid for African American Men</title><abstract>Background. The disproportionate burden of prostate cancer (PrCA) among
   African American (AA) men amplifies the need for informed decisions
   about PrCA screening. To create a computer-based decision aid (CBDA) for
   increasing prostate knowledge, decision self-efficacy, and intention to
   make an informed decision, the study implemented an iterative approach
   to develop a culturally appropriate CBDA. Method. A short CBDA prototype
   containing PrCA information and interactive activities was developed. A
   sample of 21 AA men aged 37 to 66 years in South Carolina participated
   in one of seven 90-minute focus groups and completed a 36-item survey.
   Updates were made to the CBDA based on participant feedback. The CBDA
   and heuristic evaluation surveys were then distributed to six expert
   reviewers. Ten men were also randomly selected from our sample
   population to participate in interviews regarding usability of the CBDA.
   Results. Participants and expert reviewers expressed consensus on many
   features of the CBDA, but some suggested changes to the format and
   graphics in order to enhance the CBDA's effectiveness. Development and
   evaluation processes and implications are discussed. Conclusions. Using
   CBDAs for informed decision making may be appropriate for AA men. It is
   important to engage the community and experts in an iterative
   development process to ensure that a CBDA is relevant for priority
   populations.</abstract><date>SEP 2015</date><author>Owens, Otis L.
   Friedman, Daniela B.
   Brandt, Heather M.
   Bernhardt, Jay M.
   Hebert, James R.</author></paper><paper><title>On geodesics of the rotation group SO(3)</title><abstract>Geodesics on SO(3) are characterized by constant angular velocity
   motions and as great circles on a three-sphere. The former
   interpretation is widely used in optometry and the latter features in
   the interpolation of rotations in computer graphics. The simplicity of
   these two disparate interpretations belies the complexity of the
   corresponding rotations. Using a quaternion representation for a
   rotation, we present a simple proof of the equivalence of the
   aforementioned characterizations and a straightforward method to
   establish features of the corresponding rotations.</abstract><date>NOV 2015</date><author>Novelia, Alyssa
   O'Reilly, Oliver M.</author></paper><paper><title>A new 2D graphical representation of protein sequence and its
   application</title><abstract>Graphical representation is a very efficient tool for visual analysis of
   protein sequences. In this paper, a novel 2D graphical representation
   scheme is proposed on the basis of a newly introduced concept, named
   characteristic model of the protein sequences. After obtaining the 2D
   graphics of protein sequences, two numerical characterizations of them
   is designed as descriptors to analyze the nine DN5 protein sequences,
   simulation and analysis results show that, comparing with existing
   methods, our method is not only visible, intuitional, and simple, but
   also has no circuit or degeneracy, and even more important, since the
   storage space required by our method is constant and has nothing to do
   with the length of protein sequences, then it can keep excellent visual
   inspection for long protein sequences.</abstract><date>SEP 2015</date><author>Wang, Lei
   Peng, Hui
   Zheng, Jinhua
   Qiu, Yanzi</author></paper><paper><title>Formative questioning in computer learning environments: a course for
   pre-service mathematics teachers</title><abstract>This paper focuses on a specific aspect of formative assessment, namely
   questioning. Given that computers have gained widespread use in learning
   and teaching, specific attention should be made when organizing
   formative assessment in computer learning environments (CLEs). A course
   including various workshops was designed to develop knowledge and skills
   of questioning in CLEs. This study investigates how pre-service
   mathematics teachers used formative questioning with technological tools
   such as Geogebra and Graphic Calculus software. Participants are 35
   pre-service mathematics teachers. To analyse formative questioning, two
   types of questions are investigated: mathematical questions and
   technical questions. Data were collected through lesson plans, teaching
   notes, interviews and observations. Descriptive statistics of the number
   of questions in the lesson plans before and after the workshops are
   presented. Examples of two types of questions are discussed using the
   theoretical framework. One pre-service teacher was selected and a deeper
   analysis of the way he used questioning during his three lessons was
   also investigated. The findings indicated an improvement in using
   technical questions for formative purposes and that the course provided
   a guideline in planning and using mathematical and technical questions
   in CLEs.</abstract><date>NOV 17 2015</date><author>Akkoc, Hatice</author></paper><paper><title>GPU Acceleration for Fixed Complexity Sphere Decoder in Large MIMO
   Uplink Systems</title><abstract>Large MIMO constitutes a principal technology for next generation
   wireless communication systems. Detection algorithms for large MIMO
   schemes tend to require long simulation times on conventional computer
   systems. In order to improve simulation efficiency of decoders in large
   MIMO systems, we propose to use General Purpose Graphic Processing Units
   (GPGPU). This paper considers the implementation of a Fixed Complexity
   Sphere Decoder (FCSD) for large scale MIMO uplink systems using the
   framework of GPGPU-Computed Unified Device Architecture (CUDA). Using
   GPGPU, significant speedups were found for large MIMO systems and
   constellation sizes, over conventional CPU implementation, while
   providing the same Bit Error Rate (BER) performance. Hence the proposed
   approach shows great potential of GPGPU to accelerate computer
   simulations of large MIMO systems.</abstract><date>2015</date><author>Chen, Tianpei
   Leib, Harry</author></paper><paper><title>An OpenCL-Based Cross-Platform Monte Carlo Dose Engine (oclMC) for
   Coupled Photon-Electron Transport</title><abstract></abstract><date>JUN 2015</date><author>Tian, Z.
   Shi, F.
   Folkerts, M.
   Qin, N.
   Jiang, S.
   Jia, X.</author></paper><paper><title>Graphical Processing Unit (GPU) Accelerated Solution of
   Multi-Dimensional Population Balances Using High Resolution Finite
   Volume Algorithm</title><abstract>Population balance modeling is a widely used approach to describe,
   crystallization processes, taking into accountnot only the primary
   phenomena like nucleation and growth, as well asparticle agglomeration
   and breakage which can be extended to multivariate cases where more
   internal coordinates i.e. particle properties can be used. The solution
   of the generated partial differential equation is not trivial due to its
   hyperbolic nature which causes numerical dispersion and diffusion. The
   high resolution finite volume algorithms are able to solve
   multidimensional population balance equations avoiding these unwanted
   phenomena. Unfortunately, the computational time is significantly
   larger, compared to e.g. moments based solutions. More recently there is
   an increased interest to apply parallel computations using GPU-s due to
   its massively parallel hardware architecture. The current study presents
   two accelerating possibilities for finite volume solution of population
   balance equations: compiled C++ code executed on CPU and CUDA C++ code
   running on the GPU. The case studies demonstrate that the code running
   on the GPU was between 6-35 times faster than the compiled C++ code and
   4-6 times faster than the standard MatLab function. This significant
   improvement in computational time enablesthe application of model-based
   control approaches in real time even in case of multidimensional
   population balance models.</abstract><date>2015</date><author>Szilagyi, Botond
   Nagy, Zoltan K.</author></paper><paper><title>A new algorithm for design, operation and cost assessment of struvite
   (MgNH4PO4) precipitation processes</title><abstract>Deliberate struvite (MgNH4PO4) precipitation from wastewater streams has
   been the topic of extensive research in the last two decades and is
   expected to gather worldwide momentum in the near future as a P-reuse
   technique. A wide range of operational alternatives has been reported
   for struvite precipitation, including the application of various Mg(II)
   sources, two pH elevation techniques and several Mg:P ratios and pH
   values. The choice of each operational parameter within the struvite
   precipitation process affects process efficiency, the overall cost and
   also the choice of other operational parameters. Thus, a comprehensive
   simulation program that takes all these parameters into account is
   essential for process design. This paper introduces a systematic
   decision-supporting tool which accepts a wide range of possible
   operational parameters, including unconventional Mg(II) sources (i.e.
   seawater and seawater nanofiltration brines). The study is supplied with
   a free-of-charge computerized tool ([GRAPHICS]) which links two computer
   platforms (Python and PHREEQC) for executing thermodynamic calculations
   according to predefined kinetic considerations. The model can be (inter
   alia) used for optimizing the struvite-fluidized bed reactor process
   operation with respect to P removal efficiency, struvite purity and
   economic feasibility of the chosen alternative. The paper describes the
   algorithm and its underlying assumptions, and shows results (i.e.
   effluent water quality, cost breakdown and P removal efficiency) of
   several case studies consisting of typical wastewaters treated at
   various operational conditions.</abstract><date>AUG 3 2015</date><author>Birnhack, Liat
   Nir, Oded
   Telzhenski, Marina
   Lahav, Ori</author></paper><paper><title>NGL Viewer: a web application for molecular visualization</title><abstract>The NGL Viewer (http://proteinformatics.charite.de/ngl) is a web
   application for the visualization of macromolecular structures. By fully
   adopting capabilities of modern web browsers, such as WebGL, for
   molecular graphics, the viewer can interactively display large molecular
   complexes and is also unaffected by the retirement of third-party
   plug-ins like Flash and Java Applets. Generally, the web application
   offers comprehensive molecular visualization through a graphical user
   interface so that life scientists can easily access and profit from
   available structural data. It supports common structural file-formats
   (e.g. PDB, mmCIF) and a variety of molecular representations (e.g.
   'cartoon, spacefill, licorice'). Moreover, the viewer can be embedded in
   other web sites to provide specialized visualizations of entries in
   structural databases or results of structure-related calculations.</abstract><date>JUL 1 2015</date><author>Rose, Alexander S.
   Hildebrand, Peter W.</author></paper><paper><title>A Low-cost System for Generating Near-realistic Virtual Actors</title><abstract>Generating virtual actors is one of the most challenging fields in
   computer graphics. The reconstruction of a realistic virtual actor has
   been paid attention by the academic research and the film industry to
   generate human-like virtual actors. Many movies were acted by human-like
   virtual actors, where the audience cannot distinguish between real and
   virtual actors. The synthesis of realistic virtual actors is considered
   a complex process. Many techniques are used to generate a realistic
   virtual actor; however they usually require expensive hardware
   equipment. In this paper, a low-cost system that generates
   near-realistic virtual actors is presented. The facial features of the
   real actor are blended with a virtual head that is attached to the
   actor's body. Comparing with other techniques that generate virtual
   actors, the proposed system is considered a low-cost system that
   requires only one camera that records the scene without using any
   expensive hardware equipment. The results of our system show that the
   system generates good near-realistic virtual actors that can be used on
   many applications.</abstract><date>JUN 2015</date><author>Afifi, Mahmoud
   Hussain, Khaled F.
   Ibrahim, Hosny M.
   Omar, Nagwa M.</author></paper><paper><title>High Precision Conservative Surface Mesh Generation for Swept Volumes</title><abstract>We present a novel, efficient, and flexible scheme to generate a
   high-quality mesh that approximates the outer boundary of a swept
   volume. Our approach comes with two guarantees. First, the approximation
   is conservative, i.e., the swept volume is enclosed by the generated
   mesh. Second, the one-sided Hausdorff distance of the generated mesh to
   the swept volume is upper bounded by a user defined tolerance.
   Exploiting this tolerance the algorithm generates a mesh that is adapted
   to the local complexity of the swept volume boundary, keeping the
   overall output complexity remarkably low. The algorithm is two-phased:
   the actual sweep and the mesh generation. In the sweeping phase, we
   introduce a general framework to compute a compressed voxelization. The
   phase is tailored for an easy application of parallelization techniques.
   We show this for our exemplary implementation and provide a multicore
   solution, as well as a GPU-based solution using CUDA. For the meshing
   phase we utilize and extend the well known Delaunay refinement such that
   it generates an adaptive conservative approximation that obeys the user
   defined upper bound on the one-sided Hausdorff distance to the swept
   volume. The approach is able to handle inputs of high complexity and
   compute an approximation with a very high precision, which we
   demonstrate on real industrial data sets.Note to Practitioners-This work
   is motivated by the following problem we were posed by a car
   manufacturer: To measure the movement of an engine during test drives,
   sensors were placed onto the motor compartment of a car. With this setup
   the sensors recorded the position and orientation of the engine every
   5ms. The reason for these test drives were clearance checks between
   parts of the engine and other components, fixated on the chassis, e.g.,
   the oil pan and a neighboring component, cf. Fig. 1. We are now
   interested in the following question: Which volume in space does the
   vibrating part of the engine occupy? We here present a method that
   approximatively computes the outer boundary of this volume that is also
   called the swept volume. The model that generates the volume (hence
   generator) will be a CADmodel, that is swept (hence swept volume) along
   the sequence of motions (trajectory).A trajectory can of course also
   come from CAD and/or assembly planning, as for instance in [10]. The
   scenario depicted in Fig. 2 shows a user-created path during
   maintainability analysis. The swept volume (or rather its outer
   boundary) can be used to verify that the path is collision free and that
   spatial tolerances are not violated. In contrast to previous methods
   that compute the swept volume, our swept volume approximation is
   reliable, meaning, that we give guarantees on the quality of our
   approximation in terms of how close we are to the actual swept
   volume.Future work would be, for instance, the extension of our approach
   to kinematic chains, e.g., the swept volume of a car seat under all
   possible configurations.</abstract><date>JAN 2015</date><author>von Dziegielewski, Andreas
   Hemmer, Michael
   Schoemer, Elmar</author></paper><paper><title>A Real-time GPU Implementation of the SIFT Algorithm for Large-Scale
   Video Analysis Tasks</title><abstract>The SIFT algorithm is one of the most popular feature extraction methods
   and therefore widely used in all sort of video analysis tasks like
   instance search and duplicate/ near-duplicate detection. We present an
   efficient GPU implementation of the SIFT descriptor extraction algorithm
   using CUDA. The major steps of the algorithm are presented and for each
   step we describe how to efficiently parallelize it massively, how to
   take advantage of the unique capabilities of the GPU like shared memory
   / texture memory and how to avoid or minimize common GPU performance
   pitfalls. We compare the GPU implementation with the reference CPU
   implementation in terms of runtime and quality and achieve a speedup
   factor of approximately 3 - 5 for SD and 5 - 6 for Full HD video with
   respect to a multi-threaded CPU implementation, allowing us to run the
   SIFT descriptor extraction algorithm in real-time on SD video.
   Furthermore, quality tests show that the GPU implementation gives the
   same quality as the reference CPU implementation from the HessSIFT
   library. We further describe the benefits of GPU-accelerated SIFT
   descriptor calculation for video analysis applications such as
   near-duplicate video detection.</abstract><date>2015</date><author>Fassold, Hannes
   Rosner, Jakub</author></paper><paper><title>Resolution of an inverse thermal problem using parallel processing on
   shared-memory multiprocessor architectures</title><abstract>Advances in multi-cores CPUs and in Graphics Processors Units (GPUs) are
   attracting a lot of attention of the scientific community due to their
   parallel processing power in conjunction with their low cost. In recent
   years the resolution of inverse thermal problems (ITP) is gaining
   increasing importance and attention in simulation-based applied science
   and engineering. However, the resolutions of these problems are very
   sensitive to random errors and the computer cost is high. In an attempt
   to improve the computational performance to solve an ITP, the
   computational power of multi-core architectures was used and analysed;
   mainly those offered by the GPU via Compute Unified Device Architecture
   (CUDA) and multi-cores CPUs via Pthreads. Also, we developed the
   implementation of the Preconditioned Conjugate Gradient method as a
   kernel on GPU to solve several sparse linear systems. Our CUDA and
   Pthreads-based systems are, respectively, two and four times faster than
   the serial version, while maintaining comparable convergence behaviour.</abstract><date>FEB 17 2015</date><author>Ansoni, J. L.
   Brandi, A. C.
   Seleghim, P., Jr.</author></paper><paper><title>I-COMS: Interprotein-COrrelated Mutations Server</title><abstract>Interprotein contact prediction using multiple sequence alignments
   (MSAs) is a useful approach to help detect protein-protein interfaces.
   Different computational methods have been developed in recent years as
   an approximation to solve this problem. However, as there are
   discrepancies in the results provided by them, there is still no
   consensus on which is the best performing methodology. To address this
   problem, I-COMS (interprotein COrrelated Mutations Server) is presented.
   I-COMS allows to estimate covariation between residues of different
   proteins by four different covariation methods. It provides a graphical
   and interactive output that helps compare results obtained using
   different methods. I-COMS automatically builds the required MSA for the
   calculation and produces a rich visualization of either intraprotein
   and/or interprotein covariating positions in a circos representation.
   Furthermore, comparison between any two methods is available as well as
   the overlap between any or all four methodologies. In addition, as a
   complementary source of information, a matrix visualization of the
   corresponding scores is made available and the density plot distribution
   of the inter, intra and inter+intra scores are calculated. Finally, all
   the results can be downloaded (including MSAs, scores and graphics) for
   comparison and visualization and/or for further analysis.</abstract><date>JUL 1 2015</date><author>Iserte, Javier
   Simonetti, Franco L.
   Zea, Diego J.
   Teppa, Elin
   Marino-Buslje, Cristina</author></paper><paper><title>On Longest Repeat Queries Using GPU</title><abstract>Repeat finding in strings has important applications in subfields such
   as computational biology. The challenge of finding the longest repeats
   covering particular string positions was recently proposed and solved
   by. Ileri et al., using a total of the optimal O(n) time and space,
   where n is the string size. However, their solution can only find the
   left-most longest repeat for each of the n string position. It is also
   not known how to parallelize their solution. In this paper, we propose a
   new solution for longest repeat finding, which although is theoretically
   suboptimal in time but is conceptually simpler and works faster and uses
   less memory space in practice than the optimal solution. Further, our
   solution can find all longest repeats of every string position, while
   still maintaining a faster processing speed and less memory space usage.
   Moreover, our solution is parallelizable in the shared memory
   architecture (SMA), enabling it to take advantage of the modern
   multi-processor computing platforms such as the general-purpose graphics
   processing units (GPU). We have implemented both the sequential and
   parallel versions of our solution. Experiments with both biological and
   non-biological data show that our sequential and parallel solutions are
   faster than the optimal solution by a factor of 2-3.5 and 6-14,
   respectively, and use less memory space.</abstract><date>2015</date><author>Tian, Yun
   Xu, Bojian</author></paper><paper><title>A new approach for optical assessment of directional anisotropy in
   turbid media</title><abstract>A study of polarized light transport in scattering media exhibiting
   directional anisotropy or linear birefringence is presented in this
   paper. Novel theoretical and experimental methodologies for the
   quantification of birefringent alignment based on out-of-plane polarized
   light transport are presented here. A polarized Monte Carlo model and a
   polarimetric imaging system were devised to predict and measure the
   impact of birefringence on an impinging linearly polarized light beam.
   Ex-vivo experiments conducted on bovine tendon, a biological sample
   consisting of highly packed type I collagen fibers with birefringent
   property, showed good agreement with the analytical
   results.[GRAPHICS]Top view geometry of the in-plane (a) and the
   out-of-plane (b) detection. Letter C indicates the location of the
   detection arm.</abstract><date>JAN 2016</date><author>Ghassemi, Pejhman
   Moffatt, Lauren T.
   Shupp, Jeffrey W.
   Ramella-Roman, Jessica C.</author></paper><paper><title>Image compression based on GPU encoding</title><abstract>With the rapid development of digital technology, the data increased
   greatly in both static image and dynamic video image. It is noticeable
   how to decrease the redundant data in order to save or transmit
   information more efficiently. So the research on image compression
   becomes more and more important. Using GPU to achieve higher compression
   ratio has superiority in interactive remote visualization. Contrast to
   CPU, GPU may be a good way to accelerate the image compression.
   Currently, GPU of NIVIDIA has evolved into the eighth generation, which
   increasingly dominates the high-powered general purpose computer field.
   This paper explains the way of GPU encoding image. Some experiment
   results are also presented.</abstract><date>2015</date><author>Bai, Zhaofeng
   Qiu, Yuehong</author></paper><paper><title>GPU-Based Hybrid Method for Electromagnetic Scattering of Electrically
   Large Objects</title><abstract>A GPU-based parallel hybrid method is proposed to accelerate the
   electromagnetic scattering from electrically large complex structures in
   high frequency. The MPO and the GRECO hybrid method can simplify the
   computations of the diffraction fields of the wedges by modifying the
   surface-normal vector of the target to redefine the surface equivalent
   currents. By using GPU-based reduction operation, we design a fast
   parallel summing method for the hybrid integrals over illuminated
   triangles. The hybrid method is validated by comparing the numerical
   results with those obtained through the serial computing program on a
   central processing unit, as well as through FEKO method of moments,
   which showed good agreement.</abstract><date>2015</date><author>Zhu Yan-ju
   Xie Shu-guo</author></paper><paper><title>Untangling polygonal and polyhedral meshes via mesh optimization</title><abstract>We propose simple and efficient optimization-based untangling strategies
   for 2D polygonal and 3D polyhedral meshes. The first approach uses a
   size-based mesh metric, which eliminates inverted elements by averaging
   element size over the entire mesh. The second method uses a hybrid
   quality metric, which untangles inverted elements by simultaneously
   averaging element size and improving element shape. The last method
   using a variant of the hybrid quality metric gives a high penalty for
   inverted elements and employs an adaptive sigmoid function for handling
   various mesh sizes. Numerical experiments are presented to show the
   effectiveness of the proposed untangling strategies for various 2D
   polygonal and 3D polyhedral meshes.</abstract><date>JUL 2015</date><author>Kim, Jibum
   Chung, Jaeyong</author></paper><paper><title>Random Partial Haar Wavelet Transformation for Single Instruction
   Multiple Threads</title><abstract>Many researchers expect the compressive sensing and sparse recovery
   problem can overcome the limitation of conventional digital techniques.
   However, these new approaches require to solve the l1 norm optimization
   problems when it comes to signal reconstruction. In the signal
   reconstruction process, the transform computation by multiplication of a
   random matrix and a vector consumes considerable computing power. To
   address this issue, parallel processing is applied to the optimization
   problems. In particular, due to huge size of original signal, it is hard
   to store the random matrix directly in memory, which makes one need to
   design a procedural approach in handling the random matrix. This paper
   presents a new parallel algorithm to calculate random partial Haar
   wavelet transform based on Single Instruction Multiple Threads (SIMT)
   platform.</abstract><date>2015</date><author>???</author></paper><paper><title>CUDA and MPI performances on the computation of Rossi-alpha distribution
   from pulsed neutron sources</title><abstract>This study presents the methodology to calculate the Rossi-alpha
   distribution for a subcritical assembly driven by an external pulsed
   neutron source through MCNP6 computer simulations. The Rossi-alpha
   distribution is obtained from the signal (e.g., He-3(n,p) reactions) of
   a neutron detector placed in one experimental channel of the subcritical
   assembly. The detector signal is obtained from MCNP6 computer
   simulations modeling a single pulse of the external neutron source. The
   MCNP6 output data for a single pulse of the external neutron source must
   be processed to take into account the effect of delayed neutrons born in
   all previous source pulses. The calculation of the Rossi-a distribution
   from a pulsed neutron source is time consuming because of the huge
   amount of processed data. Consequently, the algorithms introduced in
   this work use parallel computing platforms, e.g., CUDA or MPI, to reduce
   the computing time. Published by Elsevier Ltd.</abstract><date>JAN 2015</date><author>Talamo, Alberto</author></paper><paper><title>Portal-Masked Environment Map Sampling</title><abstract>We present a technique to efficiently importance sample distant,
   all-frequency illumination in indoor scenes. Standard environment
   sampling is inefficient in such cases since the distant lighting is
   typically only visible through small openings (e.g. windows). This
   visibility is often addressed by manually placing a portal around each
   window to direct samples towards the openings; however, uniformly
   sampling the portal (its area or solid angle) disregards the possibly
   high frequency environment map. We propose a new portal importance
   sampling technique which takes into account both the environment map and
   its visibility through the portal, drawing samples proportional to the
   product of the two. To make this practical, we propose a novel,
   portal-rectified reparametrization of the environment map with the key
   property that the visible region induced by a rectangular portal
   projects to an axis-aligned rectangle. This allows us to sample
   according to the desired product distribution at an arbitrary shading
   location using a single (precomputed) summed-area table per portal. Our
   technique is unbiased, relevant to many renderers, and can also be
   applied to rectangular light sources with directional emission profiles,
   enabling efficient rendering of non-diffuse light sources with soft
   shadows.</abstract><date>JUL 2015</date><author>Bitterli, Benedikt
   Novak, Jan
   Jarosz, Wojciech</author></paper><paper><title>GGEMS-Brachy: GPU GEant4-based Monte Carlo simulation for brachytherapy
   applications</title><abstract>In brachytherapy, plans are routinely calculated using the AAPM TG43
   formalism which considers the patient as a simple water object. An
   accurate modeling of the physical processes considering patient
   heterogeneity using Monte Carlo simulation (MCS) methods is currently
   too time-consuming and computationally demanding to be routinely used.
   In this work we implemented and evaluated an accurate and fast MCS on
   Graphics Processing Units (GPU) for brachytherapy low dose rate (LDR)
   applications. A previously proposed Geant4 based MCS framework
   implemented on GPU (GGEMS) was extended to include a hybrid GPU
   navigator, allowing navigation within voxelized patient specific images
   and analytically modeled I-125 seeds used in LDR brachytherapy. In
   addition, dose scoring based on track length estimator including
   uncertainty calculations was incorporated. The implemented GGEMS-brachy
   platform was validated using a comparison with Geant4 simulations and
   reference datasets. Finally, a comparative dosimetry study based on the
   current clinical standard (TG43) and the proposed platform was performed
   on twelve prostate cancer patients undergoing LDR brachytherapy.
   Considering patient 3D CT volumes of 400 x 250 x 65 voxels and an
   average of 58 implanted seeds, the mean patient dosimetry study run time
   for a 2% dose uncertainty was 9.35 s (approximate to 500 ms 10(-6)
   simulated particles) and 2.5 s when using one and four GPUs,
   respectively. The performance of the proposed GGEMS-brachy platform
   allows envisaging the use of Monte Carlo simulation based dosimetry
   studies in brachytherapy compatible with clinical practice. Although the
   proposed platform was evaluated for prostate cancer, it is equally
   applicable to other LDR brachytherapy clinical applications. Future
   extensions will allow its application in high dose rate brachytherapy
   applications.</abstract><date>JUL 7 2015</date><author>Lemarechal, Yannick
   Bert, Julien
   Falconnet, Claire
   Despres, Philippe
   Valeri, Antoine
   Schick, Ulrike
   Pradier, Olivier
   Garcia, Marie-Paule
   Boussion, Nicolas
   Visvikis, Dimitris</author></paper><paper><title>NiftySim: A GPU-based nonlinear finite element package for simulation of
   soft tissue biomechanics</title><abstract>NiftySim, an open-source finite element toolkit, has been designed to
   allow incorporation of high-performance soft tissue simulation
   capabilities into biomedical applications. The toolkit provides the
   option of execution on fast graphics processing unit (GPU) hardware,
   numerous constitutive models and solid-element options, membrane and
   shell elements, and contact modelling facilities, in a simple to use
   library.The toolkit is founded on the total Lagrangian explicit dynamics
   (TLEDs) algorithm, which has been shown to be efficient and accurate for
   simulation of soft tissues. The base code is written in C, and GPU
   execution is achieved using the nVidia CUDA framework. In most cases,
   interaction with the underlying solvers can be achieved through a single
   Simulator class, which may be embedded directly in third-party
   applications such as, surgical guidance systems. Advanced capabilities
   such as contact modelling and nonlinear constitutive models are also
   provided, as are more experimental technologies like reduced order
   modelling. A consistent description of the underlying solution
   algorithm, its implementation with a focus on GPU execution, and
   examples of the toolkit's usage in biomedical applications are
   provided.Efficient mapping of the TLED algorithm to parallel hardware
   results in very high computational performance, far exceeding that
   available in commercial packages.The NiftySim toolkit provides
   high-performance soft tissue simulation capabilities using GPU
   technology for biomechanical simulation research applications in medical
   image computing, surgical simulation, and surgical guidance
   applications.</abstract><date>JUL 2015</date><author>Johnsen, Stian F.
   Taylor, Zeike A.
   Clarkson, Matthew J.
   Hipwell, John
   Modat, Marc
   Eiben, Bjoern
   Han, Lianghao
   Hu, Yipeng
   Mertzanidou, Thomy
   Hawkes, David J.
   Ourselin, Sebastien</author></paper><paper><title>A Comparative Study of Parallel RANSAC Implementations in 3D Space</title><abstract>RANSAC (RAndom SAmple Consensus) is an iterative method for estimating
   the parameters of a certain mathematical model from a set of data which
   may contain a large number of outliers (noisy points). The main problem
   of the RANSAC algorithm is that it is too expensive in terms of
   execution time when real-time processing is needed (30 fps). In view of
   the importance of this algorithm and the rise of parallelism-based
   technologies, we analyze and compare in this work various parallel
   implementations based on different techniques (OpenMP, POSIX Threads,
   and CUDA). To the best of our knowledge, no other articles have
   attempted a similar study. In order to make this first study, we have
   used some standard metrics in parallelism (Runtime and Speedup) and some
   specific metrics used in evaluating search strategies (Precision,
   Recall, and F-Score). Furthermore, the experiments have been executed in
   different hardware alternatives in order to present a more complete
   study. The conclusions of our study show the advantages and
   disadvantages of the different parallel implementations.</abstract><date>OCT 2015</date><author>Hidalgo-Paniagua, Alejandro
   Vega-Rodriguez, Miguel A.
   Pavon, Nieves
   Ferruz, Joaquin</author></paper><paper><title>Three-directional motion-compensation mask-based novel look-up table on
   graphics processing units for video-rate generation of digital
   holographic videos of three-dimensional scenes</title><abstract>A three-directional motion-compensation mask-based novel look-up table
   method is proposed and implemented on graphics processing units (GPUs)
   for video-rate generation of digital holographic videos of
   three-dimensional (3D) scenes. Since the proposed method is designed to
   be well matched with the software and memory structures of GPUs, the
   number of compute-unified-device-architecture kernel function calls can
   be significantly reduced. This results in a great increase of the
   computational speed of the proposed method, allowing video-rate
   generation of the computer-generated hologram (CGH) patterns of 3D
   scenes. Experimental results reveal that the proposed method can
   generate 39.8 frames of Fresnel CGH patterns with 1920 x 1080 pixels per
   second for the test 3D video scenario with 12,088 object points on dual
   GPU boards of NVIDIA GTX TITANs, and they confirm the feasibility of the
   proposed method in the practical application fields of
   electroholographic 3D displays. (C) 2015 Optical Society of America</abstract><date>JAN 20 2016</date><author>Kwon, Min-Woo
   Kim, Seung-Cheol
   Kim, Eun-Soo</author></paper><paper><title>OVERVIEW OF RISK-ESTIMATION TOOLS FOR PRIMARY PREVENTION OF
   CARDIOVASCULAR DISEASES IN EUROPEAN POPULATIONS</title><abstract>To identify persons with a high risk for cardiovascular diseases (CVD)
   special tools (scores, charts, graphics or computer programs) for
   CVD-risk assessment based on levels of the certain risk factors have
   been constructed. The applicability of these instruments depends on the
   derivation cohorts, considered risk factors and endpoints, applied
   statistical methods as well as used formats.The review addresses the
   risk-estimation tools for primary prevention of CVD potentially relevant
   for European populations. The risk-estimation tools were identified
   using two previously published systematic reviews as well as conducting
   a literature search in MEDLINE and a manual search. Only instruments
   were considered which were derived from cohorts of at least 1,000
   participants of one gender without pre-existing CVD, enable risk
   assessment for a period of at least 5 years, were designed for an
   age-range of at least 25 years and published after the year 2000.A
   number of risk-estimation tools for CVD derived from single European,
   several European and from non-European cohorts were identified. From a
   clinical perspective, seem to be preferable instruments for risk of CVD
   contemporary developed for the population of interest, which use easily
   accessible measures and show a high discriminating ability. Instruments,
   restricting risk-estimation to certain cardiovascular events,
   recalibrated high-accuracy tools or tools derived from European
   populations with similar risk factors distribution and CVD-incidence are
   the second choice. In younger people, calculating the relative risk or
   cardiovascular age equivalence measures may be of more benefit.</abstract><date>JUN 2015</date><author>Gorenoi, Vitali
   Hagen, Anja</author></paper><paper><title>Distortionless segmentation image fusion and coordinate system
   transformation for 3D scene reconstruction on fibre-to-chip coupling</title><abstract>A three-dimensional (3D) scene of fibre-to-chip coupling is
   reconstructed from three captured 2D graphics in which coordinate system
   transformation and distortionless segmentation image fusion algorithm
   are presented to visually guide precise alignment and nanopositioning
   procedures based on the established single lens photometric machine
   vision system. As an instance, the 3D scene on wedge-shaped fibre (WSF)
   coupling with indium phosphide (InP) photonic integrated circuit (PIC)
   chip is demonstrated, where the wedged angle is calculated to be 46.5
   degrees, differing from the nominal value of 45 degrees with the error
   &lt;3.5%, and for the longitudinal displacement, the computation of 94 m
   has the error within 7% compared with the measurement of 88 m. Under the
   guidance of 3D scene reconstruction on WSF-InP PIC, the coupling
   efficiency is 2 dB higher than that without such stereoscopic image.</abstract><date>NOV 19 2015</date><author>Liu, Xu
   Sun, Xiaohan</author></paper><paper><title>A fully parallel in time and space algorithm for simulating the
   electrical activity of a neural tissue</title><abstract>Background: The resolution of a model describing the electrical activity
   of neural tissue and its propagation within this tissue is highly
   consuming in term of computing time and requires strong computing power
   to achieve good results.New method: In this study, we present a method
   to solve a model describing the electrical propagation in neuronal
   tissue, using parareal algorithm, coupling with parallelization space
   using CUDA in graphical processing unit (GPU).Results: We applied the
   method of resolution to different dimensions of the geometry of our
   model (1-D, 2-D and 3-D). The GPU results are compared with simulations
   from a multi-core processor cluster, using message-passing interface
   (MPI), where the spatial scale was parallelized in order to reach a
   comparable calculation time than that of the presented method using GPU.
   A gain of a factor 100 in term of computational time between sequential
   results and those obtained using the GPU has been obtained, in the case
   of 3-D geometry. Given the structure of the GPU, this factor increases
   according to the fineness of the geometry used in the
   computation.Comparison with existing method(s): To the best of our
   knowledge, it is the first time such a method is used, even in the case
   of neuroscience.Conclusion: Parallelization time coupled with GPU
   parallelization space allows for drastically reducing computational time
   with a fine resolution of the model describing the propagation of the
   electrical signal in a neuronal tissue. (C) 2015 Elsevier B.V. All
   rights reserved.</abstract><date>JAN 15 2016</date><author>Bedez, Mathieu
   Belhachmi, Zakaria
   Haeberle, Olivier
   Greget, Renaud
   Moussaoui, Saliha
   Bouteiller, Jean-Marie
   Bischoff, Serge</author></paper><paper><title>MRI of the Prostate in Germany: Online Survey among Radiologists</title><abstract>Purpose: To assess structural, technical, and communicative aspects of
   dedicated MR examinations of the prostate (MRP) offered by radiologists
   in Germany.Materials and Methods: We conducted an eight-item online
   survey among members of the German Radiology Society (DRG). Radiological
   institutions were asked about their structure, i.e., either hospital
   department (HD) or private practice (PP), number of board-certified
   radiologists, postal regions, number of MRPs in 2011, MR technology and
   MR sequences applied, ways to communicate results, and feedback from
   referring physicians on results of subsequent tests and procedures.
   Submissions were cleared of redundancies and anonymized. Differences in
   the number of positive replies to each item were statistically
   significant at p &lt; 0.05 for two-tailed testing in 2x2 tables.Results:
   The survey represented board-certified radiologists in 128 institutions
   (63 HDs and 65 PPs) in 67/95 German postal regions (71 %). Almost
   two-thirds of institutions performed 11 to 50 MRPs in 2011, more often
   at 1.5 T (116/128, 91 %) than at 3.0 T (36/128, 28 %), and most
   frequently with surface coils (1.5 T, 88/116, 76 %; 3.0 T, 34/36, 94 %;
   chi-square, 1.9736, 0.1 &lt; p &lt; 0.25). About two-thirds of 1.5 T users and
   90 % of 3.0 T users applied at least one functional MR modality
   (diffusion-weighted imaging, dynamic contrast- enhanced imaging, or MR
   spectroscopy) for MRP. Reports including graphic representations of the
   prostate were applied by 21/128 institutions (16 %). Clinical feedback
   after MRP to radiologists other than upon their own request was
   infrequent (HDs, 32 - 45 %, PPs, 18 - 32 %).Conclusion: MRP was a widely
   available, small-volume examination among radiologists in Germany in
   2011. The technology mainstay was a 1.5 T surface coil examination
   including at least one functional MR modality. Dedicated reporting and
   feedback mechanisms for quality control were underdeveloped.</abstract><date>AUG 2015</date><author>Mueller-Lisse, U. G.
   Lewerich, B.
   Mueller-Lisse, U. L.
   Reiser, M.
   Scherr, M. K.</author></paper><paper><title>An explicit dynamics GPU structural solver for thin shell finite
   elements</title><abstract>With the availability of user oriented software tools, dedicated
   architectures, such as the parallel computing platform and programming
   model CUDA (Compute Unified Device Architecture) released by NVIDIA, one
   of the main producers of graphics cards, and of improved, highly
   performing GPU (Graphics Processing Unit) boards, GPGPU (General Purpose
   programming on GPU) is attracting increasing interest in the engineering
   community, for the development of analysis tools suitable to be used in
   validation/verification and virtual reality applications. For their
   inherent explicit and decoupled structure, explicit dynamics finite
   element formulations appear to be particularly attractive for
   implementations on hybrid CPU/GPU or pure GPU architectures. The issue
   of an optimized, double-precision finite element GPU implementation of
   an explicit dynamics finite element solver for elastic shell problems in
   small strains and large displacements and rotations, using unstructured
   meshes, is here addressed. The conceptual difference between a GPU
   implementation directly adapted from a standard CPU approach and a new
   optimized formulation, specifically conceived for GPUs, is discussed and
   comparatively assessed. It is shown that a speedup factor of about 5 can
   be achieved by an optimized algorithm reformulation and careful memory
   management. A speedup of more than 40 is achieved with respect of
   state-of-the art commercial codes running on CPU, obtaining real-time
   simulations in some cases, on commodity hardware. When a last generation
   GPU board is used, it is shown that a problem with more than 16 millions
   degrees of freedom can be solved in just few hours of computing time,
   opening the way to virtualization approaches for real large scale
   engineering problems. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>JUL 1 2015</date><author>Bartezzaghi, A.
   Cremonesi, M.
   Parolini, N.
   Perego, U.</author></paper><paper><title>The importance of reaming the posterior femoral cortex before inserting
   lengthening nails and calculation of the amount of reaming</title><abstract>Background: Lengthening nails have been used to correct limb length
   discrepancy caused by different etiologies, as well as for
   post-traumatic reasons. Two important lengthening nail-related
   complications are damage to the distraction mechanism and femoral
   fractures around the nail tip. As a result of the curved anatomy of the
   femur, straight nails impinge on the anterior cortex. Therefore, proper
   reshaping of the medullary canal to accommodate straight lengthening
   nails is crucial for the prevention of this problem. Reaming the dense
   posterior cortex is important when aiming to insert a lengthening nail
   without incurring anterior cortex nail tip impingement-related
   complications. Posterior femoral cortex over-reaming is a solution to
   this situation.Methods: Sixty patients received lengthening nails during
   2008-2013, (ISKD, Fitbone, Precice). Posterior cortex rigid-reaming
   technique was used successfully in 45 retrograde femoral lengthening
   cases. The preoperatively planned posterior cortex amount was reamed
   until the impingement was overcome during the operation under
   fluoroscopic control for each case. Since the preoperative determination
   of posterior cortex reaming amount is time consuming and operator
   dependent, we evaluated the X rays of the patients with computer
   software and conventional paper-based measurements. The effect of
   reaming the posterior cortical wall on the inclination of the nail tip
   to the anterior femoral cortex was detected with measurements on the
   preoperative and postoperative lateral femoral X-rays by using the
   CorelDRAW (R) Graphic Suite X6 software package (Corel, Inc., Ottawa,
   Ontario, Canada) software. On the same software, X-rays and the
   posterior reaming amount were also calculated.Results: The mean age of
   the patients was 27 years (11-42), while the mean lengthening was 5.9 cm
   (2-14). The mean consolidation index was 1.05 (0.75-1.62), and the mean
   follow-up period was 31 months (range, 18-45 months). The mean distance
   of the osteotomy site to the intercondylar notch of the femur was 81.2
   mm (+/- 16.92). The mean displacement of the nail tip position was 15.42
   mm (+/- 4.77) on the measurements on the postoperative X-rays after nail
   insertion compared to the preoperative simulations on the templates. The
   mean posterior cortex reaming thickness was 3.68 mm (+/-
   1.02).Conclusions: We derived a formula that allows the required amount
   of optimal posterior cortex reaming to be determined. No
   impingement-related complications or nail damage were observed.</abstract><date>JAN 16 2016</date><author>Kucukkaya, Metin
   Karakoyun, Ozgur
   Erol, Mehmet Fatih</author></paper><paper><title>Common influence region problems</title><abstract>In this paper we propose and solve common influence region problems.
   These problems are related to the simultaneous influence, or the
   capacity to attract customers, of two sets of facilities of different
   types. For instance, while a facility of the first type competes with
   the other facilities of the first type, it cooperates with several
   facilities of the second type. The problems studied can be applied, for
   example, to decision-making support systems for marketing and/or
   locating facilities. We present parallel algorithms, to be run on a
   Graphics Processing Unit, for approximately solving the problems
   considered here. We also provide experimental results and discuss the
   efficiency and scalability of our approach. Finally, we present the
   speedup ratios obtained when the running times of the parallel proposed
   algorithms using a GPU are compared with those obtained from their
   respective efficient sequential CPU versions. (C) 2015 Elsevier Inc. All
   rights reserved.</abstract><date>NOV 10 2015</date><author>Fort, M.
   Sellares, J. A.</author></paper><paper><title>Till All Are One: Towards a Unified Cloud IDS</title><abstract>Recently there is a trend to use cloud computing on service deployment,
   enjoying various advantages that it offers with emphasis on the economy
   which is achieved in the era of the financial crisis. However, along
   with the transformation of technology, several security issues are
   raised and especially the threat of malicious insiders. For instance,
   insiders can use their privileged position to accomplish an attack
   against the cloud infrastructure. In this paper we introduce a practical
   and efficient intrusion detection system solution for cloud based on the
   advantages of CUDA technology. The proposed solution audits the deployed
   virtual machines operation, and correlates the collected information to
   detect uncommon behavior based on Smith-Waterman algorithm. To do so, we
   collect the system calls of cloud virtual machines and compare them with
   pre-defined attack signatures. We implement the core of the detection
   module both sequentially and in parallel on CUDA technology. We evaluate
   our solution on experimental CUDA enabled cloud system in terms of
   performance using well known attack patterns. Results indicate that our
   approach improve highly the efficiency of detection in terms of
   processing time compared to a sequential implementation.</abstract><date>2015</date><author>Pitropakis, Nikolaos
   Lambrinoudakis, Costas
   Geneiatakis, Dimitris</author></paper><paper><title>GPU Accelerated SVM with Sparse Sliced EllR-T Matrix Format</title><abstract>This paper presents the SECu-SVM algorithm for solving classification
   problems. It allows for a significant acceleration of the standard SVM
   implementations by transferring the most time-consuming computations
   from the standard CPU to the Graphics Processor Units (GPU). In
   addition, highly efficient Sliced EllR-T sparse matrix format was used
   for storing the dataset in GPU memory, which requires a very low memory
   footprint and is also well adapted to parallel processing. Performed
   experiments demonstrate an acceleration of 4-100 times over LibSVM.
   Moreover, in the majority of cases the SECu-SVM is less time-consuming
   than the best sparse GPU implementations and allows for handling
   significantly larger classification datasets.</abstract><date>FEB 2015</date><author>Sopyla, Krzysztof
   Drozda, Pawel</author></paper><paper><title>Efficient Parallel GPU Design on WRF Five-Layer Thermal Diffusion Scheme</title><abstract>Satellite remote-sensing observations and ground-based radar can detect
   the weather conditions from a distance and are widely used to monitor
   the weather all around the globe. The assimilated satellite/radar data
   are passed through the weather models for weather forecasting. The
   five-layer thermal diffusion scheme is one of the weather models,
   handling with an energy budget made up of sensible, latent, and
   radiative heat fluxes. The model feature of no interactions among
   horizontal grid points makes this scheme very favorable for parallel
   processing. This study demonstrates implementation of this scheme using
   graphics processing unit (GPU) massively parallel architecture. By
   employing one NVIDIA Tesla K40 GPU, our GPU optimization effort on this
   scheme achieves a speedup of 311 x with respect to its CPU counterpart
   Fortran code running on one CPU core of Intel Xeon E5-2603, whereas the
   speedup for one CPU socket (four cores) with respect to one CPU core is
   only 3.1 x. We can even boost the speedup of this scheme to 398 x with
   respect to one CPU core when two NVIDIA Tesla K40 GPUs are applied.</abstract><date>MAY 2015</date><author>Huang, Melin
   Huang, Bormin
   Chang, Yang-Lang
   Mielikainen, Jarno
   Huang, Hung-Lung Allen
   Goldberg, Mitchell D.</author></paper><paper><title>GPU-acceleration of waveform relaxation methods for large differential
   systems</title><abstract>It is the purpose of this paper to provide an acceleration of waveform
   relaxation (WR) methods for the numerical solution of large systems of
   ordinary differential equations. The introduced technique is based on
   the employ of graphics processing units (GPUs) in order to speed-up the
   numerical integration process. A CUDA solver based on WR-Picard,
   WR-Jacobi and red-black WR-Gauss-Seidel iterations is presented and some
   numerical experiments realized on a multi-GPU machine are provided.</abstract><date>FEB 2016</date><author>Conte, Dajana
   D'Ambrosio, Raffaele
   Paternoster, Beatrice</author></paper><paper><title>Use of B-splines in fast dynamic ultrasound RF simulations</title><abstract>Synthetic ultrasound data is invaluable for applications such as tuning
   image segmentation algorithms. Unfortunately, simulation time can become
   prohibitive for phantoms consisting of a large number of
   point-scatterers. In such cases, simplified simulation algorithms can be
   useful, and trade accuracy for speed. Storage of dynamic scatterer
   phantoms requires vast amounts of disk space when the number of time
   steps is large. We propose (1) a spline-based approach for efficient
   handling of dynamic scatterer phantoms with arbitrary time resolution,
   and (2) a fast, open source C++/CUDA implementation of a previously
   published convolution-based technique (COLE by Gao et al.) with support
   for our proposed spline-based approach. We observed a speedup of several
   orders of magnitude when comparing the simulation time for our CUDA
   implementation to the simulation time from an experiment reported in the
   literature, when using a similar simulation setup. We also observed that
   the spline-based representation of scatterers can improve the simulation
   speed almost four times for a dynamic simulation using the GPU, compared
   to the alternative of uploading new scatterer data to the GPU memory at
   every time step.</abstract><date>2015</date><author>Storve, Sigurd
   Torp, Hans</author></paper><paper><title>A Fine-Grained CUDA Implementation of the Multi-objective Evolutionary
   Approach NSGA-II: Potential Impact for Computational and Systems Biology
   Applications</title><abstract>Many computational and systems biology challenges, in particular those
   related to big data analysis, can be formulated as optimization problems
   and therefore can be addressed using heuristics. Beside the typical
   optimization problems, formulated with respect to a single target, the
   possibility of optimizing multiple objectives (MO) is rapidly becoming
   more appealing. In this context, MO Evolutionary Algorithms (MOEAs) are
   one of the most widely used classes of methods to solve MO optimization
   problems. However, these methods can be particularly demanding from the
   computational point of view and, therefore, effective parallel
   implementations are needed. This fact, together with the wide diffusion
   of powerful and low-cost general-purpose Graphics Processing Units,
   promoted the development of software tools that focus on the
   parallelization of one or more computational phases among the steps
   characterizing MOEAs. In this paper we present a fine-grained
   parallelization of the Fast Non-dominating Sorting Genetic Algorithm
   (NSGA-II) for the CUDA architecture. In particular, we will discuss how
   this solution can be exploited to solve multi-objective optimization
   task in the field of computational and systems biology.</abstract><date>2015</date><author>D'Agostino, Daniele
   Pasquale, Giulia
   Merelli, Ivan</author></paper><paper><title>Implementation of Pedestrian Detection and Tracking with GPU at
   Night-time</title><abstract>This paper is about an approach for pedestrian detection and tracking
   with infrared imagery. We used the CUDA(Computer Unified Device
   Architecture) that is a parallel processing language in order to improve
   the speed of video-based pedestrian detection and tracking. The
   detection phase is performed by Adaboost algorithm based on Haar-like
   features. Adaboost classifier is trained with datasets generated from
   infrared images. After detecting the pedestrian with the Adaboost
   classifier, we proposed a particle filter tracking strategies on HSV
   histogram feature that exploit adaptively at the same time. The proposed
   approach is implemented on an NVIDIA Jetson TK1 developer board that is
   full-featured device ideal for software development within the Linux
   environment. In this paper, we presented the results of parallel
   processing with the NVIDIA GPU on the CUDA development environment for
   detection and tracking of pedestrians. We compared the object detection
   and tracking processing time for night-time images on both GPU and CPU.
   The result showed that the detection and tracking speed of the
   pedestrian with GPU is approximately 6 times faster than that for CPU.</abstract><date>2015</date><author>???
   ???
   ???</author></paper><paper><title>APBT-JPEG Image Coding Based on GPU</title><abstract>In wireless multimedia sensor networks (WMSN), the latency of
   transmission is an increasingly problem. With the improvement of
   resolution, the time cost in image and video compression is more and
   more, which seriously affects the real-time of WMSN. In JPEG system, the
   core of the system is DCT, but DCT-JPEG is not the best choice.
   Block-based DCT transform coding has serious blocking artifacts when the
   image is highly compressed at low bit rates. APBT is used in this paper
   to solve that problem, but APBT does not have a fast algorithm. In this
   paper, we analyze the structure in JPEG and propose a parallel framework
   to speed up the algorithm of JPEG on GPU. And we use all phase
   biorthogonal transform (APBT) to replace the discrete cosine transform
   (DCT) for the better performance of reconstructed image. Therefore,
   parallel APBT-JPEG is proposed to solve the real-time of WMSN and the
   blocking artifacts in DCT-JPEG in this paper. We use the CUDA toolkit
   based on GPU which is released by NVIDIA to design the parallel
   algorithm of APBT-JPEG. Experimental results show that the maximum
   speedup ratio of parallel algorithm of APBT-JPEG can reach more than 100
   times with a very low version GPU, compared with conventional serial
   APBT-JPEG. And the reconstructed image using the proposed algorithm has
   better performance than the DCT-JPEG in terms of objective quality and
   subjective effect. The proposed parallel algorithm based on GPU of APBT
   also can be used in image compression, video compression, the edge
   detection and some other fields of image processing.</abstract><date>APR 30 2015</date><author>Wang, Chengyou
   Shan, Rongyang
   Zhou, Xiao</author></paper><paper><title>An efficient solution to the subset-sum problem on GPU</title><abstract>We present an algorithm to solve the subset-sum problem (SSP) of
   capacity c and n items with weights w(i),1 &lt;= i &lt;= n, spending O(n(m -
   w(min))/p) time and O(n + m - w(min)) space in the Concurrent
   Read/Concurrent Write (CRCW) PRAM model with 1 &lt;= p &lt;= m - w(min)
   processors, where w(min) is the lowest weight and m=min {c,Sigma(n)(i)
   (=)1 w(i)-c}, improving both upper-bounds. Thus, when n &lt;= c, it is
   possible to solve the SSP in O(n) time in parallel environments with low
   memory. We also show OpenMP and CUDA implementations of this algorithm
   and, on Graphics Processing Unit, we obtained better performance than
   the best sequential and parallel algorithms known so far. Copyright (c)
   2015 John Wiley &amp; Sons, Ltd.</abstract><date>JAN 2016</date><author>Curtis, V. V.
   Sanches, C. A. A.</author></paper><paper><title>Spectral turning bands for efficient Gaussian random fields generation
   on GPUs and accelerators</title><abstract>A random field (RF) is a set of correlated random variables associated
   with different spatial locations. RF generation algorithms are of
   crucial importance for many scientific areas, such as astrophysics,
   geostatistics, computer graphics, and many others. Current approaches
   commonly make use of 3D fast Fourier transform (FFT), which does not
   scale well for RF bigger than the available memory; they are also
   limited to regular rectilinear meshes. We introduce random field
   generation with the turning band method (RAFT), an RF generation
   algorithm based on the turning band method that is optimized for
   massively parallel hardware such as GPUs and accelerators. Our algorithm
   replaces the 3D FFT with a lower-order, one-dimensional FFT followed by
   a projection step and is further optimized with loop unrolling and
   blocking. RAFT can easily generate RF on non-regular (non-uniform)
   meshes and efficiently produce fields with mesh sizes bigger than the
   available device memory by using a streaming, out-of-core approach. Our
   algorithm generates RF with the correct statistical behavior and is
   tested on a variety of modern hardware, such as NVIDIA Tesla, AMD
   FirePro and Intel Phi. RAFT is faster than the traditional methods on
   regular meshes and has been successfully applied to two real case
   scenarios: planetary nebulae and cosmological simulations. Copyright (c)
   2015John Wiley &amp; Sons, Ltd.</abstract><date>NOV 2015</date><author>Hunger, Lars
   Cosenza, Biagio
   Kimeswenger, Stefan
   Fahringer, Thomas</author></paper><paper><title>Fast Wavefront Propagation (FWP) for Computing Exact Geodesic Distances
   on Meshes</title><abstract>Computing geodesic distances on triangle meshes is a fundamental problem
   in computational geometry and computer graphics. To date, two notable
   classes of algorithms, the Mitchell-Mount-Papadimitriou (MMP) algorithm
   and the Chen-Han (CH) algorithm, have been proposed. Although these
   algorithms can compute exact geodesic distances if numerical computation
   is exact, they are computationally expensive, which diminishes their
   usefulness for large-scale models and/or time-critical applications. In
   this paper, we propose the fast wavefront propagation (FWP) framework
   for improving the performance of both the MMP and CH algorithms. Unlike
   the original algorithms that propagate only a single window (a data
   structure locally encodes geodesic information) at each iteration, our
   method organizes windows with a bucket data structure so that it can
   process a large number of windows simultaneously without compromising
   wavefront quality. Thanks to its macro nature, the FWP method is less
   sensitive to mesh triangulation than the MMP and CH algorithms. We
   evaluate our FWP-based MMP and CH algorithms on a wide range of
   large-scale real-world models. Computational results show that our
   method can improve the speed by a factor of 3-10.</abstract><date>JUL 2015</date><author>Xu, Chunxu
   Wang, Tuanfeng Y.
   Liu, Yong-Jin
   Liu, Ligang
   He, Ying</author></paper><paper><title>Implementation of Wireless 3D Stereo Image Capture System and 3D
   Exaggeration Algorithm for the Region of Interest</title><abstract>In this paper, we introduce the mobile embedded system implemented for
   capturing stereo image based on two CMOS camera module. We use WinCE as
   an operating system and capture the stereo image by using device driver
   for CMOS camera interface and Direct Draw API functions. We aslo
   comments on the GPU hardware and CUDA programming for implementation of
   3D exaggeraion algorithm for ROI by adjusting and synthesizing the
   disparity value of ROI (region of interest) in real time. We comment on
   the pattern of aperture for deblurring of CMOS camera module based on
   the Kirchhoff diffraction formula and clarify the reason why we can get
   more sharp and clear image by blocking some portion of aperture or
   geometric sampling. Synthesized stereo image is real time monitored on
   the shutter glass type three-dimensional LCD monitor and disparity
   values of each segment are analyzed to prove the validness of
   emphasizing effect of ROI.</abstract><date>2015</date><author>Ham, Woonchul
   Song, Chulgyu
   Lee, Kangsan
   Badarch, Luubaatar</author></paper><paper><title>Graphics-Processor-Unit-Based Parallelization of Optimized Baseline
   Wander Filtering Algorithms for Long-Term Electrocardiography</title><abstract>Long-term electrocardiogram (ECG) often suffers from relevant noise.
   Baseline wander in particular is pronounced in ECG recordings using dry
   or esophageal electrodes, which are dedicated for prolonged
   registration. While analog high-pass filters introduce phase
   distortions, reliable offline filtering of the baseline wander implies a
   computational burden that has to be put in relation to the increase in
   signal-to-baseline ratio (SBR). Here, we present a graphics processor
   unit (GPU)-based parallelization method to speed up offline baseline
   wander filter algorithms, namely the wavelet, finite, and infinite
   impulse response, moving mean, and moving median filter. Individual
   filter parameters were optimized with respect to the SBR increase based
   on ECGs from the Physionet database superimposed to autoregressive
   modeled, real baseline wander. A Monte-Carlo simulation showed that for
   low input SBR the moving median filter outperforms any other method but
   negatively affects ECG wave detection. In contrast, the infinite impulse
   response filter is preferred in case of high input SBR. However, the
   parallelized wavelet filter is processed 500 and four times faster than
   these two algorithms on the GPU, respectively, and offers superior
   baseline wander suppression in low SBR situations. Using a signal
   segment of 64 mega samples that is filtered as entire unit, wavelet
   filtering of a seven-day high-resolution ECG is computed within less
   than 3 s. Taking the high filtering speed into account, the GPU wavelet
   filter is the most efficient method to remove baseline wander present in
   long-term ECGs, with which computational burden can be strongly reduced.</abstract><date>JUN 2015</date><author>Niederhauser, Thomas
   Wyss-Balmer, Thomas
   Haeberlin, Andreas
   Marisa, Thanks
   Wildhaber, Reto A.
   Goette, Josef
   Jacomet, Marcel
   Vogel, Rolf</author></paper><paper><title>WorldBrush: Interactive Example-based Synthesis of Procedural Virtual
   Worlds</title><abstract>We present a novel approach for the interactive synthesis and editing of
   virtual worlds. Our method is inspired by painting operations and uses
   methods for statistical example-based synthesis to automate content
   synthesis and deformation. Our real-time approach takes a form of local
   inverse procedural modeling based on intermediate statistical models:
   selected regions of procedurally and manually constructed example scenes
   are analyzed, and their parameters are stored as distributions in a
   palette, similar to colors on a painter's palette. These distributions
   can then be interactively applied with brushes and combined in various
   ways, like in painting systems. Selected regions can also be moved or
   stretched while maintaining the consistency of their content. Our method
   captures both distributions of elements and structured objects, and
   models their interactions. Results range from the interactive editing of
   2D artwork maps to the design of 3D virtual worlds, where constraints
   set by the terrain's slope are also taken into account.</abstract><date>AUG 2015</date><author>Emilien, Arnaud
   Vimont, Ulysse
   Cani, Marie-Paule
   Poulin, Pierre
   Benes, Bedrich</author></paper><paper><title>Cotton QTLdb: a cotton QTL database for QTL analysis, visualization, and
   comparison between Gossypium hirsutum and G-hirsutum x G-barbadense
   populations</title><abstract>A specialized database currently containing more than 2200 QTL is
   established, which allows graphic presentation, visualization and
   submission of QTL.In cotton quantitative trait loci (QTL), studies are
   focused on intraspecific Gossypium hirsutum and interspecific G.
   hirsutum x G. barbadense populations. These two populations are
   commercially important for the textile industry and are evaluated for
   fiber quality, yield, seed quality, resistance, physiological, and
   morphological trait QTL. With meta-analysis data based on the vast
   amount of QTL studies in cotton it will be beneficial to organize the
   data into a functional database for the cotton community. Here we
   provide a tool for cotton researchers to visualize previously identified
   QTL and submit their own QTL to the Cotton QTLdb database. The database
   provides the user with the option of selecting various QTL trait types
   from either the G. hirsutum or G. hirsutum x G. barbadense populations.
   Based on the user's QTL trait selection, graphical representations of
   chromosomes of the population selected are displayed in publication
   ready images. The database also provides users with trait information on
   QTL, LOD scores, and explained phenotypic variances for all QTL
   selected. The CottonQTLdb database provides cotton geneticist and
   breeders with statistical data on cotton QTL previously identified and
   provides a visualization tool to view QTL positions on chromosomes.
   Currently the database (Release 1) contains 2274 QTLs, and succeeding
   QTL studies will be updated regularly by the curators and members of the
   cotton community that contribute their data to keep the database
   current. The database is accessible from http://www.cottonqtldb.org.</abstract><date>AUG 2015</date><author>Said, Joseph I.
   Knapka, Joseph A.
   Song, Mingzhou
   Zhang, Jinfa</author></paper><paper><title>Accelerating elliptic curve scalar multiplication over GF(2(m)) on
   graphic hardwares</title><abstract>In this paper, we present PEG (Parallel ECC library on GPU), which is
   efficient implementation of Elliptic Curve Scalar Multiplication over GF
   (2(m)) on Graphic Processing Units. While existing ECC implementations
   over GPU focused on limited parameterizations such as (fixed scalar and
   different curves) or (different scalars and same base point), PEG covers
   all parameter options ((a) fixed scalar and variable points, (b) random
   scalars and fixed input point, and (c) random scalars and variable
   points) which are used for ECC-based protocols such as ECDH, ECDSA and
   ECIES. With GPU optimization concerns and through analyzing parameter
   types used for ECC-based protocols, we investigate promising algorithms
   at both finite field arithmetic and scalar multiplication level for
   performance optimization according to each parameterization. PEG covers
   ECC implementations over GF(2(163)), GF(2(233)) and GF(2(283)) for
   80-bit, 112-bit and 128-bit security on GTX285 and GTX480. PEG can
   achieve remarkable performance compared with MIRACL, one of the most
   famous ECC library, running on Intel i7 CPU (2.67 GHz). (C) 2014
   Elsevier Inc. All rights reserved.</abstract><date>JAN 2015</date><author>Seo, Seog Chung
   Kim, Taehong
   Hong, Seokhie</author></paper><paper><title>A Study on the Optimization of Finite Volume Effects of B-K in Lattice
   QCD by Using the CUDA</title><abstract>Lattice quantum chromodynamics (QCD) is the non-perturbative
   implementation of field theory to solve the QCD theory of quarks and
   gluons by using the Feynman path integral approach. We calculate the
   kaon CP (charge-parity) violation parameter B-K generally arising in
   theories of physics beyond the Standard Model. Because lattice
   simulations are performed on finite volume lattices, the finite volume
   effects must be considered to exactly estimate the systematic error. The
   computational cost of numerical simulations may increase dramatically as
   the lattice spacing is decreased. Therefore, lattice QCD calculations
   must be optimized to account for the finite volume effects. The
   methodology used in this study was to develop an algorithm to
   parallelize the code by using a graphic processing unit (GPU) and to
   optimize the code to achieve as close to the theoretical peak
   performance as possible. The results revealed that the calculation speed
   of the newly-developed algorithm is significantly improved compared with
   that of the current algorithm for the finite volume effects.</abstract><date>JUL 2015</date><author>Kim, Jangho
   Cho, Kihyeon</author></paper><paper><title>Visual Exploration of Data with Multithread MIC Computer Architectures</title><abstract>Knowledge mining from immense datasets requires fast, reliable and
   affordable tools for their visual and interactive exploration.
   Multidimensional scaling (MDS) is a good candidate for embedding of
   high-dimensional data into visually perceived 2-D and 3-D spaces. We
   focus here on the way to increase the computational performance of MDS
   in the context of interactive, hierarchical, visualization of big data.
   To this end we propose a parallel implementation of MDS on the modern
   Intel Many Integrated Core Architecture (MIC). We compare the timings
   obtained for MIC architecture to GPU and standard multi-core CPU
   implementations of MDS. We conclude that despite 30-40% lower
   computational performance comparing to GPU/CUDA tuned MDS codes, the MIC
   solution is still competitive due to dramatically shorter code
   production and tuning time. The integration of MIC with CPU will make
   this architecture very competitive with more volatile on technological
   changes GPU solutions.</abstract><date>2015</date><author>Pawliczek, Piotr
   Dzwinel, Witold
   Yuen, David A.</author></paper><paper><title>Combination of pharmacophore hypothesis and molecular docking to
   identify novel inhibitors of HCV NS5B polymerase</title><abstract>Hepatitis C virus (HCV) infection or HCV-related liver diseases are now
   shown to cause more than 350,000 deaths every year. Adaptability of HCV
   genome to vary its composition and the existence of multiple strains
   makes it more difficult to combat the emergence of drug-resistant HCV
   infections. Among the HCV polyprotein which has both the structural and
   non-structural regions, the non-structural protein NS5B RNA-dependent
   RNA polymerase (RdRP) mainly mediates the catalytic role of RNA
   replication in conjunction with its viral protein machinery as well as
   host chaperone proteins. Lack of such RNA-dependent RNA polymerase
   enzyme in host had made it an attractive and hotly pursued target for
   drug discovery efforts. Recent drug discovery efforts targeting HCV RdRP
   have seen success with FDA approval for sofosbuvir as a direct-acting
   antiviral against HCV infection. However, variations in drug-binding
   sites induce drug resistance, and therefore targeting allosteric sites
   could delay the emergence of drug resistance. In this study, we focussed
   on allosteric thumb site II of the non-structural protein NS5B
   RNA-dependent RNA polymerase and developed a five-feature pharmacophore
   hypothesis/model which estimated the experimental activity with a strong
   correlation of 0.971 &amp; 0.944 for training and test sets, respectively.
   Further, the Guner-Henry score of 0.6 suggests that the model was able
   to discern the active and inactive compounds and enrich the true
   positives during a database search. In this study, database search and
   molecular docking results supported by experimental HCV viral
   replication inhibition assays suggested ligands with best fitness to the
   pharmacophore model dock to the key residues involved in thumbs site II,
   which inhibited the HCV 1b viral replication in sub-micro-molar
   range.HCV nonstructural protein NS5B RNA-dependent RNA polymerase (RdRP)
   mediates the catalytic role of viral RNA replication. Lack of host
   RNA-dependent RNA polymerase enzyme had made it an attractive and hotly
   pursued target for drug discovery efforts. In this study, we developed a
   five-feature pharmacophore (3D QSAR) model for thumb site inhibitors of
   HCV RdRP, which estimated the experimental activity with a strong
   correlation of 0.971 &amp; 0.944 for training and test sets, respectively.
   Our database search and molecular docking results suggested that the
   compounds 1 and 2 with best fitness to the pharmacophore model were
   predicted to interact with key residues involved in thumbs site II and
   could inhibit the HCV RdRP activity. Further, the compounds 1 and 2
   potently inhibited HCV 1b viral replication in sub-micro-molar
   range.[GRAPHICS]</abstract><date>AUG 2015</date><author>Harikishore, Amaravadhi
   Li, Enlin
   Lee, Jia Jun
   Cho, Nam-Joon
   Yoon, Ho Sup</author></paper><paper><title>Modeling and analysis of performances for concurrent multithread
   applications on multicore and graphics processing unit systems</title><abstract>The capabilities of multicore processors lead them to be widely adopted
   in systems at any scale, since their are able to provide more computing
   power at a lower consumption and dissipation cost. System designers are
   challenged to a deeper understanding of multicore functioning in order
   to fully exploit them while keeping the optimal balance between cores
   utilization and optimal throughput, response time and energy usage.
   Besides the advancement of general purpose CPUs, the same technological
   evolution leads to the rise of GPUs, dramatic evolution of graphical
   coprocessors, that are now affordable, efficient, dedicated computing
   units, capable of parallel computing and equipped with facilities that
   make them suited for supporting the main CPU of a system in running
   ordinary applications. The availability of commercial off-the-shelf
   (COTS) multicore computers, eventually equipped with one or more GPUs,
   makes them the basic building block of data centers devoted to cloud
   applications or scientific computing. The way to optimal exploitation of
   such a wide amount of computing power passes through the ability of
   matching the best scheduling of hardware resources with the software
   characteristics of the applications. This requires appropriate models
   and evaluation methods. Simulation and analytical techniques are
   essential tools to support the design and the management process of such
   architectures, but a sound characterization of the workloads is
   required. Typical workloads consist in multithreaded applications, with
   different characteristics, that dynamically span over the cores of
   multiple machines, connected by fast networks. In this paper we propose
   several parametric performance models for different configurations of
   multicore machines, with or without GPU support, running multiple class
   multithreaded applications, aiming to supply a detailed modeling help
   for complex data centers. Copyright (c) 2015John Wiley &amp; Sons, Ltd.</abstract><date>FEB 2016</date><author>Cerotti, D.
   Gribaudo, M.
   Iacono, M.
   Piazzolla, P.</author></paper><paper><title>GPU-UPGMA: high-performance computing for UPGMA algorithm based on
   graphics processing units</title><abstract>Constructing phylogenetic trees is of priority concern in computational
   biology, especially for developing biological taxonomies. As a
   conventional means of constructing phylogenetic trees, unweighted pair
   group method with arithmetic (UPGMA) is also an extensively adopted
   heuristic algorithm for constructing ultrametric trees (UT). Although
   the UT constructed by UPGMA is often not a true tree unless the
   molecular clock assumption holds, UT is still useful for the clocklike
   data. Moreover, UT has been successfully adopted in other problems,
   including orthologous-domain classification and multiple sequence
   alignment. However, previous implementations of the UPGMA method have a
   limited ability to handle large taxa sets efficiently. This work
   describes a novel graphics processing unit (GPU)-UPGMA approach, capable
   of providing rapid construction of extremely large datasets for
   biologists. Experimental results indicate that the proposed GPU-UPGMA
   approach achieves an approximately 95x speedup ratio on NVIDIA Tesla
   C2050 GPU over the implementation with 2.13GHz CPU. The developed
   techniques in GPU-UPGMA also can be applied to solve the classification
   problem for large data set with more than tens of thousands items in the
   future.Copyright (c) 2014 John Wiley &amp; Sons, Ltd.</abstract><date>SEP 10 2015</date><author>Lin, Yu-Shiang
   Lin, Chun-Yuan
   Hung, Che-Lun
   Chung, Yeh-Ching
   Lee, Kual-Zheng</author></paper><paper><title>Sentiment of Emojis</title><abstract>There is a new generation of emoticons, called emojis, that is
   increasingly being used in mobile communications and social media. In
   the past two years, over ten billion emojis were used on Twitter. Emojis
   are Unicode graphic symbols, used as a shorthand to express concepts and
   ideas. In contrast to the small number of well-known emoticons that
   carry clear emotional contents, there are hundreds of emojis. But what
   are their emotional contents? We provide the first emoji sentiment
   lexicon, called the Emoji Sentiment Ranking, and draw a sentiment map of
   the 751 most frequently used emojis. The sentiment of the emojis is
   computed from the sentiment of the tweets in which they occur. We
   engaged 83 human annotators to label over 1.6 million tweets in 13
   European languages by the sentiment polarity (negative, neutral, or
   positive). About 4% of the annotated tweets contain emojis. The
   sentiment analysis of the emojis allows us to draw several interesting
   conclusions. It turns out that most of the emojis are positive,
   especially the most popular ones. The sentiment distribution of the
   tweets with and without emojis is significantly different. The
   inter-annotator agreement on the tweets with emojis is higher. Emojis
   tend to occur at the end of the tweets, and their sentiment polarity
   increases with the distance. We observe no significant differences in
   the emoji rankings between the 13 languages and the Emoji Sentiment
   Ranking. Consequently, we propose our Emoji Sentiment Ranking as a
   European language-independent resource for automated sentiment analysis.
   Finally, the paper provides a formalization of sentiment and a novel
   visualization in the form of a sentiment bar.</abstract><date>DEC 7 2015</date><author>Novak, Petra Kralj
   Smailovic, Jasmina
   Sluban, Borut
   Mozetic, Igor</author></paper><paper><title>A Hybrid CPU-GPU Real-Time Hyperspectral Unmixing Chain</title><abstract>Hyperspectral images are used in different applications in Earth and
   space science, and many of these applications exhibit real-or near
   real-time constraints. A problem when analyzing hyperspectral images is
   that their spatial resolution is generally not enough to separate
   different spectrally pure constituents (endmembers); as a result,
   several of them can be found in the same pixel. Spectral unmixing is an
   important technique for hyperspectral data exploitation, aimed at
   finding the spectral signatures of the endmembers and their associated
   abundance fractions. The development of techniques able to provide
   unmixing results in real-time is a long desired goal in the
   hyperspectral imaging community. In this paper, we describe a real-time
   hyperspectral unmixing chain based on three main steps: 1) estimation of
   the number of endmembers using the hyperspectral subspace identification
   with minimum error (HySime); 2) estimation of the spectral signatures of
   the endmembers using the vertex component analysis (VCA); and 3)
   unconstrained abundance estimation. We have developed new parallel
   implementations of the aforementioned algorithms and assembled them in a
   fully operative real-time unmixing chain using graphics processing units
   (GPUs), exploiting NVIDIA's compute unified device architecture (CUDA)
   and its basic linear algebra subroutines (CuBLAS) library, as well as
   OpenMP and BLAS for multicore parallelization. As a result, our
   real-time chain exploits both CPU (multicore) and GPU paradigms in the
   optimization. Our experiments reveal that this hybrid GPU-CPU parallel
   implementation fully meets real-time constraints in hyperspectral
   imaging applications.</abstract><date>FEB 2016</date><author>Torti, Emanuele
   Danese, Giovanni
   Leporati, Francesco
   Plaza, Antonio</author></paper><paper><title>An Aid to Generating Figures for the American Journal of Epidemiology
   Using SAS/GRAPH</title><abstract>Data visualization is an important tool that epidemiologists use to
   communicate with others in the field. The American Journal of
   Epidemiology recently acknowledged the importance of data visualization
   by inaugurating an award for the "Figure of the Year." Yet, creating
   figures that adhere to the standards of the Journal is a challenge. The
   purpose of the present article was to provide helpful hints for creating
   figures in SAS/GRAPH that meet the requirements of the Journal. It
   stresses 3 techniques: properly sizing figures overall, sizing text
   within a figure, and creating acceptable file formats. This information
   will prove useful to authors who create data-driven figures intended to
   be published in the Journal.</abstract><date>NOV 1 2015</date><author>McArdle, Patrick F.</author></paper><paper><title>Direct private query in location-based services with GPU run time
   analysis</title><abstract>Private query in location-based service allows users to request and
   receive nearest point of interest (POI) without revealing their location
   or object received. However, since the service is customized, it
   requires user-specific information. Problems arise when a user due to
   privacy or security concerns is unwilling to disclose this information.
   Previous solutions to hide them have been found to be deficient and
   sometimes inefficient. In this paper, we propose a novel idea that will
   partition objects into neighborhoods supported by database design that
   allows a user to retrieve the exact nearest POI without revealing its
   location, or the object retrieved. The paper is organized into two
   parts. In the first part, we adopted the concept of topological space to
   generalize object space. To help limit information disclosed and
   minimize transmission cost, we create disjointed neighborhoods such that
   each neighborhood contains no more than one object. We organize the
   database matrix to align with object location in the area. For
   optimization, we introduce the concept of kernel in graphical processing
   unit (GPU), and we then develop parallel implementation of our algorithm
   by utilizing the computing power of the streaming multiprocessors of GPU
   and the parallel computing platform and programming model of Compute
   Unified Device Architecture (CUDA). In the second part, we study serial
   implementation of our algorithm with respect to execution time and
   complexity. Our experiment shows a scalable design that is suitable for
   any population size with minimal impact to user experience. We also
   study GPU-CUDA parallel implementation and compared the performance with
   CPU serial processing. The results show 23.9 improvement of GPU over
   CPU. To help determine the optimal size for the parameters in our design
   or similar scalable algorithm, we provide analysis and model for
   predicting GPU execution time based on the size of the chosen parameter.</abstract><date>FEB 2015</date><author>Asanya, Charles
   Guha, Ratan</author></paper><paper><title>Microvascular anatomy of spinal dural arteriovenous fistulas:
   arteriovenous connections and their relationships with the dura mater</title><abstract>OBJECT The microvascular anatomy of spinal dural arteriovenous fistulas
   (AVFs), especially the relationships of the vessels with the dura mater,
   has yet to be angiographically demonstrated in detail and proven
   histologically.METHODS From January 2012 through April 2014, a total of
   7 patients with spinal dural AVFs in the thoracic region underwent open
   microsurgical obliteration at Tokyo Metropolitan Neurological Hospital.
   The microvascular anatomy of spinal dural AVFs was comprehensively
   assessed by using advanced microangiography, including 3D computer
   graphics and intraoperative indocyanine green video angiography, and by
   histological findings.RESULTS The 2 microangiography techniques revealed
   the spatial course and in vivo blood flow of the meningeal vessels and
   their relationships with the dura mater in sufficient detail. The
   meningeal branch of the intercostal artery split into multiple meningeal
   vessels on the outer dural surface adjacent to the root sleeve. After
   crossing the dura mater to the inner dural surface, these vessels
   gathered and joined a single intradural draining vessel. On the inner
   dural surface, the single draining vessel was fed by the surrounding
   multiple meningeal vessels, which appeared to be caput medusae.
   Histological findings revealed that the structure of the meningeal
   branch of the intercostal artery corresponded to that of a normal
   artery. The structure of intradural draining vessels corresponded to
   that of a vein modified by retrograde arterial inflow. On the inner
   dural surface, more than 1 meningeal artery gathered and joined with the
   proximal radiculomedullary vein.CONCLUSIONS Spinal dural AVFs are
   located on the inner dural surface, where multiple direct AV connections
   between more than 1 meningeal feeding artery and a single proximal
   radiculomedullary vein occur at the site where the vein connects to the
   dura mater.</abstract><date>OCT 2015</date><author>Takai, Keisuke
   Komori, Takashi
   Taniguchi, Makoto</author></paper><paper><title>Numerical investigation of the effect of boundary conditions for a
   highly rarefied gas flow using the GPU accelerated Boltzmann solver</title><abstract>The effect of the gas-surface interaction model on the rarefied gas flow
   between parallel plates is investigated on the basis of the Boltzmann
   kinetic equation. The Cercignani-Lampis model for diffuse scattering
   with incomplete energy accommodation is provided as the boundary
   condition on plates. The numerical analysis of the heat transfer problem
   between parallel plates with uniform and sinusoidal temperature
   distributions is carried out. The computational algorithm is adapted for
   solving the Boltzmann equation onto Graphics Processing Units (GPUs).
   The speedup of the GPU-accelerated computation is up to 50 times as
   compared to the CPU one. It was found that a non-uniform temperature
   distribution on plates induced a steady flow. In addition, the flow
   field strongly depends on the value of accommodation coefficients
   imposed in the Cercignani-Lampis model and this effect is more visible
   for high Knudsen numbers. Presented results are in good agreement with
   open literature ones obtained by means of the direct simulation Monte
   Carlo method. (C) 2014 Elsevier Ltd. All rights reserved.</abstract><date>MAR 30 2015</date><author>Rovenskaya, Olga
   Croce, Giulio</author></paper><paper><title>Ocean Wave Simulation Based on Wind Field</title><abstract>Ocean wave simulation has a wide range of applications in movies, video
   games and training systems. Wind force is the main energy resource for
   generating ocean waves, which are the result of the interaction between
   wind and the ocean surface. While numerous methods to handle simulating
   oceans and other fluid phenomena have undergone rapid development during
   the past years in the field of computer graphic, few of them consider to
   construct ocean surface height field from the perspective of wind force
   driving ocean waves. We introduce wind force to the construction of the
   ocean surface height field through applying wind field data and
   wind-driven wave particles. Continual and realistic ocean waves result
   from the overlap of wind-driven wave particles, and a strategy was
   proposed to control these discrete wave particles and simulate an
   endless ocean surface. The results showed that the new method is capable
   of obtaining a realistic ocean scene under the influence of wind fields
   at real time rates.</abstract><date>JAN 25 2016</date><author>Li, Zhongyi
   Wang, Hao</author></paper><paper><title>ASPECTS OF GENERATING 3D SURFACES WITH APPLICATIONS IN DRIVING
   SIMULATORS</title><abstract>Driving simulators are complex technical entities for reproducing the
   phenomena of real driving. One of their essential components is the
   visual simulator. For the purpose of creating the visual simulator it is
   of utmost importance to generate the 3D surfaces such that they simulate
   both the road and the environment. In this paper, the problem of terrain
   generation has been encountered. The generation of such surfaces,
   suitable with real ones, is a natural issue. In the last few decades
   much research was done and many attempts were made in order to generate
   random surfaces, as real looking as possible, theoretically justified
   and easy to model. The purpose of our research is to propose an
   alternative method for generation of 3D surfaces for different roads by
   using two uncommon algorithms which were initially designed for other
   purposes: the Douglas-Peucker algorithm and the so called onion peeling
   algorithm.</abstract><date>OCT-DEC 2015</date><author>Ilea, Lucian
   Munteanu, Ligia
   Dumitriu, Dan
   Dudescu, Mircea
   Brisan, Cornel
   Chiroiu, Veturia</author></paper><paper><title>GPU MrBayes V3.1: MrBayes on Graphics Processing Units for Protein
   Sequence Data</title><abstract>We present a modified GPU (graphics processing unit) version of MrBayes,
   called ta(MC)(3) (GPU MrBayes V3.1), for Bayesian phylogenetic inference
   on protein data sets. Our main contributions are 1) utilizing 64-bit
   variables, thereby enabling ta(MC)(3) to process larger data sets than
   MrBayes; and 2) to use Kahan summation to improve accuracy, convergence
   rates, and consequently runtime. Versus the current fastest software, we
   achieve a speedup of up to around 2.5 (and up to around 90 vs. serial
   MrBayes), and more on multi-GPU hardware. GPU MrBayes V3.1 is available
   from http://sourceforge.net/projects/mrbayes-gpu/.</abstract><date>SEP 2015</date><author>Pang, Shuai
   Stones, Rebecca J.
   Ren, Ming-Ming
   Liu, Xiao-Guang
   Wang, Gang
   Xia, Hong-ju
   Wu, Hao-Yang
   Liu, Yang
   Xie, Qiang</author></paper><paper><title>Collision detection of convex polyhedra on the NVIDIA GPU architecture
   for the discrete element method</title><abstract>Convex polyhedra represent granular media well. This geometric
   representation may be critical in obtaining realistic simulations of
   many industrial processes using the discrete element method (DEM).
   However detecting collisions between the polyhedra and surfaces that
   make up the environment and the polyhedra themselves is computationally
   expensive. This paper demonstrates the significant computational
   benefits that the graphical processor unit (GPU) offers DEM. As we show,
   this requires careful consideration due to the architectural differences
   between CPU and GPU platforms. This paper describes the DEM algorithms
   and heuristics that are optimized for the parallel NVIDIA Kepler GPU
   architecture in detail. This includes a GPU optimized collision
   detection algorithm for convex polyhedra based on the separating plane
   (SP) method. In addition, we present heuristics optimized for the
   parallel NVIDIA Kepler GPU architecture. Our algorithms have
   minimalistic memory requirements, which enables us to store data in the
   limited but high bandwidth constant memory on the GPU. We systematically
   verify the DEM implementation, where after we demonstrate the
   computational scaling on two large-scale simulations. We are able
   achieve a new performance level in DEM by simulating 34 million
   polyhedra on a single NVIDIA K6000 GPU. We show that by using the GPU
   with algorithms tailored for the architecture, large scale industrial
   simulations are possible on a single graphics card. (C) 2014 Elsevier
   Inc. All rights reserved.</abstract><date>SEP 15 2015</date><author>Govender, Nicolin
   Wilke, Daniel N.
   Kok, Schalk</author></paper><paper><title>Acceleration of discrete stochastic biochemical simulation using GPGPU</title><abstract>For systems made up of a small number of molecules, such as a
   biochemical network in a single cell, a simulation requires a stochastic
   approach, instead of a deterministic approach. The stochastic simulation
   algorithm (SSA) simulates the stochastic behavior of a spatially
   homogeneous system. Since stochastic approaches produce different
   results each time they are used, multiple runs are required in order to
   obtain statistical results; this results in a large computational cost.
   We have implemented a parallel method for using SSA to simulate a
   stochastic model; the method uses a graphics processing unit (GPU),
   which enables multiple realizations at the same time, and thus reduces
   the computational time and cost. During the simulation, for the purpose
   of analysis, each time course is recorded at each time step. A
   straightforward implementation of this method on a GPU is about 16 times
   faster than a sequential simulation on a CPU with hybrid
   parallelization; each of the multiple simulations is run simultaneously,
   and the computational tasks within each simulation are parallelized. We
   also implemented an improvement to the memory access and reduced the
   memory footprint, in order to optimize the computations on the GPU. We
   also implemented an asynchronous data transfer scheme to accelerate the
   time course recording function. To analyze the acceleration of our
   implementation on various sizes of model, we performed SSA simulations
   on different model sizes and compared these computation times to those
   for sequential simulations with a CPU. When used with the improved time
   course recording function, our method was shown to accelerate the SSA
   simulation by a factor of up to 130.</abstract><date>FEB 13 2015</date><author>Sumiyoshi, Kei
   Hirata, Kazuki
   Hiroi, Noriko
   Funahashi, Akira</author></paper><paper><title>Comparison of Acceleration Techniques for Selected Low-Level
   Bioinformatics Operations</title><abstract>Within the recent years clock rates of modern processors stagnated while
   the demand for computing power continued to grow. This applied
   particularly for the fields of life sciences and bioinformatics, where
   new technologies keep on creating rapidly growing piles of raw data with
   increasing speed. The number of cores per processor increased in an
   attempt to compensate for slight increments of clock rates. This
   technological shift demands changes in software development, especially
   in the field of high performance computing where parallelization
   techniques are gaining in importance due to the pressing issue of large
   sized datasets generated by e.g., modern genomics. This paper presents
   an overview of state-of-the-art manual and automatic acceleration
   techniques and lists some applications employing these in different
   areas of sequence informatics. Furthermore, we provide examples for
   automatic acceleration of two use cases to show typical problems and
   gains of transforming a serial application to a parallel one. The paper
   should aid the reader in deciding for a certain techniques for the
   problem at hand. We compare four different state-of-the-art automatic
   acceleration approaches (OpenMP, PluTo-SICA, PPCG, and OpenACC). Their
   performance as well as their applicability for selected use cases is
   discussed. While optimizations targeting the CPU worked better in the
   complex k-mer use case, optimizers for Graphics Processing Units (GPUs)
   performed better in the matrix multiplication example. But performance
   is only superior at a certain problem size due to data migration
   overhead. We show that automatic code parallelization is feasible with
   current compiler software and yields significant increases in execution
   speed. Automatic optimizers for CPU are mature and usually no additional
   manual adjustment is required. In contrast, some automatic parallelizers
   targeting GPUs still lack maturity and are limited to simple statements
   and structures.</abstract><date>FEB 10 2016</date><author>Langenkaemper, Daniel
   Jakobi, Tobias
   Feld, Dustin
   Jelonek, Lukas
   Goesmann, Alexander
   Nattkemper, Tim W.</author></paper><paper><title>Association between Lamina Cribrosa Position Change and Glaucomatous
   Visual Field Progression</title><abstract></abstract><date>JUN 2015</date><author>Abumasmah, Ramiz
   Ren, Ruojin
   Ghassibi, Mark
   Chien, Jason L.
   Adleyba, Olga
   Tello, Celso
   Liebmann, Jeffrey M.
   Ritch, Robert
   Park, Sung Chul (Sean)</author></paper><paper><title>Mobile Volume Rendering: Past, Present and Future</title><abstract>Volume rendering has been a relevant topic in scientific visualization
   for the last decades. However, the exploration of reasonably big volume
   datasets requires considerable computing power, which has limited this
   field to the desktop scenario. But the recent advances in mobile
   graphics hardware have motivated the research community to overcome
   these restrictions and to bring volume graphics to these ubiquitous
   handheld platforms. This survey presents the past and present work on
   mobile volume rendering, and is meant to serve as an overview and
   introduction to the field. It proposes a classification of the current
   efforts and covers aspects such as advantages and issues of the mobile
   platforms, rendering strategies, performance and user interfaces. The
   paper ends by highlighting promising research directions to motivate the
   development of new and interesting mobile volume solutions.</abstract><date>FEB 2016</date><author>Noguera, Jose M.
   Roberto Jimenez, J.</author></paper><paper><title>Synthesis, molecular docking and biological evaluation of novel
   bis-pyrazole derivatives for analgesic, anti-inflammatory and
   antimicrobial activities</title><abstract>A new series of bis-pyrazoles were synthesized by Michael addition of
   hydrazine to chalcones. The starting-material-substituted acetophenones
   required for the synthesis of chalcones were prepared from itaconic
   anhydride. The newly synthesized compounds were characterized by IR,
   H-1-NMR, C-13-NMR, mass spectral and analytical data. All the
   synthesized compounds were evaluated for in vivo analgesic,
   anti-inflammatory and in vitro antimicrobial activities. Among the
   tested compounds, 5a, 5b and 5d showed potential anti-inflammatory and
   analgesic activities. Further anti-inflammatory results were supported
   by in silico docking study, in which tested bis-pyrazoles were found to
   be more selective toward COX-2 (PDB ID: 1CX2) rather than COX-1 (PDB ID:
   1CQE). The LD50 values for these products 5(a-l) showed a high safety
   margin with a dose level &gt; 2000 mg/kg. Among all synthesized compounds,
   N-[4-(5-(4-bromophenyl)-1-phenyl-1H-pyrazol-3-yl)phenyl-2-(3-hydroxy-1-p
   henyl-1H-pyrazol-4-yl)] acetamide (5b) emerged as most potent molecule
   with anti-inflammatory, analgesic and antimicrobial
   properties.[GRAPHICS].</abstract><date>DEC 2015</date><author>Nayak, Prakash S.
   Narayana, B.
   Sarojini, B. K.
   Fernades, Jennifer
   Bharath, B. R.
   Madhu, L. N.</author></paper><paper><title>Numerical Simulation of Melting with Natural Convection Based on Lattice
   Boltzmann Method and Performed with CUDA Enabled GPU</title><abstract>A new solver is developed to numerically simulate the melting phase
   change with natural convection. This solver was implemented on a single
   Nvidia GPU based on the CUDA technology in order to simulate the melting
   phase change in a 2D rectangular enclosure. The Rayleigh number is of
   the order of magnitude of 10(8) and Prandlt is 50. The hybrid thermal
   lattice Boltzmann method (HTLBM) is employed to simulate the natural
   convection in the liquid phase, and the enthalpy formulation is used to
   simulate the phase change aspect. The model is validated by experimental
   data and published analytic results. The simulation results manifest a
   strong convection in the melted phase and a different flow pattern from
   the reference results with low Rayleigh number. In addition, the
   computational performance is estimated for single precision arithmetic,
   and this solver yields 703.31 MLUPS and 61.89GB/s device to device data
   throughput on a Nvidia Tesla C2050 GPU.</abstract><date>MAY 2015</date><author>Gong, Wei
   Johannes, Kevyn
   Kuznik, Frederic</author></paper><paper><title>Rear obstacle detection system with fisheye stereo camera using HCT</title><abstract>A vision based rear obstacle detection system is one of the most
   essential technologies, which can be used in many applications such as a
   parking assistance systems and intelligent vehicles. Stereo matching
   based obstacle detection methods have two inherent limitations,
   including sensitivity to illumination variation and high computational
   complexity. To overcome these problems, we present a hierarchical census
   transform (HCT)-based stereo matching method, and proposes a real-time
   rear obstacle detection system using fisheye stereo cameras. It includes
   two major components: firstly, we use a hierarchical approach to improve
   computational efficiency and reduce dependency on a matching window
   size. Computation time can also be accelerated with compute unified
   device architecture (CUDA) implementation because we designed the
   proposed method for parallel processing. Secondly, our method provides
   more accurate depth information, especially for moving objects, through
   cost aggregation using color and motion information. Experimental
   results show satisfactory performance under various real parking
   environments. The performance of the proposed method is superior to
   those of the conventional methods in terms of runtime and accuracy of
   depth estimation. Moreover, correct detection rate of the proposed
   system is 12.51% better than those of other systems, while its false and
   miss detection rates are 11.09% and 12.51% lower than those of other
   detection systems. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>OCT 2015</date><author>Kim, Deukhyeon
   Choi, Jinwook
   Yoo, Hunjae
   Yang, Ukil
   Sohn, Kwanghoon</author></paper><paper><title>Path-space Motion Estimation and Decomposition for Robust Animation
   Filtering</title><abstract>Renderings of animation sequences with physics-based Monte Carlo light
   transport simulations are exceedingly costly to generate frame-by-frame,
   yet much of this computation is highly redundant due to the strong
   coherence in space, time and among samples. A promising approach pursued
   in prior work entails subsampling the sequence in space, time, and
   number of samples, followed by image-based spatio-temporal upsampling
   and denoising. These methods can provide significant performance gains,
   though major issues remain: firstly, in a multiple scattering
   simulation, the final pixel color is the composite of many different
   light transport phenomena, and this conflicting information causes
   artifacts in image-based methods. Secondly, motion vectors are needed to
   establish correspondence between the pixels in different frames, but it
   is unclear how to obtain them for most kinds of light paths (e.g. an
   object seen through a curved glass panel). To reduce these ambiguities,
   we propose a general decomposition framework, where the final pixel
   color is separated into components corresponding to disjoint subsets of
   the space of light paths. Each component is accompanied by motion
   vectors and other auxiliary features such as reflectance and surface
   normals. The motion vectors of specular paths are computed using a
   temporal extension of manifold exploration and the remaining components
   use a specialized variant of optical flow. Our experiments show that
   this decomposition leads to significant improvements in three
   image-based applications: denoising, spatial upsampling, and temporal
   interpolation.</abstract><date>JUL 2015</date><author>Zimmer, Henning
   Rousselle, Fabrice
   Jakob, Wenzel
   Wang, Oliver
   Adler, David
   Jarosz, Wojciech
   Sorkine-Hornung, Olga
   Sorkine-Hornung, Alexander</author></paper><paper><title>Localized discrete Laplace-Beltrami operator over triangular mesh</title><abstract>The Laplace-Beltrami operator is the foundation of describing geometric
   partial differential equations, and it also plays an important role in
   the fields of computational geometry, computer graphics and image
   processing, such as surface parameterization, shape analysis, matching
   and interpolation. However, constructing the discretized
   Laplace-Beltrami operator with convergent property has been an open
   problem. In this paper we propose a new discretization scheme of the
   Laplace-Beltrami operator over triangulated surfaces. We prove that our
   discretization of the Laplace-Beltrami operator converges to the
   Laplace-Beltrami operator at every point of an arbitrary smooth surface
   as the size of the triangular mesh over the surface tends to zero.
   Numerical experiments are conducted, which support the theoretical
   analysis. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Li, Xinge
   Xu, Guoliang
   Zhang, Yongjie Jessica</author></paper><paper><title>Blind Multirigid Retrospective Motion Correction of MR Images</title><abstract>PurposePhysiological nonrigid motion is inevitable when imaging, e.g.,
   abdominal viscera, and can lead to serious deterioration of the image
   quality. Prospective techniques for motion correction can handle only
   special types of nonrigid motion, as they only allow global correction.
   Retrospective methods developed so far need guidance from navigator
   sequences or external sensors. We propose a fully retrospective nonrigid
   motion correction scheme that only needs raw data as an input.MethodsOur
   method is based on a forward model that describes the effects of
   nonrigid motion by partitioning the image into patches with locally
   rigid motion. Using this forward model, we construct an objective
   function that we can optimize with respect to both unknown motion
   parameters per patch and the underlying sharp image.ResultsWe evaluate
   our method on both synthetic and real data in 2D and 3D. In vivo data
   was acquired using standard imaging sequences. The correction algorithm
   significantly improves the image quality. Our compute unified device
   architecture (CUDA)-enabled graphic processing unit implementation
   ensures feasible computation times.ConclusionThe presented technique is
   the first computationally feasible retrospective method that uses the
   raw data of standard imaging sequences, and allows to correct for
   nonrigid motion without guidance from external motion sensors. Magn
   Reson Med 73:1457-1468, 2015. (c) 2014 Wiley Periodicals, Inc.</abstract><date>APR 2015</date><author>Loktyushin, Alexander
   Nickisch, Hannes
   Pohmann, Rolf
   Schoelkopf, Bernhard</author></paper><paper><title>Fast GPU-based Monte Carlo simulations for LDR prostate brachytherapy</title><abstract>The aim of this study was to evaluate the potential of bGPUMCD, a Monte
   Carlo algorithm executed on Graphics Processing Units (GPUs), for fast
   dose calculations in permanent prostate implant dosimetry. It also aimed
   to validate a low dose rate brachytherapy source in terms of TG-43
   metrics and to use this source to compute dose distributions for
   permanent prostate implant in very short times.The physics of bGPUMCD
   was reviewed and extended to include Rayleigh scattering and
   fluorescence from photoelectric interactions for all materials involved.
   The radial and anisotropy functions were obtained for the Nucletron
   SelectSeed in TG-43 conditions. These functions were compared to those
   found in the MD Anderson Imaging and Radiation Oncology Core
   brachytherapy source registry which are considered the TG-43 reference
   values. After appropriate calibration of the source, permanent prostate
   implant dose distributions were calculated for four patients and
   compared to an already validated Geant4 algorithm.The radial function
   calculated from bGPUMCD showed excellent agreement (differences within
   1.3%) with TG-43 accepted values. The anisotropy functions at r = 1 cm
   and r = 4 cm were within 2% of TG-43 values for angles over 17.5
   degrees. For permanent prostate implants, Monte Carlo-based dose
   distributions with a statistical uncertainty of 1% or less for the
   target volume were obtained in 30 s or less for 1 x 1 x 1 mm(3)
   calculation grids. Dosimetric indices were very similar (within 2.7%) to
   those obtained with a validated, independent Monte Carlo code (Geant4)
   performing the calculations for the same cases in a much longer time
   (tens of minutes to more than a hour).bGPUMCD is a promising code that
   lets envision the use of Monte Carlo techniques in a clinical
   environment, with sub-minute execution times on a standard workstation.
   Future work will explore the use of this code with an inverse planning
   method to provide a complete Monte Carlo-based planning solution.</abstract><date>JUL 7 2015</date><author>Bonenfant, Eric
   Magnoux, Vincent
   Hissoiny, Sami
   Ozell, Benoit
   Beaulieu, Luc
   Despres, Philippe</author></paper><paper><title>Fast quantum Monte Carlo on a GPU</title><abstract>We present a scheme for the parallelization of quantum Monte Carlo
   method on graphical processing units, focusing on variational Monte
   Carlo simulation of bosonic systems. We use asynchronous execution
   schemes with shared memory persistence, and obtain an excellent
   utilization of the accelerator. The CUDA code is provided along with a
   package that simulates liquid helium-4. The program was benchmarked on
   several models of Nvidia CPU, including Fermi GTX560 and M2090, and the
   Kepler architecture 1(20 GPU. Special optimization was developed for the
   Kepler cards, including placement of data structures in the register
   space of the Kepler GPUs. Kepler-specific optimization is
   discussed.Program SummaryProgram title: QLCatalogue identifier:
   AEUP_v1_0Program summary URL:
   http://cpc.cs.qub.ac.uk/summaries/AEUP_v1_0.htmlProgram obtainable from:
   CPC Program Library, Queen's University, Belfast, N. IrelandLicensing
   provisions: Standard CPC licence,
   http://cpc.cs.qub.ac.uk/licence/licence.htmlNo. of lines in,distributed
   program, including test data, etc.: 40170No. of bytes in distributed
   program, including test data, etc.: 1223080Distribution format:
   tar.gzProgramming language: CUDA-C, C, Fortran.Computer: Any computer
   with a CUDA-enabled GPU.Operating system: Linux.RAM: Typical execution
   uses as much RAM as is available on the CPU; usually between 1 GB and 12
   GB. Minimal requirement is I MB.Classification: 4.12, 7.7.Nature
   ofproblem: QL package executes variational Monte Carlo for liquid
   helium-4 with Aziz II interaction potential and a Jastrow pair product
   trial wavefunction. Sampling is performed with a Metropolis scheme
   applied to single-particle updates. With minimal changes, the package
   can be applied to other bosonic fluids, given a pairwise interaction
   potential and a wavefunction in the form of a product of one- and
   two-body correlation factors.Solution method: The program is
   parallelized for execution with Nvidia GPU. By design, the generation of
   new configurations is performed with shared memory persistence and the
   asynchronous execution allows for the CPU load masking.Restrictions:
   Code is limited to variational Monte Carlo. Due to the limitation of the
   shared memory of GPU, only systems under 2000 particles can be treated
   on the Fermi generation cards, and up to 10000 on Kepler cards.Running
   time: Because of the statistical nature of Monte Carlo calculations,
   computations may be chained indefinitely to improve statistical
   accuracy. As an example, using the QL package, the energy of a liquid
   helium system with 1952 atoms can be computed to within 1 mK per atom in
   less than 20 min. This corresponds to the relative error of 10(-4). It
   is unlikely that a higher accuracy may be needed. (C) 2014 Published by
   Elsevier B.V.</abstract><date>FEB 2015</date><author>Lutsyshyn, Y.</author></paper><paper><title>Fast Narrow-Baseline Stereo Matching Using CUDA Compatible GPUs</title><abstract>The Phase Correlation(PC) method demonstrates high robustness and
   accuracy for measuring the very subtle disparities from stereo image
   pairs, where the baseline (or the base-to-height ratio) is
   unconventionally narrowed. However, this method remains inherently
   computationally expensive. In this paper, an adaptive PC based stereo
   matching method is proposed, aiming to achieve higher speed and better
   stereo quality compared to the existing methods, while also preserving
   the quality of PC. Improvement was achieved both algorithmically and
   architecturally, via carefully dividing the computing tasks among
   multiprocessors of the GPUs under a novel global-pixel correlation
   framework. Experimental results on our hardware settings show that the
   method achieves as high as 64x and 24x speedup compared to single
   threaded and multi-threaded implementation running on a multi-core CPU
   system, respectively.</abstract><date>2015</date><author>Chen, Tao
   Liu, Yiguang
   Li, Jie
   Wu, Pengfei</author></paper><paper><title>Multi-frequency sweeping method for periodic steady-state computations
   on the graphics processor unit</title><abstract>This paper presents the parallelization of the multi-frequency hybrid
   backward/forward sweeping (BFS) technique on a graphics processor unit
   (GPU). Primarily, the intrinsic layer structure of a radial network,
   typical topology of distribution systems, and its multi-frequency
   behavior are exploited for parallelization of the hybrid BFS method on
   the GPU. The less computational demanding tasks, e.g., error computation
   and simple vectorized operations, are assigned to the CPU. The network
   solution is performed in the Matlab (R) environment using compute
   unified device architecture (CUDA). The computational time required by
   the GPU/CPU BFS implementation is compared with a CPU-only program by
   solving four networks of different sizes. Validation of the
   multi-frequency BFS results is made through a CPU implementation of a
   Newton-type solution scheme. The significant reduction in the
   computational time of the parallelized GPU implementation of the hybrid
   NS method combined with its ability to include a wide range of
   frequencies and to handle nonlinear components makes it suitable for
   real-time online applications. (C) 2014 Elsevier B.V. All rights
   reserved.</abstract><date>APR 2015</date><author>Morales-Aguilar, Eric
   Ramirez, Abner
   Matar, Mahmoud</author></paper><paper><title>GPU implementation of a parallel two-list algorithm for the subset-sum
   problem</title><abstract>The subset-sum problem is a well-known non-deterministic polynomial-time
   complete (NP-complete) decision problem. This paper proposes a novel and
   efficient implementation of a parallel two-list algorithm for solving
   the problem on a graphics processing unit (GPU) using Compute Unified
   Device Architecture (CUDA). The algorithm is composed of a generation
   stage, a pruning stage, and a search stage. It is not easy to
   effectively implement the three stages of the algorithm on a GPU. Ways
   to achieve better performance, reasonable task distribution between CPU
   and GPU, effective GPU memory management, and CPU-GPU communication cost
   minimization are discussed. The generation stage of the algorithm adopts
   a typical recursive divide-and-conquer strategy. Because recursion
   cannot be well supported by current GPUs with compute capability less
   than 3.5, a new vector-based iterative implementation mechanism is
   designed to replace the explicit recursion. Furthermore, to optimize the
   performance of the GPU implementation, this paper improves the three
   stages of the algorithm. The experimental results show that the GPU
   implementation has much better performance than the CPU implementation
   and can achieve high speedup on different GPU cards. The experimental
   results also illustrate that the improved algorithm can bring
   significant performance benefits for the GPU implementation. Copyright
   (C) 2014 John Wiley &amp; Sons, Ltd.</abstract><date>JAN 2015</date><author>Wan, Lanjun
   Li, Kenli
   Liu, Jing
   Li, Keqin</author></paper><paper><title>A Collection-Oriented Programming Model for Performance Portability</title><abstract>This paper describes Surge, a collection-oriented programming model that
   enables programmers to compose parallel computations using nested
   high-level data collections and operators. Surge exposes a code
   generation interface, decoupled from the core computation, that enables
   programmers and autotuners to easily generate multiple implementations
   of the same computation on various parallel architectures such as
   multi-core CPUs and GPUs. By decoupling computations from
   architecture-specific implementation, programmers can target multiple
   architectures more easily, and generate a search space that facilitates
   optimization and customization for specific architectures. We express in
   Surge four real-world benchmarks from domains such as sparse
   linear-algebra and machine learning and from the same
   performance-portable specification, generate OpenMP and CUDA C++
   implementations. Surge generates efficient, scalable code which achieves
   up to 1.32x speedup over handcrafted, well-optimized CUDA code.</abstract><date>AUG 2015</date><author>Muralidharan, Saurav
   Garland, Michael
   Catanzaro, Bryan
   Sidelnik, Albert
   Hall, Mary</author></paper><paper><title>Analysis of a Grounding System Under a Lightning Stroke</title><abstract>This paper studies the electric response of a common lattice tower when
   it is hit by a lightning stroke. Although some studies have been already
   done about this subject, the novelty of this one lies in the usage of a
   highly accurate electric model of an actual lattice tower. This model
   allows to calculate the impedance variation with different frequencies
   by using an adaptation of the well-known method of moments. Moreover,
   the authors also analyze how the impedance is affected when the
   grounding parameters and the connection to ground conditions are
   modified. These results are lately used to explain the electric behavior
   of the tower under two kinds of lightning impulses and different
   grounding configurations.</abstract><date>NOV-DEC 2015</date><author>Vilacha, C.
   Otero, Antonio F.
   Moreira, J. C.
   Miguez, E.</author></paper><paper><title>Splicing Express: a software suite for alternative splicing analysis
   using next-generation sequencing data</title><abstract>Motivation. Alternative splicing events (ASEs) are prevalent in the
   transcriptome of eukaryotic species and are known to influence many
   biological phenomena. The identification and quantification of these
   events are crucial for a better understanding of biological processes.
   Next-generation DNA sequencing technologies have allowed deep
   characterization of transcriptomes and made it possible to address these
   issues. ASEs analysis, however, represents a challenging task especially
   when many different samples need to be compared. Some popular tools for
   the analysis of ASEs are known to report thousands of events without
   annotations and/or graphical representations. A new tool for the
   identification and visualization of ASEs is here described, which can be
   used by biologists without a solid bioinformatics background.Results. A
   software suite named Splicing Express was created to perform ASEs
   analysis from transcriptome sequencing data derived from next-generation
   DNA sequencing platforms. Its major goal is to serve the needs of
   biomedical researchers who do not have bioinformatics skills. Splicing
   Express performs automatic annotation of transcriptome data (GTF files)
   using gene coordinates available from the UCSC genome browser and allows
   the analysis of data from all available species. The identification of
   ASEs is done by a known algorithm previously implemented in another tool
   named Splooce. As a final result, Splicing Express creates a set of HTML
   files composed of graphics and tables designed to describe the
   expression profile of ASEs among all analyzed samples. By using RNA-Seq
   data from the Illumina Human Body Map and the Rat Body Map, we show that
   Splicing Express is able to perform all tasks in a straightforward way,
   identifying well-known specific events.Availability and Implementation.
   Splicing Express is written in Perl and is suitable to run only in
   UNIX-like systems.</abstract><date>NOV 19 2015</date><author>Kroll, Jose E.
   Kim, Jihoon
   Ohno-Machado, Lucila
   de Souza, Sandro J.</author></paper><paper><title>Development of genome-wide insertion/deletion markers in rice based on
   graphic pipeline platform</title><abstract>DNA markers play important roles in plant breeding and genetics. The
   Insertion/Deletion (InDel) marker is one kind of co-dominant DNA markers
   widely used due to its low cost and high precision. However, the
   canonical way of searching for InDel markers is time-consuming and
   labor-intensive. We developed an end-to-end computational solution
   (InDel Markers Development Platform, IMDP) to identify genome-wide InDel
   markers under a graphic pipeline environment. IMDP constitutes assembled
   genome sequences alignment pipeline (AGA-pipe) and next-generation
   re-sequencing data mapping pipeline (NGS-pipe). With AGA-pipe we are
   able to identify 12,944 markers between the genome of rice cultivars
   Nipponbare and 93-11. Using NGS-pipe, we reported 34,794 InDels from
   re-sequencing data of rice cultivars Wu-Yun-Geng7 and Guang-Lu-Ai4.
   Combining AGA-pipe and NGS-pipe, we developed 205,659 InDels in eight
   japonica and nine indica cultivars and 2,681 InDels showed a
   subgroup-specific pattern. Polymerase chain reaction (PCR) analysis of
   subgroup-specific markers indicated that the precision reached 90% (86
   of 95). Finally, to make them available to the public, we have
   integrated the InDels/markers information into a website (Rice InDel
   Marker Database, RIMD, ). The application of IMDP in rice will
   facilitate efficiency for development of genome-wide InDel markers, in
   addition it can be used in other species with reference genome sequences
   and NGS data.</abstract><date>NOV 2015</date><author>Lu, Yang
   Cui, Xiao
   Li, Rui
   Huang, Piaopiao
   Zong, Jie
   Yao, Danqing
   Li, Gang
   Zhang, Dabing
   Yuan, Zheng</author></paper><paper><title>AVScreen: a real-time video augmentation method</title><abstract>We present a tool for video augmentation in real-time, which we name the
   augmentation virtual screen (AVScreen). AVScreen is useful for
   developing advertisements, commercials, music videos, movies, etc. The
   main challenges for augmenting videos, in contrast to fixed images, is
   that moving objects in the foreground may occlude the region to be
   augmented in the background and that the composition can be affected by
   camera movements. Therefore, we use a procedure for
   foreground-background video segmentation in order to deal with such
   occlusions. Comparisons with foreground-background video segmentation
   methods of the state of the art in both accuracy and computational
   efficiency support our choice: we reduce around 70 % of the segmentation
   error in a popular benchmark database and achieve real-time performance.
   Moreover, a new stabilization method to augment unstable camera videos
   is presented. For augmenting video shots, we present an efficient
   graph-based method for panorama (mosaic) computation. The real-time
   performance is reached by implementing high computational demanding
   procedures in GPU. The frame rate of our method is 18 frames per second
   for a video size of 640 x 480 pixels.</abstract><date>JUN 2015</date><author>Hernandez-Lopez, Francisco J.
   Rivera, Mariano</author></paper><paper><title>NDPA: A generalized efficient parallel in-place N-Dimensional
   Permutation Algorithm</title><abstract>N-dimensional transpose/permutation is a very important operation in
   many large-scale data intensive and scientific applications. These
   applications include but not limited to oil industry i.e. seismic data
   processing, nuclear medicine, media production, digital signal
   processing and business intelligence. This paper proposes an efficient
   in-place N-dimensional permutation algorithm. The algorithm is based on
   a novel 3D transpose algorithm that was published recently. The proposed
   algorithm has been tested on 3D, 4D, 5D, 6D and 7D data sets as a proof
   of concept. This is the first contribution which is breaking the
   dimensions' limitation of the base algorithm. The suggested algorithm
   exploits the idea of mixing both logical and physical permutations
   together. In the logical permutation, the address map is transposed for
   each data unit access. In the physical permutation, actual data elements
   are swapped. Both permutation levels exploit the fast on-chip memory
   bandwidth by transferring large amount of data and allowing for
   fine-grain SIMD (Single Instruction, Multiple Data) operations. Thus,
   the performance is improved as evident from the experimental results
   section. The algorithm is implemented on NVidia GeForce GTS 250 GPU
   (Graphics Processing Unit) containing 128 cores. The rapid increase in
   GPUs performance coupled with the recent and continuous improvements in
   its programmability proved that GPUs are the right choice for
   computationally demanding tasks. The use of GPUs is the second
   contribution which reflects how strongly they fit for high performance
   tasks. The third contribution is improving the proposed algorithm
   performance to its peak as discussed in the results section. (c) 2015
   Faculty of Engineering, Alexandria University. Production and hosting by
   Elsevier B.V.</abstract><date>SEP 2015</date><author>Ali, Muhammad Elsayed
   El-shehaby, Saleh
   Abougabal, Mohamed S.</author></paper><paper><title>Expanding social mobile games beyond the device screen</title><abstract>Emerging pervasive games use sensors, graphics and networking
   technologies to provide immersive game experiences integrated with the
   real world. Existing pervasive games commonly rely on a device screen
   for providing game-related information, while overlooking opportunities
   to include new types of contextual interactions like jumping, a punching
   gesture, or even voice to be used as game inputs. We present the design
   of Spellbound, a physical mobile team-based game, to help contribute to
   our understanding of how we can design pervasive games that aim to
   nurture a spirit of togetherness. We also briefly touch upon how
   togetherness and playfulness can transform physical movement into a
   desirable activity in the user evaluation section. Spellbound is an
   outdoor pervasive team-based physical game. It takes advantage of the
   above-mentioned opportunities and integrates real-world actions like
   jumping and spinning with a virtual world. It also replaces touch-based
   input with voice interaction and provides glanceable and haptic feedback
   using custom hardware in the true spirit of social play characteristic
   of traditional children's games. We believe Spellbound is a form of
   digital outdoor gaming that anchors enjoyment on physical action, social
   interaction, and tangible feedback. Spellbound was well received in user
   evaluation playtests which confirmed that the main design objective of
   enhancing a sense of togetherness was largely met.</abstract><date>JUL 2015</date><author>Sra, Misha
   Schmandt, Chris</author></paper><paper><title>In Acute Myocardial Infarction Liver Parameters Are Associated With
   Stenosis Diameter</title><abstract>Detection of high-risk subjects in acute myocardial infarction (AMI) by
   noninvasive means would reduce the need for intracardiac catheterization
   and associated complications. Liver enzymes are associated with
   cardiovascular disease risk. A potential predictive value for liver
   serum markers for the severity of stenosis in AMI was analyzed.Patients
   with AMI undergoing percutaneous coronary intervention (PCI; n = 437)
   were retrospectively evaluated. Minimal lumen diameter (MLD) and percent
   stenosis diameter (SD) were determined from quantitative coronary
   angiography. Patients were classified according to the severity of
   stenosis (SD &gt;= 50%, n = 357; SD &lt; 50%, n = 80). Routine heart and liver
   parameters were associated with SD using random forests (RF). A
   prediction model (M10) was developed based on parameter importance
   analysis in RF.Age, alkaline phosphatase (AP), aspartate
   aminotransferase (AST), and MLD differed significantly between SD &gt;= 50
   and SD &lt; 50. Age, AST, alanine aminotransferase (ALT), and troponin
   correlated significantly with SD, whereas MLD correlated inversely with
   SD. M10 (age, BMI, AP, AST, ALT, gamma-glutamyltransferase, creatinine,
   troponin) reached an AUC of 69.7% (CI 63.8-75.5%, P &lt; 0.0001).Routine
   liver parameters are associated with SD in AMI. A small set of
   noninvasively determined parameters can identify SD in AMI, and might
   avoid unnecessary coronary angiography in patients with low risk. The
   model can be accessed via[GRAPHICS].</abstract><date>FEB 2016</date><author>Baars, Theodor
   Neumann, Ursula
   Jinawy, Mona
   Hendricks, Stefanie
   Sowa, Jan-Peter
   Kaelsch, Julia
   Riemenschneider, Mona
   Gerken, Guido
   Erbel, Raimund
   Heider, Dominik
   Canbay, Ali</author></paper><paper><title>Visual Perception of Procedural Textures: Identifying Perceptual
   Dimensions and Predicting Generation Models</title><abstract>Procedural models are widely used in computer graphics for generating
   realistic, natural-looking textures. However, these mathematical models
   are not perceptually meaningful, whereas the users, such as artists and
   designers, would prefer to make descriptions using intuitive and
   perceptual characteristics like "repetitive," "directional,"
   "structured," and so on. To make up for this gap, we investigated the
   perceptual dimensions of textures generated by a collection of
   procedural models. Two psychophysical experiments were conducted:
   free-grouping and rating. We applied Hierarchical Cluster Analysis (HCA)
   and Singular Value Decomposition (SVD) to discover the perceptual
   features used by the observers in grouping similar textures. The results
   suggested that existing dimensions in literature cannot accommodate
   random textures. We therefore utilized isometric feature mapping
   (Isomap) to establish a three-dimensional perceptual texture space which
   better explains the features used by humans in texture similarity
   judgment. Finally, we proposed computational models to map perceptual
   features to the perceptual texture space, which can suggest a procedural
   model to produce textures according to user-defined perceptual scales.</abstract><date>JUN 24 2015</date><author>Liu, Jun
   Dong, Junyu
   Cai, Xiaoxu
   Qi, Lin
   Chantler, Mike</author></paper><paper><title>Revised spectral matching algorithm for scenes with mutually
   inconsistent local transformations</title><abstract>Spectral matching (SM) is an efficient and effective greedy algorithm
   for solving the graph matching problem in feature correspondence in
   computer vision and graphics. However, the classic SM algorithm cannot
   extract correspondences well when the affinity matrix is sparse and
   reducible (i.e. its corresponding graph is not connected). This case
   often happens when the geometric deformations consist of transformations
   with local inconsistency. The authors analyse this problem and show how
   the original SM could fail in this scenario. Then, the authors propose a
   revised two-step pipeline to tackle this issue: (1) decompose the
   mutually inconsistent local deformations into several consistent
   transformations which can be solved by individual SM; (2) filter out
   incorrect correspondences through an automatic thresholding. The authors
   perform experiments to demonstrate that this modification can
   effectively handle the coarse correspondence computation in shape or
   image registration where the global transformation consists of multiple
   inconsistent local transformations.</abstract><date>OCT 2015</date><author>Chen, Peizhi
   Li, Xin</author></paper><paper><title>A Virtual Reality System for PTCD Simulation Using Direct Visuo-Haptic
   Rendering of Partially Segmented Image Data.</title><abstract>This study presents a new visuo-haptic virtual reality (VR) training and
   planning system for percutaneous transhepatic cholangio-drainage (PTCD)
   based on partially segmented virtual patient models. We only use
   partially segmented image data instead of a full segmentation and
   circumvent the necessity of surface or volume mesh models. Haptic
   interaction with the virtual patient during virtual palpation,
   ultrasound probing and needle insertion is provided. Furthermore, the VR
   simulator includes X-ray and ultrasound simulation for image-guided
   training. The visualization techniques are GPU-accelerated by
   implementation in Cuda and include real-time volume deformations
   computed on the grid of the image data. Computation on the image grid
   enables straightforward integration of the deformed image data into the
   visualization components. To provide shorter rendering times, the
   performance of the volume deformation algorithm is improved by a
   multigrid approach. To evaluate the VR training system, a user
   evaluation has been performed and deformation algorithms are analyzed in
   terms of convergence speed with respect to a fully converged solution.
   The user evaluation shows positive results with increased user
   confidence after a training session. It is shown that using partially
   segmented patient data and direct volume rendering is suitable for the
   simulation of needle insertion procedures such as PTCD. </abstract><date>2016-Jan</date><author>Fortmeier, Dirk
   Mastmeyer, Andre
   Schroder, Julian
   Handels, Heinz</author></paper><paper><title>GPUs and chaos: a new true random number generator</title><abstract>For applications where security and unpredictability is of utmost
   importance, true random number generators (TRNGs) play a heavy role
   compared to its pseudo-random counterparts. Most TRNGs obtain randomness
   from physical phenomena such as radio noise, radioactive decay or
   thermal noise that are unpredictable. These applications usually require
   external hardware to extract entropy and convert them into digital
   signals. This paper introduces a TRNGs that utilizes graphics processing
   units as the source of entropy. Its unpredictable behavior is harnessed
   by computing chaotic maps that are highly sensitive to slight changes to
   their control parameters and have pseudo-random behavior. A simple
   post-processing function based on modular addition and XOR is then used
   to achieve an unbiased output. The security of the proposed TRNG is
   evaluated using statistical test suites such as the NIST SP 800-22,
   DIEHARD and ENT, as well as entropy analysis to determine
   unpredictability. Results indicate that the proposed TRNG has strong
   statistical quality of random numbers and high throughput without the
   need of external specialized equipment.</abstract><date>DEC 2015</date><author>Teh, Je Sen
   Samsudin, Azman
   Al-Mazrooie, Mishal
   Akhavan, Amir</author></paper><paper><title>GPU Implementation of the Simplex Identification via Split Augmented
   Lagrangian</title><abstract>Hyperspectral imaging can be used for object detection and for
   discriminating between different objects based on their spectral
   characteristics. One of the main problems of hyperspectral data analysis
   is the presence of mixed pixels, due to the low spatial resolution of
   such images. This means that several spectrally pure signatures
   (endmembers) are combined into the same mixed pixel. Linear spectral
   unmixing follows an unsupervised approach which aims at inferring pure
   spectral signatures and their material fractions at each pixel of the
   scene. The huge data volumes acquired by such sensors put stringent
   requirements on processing and unmixing methods.This paper proposes an
   efficient implementation of a unsupervised linear unmixing method on
   GPUs using CUDA. The method finds the smallest simplex by solving a
   sequence of nonsmooth convex subproblems using variable splitting to
   obtain a constraint formulation, and then applying an augmented
   Lagrangian technique. The parallel implementation of SISAL presented in
   this work exploits the GPU architecture at low level, using shared
   memory and coalesced accesses to memory. The results herein presented
   indicate that the GPU implementation can significantly accelerate the
   method's execution over big datasets while maintaining the methods
   accuracy.</abstract><date>2015</date><author>Sevilla, Jorge
   Nascimento, Jose M. P.</author></paper><paper><title>Days of Endless Time</title><abstract></abstract><date>JUL-AUG 2015</date><author>Day, Charles</author></paper><paper><title>Improving the Mapping of Smith-Waterman Sequence Database Searches onto
   CUDA-Enabled GPUs.</title><abstract>Sequence alignment lies at heart of the bioinformatics. The
   Smith-Waterman algorithm is one of the key sequence search algorithms
   and has gained popularity due to improved implementations and rapidly
   increasing compute power. Recently, the Smith-Waterman algorithm has
   been successfully mapped onto the emerging general-purpose graphics
   processing units (GPUs). In this paper, we focused on how to improve the
   mapping, especially for short query sequences, by better usage of shared
   memory. We performed and evaluated the proposed method on two different
   platforms (Tesla C1060 and Tesla K20) and compared it with two classic
   methods in CUDASW++. Further, the performance on different numbers of
   threads and blocks has been analyzed. The results showed that the
   proposed method significantly improves Smith-Waterman algorithm on
   CUDA-enabled GPUs in proper allocation of block and thread numbers. </abstract><date>2015</date><author>Huang, Liang-Tsung
   Wu, Chao-Chin
   Lai, Lien-Fu
   Li, Yun-Ju</author></paper><paper><title>The Genesis of Electronic Charting</title><abstract>Significant advances in marine navigation have resulted in all major
   vessels being equipped with GPS (Global Positioning System) receivers
   today, capable of providing, highly accurate positions worldwide as an
   input to ECS (Electronic Chart System) and ECDIS (Electronic Chart
   Display:and Information System) a system that was approved by IMO
   (International Maritime Organization) for use on ships in
   1989.Similarly, most major vehicle fleet operations and, in fact, most
   automobiles are now, 5 available With devices referred to as "GPS,"
   although they consist of the same two distinct subsystems : One
   essentially a GPS receiver collects,signals from satellite constellation
   to calculate a position. Most importantly, it provides a latitude and
   longitude position, which is sent to the second Sub-system consisting of
   a computer with a graphics display that Shows this Lat/Lon position on a
   map or chart background.To manufacturers, GPS is primarily an enabling
   technology for electronic charts and mapping systems that provides the
   necessary position information so it can be displayed in relation to
   surrounding hazards. It does this without,delay and often in combination
   With other important navigation sensor data on board.Without charts as a
   background, numbers er coordinates alone can the mariner about his or:
   her relationship to a reef or Shoal up ahead. Without such systems,
   ships could net meet the suggested accuracies for "Harbor Entrance and
   Approach" in the United States .ECDIS has been recognized by many great
   leaders in the industry."It will completely change the way we do
   business... For the first time you will know where you are, not where
   you were."-RADM J. Austin Yeager, NOAA Coast &amp; geodetic survey (A New
   Way to Navigate) Geodetic Survey (A New to Navigate)"ECDIS is the most
   significant improvement in navigtion in the past 100 years."- Captain Ed
   Rollinson, U.S. Coast Guard (ECDIS- A View From the Bridge, issued by
   The Canadian Hydrographic Service and Partners, U.S. Coast Guard R&amp;D
   Center)"ECDIS... Potentially the most significant breakthrough in marine
   navigation that _ has occurred since the advent of radar almost 50 years
   ago"-Dr. Lee Alexander, Chairman of International Electrotechnical
   Commission Working Group (Leading the Way with ECPINS, issued by
   Offshore Systems Ltd. [OSL]/Offshore Systems International
   [OSI])Development and application of the first ECS is presented,
   together sequent advances to level of ECDIS.</abstract><date>NOV-DEC 2015</date><author>Lanziner, Helmut</author></paper><paper><title>Dimensionality reduction and coloured noise removal from hyperspectral
   images</title><abstract>Hyperspectral image (HSI) classification requires spectral
   dimensionality reduction and noise reduction. While common
   dimensionality reduction (DR) and denoising methods are based on linear
   algebra, we propose a multilinear algebra method to jointly achieve
   denoising reduction and DR. Multilinear tools consider multidimensional
   data as whole entity by processing jointly spatial and spectral ways.
   However, it cannot cope with the HSIs distorted by non-white noise which
   is the most realistic case and cannot preserve rare signals. First, we
   propose a new method for whitening the noise (W) in HSI. Then we propose
   a method based on multidimensional wavelet packet transform (MWPT) and
   multiway Wiener filter (MWF) which performs both non-white noise and
   spectral DR, referred to as W-MWPT-MWFdr-([GRAPHICS]). The
   Classification algorithm support vector (SVM) machines are applied to
   the output of the following DR and noise reduction methods to compare
   their efficiency: The proposed W-MWPT-MWFdr-([GRAPHICS]); prewhitening
   method associated with MWF (PMWF), principal component analysis
   (PCA(dr)), minimum noise fraction (MNFdr), PCA(dr) associated with
   Wiener filtering (PCA(dr)-Wiener), and MNFdr associated with Wiener
   filtering (MNFdr-Wiener).</abstract><date>NOV 2 2015</date><author>Bourennane, S.
   Fossati, C.</author></paper><paper><title>CHC plus RT: Coherent Hierarchical Culling for Ray Tracing</title><abstract>We propose a new technique for in-core and out-of-core GPU ray tracing
   using a generalization of hierarchical occlusion culling in the style of
   the CHC++ method. Our method exploits the rasterization pipeline and
   hardware occlusion queries in order to create coherent batches of work
   for localized shader-based ray tracing kernels. By combining hierarchies
   in both ray space and object space, the method is able to share
   intermediate traversal results among multiple rays. We exploit temporal
   coherence among similar ray sets between frames and also within the
   given frame. A suitable management of the current visibility state makes
   it possible to benefit from occlusion culling for less coherent ray
   types like diffuse reflections. Since large scenes are still a challenge
   for modern GPU ray tracers, our method is most useful for scenes with
   medium to high complexity, especially since our method inherently
   supports ray tracing highly complex scenes that do not fit in GPU
   memory. For in-core scenes our method is comparable to CUDA ray tracing
   and performs up to 5.94x better than pure shader-based ray tracing.</abstract><date>MAY 2015</date><author>Mattausch, O.
   Bittner, J.
   Jaspe, A.
   Gobbetti, E.
   Wimmer, M.
   Pajarola, R.</author></paper><paper><title>IMP 2.0: a multi-species functional genomics portal for integration,
   visualization and prediction of protein functions and networks</title><abstract>IMP (Integrative Multi-species Prediction), originally released in 2012,
   is an interactive web server that enables molecular biologists to
   interpret experimental results and to generate hypotheses in the context
   of a large cross-organism compendium of functional predictions and
   networks. The system provides biologists with a framework to analyze
   their candidate gene sets in the context of functional networks,
   expanding or refining their sets using functional relationships
   predicted from integrated high-throughput data. IMP 2.0 integrates
   updated prior knowledge and data collections from the last three years
   in the seven supported organisms (Homo sapiens, Mus musculus, Rattus
   norvegicus, Drosophila melanogaster, Danio rerio, Caenorhabditis
   elegans, and Saccharomyces cerevisiae) and extends function prediction
   coverage to include human disease. IMP identifies homologs with
   conserved functional roles for disease knowledge transfer, allowing
   biologists to analyze disease contexts and predictions across all
   organisms. Additionally, IMP 2.0 implements a new flexible platform for
   experts to generate custom hypotheses about biological processes or
   diseases, making sophisticated data-driven methods easily accessible to
   researchers. IMP does not require any registration or installation and
   is freely available for use at http://imp.princeton.edu.</abstract><date>JUL 1 2015</date><author>Wong, Aaron K.
   Krishnan, Arjun
   Yao, Victoria
   Tadych, Alicja
   Troyanskaya, Olga G.</author></paper><paper><title>(PS)(2): protein structure prediction server version 3.0</title><abstract>Protein complexes are involved in many biological processes. Examining
   coupling between subunits of a complexwould be useful to understand
   themolecular basis of protein function. Here, our updated (PS)(2) web
   server predicts the three-dimensional structures of protein complexes
   based on comparative modeling; furthermore, this server examines the
   coupling between subunits of the predicted complex by combining
   structural and evolutionary considerations. The predicted complex
   structure could be indicated and visualized by Java-based 3D graphics
   viewers and the structural and evolutionary profiles are shown and
   compared chain-by-chain. For each subunit, considerations with or
   without the packing contribution of other subunits cause the differences
   in similarities between structural and evolutionary profiles, and these
   differences imply which form, complex or monomeric, is preferred in the
   biological condition for the subunit. We believe that the (PS)(2) server
   would be a useful tool for biologists who are interested not only in the
   structures of protein complexes but also in the coupling between
   subunits of the complexes. The (PS)(2) is freely available at
   http://ps2v3.life.nctu.edu.tw/.</abstract><date>JUL 1 2015</date><author>Huang, Tsun-Tsao
   Hwang, Jenn-Kang
   Chen, Chu-Huang
   Chu, Chih-Sheng
   Lee, Chi-Wen
   Chen, Chih-Chieh</author></paper><paper><title>GPU-Based Ray Tracing Algorithm for High-Speed Propagation Prediction in
   Typical Indoor Environments</title><abstract>A fast 3-D ray tracing propagation prediction model based on virtual
   source tree is presented in this paper, whose theoretical foundations
   are geometrical optics(GO) and the uniform theory of diffraction(UTD).
   In terms of typical single room indoor scene, taking the geometrical and
   electromagnetic information into account, some acceleration techniques
   are adopted to raise the efficiency of the ray tracing algorithm. The
   simulation results indicate that the runtime of the ray tracing
   algorithm will sharply increase when the number of the objects in the
   single room is large enough. Therefore, GPU acceleration technology is
   used to solve that problem. As is known to all, GPU is good at
   calculation operation rather than logical judgment, so that tens of
   thousands of threads in CUDA programs are able to calculate at the same
   time, in order to achieve massively parallel acceleration. Finally, a
   typical single room with several objects is simulated by using the
   serial ray tracing algorithm and the parallel one respectively. It can
   be found easily from the results that compared with the serial
   algorithm, the GPU-based one can achieve greater efficiency.</abstract><date>2015</date><author>Guo, Lixin
   Guan, Xiaowei
   Liu, Zhongyu</author></paper><paper><title>Extended graph rotation systems as a model for cyclic weaving on
   orientable surfaces</title><abstract>We present an extension of the theory of graph rotation systems, which
   has been a widely used model for graph imbeddings on topological
   surfaces. The extended model is quite beyond what is needed to specify
   graph imbeddings on surfaces, and it can be used to represent and
   generate link structures immersed on surfaces. Since link structures
   immersed on surfaces can be viewed as woven images in 3D space, the
   extended graph rotation systems provide a well-formulated mathematical
   model for developing a topologically robust graphics system with strong
   interactive operations for the design of woven images in 3D
   mesh-modeling and computer-aided sculpting. (C) 2015 Elsevier B.V. All
   rights reserved.</abstract><date>OCT 1 2015</date><author>Akleman, Ergun
   Chen, Jianer
   Gross, Jonathan L.</author></paper><paper><title>Improving Remote Desktopping through Adaptive Record/Replay</title><abstract>Accessing the display of a computer remotely, is popularly called
   "remote desktopping(1)". Remote desktopping software installs at both
   the user-facing client computer and the remote server computer; it
   simulates user's input events at server, and streams the corresponding
   display changes to client, thus providing an illusion to the user of
   controlling the remote machine using local input devices (e.g.,
   keyboard/mouse). Many such remote desktopping tools are widely used.We
   show that if the remote server is a virtual machine (VM) and the client
   is reasonably powerful (e.g., current laptop and desktop grade
   hardware), VM deterministic replay capabilities can be used adaptively
   to significantly reduce the network bandwidth consumption and
   server-side CPU utilization of a remote desktopping tool. We implement
   these optimizations in a tool based on Qemu/KVM virtualization platform
   and VNC remote desktopping platform. Our tool reduces VNC's network
   bandwidth consumption by up to 9x and server-side CPU utilization by up
   to 56% for popular graphics-intensive applications. On the flip side,
   our techniques consume higher CPU/memory/disk resources at the client.
   The effect of our optimizations on user-perceived latency is negligible.</abstract><date>JUL 2015</date><author>Jaffer, Shehbaz
   Kedia, Piyus
   Bansal, Sorav</author></paper><paper><title>A proxy approach to integrate heterogeneous CAD resources for
   distributed collaborative design</title><abstract>Collaborative product design on heterogeneous computer-aided design
   (CAD) platforms has shown its significance to today's global
   manufacturing industries. This article proposes a novel collaboration
   framework for supporting distributed collaborative design with
   heterogeneous CAD resources. The unique feature of the framework is
   that, instead of directly accessing heterogeneous CAD resources, a
   so-called co-proxy model is defined and created to work as the
   replication agent representing original CAD resources during
   collaborative sessions. The co-proxy model is designed to represent
   corresponding CAD resources with both structured polygonal graphics for
   visualisation and a set of embedded virtual topological elements (VTE),
   so as to support collaborative design manipulations such as virtual
   assembly and annotation. Moreover, the co-proxy model comes with an
   external link mechanism, with which the powerful modelling and editing
   functionalities from heterogeneous CAD systems can be reasonably
   integrated into the distributed collaboration framework. The proposed
   framework together with its enabling technologies has been implemented
   into a prototype system named as Co-DMU, which is generally applicable
   for feature-based CAD modelling systems with automation interfaces
   provided.</abstract><date>JUN 3 2015</date><author>Li, Jing-Rong
   Tang, Cheng
   Wang, Qing-Hui</author></paper><paper><title>Towards High-Level Programming for Systems with Many Cores</title><abstract>Application development for modern high-performance systems with many
   cores, i.e., comprising multiple Graphics Processing Units (GPUs) and
   multi-core CPUs, currently exploits low-level programming approaches
   like CUDA and OpenCL, which leads to complex, lengthy and error-prone
   programs. In this paper, we advocate a high-level programming approach
   for such systems, which relies on the following two main principles: (a)
   the model is based on the current OpenCL standard, such that programs
   remain portable across various many-core systems, independently of the
   vendor, and all low-level code optimizations can be applied; (b) the
   model extends OpenCL with three high-level features which simplify
   many-core programming and are automatically translated by the system
   into OpenCL code. The high-level features of our programming model are
   as follows: (1) memory management is simplified and automated using
   parallel container data types (vectors and matrices); (2) a data (re)
   distribution mechanism supports data partitioning and generates
   automatic data movements between multiple GPUs; (3) computations are
   precisely and concisely expressed using parallel algorithmic patterns
   (skeletons). The well-defined skeletons allow for semantics-preserving
   transformations of SkelCL programs which can be applied in the process
   of program development, as well as in the compilation and optimization
   phase. We demonstrate how our programming model and its implementation
   are used to express several parallel applications, and we report first
   experimental results on evaluating our approach in terms of program size
   and target performance.</abstract><date>2015</date><author>Gorlatch, Sergei
   Steuwer, Michel</author></paper><paper><title>Towards real-time detection of seizures in awake rats with
   GPU-accelerated diffuse optical tomography</title><abstract>Background: Advancement in clinically relevant studies like seizure
   interruption using functional neuro imaging tools has shown that
   specific changes in hemodynamics precede and accompany seizure onset and
   propagation. However, preclinical seizure experiments need to be
   conducted in awake animals with images reconstructed and displayed in
   real-time.Methods: This article describes an approach that can be
   utilized to tackle these challenges. A subject specific head interface
   and restraining method was designed to allow for DOT to imaging of
   hemodynamic changes in unanesthetized rats during evoked acute seizures.
   Using CUDA programming model, the finite-element based nonlinear
   iterative algorithm for image reconstruction was parallelized.Results:
   Early hemodynamic changes were monitored in real time and observed tens
   of seconds prior to seizure onset. Utilizing the massive parallelization
   offered by graphic processing units (GPU), DOT was extended to online
   image reconstruction within 1 s.Comparison with existing methods:
   Pre-seizure state related hemodynamic changes were detected in awake
   rats. 3D monitoring of hemodynamic changes was performed in real time
   with our parallelized image reconstruction procedure.Conclusion: Diffuse
   optical tomography (DOT) is a promising neuroimaging tool for the
   investigation of seizures in awake animals. (C) 2014 Elsevier B.V. All
   rights reserved.</abstract><date>JAN 30 2015</date><author>Zhang, Tao
   Zhou, Junli
   Carney, Paul R.
   Jiang, Huabei</author></paper><paper><title>Interactive image filtering for level-of-abstraction texturing of
   virtual 3D scenes</title><abstract>Texture mapping is a key technology in computer graphics. For the visual
   design of 3D scenes, in particular, effective texturing depends
   significantly on how important contents are expressed, e.g., by
   preserving global salient structures, and how their depiction is
   cognitively processed by the user in an application context.
   Edge-preserving image filtering is one key approach to address these
   concerns. Much research has focused on applying image filters in a
   post-process stage to generate artistically stylized depictions.
   However, these approaches generally do not preserve depth cues, which
   are important for the perception of 3D visualization (e.g., texture
   gradient). To this end, filtering is required that processes texture
   data coherently with respect to linear perspective and spatial
   relationships. In this work, we present an approach for texturing 3D
   scenes with perspective coherence by arbitrary image filters. We propose
   decoupled deferred texturing with (1) caching strategies to
   interactively perform image filtering prior to texture mapping and (2)
   for each mipmap level separately to enable a progressive level of
   abstraction, using (3) direct interaction interfaces to parameterize the
   visualization according to spatial, semantic, and thematic data. We
   demonstrate the potentials of our method by several applications using
   touch or natural language inputs to serve the different interests of
   users in specific information, including illustrative visualization,
   focus+context visualization, geometric detail removal, and semantic
   depth of field. The approach supports frame-to-frame coherence,
   order-independent transparency, multitexturing, and content-based
   filtering. In addition, it seamlessly integrates into real-time
   rendering pipelines and is extensible for custom interaction techniques.
   (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>NOV 2015</date><author>Semmo, Amir
   Doellner, Juergen</author></paper><paper><title>Longest-edge n-section algorithms: Properties and open problems</title><abstract>In this paper we survey all known (including own recent results)
   properties of the longest-edge n-section algorithms. These algorithms
   (in classical and recently designed conforming form) are nowadays used
   in many applications, including finite element simulations, computer
   graphics, etc. as a reliable tool for controllable mesh generation. In
   addition, we present a list of open problems arising in and around this
   topic. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>FEB 2016</date><author>Korotov, Sergey
   Plaza, Angel
   Suarez, Jose P.</author></paper><paper><title>Intraoperative 3D Finite Element Computation Using CUDA</title><abstract>As the Finite Element method is becoming more pervasive in scientific
   and engineering endeavors, the application space of the method increases
   as well. This work provides indication that real-time surgical
   application is also possible. The challenge is reducing the solution
   times of this, computationally expensive method, to the point that FE on
   patient organs can be done during surgery. This paper presents an
   implementation of a Total Lagrangian Explicit Dynamic Finite Element
   algorithm on General Purpose Graphics Processing Units (GPGPUs) that
   suggests the fulfillment of this constraint.The usability of the method
   is demonstrated by conducting a broad assay on ranging model sizes and
   different GPGPUs and comparing the accuracy and speed to a CPU solution.
   In doing so, we study the effect of using single/double precision
   computation.</abstract><date>2015</date><author>Strbac, Vukasin
   Vander Sloten, Jos
   Famaey, Nele</author></paper><paper><title>Batch-Theta* for path planning to the best goal in a goal set</title><abstract>The development of 3D cameras and many navigation-supporting sensors has
   recently enabled robots to build their working maps and navigate
   accurately, making path planning popular not just on computer graphics,
   but in real environments as well. Pursuing the solutions for robot path
   planning, this paper presents a variant of searching method Theta* for
   choosing the best goal among given goals and the lowest-cost path to it,
   called Batch-Theta*. The novelty lies at the proposed line-of-sight
   checking function during the searching process and the manner that the
   method handles the batch of goals during one search instead of
   repeatedly considering a single goal or blindly doing the exhausted
   search. The analysis and simulations show that the proposed Batch-Theta*
   efficiently finds the lowest-cost path to the best goal in a given goal
   set under Theta*'s mechanism.</abstract><date>DEC 2 2015</date><author>Viet-Hung Dang
   Nguyen Duc Thang
   Hoang Huu Viet
   Le Anh Tuan</author></paper><paper><title>High Performance GPU-Based Fourier Volume Rendering</title><abstract>Fourier volume rendering (FVR) is a significant visualization technique
   that has been used widely in digital radiography. As a result of its
   O(N-2 log N) time complexity, it provides a faster alternative to
   spatial domain volume rendering algorithms that are O(N-3)
   computationally complex. Relying on the Fourier projection-slice
   theorem, this technique operates on the spectral representation of a 3D
   volume instead of processing its spatial representation to generate
   attenuation-only projections that look like X-ray radiographs. Due to
   the rapid evolution of its underlying architecture, the graphics
   processing unit (GPU) became an attractive competent platform that can
   deliver giant computational raw power compared to the central processing
   unit (CPU) on a per-dollar-basis. The introduction of the compute
   unified device architecture (CUDA) technology enables
   embarrassingly-parallel algorithms to run efficiently on CUDA-capable
   GPU architectures. In this work, a high performance GPU-accelerated
   implementation of the FVR pipeline on CUDA-enabled GPUs is presented.
   This proposed implementation can achieve a speed-up of 117x compared to
   a single-threaded hybrid implementation that uses the CPU and GPU
   together by taking advantage of executing the rendering pipeline
   entirely on recent GPU architectures.</abstract><date>2015</date><author>Abdellah, Marwan
   Eldeib, Ayman
   Sharawi, Amr</author></paper><paper><title>A high performance memetic algorithm for extremely high-dimensional
   problems</title><abstract>Throughout the last years, optimization problems on a large number of
   variables, sometimes over 1000, are becoming common. Thus, algorithms
   that can tackle them effectively, both in result quality and run time,
   are necessary. Among these specific algorithms for high-dimensional
   problems, memetic algorithms, which are the result of the hybridization
   of an evolutionary algorithm and a local improvement technique, have
   arisen as very powerful optimization systems for this type of problems.
   A very effective algorithm of this kind is the MA-SW-Chains algorithm.
   On the other hand, general purpose computing using Graphics Processing
   Units (GPUs) has become a very active field because of the high speed-up
   ratios that can be obtained when applied to problems that exhibit a high
   degree of data parallelism.In this work we present a new design of
   MA-SW-Chains to adapt it to the GPU-based massively parallel
   architecture. The experiments with the new GPU memetic technique,
   compared to the original sequential version, prove that the results are
   obtained with the same quality but with a reduction of the run time of
   two orders of magnitude. This great improvement in computing time makes
   our proposal suitable for future optimization problems with a
   dimensionality several orders of magnitude greater than current
   high-dimensionality standards (i.e. problems with millions of
   variables). The remarkable run time reduction comes at the cost of a
   higher design and implementation overhead compared to the GPU-based
   single-threaded counterpart. (C) 2014 Elsevier Inc. All rights reserved.</abstract><date>FEB 1 2015</date><author>Lastra, Miguel
   Molina, Daniel
   Benitez, Jose M.</author></paper><paper><title>Special Issue: 40 YEARS OF COMPUTER GRAPHICS IN DARMSTADT Foreword</title><abstract></abstract><date>DEC 2015</date><author>Encarnacao, Jose Luis</author></paper><paper><title>A survey on object deformation and decomposition in computer graphics</title><abstract>In a realistic world, objects are expected to change in appearance over
   time when exposed to their environment. Morphology changes can be
   important indicators of an object's make up and the environment in which
   it exists. Simulating changes in an object's appearance over time has
   become increasingly popular over the recent years. In this survey we
   will describe a number of methods used in computer graphics to simulate
   object morphology changes due natural influences, such as cracks,
   fractures, patina, corrosion, erosion, burning, melting, decay, rotting
   and withering. We will focus on approaches that consider effects which
   influence the geometry of the entire object, instead of the surface
   appearance alone. The methods described are categorised according to the
   natural phenomena that drive the appearance changes. We pay particular
   attention to the different object representation and deformation
   techniques used in current approaches. The aim of this article is to
   provide a comprehensive overview of the state of the art in the area of
   object morphology changes driven by the environment. (C) 2015 Elsevier
   Ltd. All rights reserved.</abstract><date>NOV 2015</date><author>Frerichs, Dhana
   Vidler, Andrew
   Gatzidis, Christos</author></paper><paper><title>A New Framework of GPU-Accelerated Spectral Solvers: Collocation and
   Glerkin Methods for Systems of Coupled Elliptic Equations</title><abstract>Spectral methods are useful for applications that benefit from
   high-order precisions. However, if the same number of degrees of freedom
   is used, the computational cost of a spectral method is considerably
   higher than that of a general finite difference or finite element
   method. After the investigation in Chen et al. (J Comput Phys
   250:555-564, 2013), we provide for the first time a framework of
   graphics processing units (GPU)-accelerated spectral methods for systems
   of coupled elliptic equations. The involved dense matrix computations,
   as the main obstacle for fast spectral methods on a traditional CPU,
   turns out to be an opportunity for high speedups on a many-core GPU. We
   obtain an order-of-magnitude speedup for solving 2-D and 3-D systems
   using a Kepler 20 GPU over a high-end multi-core processor, with two
   popular spectral methods, namely, the spectral collocation method and
   the spectral-Galerkin method. The new framework is applicable to systems
   of coupled second-order equations with general boundary conditions,
   where is an integer of moderate size. The ultimate goal is to apply the
   developed solver to complex and nonlinear time-dependent problems. As
   two interesting examples, a 2-D FitzHugh-Nagumo equation is solved with
   the spectral collocation method and a 3-D Cahn-Hilliard equation is
   solved with the spectral-Galerkin method. We thus demonstrate a
   practical solution for demanding problems that utilize high-order
   spatial resolution and longer run-times.</abstract><date>FEB 2015</date><author>Chen, Feng</author></paper><paper><title>A GPU accelerated Barnes-Hut tree code for FLASH4</title><abstract>We present a GPU accelerated CUDA-C implementation of the Barnes Hut
   (BH) tree code for calculating the gravitational potential on octree
   adaptive meshes. The tree code algorithm is implemented within the
   FLASH4 adaptive mesh refinement (AMR) code framework and therefore fully
   MPI parallel. We describe the algorithm and present test results that
   demonstrate its accuracy and performance in comparison to the algorithms
   available in the current FLASH4 version. We use a MacLaurin spheroid to
   test the accuracy of our new implementation and use spherical,
   collapsing cloud cores with effective AMR to carry out performance tests
   also in comparison with previous gravity solvers. Depending on the setup
   and the GPU/CPU ratio, we find a speedup for the gravity unit of at
   least a factor of 3 and up to 60 in comparison to the gravity solvers
   implemented in the FLASH4 code. We find an overall speedup factor for
   full simulations of at least factor 1.6 up to a factor of 10. (C) 2015
   Elsevier B.V. All rights reserved.</abstract><date>MAY 2016</date><author>Lukat, Gunther
   Banerjee, Robi</author></paper><paper><title>1D and 2D Shape Descriptors Applied in Fabric Drape Computer Simulation</title><abstract>Fabric drape simulations accomplished by computer graphics software can
   provide the basis for effective communication among designers,
   manufacturers and other players in the apparel industry. The goal of our
   study was to investigate various 1D and 2D shape descriptors used to
   characterize renderings of 3D drape simulations in dependence on the
   geometry of collision objects and fabric type. Image processing routines
   were implemented to extract and compute the shape descriptors while
   principal components analysis was applied to interpret the relationships
   among the parameters studied. Drapes on cube, octahedron and prism were
   found to behave in a distinctively different manner compared to those
   produced using the other six collision objects: cone, cylinder,
   dodecahedron, gen-gon, sphere, and tube. A first principal component can
   be, to a large extent, represented by the following mutually strongly
   correlated 2D shape descriptors: area, major axis length, minor axis
   length, equivalent diameter, and perimeter. Analysis using 1D shape
   descriptors confirms these findings and additionally suggests that
   rubber-based drapes contain the lowest number of folds while those on
   polyester, wool, and sometimes silk and/or satin are characterized by
   the highest number of drape folds. These results were confirmed by
   visual examination of the drapes simulated.</abstract><date>NOV-DEC 2015</date><author>Tomc, Helena Gabrijelcic
   Hladnik, Ales</author></paper><paper><title>Fast Subgraph Matching on Large Graphs using Graphics Processors</title><abstract>Subgraph matching is the task of finding all matches of a query graph in
   a large data graph, which is known as an NP-complete problem. Many
   algorithms are proposed to solve this problem using CPUs. In recent
   years, Graphics Processing Units (GPUs) have been adopted to accelerate
   fundamental graph operations such as breadth-first search and shortest
   path, owing to their parallelism and high data throughput. The existing
   subgraph matching algorithms, however, face challenges in mapping
   backtracking problems to the GPU architectures. Moreover, the previous
   GPU-based graph algorithms are not designed to handle intermediate and
   final outputs. In this paper, we present a simple and GPU-friendly
   method for subgraph matching, called GpSM, which is designed for
   massively parallel architectures. We show that GpSM outperforms the
   state-of-the-art algorithms and efficiently answers subgraph queries on
   large graphs.</abstract><date>2015</date><author>Ha-Nguyen Tran
   Kim, Jung-Jae
   He, Bingsheng</author></paper><paper><title>Accelerating Fibre Orientation Estimation from Diffusion Weighted
   Magnetic Resonance Imaging Using GPUs (vol 8, e61892, 2013)</title><abstract></abstract><date>JUN 12 2015</date><author>Hernandez, Moises
   Guerrero, Gines D.
   Cecilia, Jose M.
   Garcia, Jose M.
   Inuggi, Alberto
   Jbabdi, Saad
   Behrens, Timothy E. J.
   Sotiropoulos, Stamatios N.</author></paper><paper><title>Redundancy computation analysis and implementation of phase diversity
   based on GPU</title><abstract>Phase diversity method is not only used as an image restoration
   technique, but also as a wavefront sensor. However, its computations
   have been perceived as being too burdensome to achieve its real-time
   applications on a desktop computer platform. In this paper, the
   implementation of the phase diversity algorithm based on graphic
   processing unit (GPU) is presented. The redundancy computations for the
   pupil function, point spread function, and optical transfer function are
   analyzed. Two kinds of implementation methods based on GPU are compared:
   one is the general method which is accomplished by GPU library CUFFT
   without precision loss (method-1) and the other one performed by our own
   custom FFT with little damage of precision considering the redundant
   calculations (method-2). The results show the cost and gradient
   functions can be speeded up by method-2 in contrast with the method-1
   and the overhead of global memory access by kernel fusion can be
   reduced. For the image of 256 x 256 with the sampling factor of 3, the
   results reveal that method-2 achieves speedup of 1.83x compared with
   method-1 when the central 128 x 128 pixels of the point spread function
   are used.</abstract><date>OCT 2015</date><author>Zhang, Quan
   Bao, Hua
   Rao, Changhui
   Peng, Zhenming</author></paper><paper><title>Parallelization of CUSUM Test in a CUDA Environment</title><abstract>We have parallelized the cumulative sum (CUSUM) test of NIST's1)
   statistical random number test suite in a CUDA environment. Storing
   random walks in an array instead of in scalar variables eliminates data
   dependence. The change in data structure makes it possible to apply
   parallel scans, scatters, and reductions at each stage of the test. In
   addition, serial data exchanges between CPU and GPU are removed by
   migrating CPU's tasks to GPU. Finally we have optimized global memory
   accesses. The overall speedup is 23 times over the sequential version.
   Our results contribute to improving security of random numbers for
   cryptographic keys as well as reducing the time for evaluation of
   randomness.</abstract><date>2015</date><author>???
   ???
   ???
   ???
   ???</author></paper><paper><title>DRAWING SKILLS IN CHILDREN WITH NEURODEVELOPMENTAL DELAY AGED 2-5 YEARS</title><abstract>In typically developing children, drawing development occurs in stages
   from uncontrolled strokes to complex drawing. In this study, we examined
   drawing development in children with neurodevelopmental delay (NDD). In
   order to do so, we observed the influence of age, intraventricular
   hemorrhage (IVH) and gender on the development of drawing skills. The
   sample consisted of 52 children with NDD, aged 2 years and 6 months to 5
   years. All children were hospitalized for multidisciplinary team
   monitoring and developmental support. The evaluation of drawing
   development was administered by giving each child a blank A4 paper and
   the instruction to draw anything they wanted. All of the drawings were
   scored satisfactory or unsatisfactory. Descriptive statistics was
   employed on all relevant data to show results in frequencies and
   percentages. In order to determine differences between groups, the
   chi(2)-test was administered. The results showed greatest difference in
   drawing in children aged from 3 years to 3 years and 11 months. Children
   with lower IVH had better drawing scores than children with higher IVH
   levels. According to gender dissimilarities, a difference was found
   showing girls to have better drawing skills than boys. All study results
   pointed to the importance of early rehabilitation and continuous
   structured work with children with NDD.</abstract><date>JUN 2015</date><author>Morovic, Maja Lang
   Matijevic, Valentina
   Divljakovic, Kristina
   Kraljevic, Marija
   Dimic, Zdenka</author></paper><paper><title>High Performance Approximate Sort Algorithm Using GPUs</title><abstract>Sorting is a fundamental problem in computer science, and the strict
   sorting usually means a strict order with ascending or descending.
   However, some applications in reality don't require the strict ascending
   or descending order and the approximate ascending or descending order
   just meets the requirement.Graphics processing units (GPUs) have become
   accelerators for parallel computing. In this paper, based on the popular
   CUDA parallel computing architecture, we propose high performance
   approximate sort algorithm running on multicore GPUs. The algorithm
   divides the distribution interval of input data into multiple small
   intervals, and then uses the processing cores of GPUs to map the data
   into the different intervals in parallel. Finally by combining the small
   intervals, we can make the data between the different intervals in order
   state and the data in the same interval is disorder state. Thus we can
   get the approximate sorting result and the result is characterized by a
   general order but local disorder. By utilize the massive core of GPUs to
   parallel sort data, the algorithm can greatly shorten the execution
   time. Radix sort is the fastest GPUs-based sorting and the experimental
   results show that our approximate sort algorithm is two times as fast as
   the radix sort and far exceeds all the GPUs-based sorting.</abstract><date>2015</date><author>Xiao, Jun
   Chen, Hao
   Sun, Jianhua</author></paper><paper><title>PARALLEL WATERMARKING OF IMAGES IN THE FREQUENCY DOMAIN</title><abstract>While the internet has made it possible for the consumer to easily
   obtain images, audio, video, etc. in digital form, it has also made it
   easier to illegally obtain copyrighted material. Digital watermarking is
   a partial solution to this problem. Embedding a watermark in a legal
   version of material can help the copyright owner to identify who has an
   illegal copy. Because of the ever increasing enormity of the flow of
   information, it becomes necessary to watermark files in the least amount
   of time possible. For this reason it is natural to turn to parallel
   computing. In this work we compare the performance of three different
   implementations on a cluster of SMPs, in OpenMP, MPI, and CUDA, of a
   simple algorithm for watermarking digital images. Our experiments show
   that CUDA with one gpu is almost 300 times faster than the sequential
   version and many times faster than OpenMP and MPI using 1 up to 8 nodes.</abstract><date>JUN 2015</date><author>Bollman, Dorothy
   Bustillo, Alcibiades
   Morales, Einstein</author></paper><paper><title>Robust regression with CUDA and its application to plasma reflectometry</title><abstract>In many applications, especially those involving scientific
   instrumentation data with a large experimental error, it is often
   necessary to carry out linear regression in the presence of severe
   outliers which may adversely affect the results. Robust regression
   methods do exist, but they are much more computationally intensive,
   making it difficult to apply them in real-time scenarios. In this work,
   we resort to graphics processing unit (GPU)-based computing to carry out
   robust regression in a time-sensitive application. We illustrate the
   results and the performance gains obtained by parallelizing one of the
   most common robust regression methods, namely, least median of squares.
   Although the method has a complexity of O(n(3) log n), with GPU
   computing, it is possible to accelerate it to the point that it becomes
   usable within the required time frame. In our experiments, the input
   data come from a plasma diagnostic system installed at Joint European
   Torus, the largest fusion experiment in Europe, but the approach can be
   easily transferred to other applications.</abstract><date>NOV 2015</date><author>Ferreira, Diogo R.
   Carvalho, Pedro J.
   Fernandes, Horacio</author></paper><paper><title>Fast Disk Conformal Parameterization of Simply-Connected Open Surfaces</title><abstract>Surface parameterizations have been widely used in computer graphics and
   geometry processing. In particular, as simply-connected open surfaces
   are conformally equivalent to the unit disk, it is desirable to compute
   the disk conformal parameterizations of the surfaces. In this paper, we
   propose a novel algorithm for the conformal parameterization of a
   simply-connected open surface onto the unit disk, which significantly
   speeds up the computation, enhances the conformality and stability, and
   guarantees the bijectivity. The conformality distortions at the inner
   region and on the boundary are corrected by two steps, with the aid of
   an iterative scheme using quasi-conformal theories. Experimental results
   demonstrate the effectiveness of our proposed method.</abstract><date>DEC 2015</date><author>Choi, Pui Tung
   Lui, Lok Ming</author></paper><paper><title>HELIOS-K: AN ULTRAFAST, OPEN-SOURCE OPACITY CALCULATOR FOR RADIATIVE
   TRANSFER</title><abstract>We present an ultrafast opacity calculator that we name HELIOS-K. It
   takes a line list as an input, computes the shape of each spectral line,
   and provides an option for grouping an enormous number of lines into a
   manageable number of bins. We implement a combination of Algorithm 916
   and Gauss-Hermite quadrature to compute the Voigt profile, write the
   code in CUDA, and optimize the computation for graphics processing units
   (GPUs). We restate the theory of the k-distribution method and use it to
   reduce similar to 10(5)-10(8) lines to similar to 10-10(4) wavenumber
   bins, which may then be used for radiative transfer, atmospheric
   retrieval and general circulation models. The choice of line-wing cutoff
   for the Voigt profile is a significant source of error and affects the
   value of the computed flux by similar to 10%. This is an outstanding
   physical (rather than computational) problem, due to our incomplete
   knowledge of pressure broadening of spectral lines in the far line
   wings. We emphasize that this problem remains regardless of whether one
   performs line-by-line calculations or uses the k-distribution method and
   affects all calculations of exoplanetary atmospheres requiring the use
   of wavelength-dependent opacities. We elucidate the correlated-k
   approximation and demonstrate that it applies equally to inhomogeneous
   atmospheres with a single atomic/molecular species or homogeneous
   atmospheres with multiple species. Using a NVIDIA K20 GPU, HELIOS-K is
   capable of computing an opacity function with similar to 10(5) spectral
   lines in similar to 1 s and is publicly available as part of the
   Exoclimes Simulation Platform (www.exoclime.org).</abstract><date>AUG 1 2015</date><author>Grimm, Simon L.
   Heng, Kevin</author></paper><paper><title>A Straightforward Implementation of a GPU-accelerated ELM in R with
   NVIDIA Graphic Cards</title><abstract>General purpose computing on graphics processing units (GPGPU) is a
   promising technique to cope with nowadays arising computational
   challenges due to the suitability of GPUs for parallel processing.
   Several libraries and functions are being released to boost the use of
   GPUs in real world problems. However, many of these packages require a
   deep knowledge in GPUs' architecture and in low-level programming. As a
   result, end users find trouble in exploiting GPGPU advantages. In this
   paper, we focus on the GPU-acceleration of a prediction technique
   specially designed to deal with big datasets: the extreme learning
   machine (ELM). The intent of this study is to develop a user-friendly
   library in the open source R language and subsequently release the code
   in https://github.com/maaliam/EDMANS-elmNN-GPU.git. Therefore R users
   can freely implement it with the only requirement of having a NVIDIA
   graphic card. The most computationally demanding operations were
   identified by performing a sensitivity analysis. As a result, only
   matrix multiplications were executed in the GPU as they take around 99%
   of total execution time. A speedup rate up to 15 times was obtained with
   this GPU-accelerated ELM in the most computationally expensive
   scenarios. Moreover, the applicability of the GPU-accelerated ELM was
   also tested with a typical case of model selection, in which genetic
   algorithms were used to fine-tune an ELM and training thousands of
   models is required. In this case, still a speedup of 6 times was
   obtained.</abstract><date>2015</date><author>Alia-Martinez, M.
   Antonanzas, J.
   Antonanzas-Torres, F.
   Pernia-Espinoza, A.
   Urraca, R.</author></paper><paper><title>GPU-accelerated exhaustive search for third-order epistatic interactions
   in case-control studies</title><abstract>Interest in discovering combinations of genetic markers from
   case-control studies, such as Genome Wide Association Studies (GWAS),
   that are strongly associated to diseases has increased in recent years.
   Detecting epistasis, i.e. interactions among k markers (k &gt;= 2), is an
   important but time consuming operation since statistical computations
   have to be performed for each k-tuple of measured markers. Efficient
   exhaustive methods have been proposed for k = 2, but exhaustive
   third-order analyses are thought to be impractical due to the cubic
   number of triples to be computed. Thus, most previous approaches apply
   heuristics to accelerate the analysis by discarding certain triples in
   advance. Unfortunately, these tools can fail to detect interesting
   interactions. We present GPU3SNP, a fast GPU-accelerated tool to
   exhaustively search for interactions among all marker-triples of a given
   case-control dataset. Our tool is able to analyze an input dataset with
   tens of thousands of markers in reasonable time thanks to two efficient
   CUDA kernels and efficient workload distribution techniques. For
   instance, a dataset consisting of 50,000 markers measured from 1000
   individuals can be analyzed in less than 22h on a single compute node
   with 4 NVIDIA GTX Titan boards. (C) 2015 Elsevier ay. All rights
   reserved.</abstract><date>MAY 2015</date><author>Gonzalez-Dominguez, Jorge
   Schmidt, Bertil</author></paper><paper><title>A parallel meshless dynamic cloud method on graphic processing units for
   unsteady compressible flows past moving boundaries</title><abstract>This paper presents an effort to implement a recently proposed meshless
   dynamic cloud method (Hong Wang et al., 2010) on modern high-performance
   graphic processing units (GPUs) with the compute unified device
   architecture (CUDA) programming model. Within the framework of the
   meshless method, clouds of points used as basic computational stencils
   are distributed in the whole flow domain. The spatial derivatives of the
   governing equations are discretised by the moving-least square scheme on
   every cloud of points. Roe's approximate Riemann solver is adopted to
   compute the convective flux. A dual-time stepping approach, which
   iterates in physical and pseudo temporal spaces, is employed to obtain
   the time-accurate solution. Simulation of steady compressible flows over
   a fixed aerofoil is firstly carried out to verify the GPU implementation
   of the method. Then it is extended to compute unsteady flows past
   oscillatory aerofoils. Numerical outcomes are compared with experimental
   and/or other reference results to validate the method. Significant
   performance speedup of more than an order of magnitude is verified by
   the numerical results. Systematic analysis shows that GPU is more energy
   efficient than CPU for solving aerodynamic problems. This demonstrates
   the potential of the proposed method to solve fluid-structure
   interaction problems. (c) 2014 Elsevier B.V. All rights reserved.</abstract><date>MAR 1 2015</date><author>Ma, Z. H.
   Wang, H.
   Pu, S. H.</author></paper><paper><title>Acceleration of norm-conserving Pseudopotential Plane-Wave-Based DFT
   Calculation on GPU using CUDA</title><abstract>In present study, acceleration of density functional theory calculation
   using norm-conserving pseudopotential and plane wave (NCPP-PW) basis set
   has been performed. It did not use or parallelize commonly program
   packages (such as ABINIT, VASP, PWSCF, etc.) but propose prototypical
   program to carry out self-consistent field calculations to solve
   Kohn-Sham equation and focus on Hamiltonian diagonalization part by
   using CUDA to utilize a graphical processing unit (GPU) accelerator. The
   results showed that acceleration up to 10 times speed-ups for certain
   type of GPU, namely NVidia GTX 460, for three systems: 8 silicon atoms
   in cubic unit cell (small), 10 water molecules in a box (medium), and 64
   silicon atoms in 2 x 2 x 2 cubic supercell (large).</abstract><date>2015</date><author>Fathurahman, Feradi
   Alfianto, Enggar
   Dipojono, Hermawan K.
   Martoprawiro, Muhamad. A.</author></paper><paper><title>Design and Development of Scientific Computing-Oriented Cloud Service
   Based on CUDA</title><abstract>The GPGPU (general purpose graphics processing units) has become a whole
   new area for research due to the fast development of GPU hardware and
   the programming tools such as CUDA (compute unified device
   architecture). Here, we have made a research on CUDA and its
   applications. In this paper, we clarify the architecture and characters
   of CUDA, together with the differences of programming between CUDA and
   CPU. The advantages of CUDA in some application fields are also revealed
   to provide new methods and thoughts for further study. Meanwhile, the
   rise of cloud computing provides another choice for the scientists and
   engineers. Consequently we analyze the current situation of scientific
   computing cloud and develop the architecture of cloud service on the
   basis of CUDA. At last, an optimistic outlook is given for the use of
   GPU in scientific computing area.</abstract><date>2015</date><author>Ji, Yimu
   Kuang, Zizhuo
   Pan, Qiaoyu
   Sun, Yanpeng
   Kang, Jiangbang
   Huang, Wei</author></paper><paper><title>Multimodality Neurological Data Visualization With Multi-VOI-Based DTI
   Fiber Dynamic Integration.</title><abstract>Brain lesions are usually located adjacent to critical spinal
   structures, so it is a challenging task for neurosurgeons to precisely
   plan a surgical procedure without damaging healthy tissues and nerves.
   The advancement of medical imaging technologies produces a large amount
   of neurological data, which are capable of showing a wide variety of
   brain properties. Advanced algorithms of medical data computing and
   visualization are critically helpful in efficiently utilizing the
   acquired data for disease diagnosis and brain function and structure
   exploration, which is helpful for treatment planning. In this paper, we
   describe new algorithms and a software framework for multiple volume of
   interest specified diffusion tensor imaging (DTI) fiber dynamic
   visualization. The displayed results have been integrated with a volume
   rendering pipeline for multimodality neurological data exploration. A
   depth texture indexing algorithm is used to detect DTI fiber tracts in
   graphics process units (GPUs), which makes fibers to be displayed and
   interactively manipulated with brain data acquired from functional
   magnetic resonance imaging, T 1- and T 2-weighted anatomic imaging, and
   angiographic imaging. The developed software platform is built on an
   object-oriented structure, which is transparent and extensible. It
   provides a comprehensive human-computer interface for data exploration
   and information extraction. The GPU-accelerated high-performance
   computing kernels have been implemented to enable our software to
   dynamically visualize neurological data. The developed techniques will
   be useful in computer-aided neurological disease diagnosis, brain
   structure exploration, and general cognitive neuroscience. </abstract><date>2016-Jan</date><author>Zhang, Qi
   Alexander, Murray
   Ryner, Lawrence</author></paper><paper><title>Foreword to the Special Section on the Spring Conference on Computer
   Graphics 2015 (SCCG'2015)</title><abstract></abstract><date>DEC 2015</date><author>Durikovic, Roman
   Santos, Luis Paulo</author></paper><paper><title>Which Images and Features in Graphic Cigarette Warnings Predict Their
   Perceived Effectiveness? Findings from an Online Survey of Residents in
   the UK</title><abstract>Background Many countries are implementing graphic warnings for
   cigarettes. Which graphic features influence their effectiveness remains
   unclear.Purpose To identify features of graphic warnings predicting
   their perceived effectiveness in discouraging smoking.Method Guided by
   the Common-Sense Model of responses to health threats, we
   content-analyzed 42 graphic warnings for attributes of illness risk
   representations and media features (e.g., photographs, metaphors). Using
   data from 15,536 survey participants, we conducted stratified logistic
   regressions testing which attributes predict participant selections of
   warnings as effective.Results Images of diseased body parts predicted
   greater perceived effectiveness; OR = 6.53-12.45 across smoking status
   (smoker, ex-smoker, young non-smoker) groups. Features increasing
   perceived effectiveness included images of dead or sick persons,
   children, and medical technology; focus on cancer; and photographs.
   Attributes decreasing perceived effectiveness included
   infertility/impotence, addictiveness, cigarette chemicals, cosmetic
   appearance, quitting self-efficacy, and metaphors.Conclusions These
   findings on representational and media attributes predicting perceived
   effectiveness can inform strategies for generating graphic warnings.</abstract><date>OCT 2015</date><author>Cameron, Linda D.
   Williams, Brian</author></paper><paper><title>Generalized additive models as an alternative approach to the modelling
   of the tree height-diameter relationship</title><abstract>Generalized additive models were tested using three types of smoothing
   functions as an alternative for modelling the height curve. The models
   were produced for 23 forest stands of Norway spruce (Picea abies [L.]
   Karst.) in the territory of the Training Forest Enterprise Masaryk
   Forest Kitiny. The results show that the best evaluated and recommended
   for practical use at the level of forest stand was the LOESS function
   (locally weighted scatterplot smooting) when using a greater width of
   the bandwidth. Due to the frequent overfitting and the associated
   unrealistic behaviour of the function, smoothing by spline functions
   cannot be recommended for modelling the height curve at the level of
   forest stand. It was validated that the resulting model must be assessed
   not only according to the calculated quality criteria, but also
   depending on the graphic pattern of the model which must ensure that the
   height curve pattern is realistic. The quality of the resulting models
   (with LOESS function) was assessed to be high, mainly due to the very
   precise determination of model heights.</abstract><date>JUN 2015</date><author>Adamec, Z.
   Drapela, K.</author></paper><paper><title>Improving the Mapping of Smith-Waterman Sequence Database Searches onto
   CUDA-Enabled GPUs</title><abstract>Sequence alignment lies at heart of the bioinformatics. The
   Smith-Waterman algorithm is one of the key sequence search algorithms
   and has gained popularity due to improved implementations and rapidly
   increasing compute power. Recently, the Smith-Waterman algorithm has
   been successfully mapped onto the emerging general-purpose graphics
   processing units (GPUs). In this paper, we focused on how to improve the
   mapping, especially for short query sequences, by better usage of shared
   memory. We performed and evaluated the proposed method on two different
   platforms (Tesla C1060 and Tesla K20) and compared it with two classic
   methods in CUDASW++. Further, the performance on different numbers of
   threads and blocks has been analyzed. The results showed that the
   proposed method significantly improves Smith-Waterman algorithm on
   CUDA-enabled GPUs in proper allocation of block and thread numbers.</abstract><date>2015</date><author>Huang, Liang-Tsung
   Wu, Chao-Chin
   Lai, Lien-Fu
   Li, Yun-Ju</author></paper><paper><title>Photochemical Fate of Amicarbazone in Aqueous Media: Laboratory
   Measurement and Simulations</title><abstract>Amicarbazone (AMZ) is an extensively used, broad-spectrum triazolinone
   herbicide. The literature is scarce regarding experimental data on AMZ
   photodegradation, whose fate in natural waters has not yet been
   investigated in detail. By combining laboratory experiments using
   isolated natural organic matter, literature data, and mathematical
   simulations, we investigated the sunlight-driven direct and indirect
   degradation of AMZ. We show that the reaction with hydroxyl radicals
   ([GRAPHICS]) is the main pathway leading to AMZ degradation, with
   measured second-order reaction rate constant (k(AMZ, OH)) equal to
   2.05x10(10) L/mol center dot s. Simulations suggest that amicarbazone
   degradation is favored in shallow water bodies containing low dissolved
   organic carbon (DOC) and bicarbonate/carbonate concentrations, with
   herbicide half-life varying from about less than 1 day to more than 2
   months. These values of t(1/2) are upper limits since the reaction with
   (CDOM)-C-3* was not considered. Finally, the cross-effects of water
   depth/[DOC] are slightly influenced by nitrate/nitrite and
   bicarbonate/carbonate concentrations, depending on the pH range.</abstract><date>AUG 1 2015</date><author>Silva, Marcela Prado
   Mostafa, Simon
   McKay, Garrett
   Rosario-Ortiz, Fernando L.
   Silva Costa Teixeira, Antonio Carlos</author></paper><paper><title>Alinea: An Advanced Linear Algebra Library for Massively Parallel
   Computations on Graphics Processing Units</title><abstract>Direct and iterative methods are often used to solve linear systems in
   engineering. The matrices involved can be large, which leads to heavy
   computations on the central processing unit. A graphics processing unit
   can be used to accelerate these computations. In this paper, we propose
   a new library, named Alinea, for advanced linear algebra. This library
   is implemented in C++, CUDA and OpenCL. It includes several linear
   algebra operations and numerous algorithms for solving linear systems.
   For both central processing unit and graphic processing unit devices,
   there are different matrix storage formats, and real and complex
   arithmetics in single- and double-precision. The CUDA version includes a
   self-tuning of the grid, i.e. threading distribution, depending upon the
   hardware configuration and the size of the problems. Numerical
   experiments and comparison with existing libraries illustrates the
   efficiency, accuracy and robustness of the proposed library.</abstract><date>AUG 2015</date><author>Magoules, Frederic
   Ahamed, Abal-Kassim Cheik</author></paper><paper><title>PLASTIC LETTERS: ALPHABET MIXING AND IDEOLOGIES OF PRINT IN UKRAINIAN
   SHOP SIGNS</title><abstract>This article examines the complex intersection of language ideologies
   shaping alphabetic choices in Ukrainian outdoor advertising and shop
   signs, focusing on alphabet mixing through the insertion of Latin
   letters into Cyrillic texts and the juxtaposition of parallel or
   alternating texts using both of these writing systems. Drawing upon
   ethnographic data from work with graphic designers and consumers as well
   as analysis of language use in signs, I argue that while alphabet mixing
   is often characterized as "faddish" or "youth-oriented" these practices
   also reflect Soviet-era ideological stances towards both Latin
   typefaces, seen as "plastic" letters associated with Western capitalism,
   and Cyrillic typefaces, seen as "rigid" forms subject to strong central
   control by the Soviet state. The increasing availability of personal
   computers with word-processing and graphic design software in Ukraine
   has both increased access by individuals to print technology, and
   promoted a new typographic aesthetic through the dissemination of
   Cyrillic fonts based on Latin, not Soviet or pre-Soviet Cyrillic,
   models.</abstract><date>DEC 2015</date><author>Dickinson, Jennifer A.</author></paper><paper><title>Physically-Accurate Fur Reflectance: Modeling, Measurement and Rendering</title><abstract>Rendering photo-realistic animal fur is a long-standing problem in
   computer graphics. Considerable effort has been made on modeling the
   geometric complexity of fur, but the reflectance of fur fibers is not
   well understood. Fur has a distinct diffusive and saturated appearance,
   that is not captured by either the Marschner hair model or the
   Kajiya-Kay model. In this paper, we develop a physically-accurate
   reflectance model for fur fibers. Based on anatomical literature and
   measurements, we develop a double cylinder model for the reflectance of
   a single fur fiber, where an outer cylinder represents the biological
   observation of a cortex covered by multiple cuticle layers, and an inner
   cylinder represents the scattering interior structure known as the
   medulla. Our key contribution is to model medulla scattering
   accurately-in contrast, for human hair, the medulla has minimal width
   and thus negligible contributions to the reflectance. Medulla scattering
   introduces additional reflection and transmission paths, as well as
   diffusive reflectance lobes. We validate our physical model with
   measurements on real fur fibers, and introduce the first database in
   computer graphics of reflectance profiles for nine fur samples. We show
   that our model achieves significantly better fits to the measured data
   than the Marschner hair reflectance model. For efficient rendering, we
   develop a method to precompute 2D medulla scattering profiles and
   analytically approximate our reflectance model with factored lobes. The
   accuracy of the approach is validated by comparing our rendering model
   to full 3D light transport simulations. Our model provides an enriched
   set of controls, where the parameters we fit can be directly used to
   render realistic fur, or serve as a starting point from which artists
   can manually tune parameters for desired appearances.</abstract><date>NOV 2015</date><author>Yan, Ling-Qi
   Tseng, Chi-Wei
   Jensen, Henrik Wann
   Ramamoorthi, Ravi</author></paper><paper><title>Designing Postdisaster Temporary Housing Facilities: Case Study in
   Indonesia</title><abstract>This study reports a case study regarding designs of postdisaster
   temporary housing facilities in Indonesian communities. Frequent natural
   disasters have caused severe damage in Indonesia. Thus, during the
   period from emergency relief to permanent residence, accommodating
   communities in disaster areas is imperative. While investigating current
   temporary housing solutions regarding the design of housing facilities,
   three problems are identified: inappropriate information exploration,
   insufficient information representation, and inconvenient information
   integration. Indonesian communities ultimately are not satisfied with
   the temporary housing, because they experience difficulty in identifying
   resettlement sites, designing housing facilities, and understanding the
   interdependence between the sites and facilities. To resolve these
   problems, the author proposes an approach consisting of geographical,
   building, and graphics information-based mechanisms. After some tests,
   the results show that, compared to conventional methods (i. e.,
   paper-based maps and drawings) with human processes, this approach more
   effectively helps to identify resettlement sites, computerize building
   models, and integrate the sites and models. Overall, this study offers a
   useful reference for similar applications in postdisaster reconstruction
   and management. (C) 2014 American Society of Civil Engineers.</abstract><date>AUG 2015</date><author>Tsai, Ming-Kuan</author></paper><paper><title>Flow-induced deformation of closed disclination lines near a spherical
   colloid immersed in a nematic host phase</title><abstract>We present nonequilibrium molecular dynamics simulations of a spherical
   colloidal particle with a chemically homogeneous surface immersed in a
   nematic liquid-crystal host phase. This setup is then placed between
   planar and atomically structured substrate surfaces that serve to fix
   the nematic far-field director . The substrates are separated by a
   sufficiently large distance such that they do not interfere directly
   with the environment of the colloid. Because of a mismatch between and
   the local homeotropic anchoring of molecules of the liquid crystal
   (i.e., mesogens) at the surface of the colloid circular defect (Saturn)
   rings arise if the host is in thermodynamic equilibrium (i.e., in the
   absence of flow). The size of these rings depends on the range of the
   mesogen-colloid interactions which we model via an attractive Yukawa
   potential. As Poiseuille flow is initiated, is deformed. The degree of
   deformation is analysed quantitatively in terms of characteristic
   geometric parameters fitted to suitable projections of . Our results
   suggest that smaller are shifted downstream while approximately
   maintaining their circular shape, whereas larger ones exhibit an elastic
   deformation in addition. We provide a simple geometric argument to
   predict the downstream shift of smaller, circular s in excellent
   agreement with the simulation data over the range of steady-state flows
   considered.[GRAPHICS]</abstract><date>JAN 17 2016</date><author>Stieger, Tillmann
   Pueschel-Schlotthauer, Sergej
   Schoen, Martin
   Mazza, Marco G.</author></paper><paper><title>Equine Anatomedia: Development, Integration and Evaluation of an
   E-Learning Resource in Applied Veterinary Anatomy</title><abstract>In Egypt, there is a growing movement to encourage veterinary
   student-centered learning using the most up to date educational
   technologies. This paper focuses on a computer-facilitated learning
   program "Equine Anatomedia", which comprises now two modules (head and
   digit) fully integrated with the applied anatomy curriculum at
   Alexandria and Damanhour Universities. The educational design of this
   program allows students and clinicians to explore anatomical concepts,
   principles and procedure guidelines in a manner more suited to their
   individual learning needs than traditional methods. The program
   comprises over 300 high quality images and diagrams, audio and video
   clips as well as animated graphics with colored keys highlighting the
   anatomical features. Staff and student feedback indicates that Equine
   Anatomedia is an effective and engaging learning tool which helps
   students to develop their knowledge in anatomy and to appreciate its
   relevance in clinical situations. In addition, it encourages
   student-staff interaction and is a useful tool in overcoming the
   challenges of limited resources and increasing numbers of students.     
    </abstract><date>2015-12</date><author>El Sharaby, Ashraf A
   Alsafy, Mohamed A. M
   El-Gendy, Samir A.A</author></paper><paper><title>Estimation of Noise Level in Complex Textured Images and Monte
   Carlo-Rendered Images</title><abstract>The several noise level estimation algorithms that have been developed
   for use in image processing and computer graphics generally exhibit good
   performance. However, there are certain special types of noisy images
   that such algorithms are not suitable for. It is particularly still a
   challenge to use the algorithms to estimate the noise levels of complex
   textured photographic images because of the inhomogeneity of the
   original scenes. Similarly, it is difficult to apply most conventional
   noise level estimation algorithms to images rendered by the Monte Carlo
   (MC) method owing to the spatial variation of the noise in such images.
   This paper proposes a novel noise level estimation method based on
   histogram modification, and which can be used for more accurate
   estimation of the noise levels in both complex textured images and
   MC-rendered images. The proposed method has good performance, is simple
   to implement, and can be efficiently used in various image-based and
   graphic applications ranging from smartphone camera noise removal to
   game background rendition.</abstract><date>JAN 31 2016</date><author>Kim, I-Gil</author></paper><paper><title>Precise Haptic Device Co-Location for Visuo-Haptic Augmented Reality</title><abstract>Visuo-haptic augmented reality systems enable users to see and touch
   digital information that is embedded in the real world. PHANToM haptic
   devices are often employed to provide haptic feedback. Precise
   co-location of computer-generated graphics and the haptic stylus is
   necessary to provide a realistic user experience. Previous work has
   focused on calibration procedures that compensate the non-linear
   position error caused by inaccuracies in the joint angle sensors. In
   this article we present a more complete procedure that additionally
   compensates for errors in the gimbal sensors and improves position
   calibration. The proposed procedure further includes software-based
   temporal alignment of sensor data and a method for the estimation of a
   reference for position calibration, resulting in increased robustness
   against haptic device initialization and external tracker noise. We
   designed our procedure to require minimal user input to maximize
   usability. We conducted an extensive evaluation with two different
   PHANToMs, two different optical trackers, and a mechanical tracker.
   Compared to state-of-the-art calibration procedures, our approach
   significantly improves the co-location of the haptic stylus. This
   results in higher fidelity visual and haptic augmentations, which are
   crucial for fine-motor tasks in areas such as medical training
   simulators, assembly planning tools, or rapid prototyping applications.</abstract><date>DEC 2015</date><author>Eck, Ulrich
   Pankratz, Frieder
   Sandor, Christian
   Klinker, Gudrun
   Laga, Hamid</author></paper><paper><title>Controlling the Simulation of Cumuliform Clouds Based on Fluid Dynamics</title><abstract>Controlling fluid simulation is one of the important research topics in
   computer graphics. In this paper, we focus on controlling the simulation
   of cumuliform cloud formation. Using a previously proposed method for
   controlling cloud simulation the convergence speed is very slow;
   therefore, it takes a long time before the clouds form the desired
   shapes. We improved the method and accelerated the convergence by
   introducing a new mechanism for controlling the amount of water vapor
   added. We demonstrate the effectiveness of the proposed method by
   several examples.</abstract><date>NOV 2015</date><author>Kawaguchi, Tatsuki
   Dobashi, Yoshinori
   Yamamoto, Tsuyoshi</author></paper><paper><title>BFS-4K: An Efficient Implementation of BFS for Kepler GPU Architectures</title><abstract>Breadth-first search (BFS) is one of the most common graph traversal
   algorithms and the building block for a wide range of graph
   applications. With the advent of graphics processing units (GPUs),
   several works have been proposed to accelerate graph algorithms and, in
   particular, BFS on such many-core architectures. Nevertheless, BFS has
   proven to be an algorithm for which it is hard to obtain better
   performance from parallelization. Indeed, the proposed solutions take
   advantage of the massively parallelism of GPUs but they are often
   asymptotically less efficient than the fastest CPU implementations. This
   paper presents BFS-4K, a parallel implementation of BFS for GPUs that
   exploits the more advanced features of GPU-based platforms (i.e., NVIDIA
   Kepler) and that achieves an asymptotically optimal work complexity. The
   paper presents different strategies implemented in BFS-4K to deal with
   the potential workload imbalance and thread divergence caused by any
   actual graph non-homogeneity. The paper presents the experimental
   results conducted on several graphs of different size and
   characteristics to understand how the proposed techniques are applied
   and combined to obtain the best performance from the parallel BFS
   visits. Finally, an analysis of the most representative BFS
   implementations for GPUs at the state of the art and their comparison
   with BFS-4K are reported to underline the efficiency of the proposed
   solution.</abstract><date>JUL 2015</date><author>Busato, Federico
   Bombieri, Nicola</author></paper><paper><title>GPU accelerated dynamic functional connectivity analysis for functional
   MRI data</title><abstract>Recent advances in multi-core processors and graphics card based
   computational technologies have paved the way for an improved and
   dynamic utilization of parallel computing techniques. Numerous
   applications have been implemented for the acceleration of
   computationally-intensive problems in various computational science
   fields including bioinformatics, in which big data problems are
   prevalent. In neuroimaging, dynamic functional connectivity (DFC)
   analysis is a computationally demanding method used to investigate
   dynamic functional interactions among different brain regions or
   networks identified with functional magnetic resonance imaging (fMRI)
   data. In this study, we implemented and analyzed a parallel DFC
   algorithm based on thread-based and block-based approaches. The
   thread-based approach was designed to parallelize DFC computations and
   was implemented in both Open Multi-Processing (OpenMP) and Compute
   Unified Device Architecture (CUDA) programming platforms. Another
   approach developed in this study to better utilize CUDA architecture is
   the block-based approach, where parallelization involves smaller parts
   of fMRI time-courses obtained by sliding-windows. Experimental results
   showed that the proposed parallel design solutions enabled by the GPUs
   significantly reduce the computation time for DFC analysis. Multicore
   implementation using OpenMP on 8-core processor provides up to 7.7x
   speed-up. GPU implementation using CUDA yielded substantial
   accelerations ranging from 18.5x to 157x speed-up once thread-based and
   block-based approaches were combined in the analysis. Proposed parallel
   programming solutions showed that multi-core processor and
   CUDA-supported GPU implementations accelerated the DFC analyses
   significantly. Developed algorithms make the DFC analyses more practical
   for multi-subject studies with more dynamic analyses. (C) 2015 Elsevier
   Ltd. All rights reserved.</abstract><date>JUL 2015</date><author>Akgun, Devrim
   Sakoglu, Uenal
   Esquivel, Johnny
   Adinoff, Bryon
   Mete, Mutlu</author></paper><paper><title>Compute Unified Device Architecture-Based Parallel Dose-Volume Histogram
   Computation</title><abstract>Radiotherapy planning is often evaluated using a dose-volume histogram
   (DVH). With the increases in the number of patient computed tomography
   (CT) slices and the development of intensity-modulated radiation therapy
   technology, the statistical process of DVH now requires a large number
   of cubic interpolations. However, the sequential single-threaded DVH
   codes can no longer meet the real-time requirements. This paper presents
   an efficient parallel DVH algorithm using the Compute Unified Device
   Architecture (CUDA) platform. The parallel algorithm first divides the
   patient's tumor targets and key organs covered by CT images into
   independent parts. These are then sampled, and the sampled points are
   loaded into the global graphics processing unit memory by CUDA streams.
   The dose matrix is loaded into the texture memory, on which the cubic
   interpolation is frequently computed. The statistical results for the
   DVH are loaded into the high-speed shared memory. We present a voting
   algorithm that reduces the occurrence of conflicts in this shared
   memory. Experimental results demonstrate an average speedup factor of
   7.29 for the CUDA-based DVH algorithm, with further improvements in
   conflict avoidance increasing this speedup to a factor of 7.92.</abstract><date>AUG 2015</date><author>Wang, Yangping
   Deng, Chong
   Li, Lian
   Dang, Jianwu</author></paper><paper><title>An analytic linear accelerator source model for GPU-based Monte Carlo
   dose calculations</title><abstract>Recently, there has been a lot of research interest in developing fast
   Monte Carlo (MC) dose calculation methods on graphics processing unit
   (GPU) platforms. A good linear accelerator (linac) source model is
   critical for both accuracy and efficiency considerations. In principle,
   an analytical source model should be more preferred for GPU-based MC
   dose engines than a phase-space file-based model, in that data loading
   and CPU-GPU data transfer can be avoided. In this paper, we presented an
   analytical field-independent source model specifically developed for
   GPU-based MC dose calculations, associated with a GPU-friendly sampling
   scheme. A key concept called phase-space-ring (PSR) was proposed. Each
   PSR contained a group of particles that were of the same type, close in
   energy and reside in a narrow ring on the phase-space plane located just
   above the upper jaws. The model parameterized the probability densities
   of particle location, direction and energy for each primary photon PSR,
   scattered photon PSR and electron PSR. Models of one 2D Gaussian
   distribution or multiple Gaussian components were employed to represent
   the particle direction distributions of these PSRs. A method was
   developed to analyze a reference phase-space file and derive
   corresponding model parameters. To efficiently use our model in MC dose
   calculations on GPU, we proposed a GPU-friendly sampling strategy, which
   ensured that the particles sampled and transported simultaneously are of
   the same type and close in energy to alleviate GPU thread divergences.
   To test the accuracy of our model, dose distributions of a set of open
   fields in a water phantom were calculated using our source model and
   compared to those calculated using the reference phase-space files. For
   the high dose gradient regions, the average distance-to-agreement (DTA)
   was within 1 mm and the maximum DTA within 2 mm. For relatively low dose
   gradient regions, the root-mean-square (RMS) dose difference was within
   1.1% and the maximum dose difference within 1.7%. The maximum relative
   difference of output factors was within 0.5%. Over 98.5% passing rate
   was achieved in 3D gamma-index tests with 2%/2 mm criteria in both an
   IMRT prostate patient case and a head-and-neck case. These results
   demonstrated the efficacy of our model in terms of accurately
   representing a reference phase-space file. We have also tested the
   efficiency gain of our source model over our previously developed
   phase-space-let file source model. The overall efficiency of dose
   calculation was found to be improved by similar to 1.3-2.2 times in
   water and patient cases using our analytical model.</abstract><date>OCT 21 2015</date><author>Tian, Zhen
   Li, Yongbao
   Folkerts, Michael
   Shi, Feng
   Jiang, Steve B.
   Jia, Xun</author></paper><paper><title>Patient Knowledge on Malaria Symptoms Is a Key to Promoting Universal
   Access of Patients to Effective Malaria Treatment in Palawan, the
   Philippines</title><abstract>IntroductionPalawan, where health care facilities are still limited, is
   one of the most malaria endemic provinces in the Philippines. Since
   1999, microscopists (community health workers) have been trained in
   malaria diagnosis and feasibility of early diagnosis and treatments have
   been enhanced throughout the province. To accelerate the universal
   access of malaria patients to diagnostic testing in Palawan, positive
   health seeking behavior should be encouraged when malaria infection is
   suspected.MethodsIn this cross-sectional study, structured interviews
   were carried out with residents (N = 218) of 20 remote malaria-endemic
   villages throughout Palawan with a history of suspected malaria from
   January to February in 2012. Structural equation modeling (SEM) was
   conducted to determine factors associated with appropriate treatment,
   which included: (1) socio-demo-graphic characteristics; (2) proximity to
   a health facility; (3) health seeking behavior; (4) knowledge on
   malaria; (5) participation in community awareness-raising
   activities.ResultsThree factors independently associated with
   appropriate treatment were identified by SEM (CMIN = 10.5, df = 11, CFI
   = 1.000, RMSEA = .000): "living near microscopist" (p &lt; 0.001), "not
   living near private pharmacy" (p &lt; 0.01), and "having severe symptoms"
   (p &lt; 0.01). "Severe symptoms" were positively correlated with more
   "knowledge on malaria symptoms" (p &lt; 0.001). This knowledge was
   significantly increased by attending "community awareness-raising
   activities by microscopists" (p &lt; 0.001).ConclusionsIn the
   resource-limited settings, microscopists played a significant role in
   providing appropriate treatment to all participants with severe malaria
   symptoms. However, it was considered that knowledge on malaria symptoms
   made participants more aware of their symptoms, and further progressed
   self-triage. Strengthening this recognition sensitivity and making
   residents aware of nearby microscopists may be the keys to accelerating
   universal access to effective malaria treatment in Palawan.</abstract><date>JUN 16 2015</date><author>Matsumoto-Takahashi, Emilie Louise Akiko
   Tongol-Rivera, Pilarita
   Villacorte, Elena A.
   Angluben, Ray U.
   Jimba, Masamine
   Kano, Shigeyuki</author></paper><paper><title>Efficient parallel boolean matrix based algorithms for computing
   composite rough set approximations</title><abstract>In information systems, there may exist multiple different types of
   attributes like categorical attributes, numerical attributes, set-valued
   attributes, interval-valued attributes, missing attributes, etc. Such
   information systems are called as composite information systems. To
   process such attributes with rough set theory, composite rough set model
   and corresponding matrix methods were introduced in our previous study.
   Calculation of rough set approximations of a concept is the key step for
   rule acquisition and attribute reduction in rough set based methods. To
   accelerate the computation process of rough set approximations, this
   paper first presents the boolean matrix representation of the lower and
   upper approximations in the composite information system, then designs a
   parallel method for computing approximations based on matrix, and
   implements it on Multi-GPU. The experiments on data sets from UCI and
   user-defined data sets show that the proposed method can accelerate the
   computation process efficiently. The Multi-GPU implementation achieves
   up to a speedup of 334.9 over the CPU implementation. (C) 2015 Elsevier
   Inc. All rights reserved.</abstract><date>FEB 1 2016</date><author>Zhang, Junbo
   Zhu, Yun
   Pan, Yi
   Li, Tianrui</author></paper><paper><title>A Graphic Processing Unit Web Server for Computing Correlation
   Coefficients for Gene Expression Data</title><abstract>Pairwise correlation analysis of gene expression profile is important
   for molecular biology research. It is usually used for building the
   regulation network, reconstructing the evolutionary process, and
   analyzing tissue or spatiotemporal specific genes. In this paper, we
   developed a GPU (Graphic Processing Unit) based web server, which can
   calculate 5 kinds of common correlation coefficient in parallel. Our web
   server is capable to deal with large scale data (e.g., processing a
   matrix size of 1000 x 1000) in seconds, which is thousands of times
   faster than single thread program. We also developed a Hadoop based jar
   package, which could further increase the efficiency for massive data on
   cloud computing platform. Users could visit the friendly web server via
   http://datamining.xmu.edu.cn/software/correlation/.</abstract><date>APR 2015</date><author>Zhao, Yuming
   Zou, Quan
   Jiang, Yi
   Wang, Guohua</author></paper><paper><title>Mixed Linear Model Approaches of Association Mapping for Complex Traits
   Based on Omics Variants</title><abstract>Precise prediction for genetic architecture of complex traits is impeded
   by the limited understanding on genetic effects of complex traits,
   especially on gene-by-gene (GxG) and gene-by-environment (GxE)
   interaction. In the past decades, an explosion of high throughput
   technologies enables omics studies at multiple levels (such as genomics,
   transcriptomics, proteomics, and metabolomics). The analyses of large
   omics data, especially two-loci interaction analysis, are very time
   intensive. Integrating the diverse omics data and environmental effects
   in the analyses also remain challenges. We proposed mixed linear model
   approaches using GPU (Graphic Processing Unit) computation to
   simultaneously dissect various genetic effects. Analyses can be
   performed for estimating genetic main effects, GxG epistasis effects,
   and GxE environment interaction effects on large-scale omics data for
   complex traits, and for estimating heritability of specific genetic
   effects. Both mouse data analyses and Monte Carlo simulations
   demonstrated that genetic effects and environment interaction effects
   could be unbiasedly estimated with high statistical power by using the
   proposed approaches.</abstract><date>JUL 30 2015</date><author>Zhang, Fu-Tao
   Zhu, Zhi-Hong
   Tong, Xiao-Ran
   Zhu, Zhi-Xiang
   Qi, Ting
   Zhu, Jun</author></paper><paper><title>Stochastic simulation and graphic visualization of mitotic processes
   (vol 51, pg 251, 2010)</title><abstract></abstract><date>JAN 1 2016</date><author>Gardner, Melissa K.
   Odde, David J.</author></paper><paper><title>Improved Half Vector Space Light Transport</title><abstract>In this paper, we present improvements to half vector space light
   transport (HSLT) [KHD14], which make this approach more practical,
   robust for difficult input geometry, and faster. Our first contribution
   is the computation of half vector space ray differentials in a different
   domain than the original work. This enables a more uniform
   stratification over the image plane during Markov chain exploration.
   Furthermore, we introduce a new multi chain perturbation in half vector
   space, which, if combined appropriately with half vector perturbation,
   makes the mutation strategy both more robust to geometric configurations
   with fine displacements and faster due to reduced number of ray casts.
   We provide and analyze the results of improved HSLT and discuss possible
   applications of our new half vector ray differentials.</abstract><date>JUL 2015</date><author>Hanika, Johannes
   Kaplanyan, Anton
   Dachsbacher, Carsten</author></paper><paper><title>GPU/CPU parallel computation of material damage</title><abstract>In this paper compute unified device architecture programming and open
   multiprocessing are used for the graphics processing unit and central
   processing unit parallel computation of material damage. The material
   damage is evaluated by a multilevel finite element analysis within
   material domains reconstructed from a high-resolution micro-focus X-ray
   computed tomography system. An effective computational method is
   investigated for solving the linear equations of finite element
   analysis. Numerical results show an encouraging trend in reducing the
   computation cost for the digital diagnosis of material damage.</abstract><date>JUL 2015</date><author>Shen, Jie
   Vela, Diego
   Singh, Ankita
   Song, Kexing
   Zhang, Guoshang
   LaFreniere, Bradon
   Chen, Hao</author></paper><paper><title>Evaluating the Accuracy of Size Perception on Screen-Based Displays:
   Displayed Objects Appear Smaller Than Real Objects</title><abstract>Accurate perception of the size of objects in computer-generated imagery
   is important for a growing number of applications that rely on absolute
   scale, such as medical visualization and architecture. Addressing this
   problem requires both the development of effective evaluation methods
   and an understanding of what visual information might contribute to
   differences between virtual displays and the real world. In the current
   study, we use 2 affordance judgments-perceived graspability of an object
   or reaching through an aperture-to compare size perception in
   high-fidelity graphical models presented on a large screen display to
   the real world. Our goals were to establish the use of perceived
   affordances within spaces near to the observer for evaluating computer
   graphics and to assess whether the graphical displays were perceived
   similarly to the real world. We varied the nature of the affordance task
   and whether or not the display enabled stereo presentation. We found
   that judgments of grasping and reaching through can be made effectively
   with screen-based displays. The affordance judgments revealed that sizes
   were perceived as smaller than in the real world. However, this
   difference was reduced when stereo viewing was enabled or when the
   virtual display was viewed before the real world.</abstract><date>SEP 2015</date><author>Stefanucci, Jeanine K.
   Creem-Regehr, Sarah H.
   Thompson, William B.
   Lessard, David A.
   Geuss, Michael N.</author></paper><paper><title>Fast Wavefront Propagation (FWP) for Computing Exact Geodesic Distances
   on Meshes.</title><abstract>Computing geodesic distances on triangle meshes is a fundamental problem
   in computational geometry and computer graphics. To date, two notable
   classes of algorithms, the Mitchell-Mount-Papadimitriou (MMP) algorithm
   and the Chen-Han (CH) algorithm, have been proposed. Although these
   algorithms can compute exact geodesic distances if numerical computation
   is exact, they are computationally expensive, which diminishes their
   usefulness for large-scale models and/or time-critical applications. In
   this paper, we propose the fast wavefront propagation (FWP) framework
   for improving the performance of both the MMP and CH algorithms. Unlike
   the original algorithms that propagate only a single window (a data
   structure locally encodes geodesic information) at each iteration, our
   method organizes windows with a bucket data structure so that it can
   process a large number of windows simultaneously without compromising
   wavefront quality. Thanks to its macro nature, the FWP method is less
   sensitive to mesh triangulation than the MMP and CH algorithms. We
   evaluate our FWP-based MMP and CH algorithms on a wide range of
   large-scale real-world models. Computational results show that our
   method can improve the speed by a factor of 3-10. </abstract><date>2015-Jul</date><author>Chunxu Xu
   Wang, Tuanfeng Y
   Yong-Jin Liu
   Ligang Liu
   Ying He</author></paper><paper><title>UPDATION OF DISTRIBUTION AND NOTES ON THE BROWN THREE TOED SLOTH
   Bradypus variegatus castaneiceps (PILOSA: BRADIPODIDAE) IN HONDURAS</title><abstract>We present 15 new records for the brown three-toed sloth, Bradypus
   variegatus castaneiceps (Pilosa: Bradypodidae) in Honduras. These
   records confirm the existence of this species in the departments of El
   Paraiso and Atlantida, and expand its geographical distribution from the
   watershed of the Rio Segovia (border with Nicaraguan) to along the
   Caribbean coast of Honduras. Additionally, we propose a potential
   spatial distribution of the species based on the relationship between
   records and physio-graphic characteristics using the program MaxEnt. We
   document the presence of a juvenile in April 2013 and include a
   discussion of the common names of three-toed sloths in Honduras. We
   furthermore evaluate the position of this species on the List of Species
   of Special Concern and the qualification of its status as least concern
   (LC) in the IUCN Red List</abstract><date>JUN 2015</date><author>Marineros, Leonel
   Reyes, Hector Orlando Portillo</author></paper><paper><title>Hardware architecture for full analytical Fraunhofer computer-generated
   holograms</title><abstract>Hardware architecture of parallel computation is proposed for generating
   Fraunhofer computer-generated holograms (CGHs). A pipeline-based
   integrated circuit architecture is realized by employing the modified
   Fraunhofer analytical formulism, which is large scale and enables all
   components to be concurrently operated. The architecture of the CGH
   contains five modules to calculate initial parameters of amplitude,
   amplitude compensation, phases, and phase compensation, respectively.
   The precalculator of amplitude is fully adopted considering the
   "reusable design" concept. Each complex operation type (such as square
   arithmetic) is reused only once by means of a multichannel selector. The
   implemented hardware calculates an 800 x 600 pixels hologram in parallel
   using 39,319 logic elements, 21,074 registers, and 12,651 memory bits in
   an Altera field-programmable gate array environment with stable
   operation at 50 MHz. Experimental results demonstrate that the quality
   of the images reconstructed from the hardware-generated hologram can be
   comparable to that of a software implementation. Moreover, the
   calculation speed is approximately 100 times faster than that of a
   personal computer with an Intel i5-3230M 2.6 GHz CPU for a triangular
   object. (C) 2015 Society of Photo-Optical Instrumentation Engineers
   (SPIE)</abstract><date>SEP 2015</date><author>Pang, Zhi-Yong
   Xu, Zong-Xi
   Xiong, Yi
   Chen, Biao
   Dai, Hui-Min
   Jiang, Shao-Ji
   Dong, Jian-Wen</author></paper><paper><title>A Numerical Study on Stratified Shear Layers With Relevance to Oil-Boom
   Failure</title><abstract>Interface dynamics of two-phase flow, with relevance for leakage of oil
   retained by mechanical oil barriers, is studied by means of a
   two-dimensional (2D) lattice-Boltzmann method (LBM) combined with a
   phase-field model for interface capturing. A multirelaxation-time (MRT)
   model of the collision process is used to obtain a numerically stable
   model at high Reynolds number flow. In the phase-field model, the
   interface is given a finite but small thickness, where the fluid
   properties vary continuously across a thin interface layer. Surface
   tension is modeled as a volume force in the transition layer. The
   numerical model is implemented for simulations with the graphic
   processing unit (GPU) of a desktop personal computer. Verification tests
   of the model are presented. The model is then applied to simulate
   gravity currents (GCs) obtained from a lock-exchange configuration,
   using fluid parameters relevant for those of oil and water. Interface
   instability phenomena are observed, and obtained numerical results are
   in good agreement with theory. This work demonstrates that the numerical
   model presented can be used as a numerical tool for studies of
   stratified shear flows with relevance to oil-boom failure.</abstract><date>AUG 2015</date><author>Kristiansen, David
   Faltinsen, Odd M.</author></paper><paper><title>Molecular Dynamics Simulation on Designed Antibodies of HIV-1 Capsid
   Protein (p24)</title><abstract>Computational approaches have been used by the molecular biologists all
   around the world as a vital tool in developing and improving the
   functional and binding properties of proteins, particularly
   antibody-antigen (Ab-Ag) complexes. Based on a previous work, it has
   been identified that several residues located in the complementary
   determining region (CDR) of antibody fragment of FAB23.5 may serve as a
   potential point mutation modification in order to improve the binding
   interaction and the inhibitory activity between the single chain
   variable fragment (scFv) antibody and the epitope variant of HIV-1 p24
   capsid protein. In this study, several binding residues from the
   variable chain domain of scFv anti-p24 were modified. The designed
   antibody along with the bound antigen was then subjected for molecular
   dynamics (MD) simulation by using PMEMD. CUDA under Amber 12 with NVIDIA
   (R) Kepler (R) GPUs system. GPU system has been fully utilized in this
   work as a mean to speed up the calculation time. From the simulated
   model, the newly possessed binding interaction of the modified Ab-Ag
   complex were acquired by using the Molecular Mechanics/Poisson-Boltzmann
   surface area (MM-PBSA) and Molecular Mechanics/Generalized Born surface
   area (MM-GBSA), respectively.</abstract><date>2015</date><author>Karim, Hana Atiqah Abdul
   Tayapiwatana, Chatchai
   Nimmanpipug, Piyarat
   Zain, Sharifuddin M.
   Rahman, Noorsaadah Abdul
   Lee, Vannajan Sanghiran</author></paper><paper><title>Development of a compact low coherence interferometer based on GPGPU for
   fast microscopic surface measurement on turbine blades</title><abstract>Vertical scanning interferometry (VSI) techniques are widely used to
   profile microscopic surface structures of industrial products. This
   paper introduces a high-precision fast optical measurement system with
   an optimized small sensor head for the measurement of precision surfaces
   on a turbine blade or blisks (blade integrated discs). The non-contact
   measurement system is based on a low coherence interferometer (LCI),
   which is capable of fast profiling of 3D sample surface with a nanometer
   resolution and has a larger measurement range compared to conventional
   microscopes. This results in a large amount of sampled data and a high
   computational time for the evaluation of the data. For this reason, the
   used evaluation algorithm in this paper is accelerated by the Compute
   Unified Device Architecture (CUDA) technology, which allows parallel
   evaluation of the data stack on independent cores of a General Purpose
   Graphics Processing Unit (GPGPU). As a result, the GPU-based optimized
   algorithm is compared with the original CPU-based single-threaded
   algorithm to show the approximate 60x speedup of computing the Hilbert
   Transformation, which is used to find the depth position in the
   correlogram of each pixel of the sampled data. The main advantage of the
   GPU computing for the evaluation algorithm of the LCI is that it can
   reduce the time-consuming data evaluation process and further
   accelerates the whole measurement.</abstract><date>2015</date><author>Li, Yinan
   Kaestner, Markus
   Reithmeier, Eduard</author></paper><paper><title>Using distributed memory parallel computers and GPU clusters for
   multidimensional Monte Carlo integration</title><abstract>The aim of this paper is to show that the multidimensional Monte Carlo
   integration can be efficiently implemented on various distributed memory
   parallel computers and clusters of multicore nodes using recently
   developed parallel versions of linear congruential generator and lagged
   Fibonacci generator pseudorandom number generators. We show how to
   accelerate the overall performance by offloading some computations to
   Graphics Processing Units (GPUs), and we discuss how to transform
   Message Passing Interface (MPI)+OpenMP programs to MPI+OpenMP+CUDA
   model. We explain how to utilize multiple cores of CPUs together with
   multiple GPU accelerators within a single node and how to achieve
   reasonable load balancing of all computational resources of
   GPU-accelerated multicore nodes. We present and discuss the results of
   experiments performed on the following target architectures: IBM Blue
   Gene/Q parallel computer, a cluster of Intel Xeon E5-2660 servers, and a
   Tesla-based GPU cluster with Intel Xeon X5650 multicore processors. The
   results are presented from two points of view: strong scaling and weak
   scaling. We also compare the performance of all considered
   architectures. Copyright (c) 2014 John Wiley &amp; Sons, Ltd.</abstract><date>MAR 25 2015</date><author>Szalkowski, Dominik
   Stpiczynski, Przemyslaw</author></paper><paper><title>Full tensor gravity gradiometry data inversion: Performance analysis of
   parallel computing algorithms</title><abstract>We apply reweighted inversion focusing to full tensor gravity
   gradiometry data using message-passing interface (MPI) and compute
   unified device architecture (CUDA) parallel computing algorithms, and
   then combine MPI with CUDA to formulate a hybrid algorithm. Parallel
   computing performance metrics are introduced to analyze and compare the
   performance of the algorithms. We summarize the rules for the
   performance evaluation of parallel algorithms. We use model and real
   data from the Vinton salt dome to test the algorithms. We find good
   match between model and real density data, and verify the high
   efficiency and feasibility of parallel computing algorithms in the
   inversion of full tensor gravity gradiometry data.</abstract><date>SEP 2015</date><author>Hou Zhen-Long
   Wei Xiao-Hui
   Huang Da-Nian
   Sun Xu</author></paper><paper><title>The Role of Packaging Sites in Efficient and Specific Virus Assembly</title><abstract>During the life cycle of many single-stranded RNA viruses, including
   many human pathogens, a protein shell called the capsid spontaneously
   assembles around the viral genome. Understanding the mechanisms by which
   capsid proteins selectively assemble around the viral RNA amidst diverse
   host RNAs is a key question in virology. In one proposed mechanism,
   short sequences (packaging sites) within the genomic RNA promote rapid
   and efficient assembly through specific interactions with the capsid
   proteins. In this work, we develop a coarse-grained particle-based
   computational model for capsid proteins and RNA that represents protein
   RNA interactions arising both from nonspecific electrostatics and from
   specific packaging site interactions. Using Brownian dynamics
   simulations, we explore how the efficiency and specificity of assembly
   depend on solution conditions (which control protein protein and
   nonspecific protein RNA interactions) and the strength and number of
   packaging sites. We identify distinct regions in parameter space in
   which packaging sites lead to highly specific assembly via different
   mechanisms and others in which packaging sites lead to kinetic traps. We
   relate these computational predictions to in vitro assays for
   specificity in which cognate viral RNAs compete against non-cognate RNAs
   for assembly by capsid proteins. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>JUL 31 2015</date><author>Perlmutter, Jason D.
   Hagan, Michael F.</author></paper><paper><title>A VARIATIONAL APPROACH FOR DETECTING FEATURE LINES ON MESHES</title><abstract>Feature lines are fundamental shape descriptors and have been
   extensively applied to computer graphics, computer-aided design, image
   processing, and non-photorealistic rendering. This paper introduces a
   unified variational framework for detecting generic feature lines on
   polygonal meshes. The classic Mumford-Shah model is extended to
   surfaces. Using G-convergence method and discrete differential geometry,
   we discretize the proposed variational model to sequential coupled
   sparse linear systems. Through quadratic polynomials fitting, we develop
   a method for extracting valleys of functions defined on surfaces. Our
   approach provides flexible and intuitive control over the detecting
   procedure, and is easy to implement. Several measure functions are
   devised for different types of feature lines, and we apply our approach
   to various polygonal meshes ranging from synthetic to measured models.
   The experiments demonstrate both the effectiveness of our algorithms and
   the visual quality of results.</abstract><date>2016</date><author>Tong, Weihua
   Tai, Xuecheng</author></paper><paper><title>Five-dimensional ultrasound system for soft tissue visualization</title><abstract>A five-dimensional ultrasound (US) system is proposed as a real-time
   pipeline involving fusion of 3D B-mode data with the 3D ultrasound
   elastography (USE) data as well as visualization of these fused data and
   a real-time update capability over time for each consecutive scan. 3D
   B-mode data assist in visualizing the anatomy of the target organ, and
   3D elastography data adds strain information.We investigate the
   feasibility of such a system and show that an end-to-end real-time
   system, from acquisition to visualization, can be developed. We present
   a system that consists of (a) a real-time 3D elastography algorithm
   based on a normalized cross-correlation (NCC) computation on a GPU; (b)
   real-time 3D B-mode acquisition and network transfer; (c) scan
   conversion of 3D elastography and B-mode volumes (if acquired by 4D
   wobbler probe); and (d) visualization software that fuses, visualizes,
   and updates 3D B-mode and 3D elastography data in real time.We achieved
   a speed improvement of 4.45-fold for the threaded version of the
   NCC-based 3D USE versus the non-threaded version. The maximum speed was
   79 volumes/s for 3D scan conversion. In a phantom, we validated the
   dimensions of a 2.2-cm-diameter sphere scan-converted to B-mode volume.
   Also, we validated the 5D US system visualization transfer function and
   detected 1- and 2-cm spherical objects (phantom lesion). Finally, we
   applied the system to a phantom consisting of three lesions to delineate
   the lesions from the surrounding background regions of the phantom.A 5D
   US system is achievable with real-time performance. We can distinguish
   between hard and soft areas in a phantom using the transfer functions.</abstract><date>DEC 2015</date><author>Deshmukh, Nishikant P.
   Caban, Jesus J.
   Taylor, Russell H.
   Hager, Gregory D.
   Boctor, Emad M.</author></paper><paper><title>Research on Visualization of Multi-Dimensional Real-Time Traffic Data
   Stream Based on Cloud Computing</title><abstract>Based on efficient continuous parallel query series algorithm supporting
   multi-objective optimization, by using visual graphics technology for
   traffic data streams for efficient real-time graphical visualization, it
   improve human-computer interaction, to realize real-time and visual data
   analysis and to improve efficiency and accuracy of the analysis. This
   paper employs data mining processing and statistical analysis on
   real-time traffic data stream, based on the parameters standards of
   various data mining algorithms, and by using computer graphics and image
   processing technology, converts graphics or images and make them
   displayed on the screen according to the system requirements, in order
   to track, forecast and maintain the operating condition of all traffic
   service systems effectively. (C) 2016 The Authors. Published by Elsevier
   Ltd.</abstract><date>2016</date><author>Jia Chaolong
   Wang Hanning
   Wei Lili</author></paper><paper><title>Guidewire path determination for intravascular applications</title><abstract>Vascular diseases are among the major causes of death in developed
   countries and the treatment of those pathologies may require
   endovascular interventions, in which the physician utilizes guidewires
   and catheters through the vascular system to reach the injured vessel
   region. Several computational studies related to endovascular procedures
   are in constant development. Thus, predicting the guidewire path may be
   of great value for both physicians and researchers. However, attaining
   good accuracy and precision is still an important issue. We propose a
   method to simulate and predict the guidewire and catheter path inside a
   blood vessel based on equilibrium of a new set of forces, which leads,
   iteratively, to the minimum energy configuration. This technique was
   validated with phantoms using a empty set0.33mm stainless steel
   guidewire and compared to other relevant methods in the literature. This
   method presented RMS error 0.30mm and 0.97mm, which represents less than
   2% and 20% of the lumen diameter of the phantom, in 2D and 3D cases,
   respectively. The proposed technique presented better results than other
   methods from the literature, which were included in this work for
   comparison. Moreover, the algorithm presented low variation
   (&lt;inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink") due to the
   variation of the input parameters. Therefore, even for a wide range of
   different parameters configuration, similar results are presented for
   the proposed approach, which is an important feature and makes this
   technique easier to work with. Since this method is based on basic
   physics, it is simple, intuitive, easy to learn and easy to adapt.</abstract><date>APR 25 2016</date><author>Cardoso, Fernando M.
   Furuie, Sergio S.</author></paper><paper><title>Fast computation of bare soil surface roughness on a Fermi GPU</title><abstract>Surface roughness is an important factor in bare soil microwave
   radiation for the observation of the Earth. Correlation length and
   standard deviation of surface height are the two statistical parameters
   that describe surface roughness. However, when the number of data points
   is large, the calculation of surface roughness parameters becomes
   time-consuming.Therefore, it is desired to have a high-performance
   computing facility to execute this task. A Graphics Processing Unit
   (GPU) provides hundreds of computing cores along with a high memory
   bandwidth. To carry out a parallel implementation of the algorithms,
   Compute Unified Device Architecture (CUDA) provides researchers with an
   easy way to execute multiple threads in parallel on GPUs. In this paper,
   we propose a GPU-based parallel computing method for 2D surface
   roughness estimation. We use an NVIDIA GeForce GTX 590 graphics card to
   run the CUDA implementation. The experimental input data is collected by
   our in-house surface roughness tester which is designed based on the
   laser triangulation principle, giving sample data points of up to
   52,040. According to the experimental results, the serial CPU version of
   the implementation takes 5422 s whereas our GPU implementation takes
   only 47 s, resulting a significant 115 x speedup. (C) 2015 Elsevier Ltd.
   All rights reserved.</abstract><date>SEP 2015</date><author>Li, Xiaojie
   Song, Changhe
   Lopez, Sebastian
   Li, Yunsong
   Lopez, Jose F.</author></paper><paper><title>Adaptation of quadtree meshes in the scaled boundary finite element
   method for crack propagation modelling</title><abstract>A crack propagation modelling technique combining the scaled boundary
   finite element method and quadtree meshes is developed. This technique
   automatically satisfies the compatibility requirement between adjacent
   quadtree cells irrespective of the presence of hanging nodes. The
   quadtree structure facilitates efficient data storage and rapid
   computations. Only a single cell is required to accurately model the
   stress field near crack tips. Crack growth is modelled by splitting the
   cells in the mesh into two. The resulting polygons are directly modelled
   by the scaled boundary formulation with minimal changes to the mesh.
   Four numerical examples demonstrate the salient features of the
   technique. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>AUG 2015</date><author>Ooi, E. T.
   Man, H.
   Natarajan, S.
   Song, C.</author></paper><paper><title>Parallelization of 3D MPDATA Algorithm Using Many Graphics Processors</title><abstract>EULAG (Eulerian/semi-Lagrangian fluid solver) is an established
   numerical model for simulating thermo-fluid flows across a wide range of
   scales and physical scenarios. The multidimensional positive definite
   advection transport algorithm (MPDATA) is among the most time-consuming
   components of EULAG. In this study, we focus on adapting the 3D MPDATA
   computations to clusters with graphics processors. Our approach is based
   on a hierarchical decomposition including the level of cluster, as well
   as an optimized distribution of computations between GPU resources
   within each node. To implement the resulting computing scheme, the MPI
   standard is used across nodes, while CUDA is applied inside nodes. We
   present performance results for the 3D MPDATA code running on the NVIDIA
   GeForce GTX TITAN graphics card, as well as on the Piz Daint cluster
   equipped with NVIDIA Tesla K20x GPUs. In particular, the sustained
   performance of 138 Gflop/s is achieved for a single GPU, which scales up
   to more than 11 Tflop/s for 256 GPUs.</abstract><date>2015</date><author>Rojek, Krzysztof
   Wyrzykowski, Roman</author></paper><paper><title>Rapid motion planning algorithm for optimal UVMS interventions in
   semi-structured environments using GPUs</title><abstract>In this work, a novel local motion planning algorithm is presented, for
   underwater vehicle manipulator systems (UVMS) that perform autonomous
   underwater inspection operations. An optimization problem is formulated
   considering the collision avoidance, the approximation of the given task
   curve and critical optimization criteria. The searching method is based
   on an evolutionary algorithm and it is able to generate a local motion
   plan using continuously updated sensor information. The working
   environment is represented by a Bump-surface entity, constantly updated
   by a parallel algorithm implemented on a graphical processing unit
   (GPU). A trained artificial neural network is used for the fast
   approximation of the considered dexterity index. The local planner can
   cope with unknown obstacles inside the workspace while executing the
   task and pursuing high performance configurations in the free space. A
   welding inspection on an underwater tube structure is considered as the
   validation scenario, while a UVMS with a mounted six degrees of freedom
   manipulator is assigned to perform the task. (C) 2015 Elsevier B.V. All
   rights reserved.</abstract><date>DEC 2015</date><author>Sotiropoulos, Panagiotis
   Kolonias, Vasileios
   Aspragathos, Nikos
   Housos, Efthymios</author></paper><paper><title>Osteolytica: An automated image analysis software package that rapidly
   measures cancer-induced osteolytic lesions in in vivo models with
   greater reproducibility compared to other commonly used methods</title><abstract>Methods currently used to analyse osteolytic lesions caused by
   malignancies such as multiple myeloma and metastatic breast cancer vary
   from basic 2-D X-ray analysis to 2-D images of micro-CT datasets
   analysed with non-specialised image software such as ImageJ. However,
   these methods have significant limitations. They do not capture 3-D
   data, they are time-consuming and they often suffer from inter-user
   variability. We therefore sought to develop a rapid and reproducible
   method to analyse 3-D osteolytic lesions in mice with cancer induced
   bone disease. To this end, we have developed Osteolytica, an image
   analysis software method featuring an easy to use, step-by-step
   interface to measure lytic bone lesions. Osteolytica utilises novel
   graphics card acceleration (parallel computing) and 3-D rendering to
   provide rapid reconstruction and analysis of osteolytic lesions. To
   evaluate the use of Osteolytica we analysed tibial micro-CT datasets
   from murine models of cancer-induced bone disease and compared the
   results to those obtained using a standard ImageJ analysis method.
   Firstly, to assess inter-user variability we deployed four independent
   researchers to analyse tibial datasets from the U266-NSG murine model of
   myeloma. Using ImageJ, inter-user variability between the bones was
   substantial (+/- 19.6%), in contrast to using Osteolytica, which
   demonstrated minimal variability (+/- 0.5%). Secondly, tibial datasets
   from U266-bearing NSG mice or BALB/c mice injected with the metastatic
   breast cancer cell line 4T1 were compared to tibial datasets from aged
   and sex-matched non-tumour control mice. Analyses by both Osteolytica
   and ImageJ showed significant increases in bone lesion area in
   tumour-bearing mice compared to control mice. These results confirm that
   Osteolytica performs as well as the current 2-D ImageJ osteolytic lesion
   analysis method. However, Osteolytica is advantageous in that it
   analyses over the entirety of the bone volume (as opposed to selected
   2-D images), it is a more rapid method and it has less user variability.
   (c) 2015 The Authors. Published by Elsevier Inc.</abstract><date>FEB 2016</date><author>Evans, H. R.
   Karmakharm, T.
   Lawson, M. A.
   Walker, R. E.
   Harris, W.
   Fellows, C.
   Huggins, I. D.
   Richmond, P.
   Chantry, A. D.</author></paper><paper><title>Oxygenation measurement by multi-wavelength oxygen-dependent
   phosphorescence and delayed fluorescence: catchment depth and
   application in intact heart</title><abstract>Oxygen delivery and metabolism represent key factors for organ function
   in health and disease. We describe the optical key characteristics of a
   technique to comprehensively measure oxygen tension (PO2) in myocardium,
   using oxygen-dependent quenching of phosphorescence and delayed
   fluorescence of porphyrins, by means of Monte Carlo simulations and ex
   vivo experiments. Oxyphor G2 (microvascular PO2) was excited at 442 nm
   and 632 nm and protoporphyrin IX (mitochondrial PO2) at 510 nm. This
   resulted in catchment depths of 161 (86) mu m, 350 (307) mu m and 262
   (255) mu m respectively, as estimated by Monte Carlo simulations and ex
   vivo experiments (brackets). The feasibility to detect changes in
   oxygenation within separate anatomical compartments is demonstrated in
   rat heart in vivo.[GRAPHICS].</abstract><date>AUG 2015</date><author>Balestra, Gianmarco M.
   Aalders, Maurice C. G.
   Specht, Patricia A. C.
   Ince, Can
   Mik, Egbert G.</author></paper><paper><title>A nodal discontinuous Galerkin method for reverse-time migration on GPU
   clusters</title><abstract>Improving both accuracy and computational performance of numerical tools
   is a major challenge for seismic imaging and generally requires
   specialized implementations to make full use of modern parallel
   architectures. We present a computational strategy for reverse-time
   migration (RTM) with accelerator-aided clusters. A new imaging condition
   computed from the pressure and velocity fields is introduced. The model
   solver is based on a high-order discontinuous Galerkin time-domain
   (DGTD) method for the pressure-velocity system with unstructured meshes
   and multirate local time stepping. We adopted the MPI+X approach for
   distributed programming where X is a threaded programming model. In this
   work we chose OCCA, a unified framework that makes use of major
   multithreading languages (e.g. CUDA and OpenCL) and offers the
   flexibility to run on several hardware architectures. DGTD schemes are
   suitable for efficient computations with accelerators thanks to
   localized element-to-element coupling and the dense algebraic operations
   required for each element. Moreover, compared to high-order
   finite-difference schemes, the thin halo inherent to DGTD method reduces
   the amount of data to be exchanged between MPI processes and storage
   requirements for RTM procedures. The amount of data to be recorded
   during simulation is reduced by storing only boundary values in memory
   rather than on disk and recreating the forward wavefields. Computational
   results are presented that indicate that these methods are strong
   scalable up to at least 32 GPUs for a three-dimensional RTM case.</abstract><date>NOV 2015</date><author>Modave, A.
   St-Cyr, A.
   Mulder, W. A.
   Warburton, T.</author></paper><paper><title>A Survey of Digital Earth</title><abstract>The creation of a digital representation of the Earth and its associated
   data is a complex and difficult task. The incredible size of geospatial
   data and differences between data sets pose challenges related to big
   data, data creation, and data integration. Advances in globe
   representation and visualization have made use of Discrete Global Grid
   Systems (DGGSs) that discretize the globe into a set of cells to which
   data are assigned. DGGSs are well studied and important in the GIS, OGC,
   and Digital Earth communities but have not been well-introduced to the
   computer graphics community. In this paper, we provide an overview of
   DGGSs and their use in digitally representing the Earth, describe
   several current Digital Earth systems and their methods of Earth
   representation, and list a number of applications of Digital Earths with
   related works. Moreover, we discuss the key research areas and related
   papers from computer graphics that are useful for a Digital Earth
   system, such as advanced techniques for geospatial data creation and
   representation. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>DEC 2015</date><author>Mahdavi-Amiri, Ali
   Alderson, Troy
   Samavati, Faramarz</author></paper><paper><title>iUbiq-Lys: prediction of lysine ubiquitination sites in proteins by
   extracting sequence evolution information via a gray system model</title><abstract>As one of the most important posttranslational modifications (PTMs),
   ubiquitination plays an important role in regulating varieties of
   biological processes, such as signal transduction, cell division,
   apoptosis, and immune response. Ubiquitination is also named "lysine
   ubiquitination" because it occurs when an ubiquitin is covalently
   attached to lysine (K) residues of targeting proteins. Given an
   uncharacterized protein sequence that contains many lysine residues,
   which one of them is the ubiquitination site, and which one is of
   non-ubiquitination site? With the avalanche of protein sequences
   generated in the postgenomic age, it is highly desired for both basic
   research and drug development to develop an automated method for rapidly
   and accurately annotating the ubiquitination sites in proteins. In view
   of this, a new predictor called "iUbiq-Lys" was developed based on the
   evolutionary information, gray system model, as well as the general form
   of pseudo-amino acid composition. It was demonstrated via the rigorous
   cross-validations that the new predictor remarkably outperformed all its
   counterparts. As a web-server, iUbiq-Lys is accessible to the public
   at[GRAPHICS]. For the convenience of most experimental scientists, we
   have further provided a protocol of step-by-step guide, by which users
   can easily get their desired results without the need to follow the
   complicated mathematics that were presented in this paper just for the
   integrity of its development process.</abstract><date>AUG 3 2015</date><author>Qiu, Wang-Ren
   Xiao, Xuan
   Lin, Wei-Zhong
   Chou, Kuo-Chen</author></paper><paper><title>MIMOPack: a high-performance computing library for MIMO communication
   systems</title><abstract>This paper presents MIMOPack, a set of optimized functions to perform
   some of the most complex stages in multiple-input multiple-output (MIMO)
   communication systems such as channel coding, preprocessing, precoding
   and detection. These functions are optimized to be run in a wide range
   of architectures increasing the portability of scientific codes between
   different computing environments. MIMOPack aims to become a useful
   library for the research community facilitating to the programmer the
   development of adaptable parallel applications and also to speed up
   simulation platforms used to assess different technologies proposed by
   several companies involved in standarization processes.</abstract><date>FEB 2015</date><author>Ramiro, Carla
   Vidal, Antonio M.
   Gonzalez, Alberto</author></paper><paper><title>Simulation of particle motion in a closed conduit validated against
   experimental data</title><abstract>Motion of a number of spherical particles in a closed conduit is
   examined by means of both simulation and experiment. The bed of the
   conduit is covered by stationary spherical particles of the size of the
   moving particles. The flow is driven by experimentally measured velocity
   profiles which are inputs of the simulation. Altering input velocity
   profiles generates various trajectory patterns. The lattice Boltzmann
   method (LBM) based simulation is developed to study mutual interactions
   of the flow and the particles. The simulation enables to model both the
   particle motion and the fluid flow. The entropic LBM is employed to deal
   with the flow characterized by the high Reynolds number. The entropic
   modification of the LBM along with the enhanced refinement of the
   lattice grid yield an increase in demands on computational resources.
   Due to the inherently parallel nature of the LBM it can be handled by
   employing the Parallel Computing Toolbox (MATLAB) and other
   transformations enabling usage of the CUDA GPU computing technology. The
   trajectories of the particles determined within the LBM simulation are
   validated against data gained from the experiments. The compatibility of
   the simulation results with the outputs of experimental measurements is
   evaluated. The accuracy of the applied approach is assessed and
   stability and efficiency of the simulation is also considered.</abstract><date>2015</date><author>Dolansky, Jindrich</author></paper><paper><title>Solitary Langerhans cell histiocytosis of the occipital condyle: a case
   report and review of the literature</title><abstract>Despite the recent advent of various radiographic imaging techniques, it
   is still very difficult to correctly distinguish a pediatric osteolytic
   lesion in the occipital condyle, which makes it further complicated to
   decide on the necessity of and the adequate timing for radical resection
   and craniocervical fusions. To establish a legitimate therapeutic
   strategy for this deep-seated lesion, surgical biopsy is a reasonable
   choice for first-line intervention. The choice of surgical approach
   becomes very important because a sufficient amount of histological
   specimen must be obtained to confirm the diagnosis but, ideally, the
   residual bony structures and the muscular structures should be preserved
   so as not to increase craniocervical instability. In this report, we
   present our experience with a case of solitary Langerhans cell
   histiocytosis (LCH) involving the occipital condyle that was
   successfully treated with minimally invasive surgical biopsy with a far
   lateral condylar approach supported by preoperative 3D computer graphic
   simulation.An 8-year-old girl presented with neck pain. Magnetic
   resonance imaging and computed tomography (CT) revealed an osteolytic
   lesion of the left occipital condyle. At surgery, the patient was placed
   in the prone position. A 3-cm skin incision was made in the posterior
   auricular region, and the sternocleidomastoid and splenius capitis
   muscles were dissected in the middle of the muscle bundle along the
   direction of the muscle fiber. Under a navigation system, we approached
   the occipital condyle through the space between the longissimus capitis
   muscle and the posterior belly of the digastric muscle and lateral to
   the superior oblique muscle, verifying each muscle at each depth of the
   surgical field and, finally, obtained sufficient surgical specimen.
   After the biopsy, her craniocervical instability had not worsened, and
   chemotherapy was performed. Twelve weeks after chemotherapy, her neck
   pain had gradually disappeared along with her torticollis, and CT showed
   remission of the lesion and marked regeneration of the left occipital
   condyle. Within our knowledge, this is the first reported case of LCH
   involving the occipital condyle. Although very rare, our case indicated
   that LCH can be an alternative in the differential diagnosis of
   osteolytic lesions in the craniocervical junction, in which early bone
   regeneration with sufficient cervical stability is expected after
   chemotherapy.In cases of pediatric osteolytic lesions, when they
   initially presented with apparent cervical instability, craniocervical
   fusion may possibly become unnecessary after a series of treatments.
   Thus, the effort to maximally preserve the musculoskeletal structure
   should be made until its histological diagnosis is finally confirmed.</abstract><date>FEB 2016</date><author>Teranishi, Yu
   Shin, Masahiro
   Yoshino, Masanori
   Saito, Nobuhito</author></paper><paper><title>Stacked Multilayer Self-Organizing Map for Background Modeling</title><abstract>In this paper, a new background modeling method called stacked
   multilayer self-organizing map background model (SMSOM-BM) is proposed,
   which presents several merits such as strong representative ability for
   complex scenarios, easy to use, and so on. In order to enhance the
   representative ability of the background model and make the parameters
   learned automatically, the recently developed idea of representative
   learning (or deep learning) is elegantly employed to extend the existing
   single-layer self-organizing map background model to a multilayer one
   (namely, the proposed SMSOM-BM). As a consequence, the SMSOM-BM gains
   several merits including strong representative ability to learn
   background model of challenging scenarios, and automatic determination
   for most network parameters. More specifically, every pixel is modeled
   by a SMSOM, and spatial consistency is considered at each layer. By
   introducing a novel over-layer filtering process, we can train the
   background model layer by layer in an efficient manner. Furthermore, for
   real-time performance consideration, we have implemented the proposed
   method using NVIDIA CUDA platform. Comparative experimental results show
   superior performance of the proposed approach.</abstract><date>SEP 2015</date><author>Zhao, Zhenjie
   Zhang, Xuebo
   Fang, Yongchun</author></paper><paper><title>Initial results on computational performance of Intel many integrated
   core, sandy bridge, and graphical processing unit architectures:
   implementation of a 1D c++/OpenMP electrostatic particle-in-cell code</title><abstract>We present initial comparison performance results for Intel many
   integrated core (MIC), Sandy Bridge (SB), and graphical processing unit
   (GPU). A 1D explicit electrostatic particle-in-cell code is used to
   simulate a two-stream instability in plasma. We compare the computation
   times for various number of cores/threads and compiler options. The
   parallelization is implemented via OpenMP with a maximum thread number
   of 128. Parallelization and vectorization on the GPU is achieved with
   modifying the code syntax for compatibility with CUDA. We assess the
   speedup due to various auto-vectorization and optimization level
   compiler options. Our results show that the MIC is several times slower
   than SB for a single thread, and it becomes faster than SB when the
   number of cores increases with vectorization switched on. The compute
   times for the GPU are consistently about six to seven times faster than
   the ones for MIC. Compared with SB, the GPU is about two times faster
   for a single thread and about an order of magnitude faster for 128
   threads. The net speedup, however, for MIC and GPU are almost the same.
   An initial attempt to offload parts of the code to the MIC coprocessor
   shows that there is an optimal number of threads where the speedup
   reaches a maximum. Copyright (c) 2014 John Wiley &amp; Sons, Ltd.</abstract><date>MAR 10 2015</date><author>Vapirev, A.
   Deca, J.
   Lapenta, G.
   Markidis, S.
   Hur, I.
   Cambier, J. -L.</author></paper><paper><title>PRROC: computing and visualizing precision-recall and receiver operating
   characteristic curves in R</title><abstract>Precision-recall (PR) and receiver operating characteristic (ROC) curves
   are valuable measures of classifier performance. Here, we present the
   R-package PRROC, which allows for computing and visualizing both PR and
   ROC curves. In contrast to available R-packages, PRROC allows for
   computing PR and ROC curves and areas under these curves for
   soft-labeled data using a continuous interpolation between the points of
   PR curves. In addition, PRROC provides a generic plot function for
   generating publication-quality graphics of PR and ROC curves.</abstract><date>AUG 1 2015</date><author>Grau, Jan
   Grosse, Ivo
   Keilwagen, Jens</author></paper><paper><title>Preselective Screening for Linear-Scaling Exact Exchange-Gradient
   Calculations for Graphics Processing Units and General Strong-Scaling
   Massively Parallel Calculations</title><abstract>We present an extension of our recently presented PreLinK scheme (J.
   Chem. Phys. 2013, 138, 134114) for the exact exchange contribution to
   nuclear forces. The significant contributions to the exchange gradient
   are determined by preselection based on accurate shell-pair
   contributions to the SCF exchange energy prior to the calculation.
   Therefore, our method is highly suitable for massively parallel
   electronic structure calculations because of an efficient load balancing
   of the significant contributions only and an unhampered control flow.
   The efficiency of our method is shown for several illustrative
   calculations on single GPU servers, as well as for hybrid MPI/CUDA
   parallel calculations with the largest system comprising 3369 atoms and
   26952 basis functions.</abstract><date>MAR 2015</date><author>Kussmann, Joerg
   Ochsenfeld, Christian</author></paper><paper><title>Visualization Quality Enhancement of Three Dimensional Swept Source
   Optical Coherence Tomography Data</title><abstract></abstract><date>JUN 2015</date><author>Glittenberg, Carl G. O.
   Binder, Susanne</author></paper><paper><title>The Lattice Boltzmann Method Implemented on the GPU to Simulate the
   Turbulent Flow Over a Square Cylinder Confined in a Channel</title><abstract>The lattice Boltzmann method (LBM) is a relatively new method for fluid
   flow simulations, and is recently gaining popularity due to its simple
   algorithm and parallel scalability. Although the method has been
   successfully applied to a wide range of flow physics, its capabilities
   in simulating turbulent flow is still under-validated. Hence, in this
   paper, a 3D LBM code was developed to investigate the validity of the
   LBM for turbulent flow simulations through large eddy simulations (LES).
   A GPU enabled LBM code was developed, and validated against a benchmark
   test case involving the flow over a square cylinder in square channel.
   The flow results showed good agreement with literature, and speedups of
   over 150 times were observed when two GPUs were used in parallel.
   Turbulent flow simulations were then conducted using LES with the
   Smagorinsky subgrid model. The methodology was first validated by
   computing the fully developed turbulent channel flow, and comparing the
   results against direct numerical simulation results. The results were in
   good agreement despite the relatively coarse grid. The code was then
   used to simulate the turbulent flow over a square cylinder confined in a
   channel. In order to emulate a realistic inflow at the channel inlet, an
   auxiliary simulation consisting of a fully developed turbulent channel
   flow was run in conjunction, and its velocity profile was used to
   enforce the inlet boundary condition for the cylinder flow simulation.
   Comparison of the results with experimental and numerical results
   revealed that the presence of the turbulent flow structures at the inlet
   can significantly influence the resulting flow field around the
   cylinder.</abstract><date>APR 2015</date><author>Koda, Yusuke
   Lien, Fue-Sang</author></paper><paper><title>Elements of Style: Learning Perceptual Shape Style Similarity</title><abstract>The human perception of stylistic similarity transcends structure and
   function: for instance, a bed and a dresser may share a common style. An
   algorithmically computed style similarity measure that mimics human
   perception can benefit a range of computer graphics applications.
   Previous work in style analysis focused on shapes within the same class,
   and leveraged structural similarity between these shapes to facilitate
   analysis. In contrast, we introduce the first structure-transcending
   style similarity measure and validate it to be well aligned with human
   perception of stylistic similarity. Our measure is inspired by
   observations about style similarity in art history literature, which
   point to the presence of similarly shaped, salient, geometric elements
   as one of the key indicators of stylistic similarity. We translate these
   observations into an algorithmic measure by first quantifying the
   geometric properties that make humans perceive geometric elements as
   similarly shaped and salient in the context of style, then employing
   this quantification to detect pairs of matching style related elements
   on the analyzed models, and finally collating the element-level
   geometric similarity measurements into an object-level style measure
   consistent with human perception. To achieve this consistency we employ
   crowdsourcing to quantify the different components of our measure; we
   learn the relative perceptual importance of a range of elementary shape
   distances and other parameters used in our measurement from 50K
   responses to cross-structure style similarity queries provided by over
   2500 participants. We train and validate our method on this dataset,
   showing it to successfully predict relative style similarity with near
   90% accuracy based on 10-fold cross-validation.</abstract><date>AUG 2015</date><author>Lun, Zhaoliang
   Kalogerakis, Evangelos
   Sheffer, Alla</author></paper><paper><title>The Real Dynamics of Bieberbach's Example</title><abstract>Bieberbach constructed, in 1933, domains in C-2 which were biholomorphic
   to C-2 but not dense. The existence of such domains was unexpected. The
   special domains Bieberbach considered are basins of attraction of a
   cubic Henon map. This classical method of construction is one of the
   first applications of dynamical systems to complex analysis. In this
   paper, the boundaries of the real sections of Bieberbach's domains will
   be calculated explicitly as the stable manifolds of the saddle points.
   The real filled Julia sets and the real Julia sets of Bieberbach's map
   will also be calculated explicitly and illustrated with computer
   generated graphics. Basic differences between real and the complex
   dynamics will be shown.</abstract><date>OCT 2015</date><author>Hayes, Sandra
   Hundemer, Axel
   Milliken, Evan
   Moulinos, Tasos</author></paper><paper><title>Advanced free-form deformation and Kullback-Lieblier divergence measure
   for digital elevation model registration</title><abstract>Registration is the process of transforming various images of the same
   object, place, etc. to confer one coordinate system, be they multimodal,
   multi-temporal of the same place or images of different places having
   certain common characteristics. Many methods have been tried for image
   registration; however, only a handful of methods may be said to work for
   digital elevation model (DEM) registration. The motivation of the
   present work is to perform a robust and efficient algorithm to perform
   Elevation model image registration. In this paper, we present a new
   approach for DEM registration, particularly for multimodal or
   multi-temporal DEMs. For time efficient, robust and precise registration
   of DEMs, a novel idea has been proposed using rational DMS-spline
   volumes (Xu et al. in J Comput Sci Technol 23(5):862-873, 1996;
   McDonnell and Qin in J Gr Tools 12(3):24-41, 2007) (the term DMS is
   acronym for the three authors, namely Dahmen et al. in Math Comput
   59(199):97-115, 1997). This is based on the usage of arbitrary topology
   lattices (MacCracken and Joy in 23rd annual conference on computer
   graphics and interactive techniques proceedings. ACM-SIGGRAPH, pp
   181-188, 1996; Feng et al. in Vis Comput Int Comput Gr 22(1):28-42,
   2005). Using free-form deformation has lead to the usage of global and
   local transformations for registration of candidate image domain to
   reference image domain. Also, a similarity measure based on
   Kullback-Leibler divergence (KLD) has been used for measuring robustness
   of the method so proposed. The task of registration is achieved by
   minimizing the cost function using rational DMS-spline functions for
   local registration. After experimentations, the results show that the
   registration process, of registration of candidate DEM to reference DEM,
   could be completed successfully. Comparison of similarity measurement
   methods such as mutual information, correlation coefficient and peak
   signal-to-noise ratio with that of KLD-based has been performed.
   Comparative study with existing works suggests that the presented scheme
   is better, when compared with respect to above mentioned parameters.</abstract><date>OCT 2015</date><author>Dawn, Suma
   Saxena, Vikas
   Sharma, Bhu Dev</author></paper><paper><title>Two-level parallelization of a fluid mechanics algorithm exploiting
   hardware heterogeneity</title><abstract>The prospect of wildly heterogeneous computer systems has led to a
   renewed discussion of programming approaches in high-performance
   computing, of which computational fluid dynamics is a major field. The
   challenge consists in harvesting the performance of all available
   hardware components while retaining good programmability. In particular
   the use of graphic cards is an important trend. This is addressed in the
   present paper by devising a hybrid programming model to create a
   heterogeneous data-parallel computation with a single source code. The
   concept is demonstrated for a one-dimensional spectral-element
   discretization of a fluid dynamics problem. To exploit the additional
   hardware available when coupling GPGPU-accelerated processes with excess
   CPU cores, a straight-forward load balancing model for such
   heterogeneous environments is developed. The paper presents a large
   number of run time measurements and demonstrates that the achieved
   performance gains are close to optimal. This provides valuable
   information for the implementation of fluid dynamics codes on modern
   heterogeneous hardware. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>AUG 31 2015</date><author>Huismann, Immo
   Stiller, Joerg
   Froehlich, Jochen</author></paper><paper><title>Image Stitching System Based on ORB Feature-Based Technique and
   Compensation Blending</title><abstract>The construction of a high-resolution panoramic image from a sequence of
   input overlapping images of the same scene is called image
   stitching/mosaicing. It is considered as an important, challenging topic
   in computer vision, multimedia, and computer graphics. The quality of
   the mosaic image and the time cost are the two primary parameters for
   measuring the stitching performance. Therefore, the main objective of
   this paper is to introduce a high-quality image stitching system with
   least computation time. First, we compare many different features
   detectors. We test Harris corner detector, SIFT, SURF, FAST,
   GoodFeaturesToTrack, MSER, and ORB techniques to measure the detection
   rate of the corrected keypoints and processing time. Second, we
   manipulate the implementation of different common categories of image
   blending methods to increase the quality of the stitching process. From
   experimental results, we conclude that ORB algorithm is the fastest,
   more accurate, and with higher performance. In addition, Exposure
   Compensation is the highest stitching quality blending method. Finally,
   we have generated an image stitching system based on ORB using Exposure
   Compensation blending method.</abstract><date>SEP 2015</date><author>Adel, Ebtsam
   Elmogy, Mohammed
   Elbakry, Hazem</author></paper><paper><title>A stencil-based implementation of Parareal in the C plus plus domain
   specific embedded language STELLA</title><abstract>In view of the rapid rise of the number of cores in modern
   supercomputers, time-parallel methods that introduce concurrency along
   the temporal axis are becoming increasingly popular. For the solution of
   time-dependent partial differential equations, these methods can add
   another direction for concurrency on top of spatial parallelization. The
   paper presents an implementation of the time-parallel Parareal method in
   a C++ domain specific language for stencil computations (STELLA). STELLA
   provides both an OpenMP and a CUDA backend for a shared memory
   parallelization, using the CPU or GPU inside a node for the spatial
   stencils. Here, we intertwine this node-wise spatial parallelism with
   the time-parallel Parareal. This is done by adding an MPI-based
   implementation of Parareal, which allows us to parallelize in time
   across nodes. The performance of Parareal with both backends is analyzed
   in terms of speedup, parallel efficiency and energy-to-solution for an
   advection-diffusion problem with a time-dependent diffusion coefficient.
   (C) 2014 Elsevier Inc. All rights reserved.</abstract><date>SEP 15 2015</date><author>Arteaga, Andrea
   Ruprecht, Daniel
   Krause, Rolf</author></paper><paper><title>GPURFSCREEN: a GPU based virtual screening tool using random forest
   classifier.</title><abstract>BACKGROUND: In-silico methods are an integral part of modern drug
   discovery paradigm. Virtual screening, an in-silico method, is used to
   refine data models and reduce the chemical space on which wet lab
   experiments need to be performed. Virtual screening of a ligand data
   model requires large scale computations, making it a highly time
   consuming task. This process can be speeded up by implementing
   parallelized algorithms on a Graphical Processing Unit (GPU).RESULTS:
   Random Forest is a robust classification algorithm that can be employed
   in the virtual screening. A ligand based virtual screening tool
   (GPURFSCREEN) that uses random forests on GPU systems has been proposed
   and evaluated in this paper. This tool produces optimized results at a
   lower execution time for large bioassay data sets. The quality of
   results produced by our tool on GPU is same as that on a regular serial
   environment.CONCLUSION: Considering the magnitude of data to be
   screened, the parallelized virtual screening has a significantly lower
   running time at high throughput. The proposed parallel tool outperforms
   its serial counterpart by successfully screening billions of molecules
   in training and prediction phases.</abstract><date>2016</date><author>Jayaraj, P B
   Ajay, Mathias K
   Nufail, M
   Gopakumar, G
   Jaleel, U C A</author></paper><paper><title>Acceleration of free-vibrations analysis with the Dual Reciprocity BEM
   based on H-matrices and CUDA</title><abstract>Purpose - The purpose of this paper is to present a novel strategy used
   for acceleration of free-vibration analysis, in which the hierarchical
   matrices structure and Compute Unified Device Architecture (CUDA)
   platform is applied to improve the performance of the traditional dual
   reciprocity boundary element method (DRBEM).Design/methodology/approach
   - The DRBEM is applied in forming integral equation to reduce
   complexity. In the procedure of optimization computation, H-Matrices are
   introduced by applying adaptive cross-approximation method. At the same
   time, this paper proposes a high-efficiency parallel algorithm using
   CUDA and the counterpart of the serial effective algorithm in H-Matrices
   for inverse arithmetic operation.Findings - The analysis for
   free-vibration could achieve impressive time and space efficiency by
   introducing hierarchical matrices technique. Although the serial
   algorithm based on H-Matrices could obtain fair performance for complex
   inversion operation, the CUDA parallel algorithm would further double
   the efficiency. Without much loss in accuracy according to the
   examination of the numerical example, the relative error appeared in
   approximation process can be fixed by increasing degrees of freedoms or
   introducing certain amount of internal points.Originality/value - The
   paper proposes a novel effective strategy to improve computational
   efficiency and decrease memory consumption of free-vibration problems.
   H-Matrices structure and parallel operation based on CUDA are introduced
   in traditional DRBEM.</abstract><date>2015</date><author>Wei, Yixiong
   Wang, Qifu
   Huang, Yunbao
   Wang, Yingjun
   Xia, Zhaohui</author></paper><paper><title>Segmentation-based semi-regular remeshing of 3D models using
   curvature-adapted subdivision surface fitting</title><abstract>This paper proposes a novel method of semi-regular remeshing for
   triangulated surfaces to achieve superior triangles lead to advanced
   visualization of 3D model. It is based on mesh segmentation and
   subdivision surface fitting which uses curvature-adapted polygon
   patches. Our contribution lies in building a sophisticated system with
   three stages, i.e., curvature-aware mesh segmentation, submesh surface
   fitting to generate a high-quality semi-regular mesh and finally,
   stitching the segments using an efficient algorithm. Our method uses
   centroidal Voronoi tessellation and Lloyd's relaxation to generate
   curvature-adapted site centers. Geodesic distances from site centers are
   used for labeling segments and indexing corner vertices for each segment
   boundary. Using information of site centers and corner vertices,
   feature-adapted polygonal patches are generated for each segment. These
   patches are then subdivided and optimized using squared distance metric
   to adjust position of the subdivision sampling with segment details and
   prevent oversampling. At last, an efficient stitching algorithm is
   introduced to connect regular submeshes together and build the final
   semi-regular mesh. We have demonstrated the results of our semi-regular
   remeshing algorithm on meshes with different topology and complexity and
   compared them with known methods. Superior triangle quality with higher
   aspect ratio together with acceptable distortion error is achieved
   according to the experimental results.</abstract><date>FEB 2016</date><author>Mansouri, Saeid
   Ebrahimnezhad, Hossein</author></paper><paper><title>Diagnostic ability for glaucomatous optic neuropathy of Humphrey
   perimetry, Octopus perimetry and Cirrus OCT</title><abstract></abstract><date>JUN 2015</date><author>Monsalve, Blanca
   Ferreras, Antonio
   Calvo, Pilar
   Ruiz, Gema
   Monsalve, Juan</author></paper><paper><title>CUDA-NP: Realizing Nested Thread-Level Parallelism in GPGPU Applications</title><abstract>Parallel programs consist of series of code sections with different
   thread-level parallelism (TLP). As a result, it is rather common that a
   thread in a parallel program, such as a GPU kernel in CUDA programs,
   still contains both sequential code and parallel loops. In order to
   leverage such parallel loops, the latest NVIDIA Kepler architecture
   introduces dynamic parallelism, which allows a GPU thread to start
   another GPU kernel, thereby reducing the overhead of launching kernels
   from a CPU. However, with dynamic parallelism, a parent thread can only
   communicate with its child threads through global memory and the
   overhead of launching GPU kernels is non-trivial even within GPUs. In
   this paper, we first study a set of GPGPU benchmarks that contain
   parallel loops, and highlight that these benchmarks do not have a very
   high loop count or high degree of TLP. Consequently, the benefits of
   leveraging such parallel loops using dynamic parallelism are too limited
   to offset its overhead. We then present our proposed solution to exploit
   nested parallelism in CUDA, referred to as CUDA-NP. With CUDA-NP, we
   initially enable a high number of threads when a GPU program starts, and
   use control flow to activate different numbers of threads for different
   code sections. We implement our proposed CUDA-NP framework using a
   directive-based compiler approach. For a GPU kernel, an application
   developer only needs to add OpenMP-like pragmas for parallelizable code
   sections. Then, our CUDA-NP compiler automatically generates the
   optimized GPU kernels. It supports both the reduction and the scan
   primitives, explores different ways to distribute parallel loop
   iterations into threads, and efficiently manages on-chip resource. Our
   experiments show that for a set of GPGPU benchmarks, which have already
   been optimized and contain nested parallelism, our proposed CUDA-NP
   framework further improves the performance by up to 6.69 times and 2.01
   times on average.</abstract><date>JAN 2015</date><author>Yang, Yi
   Li, Chao
   Zhou, Huiyang</author></paper><paper><title>Massively parallel sampling of lattice proteins reveals foundations of
   thermal adaptation</title><abstract>Evolution of proteins in bacteria and archaea living in different
   conditions leads to significant correlations between amino acid usage
   and environmental temperature. The origins of these correlations are
   poorly understood, and an important question of protein theory,
   physics-based prediction of types of amino acids overrepresented in
   highly thermostable proteins, remains largely unsolved. Here, we extend
   the random energy model of protein folding by weighting the interaction
   energies of amino acids by their frequencies in protein sequences and
   predict the energy gap of proteins designed to fold well at elevated
   temperatures. To test the model, we present a novel scalable algorithm
   for simultaneous energy calculation for many sequences in many
   structures, targeting massively parallel computing architectures such as
   graphics processing unit. The energy calculation is performed by
   multiplying two matrices, one representing the complete set of
   sequences, and the other describing the contact maps of all structural
   templates. An implementation of the algorithm for the CUDA platform is
   available at http://www.github.com/kzeldovich/galeprot and calculates
   protein folding energies over 250 times faster than a single central
   processing unit. Analysis of amino acid usage in 64-mer cubic lattice
   proteins designed to fold well at different temperatures demonstrates an
   excellent agreement between theoretical and simulated values of energy
   gap. The theoretical predictions of temperature trends of amino acid
   frequencies are significantly correlated with bioinformatics data on 191
   bacteria and archaea, and highlight protein folding constraints as a
   fundamental selection pressure during thermal adaptation in biological
   evolution. (C) 2015 AIP Publishing LLC.</abstract><date>AUG 7 2015</date><author>Venev, Sergey V.
   Zeldovich, Konstantin B.</author></paper><paper><title>Flow Aligned Surfacing of Curve Networks</title><abstract>We propose a new approach for automatic surfacing of 3D curve networks,
   a long standing computer graphics problem which has garnered new
   attention with the emergence of sketch based modeling systems capable of
   producing such networks. Our approach is motivated by recent studies
   suggesting that artist-designed curve networks consist of descriptive
   curves that convey intrinsic shape properties, and are dominated by
   representative flow lines designed to convey the principal curvature
   lines on the surface. Studies indicate that viewers complete the
   intended surface shape by envisioning a surface whose curvature lines
   smoothly blend these flow-line curves. Following these observations we
   design a surfacing framework that automatically aligns the curvature
   lines of the constructed surface with the representative flow lines and
   smoothly interpolates these representative flow, or curvature directions
   while minimizing undesired curvature variation. Starting with an initial
   triangle mesh of the network, we dynamically adapt the mesh to maximize
   the agreement between the principal curvature direction field on the
   surface and a smooth flow field suggested by the representative
   flow-line curves. Our main technical contribution is a framework for
   curvature-based surface modeling, that facilitates the creation of
   surfaces with prescribed curvature characteristics. We validate our
   method via visual inspection, via comparison to artist created and
   ground truth surfaces, as well as comparison to prior art, and confirm
   that our results are well aligned with the computed flow fields and with
   viewer perception of the input networks.</abstract><date>AUG 2015</date><author>Pan, Hao
   Liu, Yang
   Sheffer, Alla
   Vining, Nicholas
   Li, Chang-Jian
   Wang, Wenping</author></paper><paper><title>An Effective CUDA Parallelization of Projection in Iterative Tomography
   Reconstruction</title><abstract>Projection and back-projection are the most computationally intensive
   parts in Computed Tomography (CT) reconstruction, and are essential to
   acceleration of CT reconstruction algorithms. Compared to
   back-projection, parallelization efficiency in projection is highly
   limited by racing condition and thread unsynchronization. In this paper,
   a strategy of Fixed Sampling Number Projection (FSNP) is proposed to
   ensure the operation synchronization in the ray-driven projection with
   Graphical Processing Unit (GPU). Texture fetching is also used utilized
   to further accelerate the interpolations in both projection and
   back-projection. We validate the performance of this FSNP approach using
   both simulated and real conebeam CT data. Experimental results show that
   compare to the conventional approach, the proposed FSNP method together
   with texture fetching is 10 similar to 16 times faster than the
   conventional approach based on global memory, and thus leads to more
   efficient iterative algorithm in CT reconstruction.</abstract><date>NOV 30 2015</date><author>Xie, Lizhe
   Hu, Yining
   Yan, Bin
   Wang, Lin
   Yang, Benqiang
   Liu, Wenyuan
   Zhang, Libo
   Luo, Limin
   Shu, Huazhong
   Chen, Yang</author></paper><paper><title>Spins Dynamics in a Dissipative Environment: Hierarchal Equations of
   Motion Approach Using a Graphics Processing Unit (GPU)</title><abstract>A system with many energy states coupled to a harmonic oscillator bath
   is considered. To study quantum non-Markovian system-bath dynamics
   numerically rigorously and nonperturbatively, we developed a computer
   code for the reduced hierarchy equations of motion (HEOM) for a graphics
   processor unit (GPU) that can treat the system as large as 4096 energy
   states. The code employs a Pade spectrum decomposition (PSD) for a
   construction of HEOM and the exponential integrators. Dynamics of a
   quantum spin glass system are studied by calculating the free induction
   decay signal for the cases of 3 x 2 to 3 x 4 triangular lattices with
   antiferromagnetic interactions. We found that spins relax faster at
   lower temperature due to transitions through a quantum coherent state,
   as represented by the offdiagonal elements of the reduced density
   matrix, while it has been known that the spins relax slower due to
   suppression of thermal activation in a classical case. The decay of the
   spins are qualitatively similar regardless of the lattice sizes. The
   pathway of spin relaxation is analyzed under a sudden temperature drop
   condition. The Compute Unified Device Architecture (CUDA) based source
   code used in the present calculations is provided as Supporting
   Information.</abstract><date>AUG 2015</date><author>Tsuchimoto, Masashi
   Tanimura, Yoshitaka</author></paper><paper><title>Parallel hybrid PSO with CUDA for 1D heat conduction equation</title><abstract>Objectives: We propose a parallel hybrid particle swarm optimization
   (PHPSO) algorithm to reduce the computation cost because solving the
   one-dimensional (1D) heat conduction equation requires large
   computational cost which imposes a great challenge to both common
   hardware and software equipments.Background: Over the past few years,
   GPUs have quickly emerged as inexpensive parallel processors due to
   their high computation power and low price, The CUDA library can be used
   by Fortran, C, C++, and by other languages and it is easily programmed.
   Using GPU and CUDA can efficiently reduce the computation time of
   solving heat conduction equation.Methods: Firstly, a spline difference
   method is used to discrete 1D heat conduction equation into the form of
   linear equation systems, secondly, the system of linear equations is
   transformed into an unconstrained optimization problem, finally, it is
   solved by using the PHPSO algorithm. The PHPSO is based on CUDA by
   hybridizing the PSO and conjugate gradient method (CGM).Results: A
   numerical case is given to illustrate the effectiveness and efficiency
   of our proposed method. Comparison of three parallel algorithms shows
   that the PHPSO is competitive in terms of speedup and standard
   deviation. The results also show that using PHPSO to solve the
   one-dimensional heat conduction equation can outperform two parallel
   algorithms as well as HPSO itself.Conclusions: It is concluded that the
   PHPSO is an efficient and effective approach towards the 1D heat
   conduction equation, as it is shown to be with strong robustness and
   high speedup. (C) 2014 Elsevier Ltd. All rights reserved.</abstract><date>MAR 30 2015</date><author>Ouyang, Aijia
   Tang, Zhuo
   Zhou, Xu
   Xu, Yuming
   Pan, Guo
   Li, Keqin</author></paper><paper><title>Anti-aliasing in image based shadow generation techniques: a
   comprehensive survey</title><abstract>This research provides an overview of popular and widely used techniques
   to overcome aliasing in image based shadow generation techniques.
   Aliasing is a critical drawback of image based techniques in shadow
   generation. Many techniques are proposed to enhance the anti-aliasing.
   We have classified and systemized these techniques. The main goal of
   this paper is to provide researchers with background on a variety of
   techniques to reduce the aliasing so as make it easier for them to
   choose the method best suited to their aims. During categorizing the
   anti-aliasing techniques, well-known techniques to enhance aliasing is
   described detail, along with a discussion of the advantages and
   drawbacks of each. The algorithms are also comprehensively summarized
   and analysed in depth. It is also hoped that our analysis helps
   researchers find solutions to the shortcomings of each technique.</abstract><date>SEP 2015</date><author>Kolivand, Hoshang
   Sunar, Mohd Shahrizal</author></paper><paper><title>From Sensory Signals to Modality-Independent Conceptual Representations:
   A Probabilistic Language of Thought Approach</title><abstract>People learn modality-independent, conceptual representations from
   modality-specific sensory signals. Here, we hypothesize that any system
   that accomplishes this feat will include three components: a
   representational language for characterizing modality-independent
   representations, a set of sensory-specific forward models for mapping
   from modality-independent representations to sensory signals, and an
   inference algorithm for inverting forward models-that is, an algorithm
   for using sensory signals to infer modality-independent representations.
   To evaluate this hypothesis, we instantiate it in the form of a
   computational model that learns object shape representations from visual
   and/or haptic signals. The model uses a probabilistic grammar to
   characterize modality-independent representations of object shape, uses
   a computer graphics toolkit and a human hand simulator to map from
   object representations to visual and haptic features, respectively, and
   uses a Bayesian inference algorithm to infer modality-independent object
   representations from visual and/or haptic signals. Simulation results
   show that the model infers identical object representations when an
   object is viewed, grasped, or both. That is, the model's percepts are
   modality invariant. We also report the results of an experiment in which
   different subjects rated the similarity of pairs of objects in different
   sensory conditions, and show that the model provides a very accurate
   account of subjects' ratings. Conceptually, this research significantly
   contributes to our understanding of modality invariance, an important
   type of perceptual constancy, by demonstrating how modality-independent
   representations can be acquired and used. Methodologically, it provides
   an important contribution to cognitive modeling, particularly an
   emerging probabilistic language-of-thought approach, by showing how
   symbolic and statistical approaches can be combined in order to
   understand aspects of human perception.</abstract><date>NOV 2015</date><author>Erdogan, Goker
   Yildirim, Ilker
   Jacobs, Robert A.</author></paper><paper><title>Harnessing the power of disgust: a randomized trial to reduce
   high-calorie food appeal through implicit priming</title><abstract>Background: In our increasingly obesogenic environment, in which
   high-calorie convenience foods are readily available, food choices can
   drastically affect weight and overall health: Learned food preferences,
   which are developed through repeated pairings with positively and
   negatively valenced stimuli, can contribute to obesity susceptibility if
   positive attitudes toward high-calorie foods are developed. Thus, the
   modification of automatic associations with food may be a viable
   strategy to promote healthier eating behaviors.Objective: In this study,
   we investigated the ability of an implicit priming (IP) intervention to
   alter responses to visual food cues by using an evaluative conditioning
   approach. The main objective was to implicitly (i.e., below conscious
   perception) associate disgust with high-calorie foods with the aim of
   reducing liking of these foods.Design: Participants were randomly
   assigned to active or control IP. In active IP (n = 22), high-calorie
   food images were implicitly primed with negatively valenced images, and
   low-calorie food images were implicitly primed with positively valenced
   images. In control IP (n = 20), all food images were primed with neutral
   images of fixation crosses. Food images were rated on the desire to eat
   immediately before and after EP.Results: A significant main effect of
   calorie (high compared with low; P &lt; 0.001) and a significant
   calorie-by-group (active compared with control) interaction (P = 0.025)
   were observed. Post hoc tests identified a significantly greater
   high-calorie rating decline after active IP than after control IP (P =
   0.036). Furthermore, there was significantly greater change in
   high-calorie ratings than in low-calorie ratings in the active group (P
   = 0.001). Active IP effects extended to high-calorie foods not
   specifically included in the intervention, which suggested an effect
   generalization. Moreover, a greater change in high-calorie ratings than
   in low-calorie ratings persisted 3-5 d after active IP (P &lt; 0.007),
   which suggested lasting effects.Conclusion: This study provides initial
   evidence that IP can be used to alter high-calorie food preferences,
   which could promote healthier eating habits. This trial was registered
   at clinicaltrials.gov as NCT02347527.</abstract><date>AUG 2015</date><author>Legget, Kristina T.
   Cornier, Marc-Andre
   Rojas, Donald C.
   Lawful, Benjamin
   Tregellas, Jason R.</author></paper><paper><title>The Affine Particle-In-Cell Method</title><abstract>Hybrid Lagrangian/Eulerian simulation is commonplace in computer
   graphics for fluids and other materials undergoing large deformation. In
   these methods, particles are used to resolve transport and topological
   change, while a background Eulerian grid is used for computing
   mechanical forces and collision responses. Particlein-Cell (PIC)
   techniques, particularly the Fluid Implicit Particle (FLIP) variants
   have become the norm in computer graphics calculations. While these
   approaches have proven very powerful, they do suffer from some well
   known limitations. The original PIC is stable, but highly dissipative,
   while FLIP, designed to remove this dissipation, is more noisy and at
   times, unstable. We present a novel technique designed to retain the
   stability of the original PIC, without suffering from the noise and
   instability of FLIP. Our primary observation is that the dissipation in
   the original PIC results from a loss of information when transferring
   between grid and particle representations. We prevent this loss of
   information by augmenting each particle with a locally affine, rather
   than locally constant, description of the velocity. We show that this
   not only stably removes the dissipation of PIC, but that it also allows
   for exact conservation of angular momentum across the transfers between
   particles and grid.</abstract><date>AUG 2015</date><author>Jiang, Chenfanfu
   Schroeder, Craig
   Selle, Andrew
   Teran, Joseph
   Stomakhin, Alexey</author></paper><paper><title>DisseminACTION: disseminating science in the information age
   (www.action-euproject.eu: a website for researchers and parents)</title><abstract>www.action-euproject.eu is a website designed at the University of
   Cagliari, by the Department of Surgery, Faculty of Medicine, within the
   project "ACTION-Aggression in Children: unravelling gene-environment
   interplay to inform Treatment and InterventiON strategies", a
   collaborative project which includes twelve international partners,
   funded under the 7th Framework Programme for Research, technological
   Development and Demonstration.Its aim is to properly disseminate
   official news, events, medical discoveries carried out within the
   project, with an intent to connect European researchers and citizens
   with the official source of ACTION's scientific research. One of the
   main problems of the so called "web 2.0" is represented by the growth of
   viral misinformation, which contributes to create rumours and hoaxes
   around scientific threads. In order to avoid this kind of problems,
   www.action-euproject.eu is also designed to directly reach its audience
   even with social networks integration and with newsletters.Informatics
   is the discipline that studies the information processing through
   automated elaborations. The term appears for the first time in 1957, and
   since that time Computer Science has grown, reaching an unthinkable
   evolution, so that the common devices we use in our everyday lives
   (personal computers,notebooks, tablets, smartphones) are more powerful
   than the NASA calculators at the time of moon's landing. This evolution
   leads to privacy and security matters: our devices process everyday an
   important number of sensitive data, and are everyday exposed to the
   risks of computer security.This website has been designed following
   usability guidelines, with a logical sitemap, an easy system of options,
   a clear graphic style, a responsive graphic template and a robust
   Content Management System, in order to ensure the website security and a
   rigid privacy policy.</abstract><date>OCT 2015</date><author>Mauri, Matteo</author></paper><paper><title>A Parallel SIFT Algorithm for Image Matching Under Intelligent Vehicle
   Environment</title><abstract>Based on hardware architecture of CUDA (Compute Unified Device
   Architecture), this paper not only makes full use of multithreaded and
   parallelism in GPU (Graphic Processing Unit, the image Processing Unit),
   but also takes advantage of the memory to improve parallelization of
   SIFT algorithm. What's more, two-dimensional thread structure is adopted
   when storing the image data and variable blockIdx which is built in the
   device is used for mapping width and height in pixels of the
   two-dimensional images. Thus, it can improve the efficiency of parallel
   by making full use of thread parallelism and two-dimensional features of
   thread grid. The experimental results show that the matching accuracy
   and speed of the algorithm have greatly improved compared to the
   traditional serial SIFT algorithm, and the maximum acceleration ratio
   can reach 21.43 in this experiment making image parallel matching
   possible under the smart vehicle environment.</abstract><date>2015</date><author>Liu, Hui-Qi
   Li, Yuan-yuan
   Li, Tian-tian</author></paper><paper><title>Combined use of diffusion tensor tractography and multifused
   contrast-enhanced FIESTA for predicting facial and cochlear nerve
   positions in relation to vestibular schwannoma</title><abstract>OBJECT The authors assessed whether the combined use of diffusion tensor
   tractography (DTT) and contrast-enhanced (CE) fast imaging employing
   steady-state acquisition (FIESTA) could improve the accuracy of
   predicting the courses of the facial and cochlear nerves before
   surgery.METHODS The population was composed of 22 patients with
   vestibular schwannoma in whom both the facial and cochlear nerves could
   be identified during surgery. According to DTT, depicted fibers running
   from the internal auditory canal to the brainstem were judged to
   represent the facial or vestibulocochlear nerve. With regard to imaging,
   the authors investigated multifused CE-FIESTA scans, in which all 3D
   vessel models were shown simultaneously, from various angles. The
   low-intensity areas running along the tumor from brainstem to the
   internal auditory canal were judged to represent the facial or
   vestibulocochlear nerve.RESULTS For all 22 patients, the rate of fibers
   depicted by DTT coinciding with the facial nerve was 13.6% (3/22), and
   that of fibers depicted by DTT coinciding with the cochlear nerve was
   63.6% (14/22). The rate of candidates for nerves predicted by multifused
   CE-FIESTA coinciding with the facial nerve was 59.1% (13/22), and that
   of candidates for nerves predicted by multifused CE-FIESTA coinciding
   with the cochlear nerve was 4.5% (1/22). The rate of candidates for
   nerves predicted by combined DTT and multifused CE-FIESTA coinciding
   with the facial nerve was 63.6% (14/22), and that of candidates for
   nerves predicted by combined DTT and multifused CE-FIESTA coinciding
   with the cochlear nerve was 63.6% (14/22). The rate of candidates
   predicted by DTT coinciding with both facial and cochlear nerves was
   0.0% (0/22), that of candidates predicted by multifused CE-FIESTA
   coinciding with both facial and cochlear nerves was 4.5% (1/22), and
   that of candidates predicted by combined DTT and multifused CE-FIESTA
   coinciding with both the facial and cochlear nerves was 45.5%
   (10/22).CONCLUSIONS By using a combination of DTT and multifused
   CE-FIESTA, the-authors were able to increase the number of vestibular
   schwannoma patients for whom predicted results corresponded with the
   courses of both the facial and cochlear nerves, a result-that has been
   considered difficult to achieve by use of a single modality only.
   Although the 3D image including these prediction results helped with
   comprehension of the 3D operative anatomy, the reliability of prediction
   remains to be established.</abstract><date>DEC 2015</date><author>Yoshino, Masanori
   Kin, Taichi
   Ito, Akihiro
   Saito, Toki
   Nakagawa, Daichi
   Ino, Kenji
   Kamada, Kyousuke
   Mori, Harushi
   Kunimatsu, Akira
   Nakatomi, Hirofumi
   Oyama, Hiroshi
   Saito, Nobuhito</author></paper><paper><title>Origin of the balanomorph barnacles (Crustacea, Cirripedia, Thoracica):
   new evidence from the Late Cretaceous (Campanian) of Sweden</title><abstract>New material of thoracican cirripedes, traditionally assigned to
   Brachylepadomorpha and basal Balanomorpha, is described from abundant
   isolated plates collected from sediment deposited between boulders on a
   rocky coastline of Late Campanian age (c. 80 Ma) at Ivo Klack in Scania,
   southern Sweden. Two new genera, Epibrachylepas Gale gen. nov. and
   Parabrachylepas Gale gen. nov. (type species P. ifoensis Withers, 1935)
   are described, as is a new species, Epibrachylepas newmani Gale sp. nov.
   Pachydiadema cretacea Withers, 1935 and Brachylepas guascoi (Bosquet,
   1857) are redescribed on the basis of extensive new material. It is
   concluded that the long-held homologies between lateral plates of
   pedunculate cirripedes and balanomorphs are incorrect, and a new
   nomenclature is proposed for the latter group. Cladistic analysis based
   on 40 morphological characters of 12 species yields a consensus tree
   showing successive Brachylepas species and Pachydiadema as sister taxa
   to the crown group balanomorphs, which are here called Neobalanomorpha
   Gale suborder nov. Both 'Brachylepadomorpha' and 'Brachylepadidae' are
   paraphyletic, and together with P. cretacea form a morphocline leading
   from pedunculate ancestors (Pycnolepas articulata), through to basal
   sessile forms (B. naissanti, B. guascoi) and on to taxa identified as
   basal balanomorphs (Parabrachylepas, Epibrachylepas, Pachydiadema). The
   functional significance of the progressive changes is discussed with
   reference to living taxa. It is suggested that the radiation of
   Neobalanomorpha, dominant shallow water thoracicans in the Cenozoic,
   postdated the K-Pg near-extinction of more basal sessile barnacle
   groups.[GRAPHICS]</abstract><date>SEP 2 2015</date><author>Gale, Andrew Scott
   Sorensen, Anne Mehlin</author></paper><paper><title>Optimizations of the GPU-based Three-Dimensional FDTD Program with CPML
   boundary condition on Kepler Architecture GPU</title><abstract>An effective way to accelerate the Finite-difference time-domain (FDTD)
   method is the use of a Graphic Processing Unit (GPU). This paper
   describes an implementation of the three dimensional FDTD method with
   CPML boundary condition on a Kepler (GK110) architecture GPU. We
   optimize the FDTD domain decomposition method on Kepler GPU. And then,
   several Kepler-based optimizations are studied and applied to the FDTD
   program. The optimized program achieved up to 270.9 times speedup
   compared to the CPU sequential version. The experiments show that 22.2%
   of the simulation time is saved compared to the GPU version without
   optimizations. The solution is also faster than previous works.</abstract><date>2015</date><author>Shao, Ran
   Linton, David
   Spence, Ivor
   Zheng, Ning</author></paper><paper><title>VisualCNA: a GUI for interactive constraint network analysis and protein
   engineering for improving thermostability</title><abstract>Constraint network analysis (CNA) is a graph theory-based rigidity
   analysis approach for linking a biomolecule's structure, flexibility,
   (thermo) stability and function. Results from CNA are highly
   information-rich and require intuitive, synchronized and interactive
   visualization for a comprehensive analysis. We developed VisualCNA, an
   easy-to-use PyMOL plug-in that allows setup of CNA runs and analysis of
   CNA results linking plots with molecular graphics representations. From
   a practical viewpoint, the most striking feature of VisualCNA is that it
   facilitates interactive protein engineering aimed at improving
   thermostability.</abstract><date>JUL 15 2015</date><author>Rathi, Prakash Chandra
   Mulnaes, Daniel
   Gohlke, Holger</author></paper><paper><title>Accessible and Quantitative Entangled Polymer Rheology Predictions,
   Suitable for Complex Flow Calculations</title><abstract>Despite strong industrial interest, only recently, have quantitative
   predictions for entangled polymer rheology become possible. Major
   advances are achieved with an alternative approach to entanglement
   modeling slip-links. For example,- the discrete slip-link model (DSM)
   allows for equilibrium and highflciw,rate preclietions with a strong
   connection to a molecular basis. Slip-link -models are applicable to
   arbitrary composition and chain architectures, but usually incur high
   numerical cost. Recent advances in coarse-graining allowed for a speedup
   of order one hundred, but further improvements are required for the
   ultimate goal simulations of nenhomogeneots,flows., On the othefhand, it
   is possible to rely on advanced computational devices to obtain
   additional sp- eed-up. 'GPUs are one of the most rapidly developing kind
   of computational devices, and provide immense computational power, but
   also have some critical restrictions on numerical operations. Since
   slip-link models have a uniquelevel of description, Pin numerical
   implementations are designed from scratch, rather than using available
   modeling frame I ivorks. n this work, we preSent a numerical algorithm
   for.a variant- of DSM the elustered flied slip-link model fully
   implemented on CUDA. Because the model is a -Well-defined mathematical
   object, the completely new GPU-optimized, algorithm produces results
   indistinguishable from the earlier CPU implementation We believe the
   performance of the algorithm will allow simulations of complex,
   nonhomogeneous flows of entangled poly:mer melts and solutions on -a
   relatively small GPU computational eluster. The single GPIJ- version of
   code for -equilibrium and homogeneous flow calculations is available for
   free download online,: which allows homogeneous flow calculations on an
   inexpensive desktop. We discuss implementation details and test
   performance and show -a comparison of the results with,experiments.</abstract><date>MAR 10 2015</date><author>Andreev, Marat
   Schieber, Jay D.</author></paper><paper><title>Clinician-Graded Electronic Facial Paralysis Assessment: The eFACE</title><abstract>Background: The subjective nature of facial aesthetics and the
   difficulties associated with quantifying facial function have made
   outcomes analysis in facial paralysis challenging. Clinicians rely on
   photographs, subjective descriptions, and scales, limiting assessment,
   communication among providers, and communication between providers and
   patients. The authors describe the development and validation of a
   comprehensive, electronic, clinician-graded facial function scale
   (eFACE), which generates an overall disfigurement score and offers
   simple graphic output for clinician communication, assessment of various
   interventions, and patient understanding. The eFACE application may be
   used in a variety of electronic devices, including smartphones, tablets,
   and computers.Methods: An instrument consisting of 16 items in a visual
   analogue scale format was developed to assess facial function and
   symmetry (the eFACE). Video recordings of subjects performing facial
   expressions were viewed, and the eFACE instrument was applied, along
   with an overall facial disfigurement score. A multiple regression
   analysis was performed to determine the best linear relationship between
   overall expert-determined disfigurement and the eFACE items. The
   resulting equation was tested by three independent facial nerve
   clinicians, using an additional series of patients, to determine both
   interrater and intrarater reliability of the instrument.Results:
   Multiple regression analysis produced good fit of eFACE parameters to
   overall expert-rated global facial disfigurement when dynamic parameters
   were weighted twice as heavily as static and synkinesis parameters.
   eFACE scores demonstrated very high interrater and intrarater
   reliability.Conclusion: The eFACE is a reliable, reproducible, and
   straightforward digital clinical measure with which to assess facial
   function and disfigurement in patients with facial paralysis.</abstract><date>AUG 2015</date><author>Banks, Caroline A.
   Bhama, Prabhat K.
   Park, Jong
   Hadlock, Charles R.
   Hadlock, Tessa A.</author></paper><paper><title>Intuitive modeling of vaporish objects</title><abstract>Attempts to model gases in computer graphics started in the late 1970s.
   Since that time, there have been many approaches developed. In this
   paper we present a non-physical method allowing to create vaporish
   objects like clouds or smoky characters. The idea is to create a few
   sketches describing the rough shape of the final vaporish object. These
   sketches will be used as condensation sets of Iterated Function Systems,
   providing intuitive control over the object. The advantages of the new
   method are: simplicity, good control of resulting shapes and ease of
   eventual object animation. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>DEC 2015</date><author>Sokolov, Dmitry
   Gentil, Christian</author></paper><paper><title>A multiple-GPU based parallel independent coefficient reanalysis method
   and applications for vehicle design</title><abstract>The main limits of reanalysis method using CUDA (Compute Unified Device
   Architecture) for large-scale engineering optimization problems are low
   efficiency on single GPU and memory bottleneck of GPU. To breakthrough
   these bottlenecks, an efficient parallel independent coefficient (IC)
   reanalysis method is developed based on multiple GPUs platform. The IC
   reanalysis procedure is reconstructed to accommodate the use of multiple
   GPUs. The matrices and vectors are successfully partitioned and prepared
   for each GPU to achieve good load balance as well as little
   communication between GPUs. This study also proposes an effective
   technique to overlap the computation and communication by using
   non-blocking communication strategy. GPUs would continue their
   succeeding tasks while communication is still carried out
   simultaneously. Furthermore, the CSR format is used in each GPU for
   saving the memory. Finally, large-scale vehicle design problems are
   implemented by the developed solver. According to the test results, the
   multi-GPU based IC reanalysis method has potential capability for
   handling the real large scale problem and reducing the design cycle. (C)
   2015 Elsevier Ltd. All rights reserved.</abstract><date>JUL 2015</date><author>He, Guanqiang
   Wang, Hu
   Li, Enying
   Huang, Guanxin
   Li, Guangyao</author></paper><paper><title>Efficient GPU Spatial-Temporal Multitasking</title><abstract>Heterogeneous computing nodes are now pervasive throughout computing,
   and GPUs have emerged as a leading computing device for application
   acceleration. GPUs have tremendous computing potential for data-parallel
   applications, and the emergence of GPUs has led to proliferation of
   GPU-accelerated applications. This proliferation has also led to systems
   in which many applications are competing for access to GPU resources,
   and efficient utilization of the GPU resources is critical to system
   performance. Prior techniques of temporal multitasking can be employed
   with GPU resources as well, but not all GPU kernels make full use of the
   GPU resources. There is, therefore, an unmet need for spatial
   multitasking in GPUs. Resources used inefficiently by one kernel can be
   instead assigned to another kernel that can more effectively use the
   resources. In this paper we propose a software-hardware solution for
   efficient spatial-temporal multitasking and a software based emulation
   framework for our system. We pair an efficient heuristic in software
   with hardware leaky-bucket based thread-block interleaving to implement
   spatial-temporal multitasking. We demonstrate our techniques on various
   GPU architecture using nine representative benchmarks from CUDA SDK. Our
   experiments on Fermi GTX480 demonstrate performance improvement by up to
   46% (average 26%) over sequential GPU task execution and 37% (average
   18%) over default concurrent multitasking. Compared with the
   state-of-the-art Kepler K20 using Hyper-Q technology, our technique
   achieves up to 40% (average 17%) performance improvement over default
   concurrent multitasking.</abstract><date>MAR 2015</date><author>Liang, Yun
   Huynh Phung Huynh
   Rupnow, Kyle
   Goh, Rick Siow Mong
   Chen, Deming</author></paper><paper><title>Nonlinear electronic excitations in crystalline solids using
   meta-generalized gradient approximation and hybrid functional in
   time-dependent density functional theory</title><abstract>We develop methods to calculate electron dynamics in crystalline solids
   in real-time time-dependent density functional theory employing
   exchange-correlation potentials which reproduce band gap energies of
   dielectrics; a meta-generalized gradient approximation was proposed by
   Tran and Blaha [Phys. Rev. Lett. 102, 226401 (2009)] (TBm-BJ) and a
   hybrid functional was proposed by Heyd, Scuseria, and Ernzerhof [J.
   Chem. Phys. 118, 8207 (2003)] (HSE). In time evolution calculations
   employing the TB-mBJ potential, we have found it necessary to adopt the
   predictor-corrector step for a stable time evolution. We have developed
   a method to evaluate electronic excitation energy without referring to
   the energy functional which is unknown for the TB-mBJ potential. For the
   HSE functional, we have developed a method for the operation of the
   Fock-like term in Fourier space to facilitate efficient use of massive
   parallel computers equipped with graphic processing units. We compare
   electronic excitations in silicon and germanium induced by femtosecond
   laser pulses using the TB-mBJ, HSE, and a simple local density
   approximation (LDA). At low laser intensities, electronic excitations
   are found to be sensitive to the band gap energy: they are close to each
   other using TB-mBJ and HSE and are much smaller in LDA. At high laser
   intensities close to the damage threshold, electronic excitation
   energies do not differ much among the three cases. (C) 2015 AIP
   Publishing LLC.</abstract><date>DEC 14 2015</date><author>Sato, Shunsuke A.
   Taniguchi, Yasutaka
   Shinohara, Yasushi
   Yabana, Kazuhiro</author></paper><paper><title>Design space exploration of SW beamformer on GPU</title><abstract>Ultrasound imaging has become one of the most widely used modalities in
   medical diagnosis today. However, real-time ultrasound imaging requires
   large amount of data transfer and massive computation and therefore
   mainly relies on a complex dedicated hardware system. A recent trend of
   a graphics processing unit (GPU) based software-based approach offers
   the advantages of flexibility and quick implementation. The GPUs have
   been reported as excellent accelerators across a wide range of
   applications. For best exploiting outstanding computational power and
   high memory bandwidth of a GPU, the paper explores the design space of
   implementing an ultrasound beamformer on a GPU platform. The design
   spaces are expanded by applying different optimization strategies to the
   beamformer on a GPU platform, and we also discuss the performance
   evaluation results on the various GPUs whose architectural
   characteristics are different to each others. The performance analysis
   shows that by optimizing CUDA code, our real-time-GPU-based beamformer
   can be successfully implemented with 181 frames per second (fps) and
   speedup of 230.6X compared with the single-threaded implementation on a
   high-performance CPU platform. Copyright (c) 2014 John Wiley &amp; Sons,
   Ltd.</abstract><date>MAY 2015</date><author>Thi Yen Phuong
   Lee, Jeong-Gun</author></paper><paper><title>Automotive fuel cell sloshing under temporally and spatially varying
   high acceleration using GPU-based Smoothed Particle Hydrodynamics (SPH)</title><abstract>Understanding how fuel sloshes in a fuel cell, as a vehicle races around
   a circuit, is an important but mostly unexplored factor when designing
   fuel containment systems. Cell designs are based on knowledge of how
   liquids slosh in other containers, with the design and placement of
   structures, such as weirs, based on engineering judgement.This work aims
   to provide better understanding for this difficult problem with a view
   to improve future designs. A Graphics Processing Unit (GPU) based
   Smoothed Particle Hydrodynamics (SPH) model is presented to simulate the
   fuel sloshing problem, with results from a simplified and real fuel cell
   geometry shown and compared against real data recorded in a vehicle. The
   vehicle motion and accelerations are included in the SPH simulations
   using a body force within the momentum equation. Results show good
   agreement between the simulation and the real fuel movement, with bulk
   motion captured well for accelerations up to 5 times gravity.Focus is
   placed on the practicality of the method for use as part of an
   industrial design process, therefore the amount of time needed to
   compute results is considered throughout. Computational performance is
   found to be within acceptable limits, while numerical accuracy is
   actively considered through the use of Kahan compensated summation. It
   is concluded that the model is successful in capturing the necessary
   fluid dynamics for it to be useful in fuel cell design. It is expected
   that the method will provide insight into current cell designs and
   highlight where improvements can be made. (C) 2015 Elsevier Ltd. All
   rights reserved.</abstract><date>MAY 2015</date><author>Longshaw, S. M.
   Rogers, B. D.</author></paper><paper><title>Scalable multi-relaxation-time lattice Boltzmann simulations on
   multi-GPU cluster</title><abstract>In this paper, the D3Q19 multi-relaxation-time lattice Boltzmann model
   is adopted to simulate three-dimensional cavity flows using graphic
   processing units (GPUs). For single CPU computations, utilizing on-chip
   memory generates three to five times speedup over adopting global memory
   alone. Also, streaming using offset reading attains another two times
   speedup over employing offset writing, For Message Passing Interface
   (MPI) based multi-CPU computations, overlapping communication and
   computation can achieve 38% improvement and provide an efficient scheme
   to improve the scalability and its performance. Numerical experiments
   show that 12 Tesla (TM) M2070 CPUs produce around 5500 million lattices
   updates per second (MLUPS) using 576(3) grid. On the other hand, three
   GTX Titans deliver 5000 MLUPS for 192(3) grids, while 12 Tesla attain
   half performance. (C) 2014 Elsevier Ltd. All rights reserved.</abstract><date>MAR 30 2015</date><author>Hong, Pei-Yao
   Huang, Li-Min
   Lin, Li-Song
   Lin, Chao-An</author></paper><paper><title>Supporting High Performance Molecular Dynamics in Virtualized Clusters
   using IOMMU, SR-IOV, and GPUDirect</title><abstract>Cloud Infrastructure-as-a-Service paradigms have recently shown their
   utility for a vast array of computational problems, ranging from
   advanced web service architectures to high throughput computing.
   However, many scientific computing applications have been slow to adapt
   to virtualized cloud frameworks. This is due to performance impacts of
   virtualization technologies, coupled with the lack of advanced hardware
   support necessary for running many high performance scientific
   applications at scale.By using KVM virtual machines that leverage both
   Nvidia GPUs and InfiniBand, we show that molecular dynamics simulations
   with LAMMPS and HOOMD run at near-native speeds. This experiment also
   illustrates how virtualized environments can support the latest parallel
   computing paradigms, including both MPI+CUDA and new GPUDirect RDMA
   functionality. Specific findings show initial promise in scaling of such
   applications to larger production deployments targeting large scale
   computational workloads.</abstract><date>JUL 2015</date><author>Younge, Andrew J.
   Walters, John Paul
   Crago, Stephen P.
   Fox, Geoffrey C.</author></paper><paper><title>Camera array calibration for light field acquisition</title><abstract>Light field cameras are becoming popular in computer vision and
   graphics, with many research and commercial applications already having
   been proposed. Various types of cameras have been developed with the
   camera array being one of the ways of acquiring a 4D light field image
   using multiple cameras. Camera calibration is essential, since each
   application requires the correct projection and ray geometry of the
   light field. The calibrated parameters are used in the light field image
   rectified from the images captured by multiple cameras. Various camera
   calibration approaches have been proposed for a single camera, multiple
   cameras, and a moving camera. However, although these approaches can be
   applied to calibrating camera arrays, they are not effective in terms of
   accuracy and computational cost. Moreover, less attention has been paid
   to camera calibration of a light field camera. In this paper, we propose
   a calibration method for a camera array and a rectification method for
   generating a light field image from the captured images. We propose a
   two-step algorithm consisting of closed form initialization and
   nonlinear refinement, which extends Zhang's well-known method to the
   camera array. More importantly, we introduce a rigid camera constraint
   whereby the array of cameras is rigidly aligned in the camera array and
   utilize this constraint in our calibration. Using this constraint, we
   obtained much faster and more accurate calibration results in the
   experiments.</abstract><date>OCT 2015</date><author>Xu, Yichao
   Maeno, Kazuki
   Nagahara, Hajime
   Taniguchi, Rin-ichiro</author></paper><paper><title>GPU-Accelerated Method of Query Selectivity Estimation for Non Equi-Join
   Conditions Based on Discrete Fourier Transform</title><abstract>Selectivity factor is obtained by database query optimizer for
   estimating the size of data that satisfy a query condition. This allows
   to choose the optimal query execution plan. In this paper we consider
   the problem of selectivity estimation for inequality predicates based on
   two attributes, therefore the proposed solution allows to estimate the
   size of data that satisfy theta-join conditions. The proposed method is
   based on Discrete Fourier Transform and convolution theorem. DFT
   spectrums are used as representations of distribution of attribute
   values. We compute selectivity either performing Inverse DFT (for an
   inequality condition based on two attributes) or avoiding it (for a
   single-attribute range one). Selectivity calculation is a time-critical
   operation performed during an on-line query preparing phase. We show
   that by applying parallel processing capabilities of Graphical
   Processing Unit, the implementation of the method satisfies the assumed
   time constraint.</abstract><date>2015</date><author>Augustyn, Dariusz Rafal
   Warchal, Lukasz</author></paper><paper><title>GPU Implementation of Physarum Cellular Automata Model</title><abstract>In the past few decades, there is an increasing number of publications
   which show that solutions to complex mathematical problems can be found
   by applying unconventional computing methods. Among other examples, the
   plasmodium of Physarum Polycephalum has been intensively used for
   solving shortest path(s) problem, various graph problems, evaluation of
   transport networks, robotic control and many other engineering
   applications. In this paper we coupled the computing abilities of slime
   mould with one of the most powerful parallel computational models,
   namely Cellular Automata (CAs). CAs can capture the essential features
   of systems which global behavior emerges from the collective effect of
   simple components, which interact locally. Moreover, a Graphical
   Processing Unit (GPU) implementation will exploit the prominent feature
   of parallelism that CA structures inherently possess in contrast to the
   serial computers, thus accelerating the response of the proposed model.
   As a result, a slime mould CA based model in graphics processing unit
   (GPU) using CUDA programming model to describe and mimic the behavior of
   a plasmodium in a maze. In this way we are able to produce a virtual lab
   speeding up significantly the biological paradigm in GPU.</abstract><date>2015</date><author>Dourvas, Nikolaos I.
   Sirakoulis, Georgios Ch.
   Tsalides, Philippos</author></paper><paper><title>Single calcium channel domain gating of synaptic vesicle fusion at fast
   synapses; analysis by graphic modeling</title><abstract>At fast-transmitting presynaptic terminals Ca2+ enter through voltage
   gated calcium channels (CaVs) and bind to a synaptic vesicle (SV)
   -associated calcium sensor (SV-sensor) to gate fusion and discharge. An
   open CaV generates a high-concentration plume, or nanodomain of Ca2+
   that dissipates precipitously with distance from the pore. At most fast
   synapses, such as the frog neuromuscular junction (NMJ), the SV sensors
   are located sufficiently close to individual CaVs to be gated by single
   nanodomains. However, at others, such as the mature rodent calyx of Held
   (calyx of Held), the physiology is more complex with evidence that CaVs
   that are both close and distant from the SV sensor and it is argued that
   release is gated primarily by the overlapping Ca2+ nanodomains from many
   CaVs. We devised a 'graphic modeling' method to sum Ca2+ from individual
   CaVs located at varying distances from the SV-sensor to determine the SV
   release probability and also the fraction of that probability that can
   be attributed to single domain gating. This method was applied first to
   simplified, low and high CaV density model release sites and then to
   published data on the contrasting frog NMJ and the rodent calyx of Held
   native synapses. We report 3 main predictions: the SV-sensor is
   positioned very close to the point at which the SV fuses with the
   membrane; single domain-release gating predominates even at synapses
   where the SV abuts a large cluster of CaVs, and even relatively remote
   CaVs can contribute significantly to single domain-based gating.</abstract><date>SEP 3 2015</date><author>Stanley, Elise F.</author></paper><paper><title>SpirPro: A Spirulina proteome database and web-based tools for the
   analysis of protein-protein interactions at the metabolic level in
   Spirulina (Arthrospira) platensis C1</title><abstract>Background: Spirulina (Arthrospira) platensis is the only cyanobacterium
   that in addition to being studied at the molecular level and subjected
   to gene manipulation, can also be mass cultivated in outdoor ponds for
   commercial use as a food supplement. Thus, encountering environmental
   changes, including temperature stresses, is common during the mass
   production of Spirulina. The use of cyanobacteria as an experimental
   platform, especially for photosynthetic gene manipulation in plants and
   bacteria, is becoming increasingly important. Understanding the
   mechanisms and protein-protein interaction networks that underlie
   low-and high-temperature responses is relevant to Spirulina mass
   production. To accomplish this goal, high-throughput techniques such as
   OMICs analyses are used. Thus, large datasets must be collected, managed
   and subjected to information extraction. Therefore, databases including
   (i) proteomic analysis and protein-protein interaction (PPI) data and
   (ii) domain/motif visualization tools are required for potential use in
   temperature response models for plant chloroplasts and photosynthetic
   bacteria.Descriptions: A web-based repository was developed including an
   embedded database, SpirPro, and tools for network visualization.
   Proteome data were analyzed integrated with protein-protein interactions
   and/or metabolic pathways from KEGG. The repository provides various
   information, ranging from raw data (2D-gel images) to associated
   results, such as data from interaction and/or pathway analyses. This
   integration allows in silico analyses of protein-protein interactions
   affected at the metabolic level and, particularly, analyses of
   interactions between and within the affected metabolic pathways under
   temperature stresses for comparative proteomic analysis. The developed
   tool, which is coded in HTML with CSS/JavaScript and depicted in
   Scalable Vector Graphics (SVG), is designed for interactive analysis and
   exploration of the constructed network. SpirPro is publicly available on
   the web at http://spirpro.sbi.kmutt.ac.th.Conclusions: SpirPro is an
   analysis platform containing an integrated proteome and PPI database
   that provides the most comprehensive data on this cyanobacterium at the
   systematic level. As an integrated database, SpirPro can be applied in
   various analyses, such as temperature stress response networking
   analysis in cyanobacterial models and interacting domain-domain analysis
   between proteins of interest.</abstract><date>JUL 29 2015</date><author>Senachak, Jittisak
   Cheevadhanarak, Supapon
   Hongsthong, Apiradee</author></paper><paper><title>A Survey on Implicit Surface Polygonization</title><abstract>Implicit surfaces (IS) are commonly used in image creation, modeling
   environments, modeling objects, and scientific data visualization. In
   this article, we present a survey of different techniques for fast
   visualization of IS. The main classes of visualization algorithms are
   identified along with the advantages of each in the context of the
   different types of IS commonly used in computer graphics. We focus
   closely on polygonization methods, as they are the most suited to fast
   visualization. Classification and comparison of existing approaches are
   presented using criteria extracted from current research. This enables
   the identification of the best strategies according to the number of
   specific requirements, such as speed, accuracy, quality, or stylization.</abstract><date>JUL 2015</date><author>de Araujo, B. R.
   Lopes, Daniel S.
   Jepp, Pauline
   Jorge, Joaquim A.
   Wyvill, Brian</author></paper><paper><title>Numerical simulation of radiative heat transfer in indoor environments
   on programmable graphics hardware</title><abstract>The efficient use of energy for heating and cooling of indoor
   environments requires an accurate prediction and analysis of radiative
   heat transfer. Therefore it is necessary to use modern computer methods
   as otherwise the computational costs may become higher than for
   convective heat transfer, which is known to be a huge computational
   problem. A key problem in calculations of radiative heat transfer is the
   problem of mutual visibility arising in the determination of view
   factors. This is the same challenge that global illumination has to cope
   with which is one of the fundamental topics in computational graphics.
   The visibility problem is efficiently solved by modern graphics
   hardware. Therefore an OpenGL-based algorithm is developed to quickly
   and accurately calculate view factors for arbitrary, complex geometries.
   Theoretical and implementation details of the applied methods are given.
   We demonstrate the advantages of the developed computational method by a
   virtual test room for a tubular radiator, a heating of a warehouse by
   ceramic infrared heaters and the heat transfer in a car cabin. (C) 2015
   Elsevier Masson SAS. All rights reserved.</abstract><date>OCT 2015</date><author>Kramer, Stephan
   Gritzki, Ralf
   Perschk, Alf
   Roesler, Markus
   Felsmann, Clemens</author></paper><paper><title>Virtual Movement of the Ankle and Subtalar Joints Using Cadaver Surface
   Models</title><abstract>Medical students in the dissection room do not fully understand the
   ankle joint for dorsiflexion and plantar flexion as well as the subtalar
   joint for inversion and eversion. Thus, a three-dimensional simulation
   of the movements would be beneficial as a complementary pedagogic tool.
   The bones and five muscles (tibialis anterior, tibialis posterior,
   fibularis longus, fibularis brevis, and fibularis tertius) of the left
   ankle and foot were outlined in serially sectioned cadaver images from
   the Visible Korean project. The outlines were verified and revised; and
   were stacked to build surface models using Mimics software. Dorsiflexion
   and plantar flexion were simulated using the models on Maya to determine
   the mediolateral axis. Then, inversion and eversion were done to
   determine the anteroposterior axis. The topographic relationship of the
   two axes with the five affecting muscles was examined to demonstrate
   correctness. The models were placed in a PDF file, with which users were
   capable of mixed display of structures. The stereoscopic image data,
   developed in this investigation, clearly explain ankle movement. These
   graphic contents, accompanied by the sectioned images, are expected to
   facilitate the development of simulation for the medical students'
   learning and the orthopedic surgeons' clinical trial.</abstract><date>SEP 2015</date><author>Shin, Dong Sun
   Chung, Min Suk</author></paper><paper><title>Automatic High-Level Data-Flow Synthesis and Optimization of Polynomial
   Datapaths Using Functional Decomposition</title><abstract>This paper concentrates on high-level data-flow optimization and
   synthesis techniques for datapath intensive designs such as those in
   Digital Signal Processing (DSP), computer graphics and embedded systems
   applications, which are modeled as polynomial computations over Z(2n1) x
   Z(2n2) x ... x Z(2nd) to Z(2m). Our main contribution in this paper is
   proposing an optimization method based on functional decomposition of
   multivariate polynomial in the form of f(x) = g(x) o h(x) + f(0) =
   g(h(x)) + f(0) to obtain good building blocks, and vanishing polynomials
   over Z(2m) to add/delete redundancy to/from given polynomial functions
   to extract further common sub-expressions. Experimental results for
   combinational implementation of the designs have shown an average saving
   of 38.85 and 18.85 percent in the number of gates and critical path
   delay, respectively, compared with the state-of-the-art techniques.
   Regarding the comparison with our previous works, the area and delay are
   improved by 10.87 and 11.22 percent, respectively. Furthermore,
   experimental results of sequential implementations have shown an average
   saving of 39.26 and 34.70 percent in the area and the latency,
   respectively, compared with the state-of-the-art techniques.</abstract><date>JUN 2015</date><author>Ghandali, Samaneh
   Alizadeh, Bijan
   Fujita, Masahiro
   Navabi, Zainalabedin</author></paper><paper><title>MULTIPLE STRING MATCHING ON A GPU USING CUDA</title><abstract>Multiple pattern matching algorithms are used to locate the occurrences
   of patterns from a finite pattern set in a large input string.
   Aho-Corasick, Set Horspool, Set Backward Oracle Matching, Wu-Manber and
   SOG, five of the most well known algorithms for multiple matching
   require an increased computing power, particularly in cases where
   large-size datasets must be processed, as is common in computational
   biology applications. Over the past years, Graphics Processing Units
   (GPUs) have evolved to powerful parallel processors outperforming CPUs
   in scientific applications. This paper evaluates the speedup of the
   basic parallel strategy and the different optimization strategies for
   parallelization of Aho-Corasick, Set Horspool, Set Backward Oracle
   Matching, Wu-Manber and SOG algorithms on a GPU.</abstract><date>JUN 2015</date><author>Kouzinopoulos, Charalampos S.
   Michailidis, Panagiotis D.
   Margaritis, Konstantinos G.</author></paper><paper><title>GeNN: a code generation framework for accelerated brain simulations</title><abstract>Large-scale numerical simulations of detailed brain circuit models are
   important for identifying hypotheses on brain functions and testing
   their consistency and plausibility. An ongoing challenge for simulating
   realistic models is, however, computational speed. In this paper, we
   present the GeNN (GPU-enhanced Neuronal Networks) framework, which aims
   to facilitate the use of graphics accelerators for computational models
   of large-scale neuronal networks to address this challenge. GeNN is an
   open source library that generates code to accelerate the execution of
   network simulations on NVIDIA GPUs, through a flexible and extensible
   interface, which does not require in-depth technical knowledge from the
   users. We present performance benchmarks showing that 200-fold speedup
   compared to a single core of a CPU can be achieved for a network of one
   million conductance based Hodgkin-Huxley neurons but that for other
   models the speedup can differ. GeNN is available for Linux, Mac OS X and
   Windows platforms. The source code, user manual, tutorials, Wiki,
   in-depth example projects and all other related information can be found
   on the project website http://genn-team.github.io/genn/.</abstract><date>JAN 7 2016</date><author>Yavuz, Esin
   Turner, James
   Nowotny, Thomas</author></paper><paper><title>High Throughput Pipeline Decoder for LDPC Convolutional Codes on GPU</title><abstract>In this letter, we present a graphics processing unit (GPU)-based LDPC
   convolutional code (LDPC-CC) pipeline decoder with optimized
   parallelism. The proposed decoder exploits different granularities of
   decoding parallelism for both the compute unified device architecture
   (CUDA) kernel execution stage and the data transfer stage. Moreover, the
   parameter selection criteria for decoder implementation are designed to
   avoid exhaustive search of all the combinations of parameters. The
   experiments are carried out on NvidiaGTX460 and GTX580 platforms. The
   results demonstrate the proposed decoder achieves about 3 times speedup
   compared to the existing GPU-based work.</abstract><date>DEC 2015</date><author>Hou, Yi
   Liu, Rongke
   Peng, Hao
   Zhao, Ling</author></paper><paper><title>Electroencephalographic Data Analysis With Visibility Graph Technique
   for Quantitative Assessment of Brain Dysfunction</title><abstract>Usual techniques for electroencephalographic (EEG) data analysis lack
   some of the important properties essential for quantitative assessment
   of the progress of the dysfunction of the human brain. EEG data are
   essentially nonlinear and this nonlinear time series has been identified
   as multi-fractal in nature. We need rigorous techniques for such
   analysis. In this article, we present the visibility graph as the
   latest, rigorous technique that can assess the degree of multifractality
   accurately and reliably. Moreover, it has also been found that this
   technique can give reliable results with test data of comparatively
   short length. In this work, the visibility graph algorithm has been used
   for mapping a time series-EEG signals-to a graph to study complexity and
   fractality of the time series through investigation of its complexity.
   The power of scale-freeness of visibility graph has been used as an
   effective method for measuring fractality in the EEG signal. The
   scale-freeness of the visibility graph has also been observed after
   averaging the statistically independent samples of the signal.
   Scale-freeness of the visibility graph has been calculated for 5 sets of
   EEG data patterns varying from normal eye closed to epileptic. The
   change in the values is analyzed further, and it has been observed that
   it reduces uniformly from normal eye closed to epileptic.</abstract><date>JUL 2015</date><author>Bhaduri, Susmita
   Ghosh, Dipak</author></paper><paper><title>A Modular Framework for EEG Web Based Binary Brain Computer Interfaces
   to Recover Communication Abilities in Impaired People</title><abstract>A Brain Computer Interface (BCI) allows communication for impaired
   people unable to express their intention with common channels.
   Electroencephalography (EEG) represents an effective tool to allow the
   implementation of a BCI. The present paper describes a modular framework
   for the implementation of the graphic interface for binary BCIs based on
   the selection of symbols in a table. The proposed system is also
   designed to reduce the time required for writing text. This is made by
   including a motivational tool, necessary to improve the quality of the
   collected signals, and by containing a predictive module based on the
   frequency of occurrence of letters in a language, and of words in a
   dictionary. The proposed framework is described in a top-down approach
   through its modules: signal acquisition, analysis, classification,
   communication, visualization, and predictive engine. The framework,
   being modular, can be easily modified to personalize the graphic
   interface to the needs of the subject who has to use the BCI and it can
   be integrated with different classification strategies, communication
   paradigms, and dictionaries/languages. The implementation of a scenario
   and some experimental results on healthy subjects are also reported and
   discussed: the modules of the proposed scenario can be used as a
   starting point for further developments, and application on severely
   disabled people under the guide of specialized personnel.</abstract><date>JAN 2016</date><author>Placidi, Giuseppe
   Petracca, Andrea
   Spezialetti, Matteo
   Iacoviello, Daniela</author></paper><paper><title>An Effective CUDA Parallelization of Projection in Iterative Tomography
   Reconstruction.</title><abstract>Projection and back-projection are the most computationally intensive
   parts in Computed Tomography (CT) reconstruction, and are essential to
   acceleration of CT reconstruction algorithms. Compared to
   back-projection, parallelization efficiency in projection is highly
   limited by racing condition and thread unsynchronization. In this paper,
   a strategy of Fixed Sampling Number Projection (FSNP) is proposed to
   ensure the operation synchronization in the ray-driven projection with
   Graphical Processing Unit (GPU). Texture fetching is also used utilized
   to further accelerate the interpolations in both projection and
   back-projection. We validate the performance of this FSNP approach using
   both simulated and real cone-beam CT data. Experimental results show
   that compare to the conventional approach, the proposed FSNP method
   together with texture fetching is 10~16 times faster than the
   conventional approach based on global memory, and thus leads to more
   efficient iterative algorithm in CT reconstruction. </abstract><date>2015</date><author>Xie, Lizhe
   Hu, Yining
   Yan, Bin
   Wang, Lin
   Yang, Benqiang
   Liu, Wenyuan
   Zhang, Libo
   Luo, Limin
   Shu, Huazhong
   Chen, Yang</author></paper><paper><title>Efficient CPU-GPU cooperative computing for solving the subset-sum
   problem</title><abstract>Heterogeneous CPU-GPU system is a powerful way to accelerate
   compute-intensive applications, such as the subset-sum problem. Many
   parallel algorithms for solving the problem have been implemented on
   graphics processing units (GPUs). However, these GPU implementations may
   fail to fully utilize all the CPU cores and the GPU resources. When the
   GPU performs computational task, only one CPU core is used to control
   the GPUs, and all the remaining CPU cores are in idle state, which leads
   to large amounts of available CPU resources being wasted. This paper
   proposes an efficient CPU-GPU cooperative computing scheme for solving
   the subset-sum problem, which enables the full utilization of all the
   computing power of both CPUs and GPUs. In order to find the most
   appropriate task distribution ratio between CPUs and GPUs, this paper
   establishes a simple but effective task distribution model. Considering
   the high CPU-GPU communication overhead and the unbalanced workload
   between CPUs and GPUs may greatly reduce the performance, an incremental
   data transfer method is proposed to reduce the CPU-GPU communication
   overhead, and a feedback-based dynamic task distribution scheme is
   designed to effectively balance the workload between CPUs and GPUs
   during runtime. The experimental results show that the CPU-GPU
   cooperative computing achieves a significant performance benefit over
   the CPU-only or GPU-only computing. Copyright (c) 2015 John Wiley &amp;
   Sons, Ltd.</abstract><date>FEB 2016</date><author>Wan, Lanjun
   Li, Kenli
   Liu, Jing
   Li, Keqin</author></paper><paper><title>2D to 3D geologic mapping transformation using virtual globes and flight
   simulators and their applications in the analysis of geodiversity in
   natural areas</title><abstract>This work describes the transformation process from 2D cartography to
   3D, simply by overlapping images in common formats (jpeg, bmp, tiff,
   png, etc.) on Google Earth's virtual globe. Arribes del Duero Natural
   Park, located west of the province of Salamanca, Spain, was the object
   of this part of the study. Other natural areas are also discussed and
   were used to establish a procedure for mapping geodiversity and for
   identifying areas of geological uniqueness and naturalness within the
   natural areas. To do this, different parametric indices were used to
   empirically generate different degrees of geological diversity in the
   Quilamas Natural Area, located south of Salamanca, Spain. Intermediate
   parametric maps were processed using two types of GIS technical:
   graphical (neighbourhood operations) and alphanumeric (calculated from
   the fields in the attribute table). Intermediate parametric maps were
   processed using two types of technical GIS: neighbourhood operations
   (graphics) and alphanumeric (calculated from the fields in the attribute
   table). These maps were used to establish areas with the greatest
   concentration of geological diversity elements and to define areas with
   a greater need for protection when planning the management of human
   activities in natural areas. Finally, the flight simulator tool, which
   was implemented in the free virtual globe and controlled using a
   keyboard or joystick, allows you to "fly" through the projected
   geological mapping of Arribes Del Duero Natural Park or view the
   parametric mapping and geodiversity in the newly created Quilamas
   Natural Area. Interoperability with the Google Maps application allows
   you to identify and observe the outcrops of the various geological
   materials in natural or anthropic terrain cuts.</abstract><date>JUN 2015</date><author>Martinez-Grana, A. M.
   Goy, J. L.
   Cimarra, C.</author></paper><paper><title>Efficient visualization of high-throughput targeted proteomics
   experiments: TAPIR</title><abstract>Motivation: Targeted mass spectrometry comprises a set of powerful
   methods to obtain accurate and consistent protein quantification in
   complex samples. To fully exploit these techniques, a cross-platform and
   open-source software stack based on standardized data exchange formats
   is required.Results: We present TAPIR, a fast and efficient Python
   visualization software for chromatograms and peaks identified in
   targeted proteomics experiments. The input formats are open,
   community-driven standardized data formats (mzML for raw data storage
   and TraML encoding the hierarchical relationships between transitions,
   peptides and proteins). TAPIR is scalable to proteome-wide targeted
   proteomics studies (as enabled by SWATH-MS), allowing researchers to
   visualize high-throughput datasets. The framework integrates well with
   existing automated analysis pipelines and can be extended beyond
   targeted proteomics to other types of analyses.</abstract><date>JUL 15 2015</date><author>Roest, Hannes L.
   Rosenberger, George
   Aebersold, Ruedi
   Malmstroem, Lars</author></paper><paper><title>Hierarchical genetic clusters for phenotypic analysis</title><abstract>Methods to obtain phenotypic information were evaluated to help breeders
   choosing the best methodology for analysis of genetic diversity in
   backcross populations. Phenotypes were simulated for 13 characteristics
   generated in 10 populations with 100 individuals each. Genotypic
   information was generated from 100 loci of which 20 were taken at random
   to determine the characteristics expressing two alleles. Dissimilarity
   measures were calculated, and genetic diversity was analyzed through
   hierarchical clustering and graphic projection of the distances. A
   backcross was performed from the two most divergent populations. A set
   of characteristics with variable heritability was taken into account.
   The environmental effect was simulated assuming x similar to N (0,
   sigma(2)). For hierarchical clusters, the following methods were used:
   Gower Method, average linkage within the cluster, average linkage among
   clusters, the furthest neighbor method, the nearest neighbor method,
   Ward's method, and the median method. The environmental effect and
   heritability of the analyzed variables had an influence on the pattern
   of hierarchical clustering populations according to the backcrossed
   generations. The nearest neighbor method was the most efficient in
   reconstructing the system of backcrossing, and it presented the highest
   cophenetic correlation. The efficiency of the nearest neighbor method
   was the highest when the analysis involved characteristics of high
   heritability.</abstract><date>OCT-DEC 2015</date><author>da Matta, Luiza Barbosa
   Oliveira Tome, Livia Gracielle
   Salgado, Caio Cesio
   Cruz, Cosme Damiao
   Silva, Leticia de Faria</author></paper><paper><title>A study of graphics hardware accelerated particle swarm optimization
   with digital pheromones</title><abstract>Programmable Graphics Processing Units (GPUs) have lately become a
   promising means to perform scientific computations. Modern GPUs have
   proven to outperform the number of floating point operations when
   compared to traditional Central Processing Units (CPUs) through inherent
   data parallel architecture and higher bandwidth capabilities. They allow
   scientific computations to be performed without noticeable degradation
   in accuracy in a fraction of the time compared to traditional CPUs at
   substantially reduced costs, making them viable alternatives to
   expensive computer clusters or workstations. GPU programmability
   however, has fostered the development of a variety of programming
   languages making it challenging to select a computing language and use
   it consistently without the pitfall of being obsolete. Some GPU
   languages are hardware specific and are designed to rake in performance
   boosts when used with their host GPUs (e.g., Nvidia Cuda). Others are
   operating system specific (e.g., Microsoft HLSL). A few are platform
   agnostic lending themselves to be used on a workstation with any CPU and
   a GPU (e.g., GLSL, OpenCL).Of a number of companies and organizations
   that implement formal optimization into their processes, only a few
   utilize GPUs. It is either because the others are either vested much
   into CPU based computing or they are not fully aware of the benefits of
   implementing population based optimization routines in GPUs. Literature
   shows a large number of research publications specifically in the field
   of optimization utilizing GPUs. However, most of them are limited to a
   specific GPU hardware or addressed specific problems. The diversity in
   current GPU hardware and software APIs present overwhelming number of
   choices making it challenging to decide where and how to begin
   transitioning to GPU based computing, impeding promising computing
   avenues that relatively is very cost effective. In this paper, the
   authors precisely intend to address some of these issues by broadly
   classifying GPU APIs into three categories: 1) Hardware vendor dependent
   GPU APIs, 2) Graphical in context APIs, and 3) Platform agnostic APIs.
   Prior work by the authors demonstrated the capability of digital
   pheromones within Particle Swarm Optimization (PSO) for searching
   n-dimensional design spaces with improved accuracy, efficiency and
   reliability in serial and parallel CPU computing environments. To study
   the impact of GPUs, the authors have taken this digital pheromone
   variant of PSO and implemented it on three GPU APIs, each representing a
   category listed above, in a simplistic sense - delegate unconstrained
   explicit objective function evaluations to GPUs. While this approach
   itself cannot be considered novel, the takeaways from implementing it on
   different GPU APIs provided a wealth of information that the authors
   believe can help optimization companies and organizations make informed
   decisions in implementing GPUs in their processes.</abstract><date>JUN 2015</date><author>Kalivarapu, Vijay
   Winer, Eliot</author></paper><paper><title>A GPU based compressible multiphase hydrocode for modelling violent
   hydrodynamic impact problems</title><abstract>This paper presents a GPU based compressible multiphase hydrocode for
   modelling violent hydrodynamic impacts under harsh conditions such as
   slamming and underwater explosion. An effort is made to extend a
   one-dimensional five-equation reduced model (Kapila et al., 2001) to
   compute three-dimensional hydrodynamic impact problems on modern
   graphics hardware. In order to deal with free-surface problems such as
   water waves, gravitational terms, which are initially absent from the
   original model, are now considered and included in the governing
   equations. A third-order finite volume based MUSCL scheme is applied to
   discretise the integral form of the governing equations. The numerical
   flux across a mesh cell face is estimated by means of the HLLC
   approximate Riemann solver. The serial CPU program is firstly
   parallelised on multi-core CPUs with the OpenMP programming model and
   then further accelerated on many-core graphics processing units (GPUs)
   using the CUDA C programming language. To balance memory usage,
   computing efficiency and accuracy on multi- and many-core processors, a
   mixture of single and double precision floating-point operations is
   implemented. The most important data like conservative flow variables
   are handled with double-precision dynamic arrays, whilst all the other
   variables/arrays like fluxes, residual and source terms are treated in
   single precision. Several benchmark test cases including water-air shock
   tubes, one-dimensional liquid cavitation tube, dam break, 2D cylindrical
   underwater explosion near a planar rigid wall, 3D spherical explosion in
   a rigid cylindrical container and water entry of a 3D rigid flat plate
   have been calculated using the present approach. The obtained results
   agree well with experiments, exact solutions and other independent
   numerical computations. This demonstrates the capability of the present
   approach to deal with not only violent free-surface impact problems but
   also hull cavitation associated with underwater explosions. Performance
   analysis reveals that the running time cost of numerical simulations is
   dramatically reduced by use of GPUs with much less consumption of
   electrical energy than on the CPU. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>OCT 5 2015</date><author>Ma, Z. H.
   Causon, D. M.
   Qian, L.
   Gu, H. B.
   Mingham, C. G.
   Ferrer, P. Martinez</author></paper><paper><title>NMF-mGPU: non-negative matrix factorization on multi-GPU systems</title><abstract>Background: In the last few years, the Non-negative Matrix Factorization
   (NMF) technique has gained a great interest among the Bioinformatics
   community, since it is able to extract interpretable parts from
   high-dimensional datasets. However, the computing time required to
   process large data matrices may become impractical, even for a parallel
   application running on a multiprocessors cluster. In this paper, we
   present NMF-mGPU, an efficient and easy-to-use implementation of the NMF
   algorithm that takes advantage of the high computing performance
   delivered by Graphics-Processing Units (GPUs). Driven by the
   ever-growing demands from the video-games industry, graphics cards
   usually provided in PCs and laptops have evolved from simple
   graphics-drawing platforms into high-performance programmable systems
   that can be used as coprocessors for linear-algebra operations. However,
   these devices may have a limited amount of on-board memory, which is not
   considered by other NMF implementations on GPU.Results: NMF-mGPU is
   based on CUDA (Compute Unified Device Architecture), the NVIDIA's
   framework for GPU computing. On devices with low memory available, large
   input matrices are blockwise transferred from the system's main memory
   to the GPU's memory, and processed accordingly. In addition, NMF-mGPU
   has been explicitly optimized for the different CUDA architectures.
   Finally, platforms with multiple GPUs can be synchronized through MPI
   (Message Passing Interface). In a four-GPU system, this implementation
   is about 120 times faster than a single conventional processor, and more
   than four times faster than a single GPU device (i.e., a super-linear
   speedup).Conclusions: Applications of GPUs in Bioinformatics are getting
   more and more attention due to their outstanding performance when
   compared to traditional processors. In addition, their relatively low
   price represents a highly cost-effective alternative to conventional
   clusters. In life sciences, this results in an excellent opportunity to
   facilitate the daily work of bioinformaticians that are trying to
   extract biological meaning out of hundreds of gigabytes of experimental
   information. NMF-mGPU can be used "out of the box" by researchers with
   little or no expertise in GPU programming in a variety of platforms,
   such as PCs, laptops, or high-end GPU clusters. NMF-mGPU is freely
   available at https://github.com/bioinfo-cnb/bionmf-gpu.</abstract><date>FEB 13 2015</date><author>Mejia-Roa, Edgardo
   Tabas-Madrid, Daniel
   Setoain, Javier
   Garcia, Carlos
   Tirado, Francisco
   Pascual-Montano, Alberto</author></paper><paper><title>A fine-grained block ILU scheme on regular structures for GPGPUs</title><abstract>Iterative methods based on block incomplete LU (BILU) factorization are
   considered highly effective for solving large-scale block-sparse linear
   systems resulting from coupled PDE systems with n equations. However,
   efforts on porting implicit PDE solvers to massively parallel
   shared-memory heterogeneous architectures, such as general-purpose
   graphics processing units (GPGPUs), have largely avoided BILU, leaving
   their enormous performance potential unfulfilled in many applications
   where the use of implicit schemes and BILU-type preconditioners/solvers
   is highly preferred. Indeed, strong inherent data dependency and high
   memory bandwidth demanded by block matrix operations render naive
   adoptions of existing sequential BILU algorithms extremely inefficient
   on GPGPUs. In this study, we present a fine-grained BILU (FGBILU) scheme
   which is particularly effective on GPGPUs. A straightforward one-sweep
   wavefront ordering is employed to resolve data dependency. Granularity
   is substantially refined as block matrix operations are carried out in a
   true element-wise approach. Particularly, the inversion of diagonal
   blocks, a well-known bottleneck, is accomplished by a parallel in-place
   Gauss Jordan elimination. As a result, FGBILU is able to offer
   low-overhead concurrent computation at 0(n(2)N(2)) scale on a 3D PDE
   domain with a linear scale of N. FGBILU has been implemented with both
   OpenACC and CUDA and tested as a block-sparse linear solver on a
   structured 3D grid. While FGBILU remains mathematically identical to
   sequential global BILU, numerical experiments confirm its exceptional
   performance on an Nvidia GPGPU. (c) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>SEP 22 2015</date><author>Luo, Lixiang
   Edwards, Jack R.
   Luo, Hong
   Mueller, Frank</author></paper><paper><title>New insights into the regulation of innate immunity by caspase-8</title><abstract>Caspase-8 is required for extrinsic apoptosis, but is also central for
   preventing a pro-inflammatory receptor interacting protein kinase (RIPK)
   3-mixed lineage kinase domain-like (MLKL)-dependent cell death pathway
   termed necroptosis. Despite these critical cellular functions, the
   impact of capase-8 deletion in the myeloid cell lineage, which forms the
   basis for innate immune responses, has remained unclear. In a recent
   article in Arthritis Research &amp; Therapy, Cuda et al. report that myeloid
   cell-restricted caspase-8 loss leads to a very mild RIPK3-dependent
   inflammatory phenotype. The presented results suggest that inflammation
   does not arise exclusively because of RIPK3-mediated necroptotic death
   but that, in the absence of caspase-8, RIPK1 and RIPK3 enhance
   microbiome-driven Toll-like receptor-induced pro-inflammatory cytokine
   production.</abstract><date>JAN 13 2016</date><author>Sagulenko, Vitaliya
   Lawlor, Kate E.
   Vince, James E.</author></paper><paper><title>A graphics-card implementation of Monte-Carlo simulations for cosmic-ray
   transport</title><abstract>A graphics card implementation of a test-particle simulation code is
   presented that is based on the CUDA extension of the C/C++ programming
   language. The original CPU version has been developed for the
   calculation of cosmic-ray diffusion coefficients in artificial
   Kolmogorov-type turbulence. In the new implementation, the magnetic
   turbulence generation, which is the most time-consuming part, is
   separated from the particle transport and is performed on a graphics
   card. In this article, the modification of the basic approach of
   integrating test particle trajectories to employ the SIMD (single
   instruction, multiple data) model is presented and verified. The
   efficiency of the new code is tested and several language-specific
   accelerating factors are discussed. For the example of isotropic
   magnetostatic turbulence, sample results are shown and a comparison to
   the results of the CPU implementation is performed. (C) 2015 Elsevier
   B.V. All rights reserved.</abstract><date>MAY 2016</date><author>Tautz, R. C.</author></paper><paper><title>Accelerating the Pace of Protein Functional Annotation With Intel Xeon
   Phi Coprocessors</title><abstract>Intel Xeon Phi is a new addition to the family of powerful parallel
   accelerators. The range of its potential applications in computationally
   driven research is broad; however, at present, the repository of
   scientific codes is still relatively limited. In this study, we describe
   the development and benchmarking of a parallel version of eFindSite, a
   structural bioinformatics algorithm for the prediction of ligand-binding
   sites in proteins. Implemented for the Intel Xeon Phi platform, the
   parallelization of the structure alignment portion of eFindSite using
   pragma-based OpenMP brings about the desired performance improvements,
   which scale well with the number of computing cores. Compared to a
   serial version, the parallel code runs 11.8 and 10.1 times faster on the
   CPU and the coprocessor, respectively; when both resources are utilized
   simultaneously, the speedup is 17.6. For example, ligand-binding
   predictions for 501 benchmarking proteins are completed in 2.1 hours on
   a single Stampede node equipped with the Intel Xeon Phi card compared to
   3.1 hours without the accelerator and 36.8 hours required by a serial
   version. In addition to the satisfactory parallel performance, porting
   existing scientific codes to the Intel Xeon Phi architecture is
   relatively straightforward with a short development time due to the
   support of common parallel programming models by the coprocessor. The
   parallel version of eFindSite is freely available to the academic
   community at www.brylinski.org/efindsite.</abstract><date>JUN 2015</date><author>Feinstein, Wei P.
   Moreno, Juana
   Jarrell, Mark
   Brylinski, Michal</author></paper><paper><title>A patient-centered symptom monitoring and reporting system for children
   and young adults with cancer (SyMon-SAYS)</title><abstract>BackgroundThis study evaluated the feasibility of implementing a
   patient-centered, technology-based symptom monitoring and reporting
   system (SyMon-SAYS) in pediatric oncology clinics using fatigue as a
   prototypic symptom. Timely identification of symptoms related to
   multi-modal therapy for children with cancer is fundamental to the
   overall success of cancer treatment. SyMon-SAYS was developed to address
   this need.ProcedurePatients with a cancer diagnosis, ages 7-21 years,
   currently on treatment, or off treatment within 6 months, were eligible.
   Patients/parents completed weekly fatigue assessments over 8 weeks via
   the internet or interactive voice response (IVR) by phone. Alert emails
   were generated when pre-defined fatigue score thresholds were met, and
   fatigue reports were forwarded to clinicians accordingly. Clinicians and
   parents/patients received cumulative graphic reports of fatigue scores
   prior to clinic visits at 4 and 8 weeks post-baseline to facilitate
   discussion. Parents/patients completed an exit survey at their last
   visit.ResultsFifty-seven patients/parents completed the study. The
   majority of patients (93%) and parents (78%) felt it was very/extremely
   easy to complete SyMon-SAYS; 95% of parents were satisfied with the
   system; 60% reported it helped deal with their child's fatigue; 70%
   reported that clinicians didn't discuss fatigue with them; 81% would be
   willing to use SyMon-SAYS to manage fatigue and other symptoms.
   Clinicians reported insufficient time to review reports, yet 71% were
   willing to receive the report on a monthly basis.ConclusionSyMon-SAYS is
   feasible and acceptable to patients and parents. Future efforts should
   focus on better integrating the system into the clinical workflow to
   improve clinicians' acceptance. Pediatr Blood Cancer 2015;62:1813-1818.
   (c) 2015 Wiley Periodicals, Inc.</abstract><date>OCT 2015</date><author>Lai, Jin-Shei
   Yount, Susan
   Beaumont, Jennifer L.
   Cella, David
   Toia, Jacquie
   Goldman, Stewart</author></paper><paper><title>CBIR Based Testing Oracles: An Experimental Evaluation of Similarity
   Functions</title><abstract>Content-Based Image Retrieval (CBIR) systems constitute an innovative
   approach to store, to compare and to query images in a database. Visual
   aspects such as color, texture or shape are used to perform such
   operations. Recently, CBIR concepts were applied to build testing
   oracles for image processing programs, where test verdicts
   (approval/disapproval) are based on similarity measures between images
   produced by the program and reference images. However, the results of a
   CBIR system may vary depending on the components employed in the system
   (feature extractors and similarity functions), and few studies assessing
   this influence have been found in the literature. Our aim is to present
   an empirical analysis of ten similarity functions in CBIR systems within
   the context of software testing with graphic outputs. A case study with
   images obtained from a computer-aided diagnosis system in mammography
   indicated some variability among image test verdicts
   (approval/disapproval) according to the similarity function choice. The
   case study also indicates the existence of some clusters of similarity
   functions with high correlation coefficients.</abstract><date>OCT 2015</date><author>Nunes, Fatima L. S.
   Delamaro, Marcio Eduardo
   Goncalves, Vagner Mendocna
   Lauretto, Marcelo De Souza</author></paper><paper><title>Topological subtleties for molecular movies</title><abstract>Synchronous movies permit visual analysis of shape perturbation during
   molecular simulations. The molecule is conceptualized as a knot and
   modeled as a spline curve. As the molecule writhes, the graphics
   approximation in each frame should display an ambient isotopic image of
   the perturbing spline. These graphics approximations raise subtleties
   for correctly rendering the embedding. A cautionary example was
   discovered through visualization experiments and the relevant
   characteristics are formally proved. (C) 2015 Elsevier B.V. All rights
   reserved.</abstract><date>JUN 15 2015</date><author>Li, J.
   Peters, T. J.
   Marinelli, K.
   Kovalev, E.
   Jordan, K. E.</author></paper><paper><title>MetaMapR: pathway independent metabolomic network analysis incorporating
   unknowns</title><abstract>Metabolic network mapping is a widely used approach for integration of
   metabolomic experimental results with biological domain knowledge.
   However, current approaches can be limited by biochemical domain or
   pathway knowledge which results in sparse disconnected graphs for real
   world metabolomic experiments. MetaMapR integrates enzymatic
   transformations with metabolite structural similarity, mass spectral
   similarity and empirical associations to generate richly connected
   metabolic networks. This open source, web-based or desktop software,
   written in the R programming language, leverages KEGG and PubChem
   databases to derive associations between metabolites even in cases where
   biochemical domain or molecular annotations are unknown. Network
   calculation is enhanced through an interface to the Chemical Translation
   System, which allows metabolite identifier translation between &gt;200
   common biochemical databases. Analysis results are presented as
   interactive visualizations or can be exported as high-quality graphics
   and numerical tables which can be imported into common network analysis
   and visualization tools.</abstract><date>AUG 15 2015</date><author>Grapov, Dmitry
   Wanichthanarak, Kwanjeera
   Fiehn, Oliver</author></paper><paper><title>Modeling Luminance Perception at Absolute Threshold</title><abstract>When human luminance perception operates close to its absolute
   threshold, i. e., the lowest perceivable absolute values, appearance
   changes substantially compared to common photopic or scotopic vision. In
   particular, most observers report perceiving temporally-varying noise.
   Two reasons are physiologically plausible; quantum noise (due to the low
   absolute number of photons) and spontaneous photochemical reactions.
   Previously, static noise with a normal distribution and no account for
   absolute values was combined with blue hue shift and blur to simulate
   scotopic appearance on a photopic display for movies and interactive
   applications (e.g., games). We present a computational model to
   reproduce the specific distribution and dynamics of scotopic noise for
   specific absolute values. It automatically introduces a
   perceptually-calibrated amount of noise for a specific luminance level
   and supports animated imagery. Our simulation runs in milliseconds at HD
   resolution using graphics hardware and favorably compares to simpler
   alternatives in a perceptual experiment.</abstract><date>JUL 2015</date><author>Kellnhofer, Petr
   Ritschel, Tobias
   Myszkowski, Karol
   Eisemann, Elmar
   Seidel, Hans-Peter</author></paper><paper><title>Towards Online Visualization and Interactive Monitoring of Real-Time CFD
   Simulations on Commodity Hardware</title><abstract>Real-time rendering in the realm of computational fluid dynamics (CFD)
   in particular and scientific high performance computing (HPC) in general
   is a comparably young field of research, as the complexity of most
   problems with practical relevance is too high for a real-time numerical
   simulation. However, recent advances in HPC and the development of very
   efficient numerical techniques allow running first optimized numerical
   simulations in or near real-time, which in return requires integrated
   and optimized visualization techniques that do not affect performance.
   In this contribution, we present concepts, implementation details and
   several application examples of a minimally-invasive, efficient
   visualization tool for the interactive monitoring of 2D and 3D turbulent
   flow simulations on commodity hardware. The numerical simulations are
   conducted with elbe, an efficient lattice Boltzmann environment based on
   NVIDIA CUDA (Compute Unified Device Architecture), which provides
   optimized numerical kernels for 2D and 3D computational fluid dynamics
   with fluid-structure interactions and turbulence.</abstract><date>SEP 2015</date><author>Koliha, Nils
   Janssen, Christian F.
   Rung, Thomas</author></paper><paper><title>Latent uncertainties of the precalculated track Monte Carlo method</title><abstract>Purpose: While significant progress has been made in speeding up Monte
   Carlo (MC) dose calculation methods, they remain too time-consuming for
   the purpose of inverse planning. To achieve clinically usable
   calculation speeds, a precalculated Monte Carlo (PMC) algorithm for
   proton and electron transport was developed to run on graphics
   processing units (GPUs). The algorithm utilizes pregenerated particle
   track data from conventional MC codes for different materials such as
   water, bone, and lung to produce dose distributions in voxelized
   phantoms. While PMC methods have been described in the past, an explicit
   quantification of the latent uncertainty arising from the limited number
   of unique tracks in the pregenerated track bank is missing from the
   paper. With a proper uncertainty analysis, an optimal number of tracks
   in the pregenerated track bank can be selected for a desired dose
   calculation uncertainty.Methods: Particle tracks were pregenerated for
   electrons and protons using EGSnrc and GEANT4 and saved in a database.
   The PMC algorithm for track selection, rotation, and transport was
   implemented on the Compute Unified Device Architecture (CUDA) 4.0
   programming framework. PMC dose distributions were calculated in a
   variety of media and compared to benchmark dose distributions simulated
   from the corresponding general-purpose MC codes in the same conditions.
   A latent uncertainty metric was defined and analysis was performed by
   varying the pregenerated track bank size and the number of simulated
   primary particle histories and comparing dose values to a "ground truth"
   benchmark dose distribution calculated to 0.04% average uncertainty in
   voxels with dose greater than 20% of D-max. Efficiency metrics were
   calculated against benchmark MC codes on a single CPU core with no
   variance reduction.Results: Dose distributions generated using PMC and
   benchmark MC codes were compared and found to be within 2% of each other
   in voxels with dose values greater than 20% of the maximum dose. In
   proton calculations, a small (&lt;= 1 mm) distance-to-agreement error was
   observed at the Bragg peak. Latent uncertainty was characterized for
   electrons and found to follow a Poisson distribution with the number of
   unique tracks per energy. A track bank of 12 energies and 60 000 unique
   tracks per pregenerated energy in water had a size of 2.4 GB and
   achieved a latent uncertainty of approximately 1% at an optimal
   efficiency gain over DOSXYZnrc. Larger track banks produced a lower
   latent uncertainty at the cost of increased memory consumption. Using an
   NVIDIA GTX 590, efficiency analysis showed a 807x efficiency increase
   over DOSXYZnrc for 16 MeV electrons in water and 508x for 16 MeV
   electrons in bone.Conclusions: The PMC method can calculate dose
   distributions for electrons and protons to a statistical uncertainty of
   1% with a large efficiency gain over conventional MC codes. Before
   performing clinical dose calculations, models to calculate dose
   contributions from uncharged particles must be implemented. Following
   the successful implementation of these models, the PMC method will be
   evaluated as a candidate for inverse planning of modulated electron
   radiation therapy and scanned proton beams. (C) 2015 American
   Association of Physicists in Medicine.</abstract><date>JAN 2015</date><author>Renaud, Marc-Andre
   Roberge, David
   Seuntjens, Jan</author></paper><paper><title>AQUAgpusph, a new free 3D SPH solver accelerated with OpenCL</title><abstract>In this paper, AQUAgpusph, a new free Smoothed Particle Hydrodynamics
   (SPH) software accelerated with OpenCL, is described. The main
   differences and progress with respect to other existing alternatives are
   considered. These are the use of the Open Computing Language (OpenCL)
   framework instead of the Compute Unified Device Architecture (CUDA), the
   implementation of the most popular boundary conditions, the easy
   customization of the code to different problems, the extensibility with
   regard to Python scripts, and the runtime output which allows the
   tracking of simulations in real time, or a higher frequency in saving
   some results without a significant performance lost. These modifications
   are shown to improve the solver speed, the results quality, and allow
   for a wider areas of application. AQUAgpusph has been designed trying to
   provide researchers and engineers with a valuable tool to test and apply
   the SPH method. Three practical applications are discussed in detail.
   The evolution of a dam break is used to quantify and compare the
   computational performance and modeling accuracy with the most popular
   SPH Graphics Processing Unit (CPU) accelerated alternatives. The
   dynamics of a coupled system, a Tuned Liquid Damper (TLD), is discussed
   in order to show the integration capabilities of the solver with
   external dynamics. Finally, the sloshing flow inside a nuclear reactor
   is simulated in order to show the capabilities of the solver to treat
   3-D problems with complex geometries and of industrial interest.Program
   summaryProgram title: AQUAgpusph 1.5Catalogue identifier:
   AEVG_v1_0Program summary URL:
   http://cpc.cs.qub.ac.uk/summaries/AEVG_v1_0.htmlProgram obtainable from:
   CPC Program Library, Queen's University, Belfast, N. IrelandLicensing
   provisions: GNU General Public License, version 3No. of lines in
   distributed program, including test data, etc.: 1702666No. of bytes in
   distributed program, including test data, etc.: 75117178Distribution
   format: tar.gzProgramming language: C++, OpenCL, Python.Computer: Linux
   based computers with OpenCL support.Operating system: Linux.Has the code
   been vectorized or parallelized?: Code is parallelized with
   OpenCL.Classification: 1.5.Nature of problem: Complex geometry or
   heavily fragmented free surface fluid dynamics problems where mesh based
   method cannot be successfully applied.Solution method: SPH is a meshless
   method where the fluid domain is discretized as a set of fluid
   particles. The fields in the fluid domain are smoothed using a kernel
   function, that allows to develop differential operators from the flow
   field values in scattered sets of particles.Running time: Using an AMD
   HD-7970 graphic device 2 x 10(5) time steps of a 2-D simulation, with
   10(5) particles and 8 x 10(2) neighs per particle, is requiring around 9
   h of computation. A more detailed performance analysis will be carried
   out in the practical application section herein. (C) 2015 Elsevier B.V.
   All rights reserved.</abstract><date>JUL 2015</date><author>Cercos-Pita, J. L.</author></paper><paper><title>Near Video-Rate Optical Coherence Elastography by Acceleration With a
   Graphics Processing Unit</title><abstract>We present a graphics processing unit (GPU)-accelerated optical
   coherence elastography (OCE) system capable of generating strain images
   (elastograms) of soft tissue at near video-rates. The system implements
   phase-sensitive compression OCE using a pipeline of GPU kernel functions
   to enable a highly parallel implementation of OCE processing using the
   OpenCL framework. Developed on a commercial-grade GPU and desktop
   computer, the system achieves a processing rate of 21 elastograms per
   second at an image size of 960 x 400 pixels, enabling high-rate
   visualization during acquisition. The system is demonstrated on both
   tissue-simulating phantoms and fresh ex vivo mouse muscle. To the best
   of our knowledge, this is the first implementation of near video-rate
   OCE and the fastest reported OCE processing rate, enabling, for the
   first time, a system capable of computing and displaying OCE elastograms
   interactively during acquisition. This advance provides new
   opportunities for medical imaging of soft tissue stiffness using optical
   methods.</abstract><date>AUG 15 2015</date><author>Kirk, Rodney W.
   Kennedy, Brendan F.
   Sampson, David D.
   McLaughlin, Robert A.</author></paper><paper><title>Design, synthesis and molecular docking analysis of some novel
   7-[(quinolin-6-yl)methyl] purines as potential c-Met inhibitors</title><abstract>HGF/c-Met signaling pathway has come into the spotlight as a promising
   therapeutic target for inhibiting tumor growth and has become one of the
   leading molecular targets in cancer. Various strategies are currently in
   development to disrupt the HGF-Met signal transduction pathway, in which
   small molecular inhibitors have been a particularly active field. On the
   basis of the structures of two c-Met inhibitors, PF-04217903 and
   JNJ-38877605, some novel 7-[(quinolin-6-yl)methyl] purines (4a-4e) were
   rationally designed on the principle of bioisosterism strategy. These
   compounds were synthesized and evaluated as novel c-Met inhibitors.
   Molecular docking experiments analyzed the results and explained the
   molecular mechanism of eminent activities to c-Met. The results showed
   that all the title compounds were active against c-Met enzyme to some
   extent. Though these compounds did not demonstrate inhibition as we
   expected, this study provided important information for building
   diversification of chemical library and molecular docking experiment
   supplied the basis for further research works.Some novel
   7-[(quinolin-6-yl)methyl] purines as potential c-Met inhibitors have
   been designed and prepared. Their synthesis and spectral
   characterization are reported here.[GRAPHICS].</abstract><date>AUG 2015</date><author>Ye, Lianbao
   Wu, Jie
   Yang, Jiebo
   Chen, Weiqiang
   Luo, Yan
   Zhang, Yanmei</author></paper><paper><title>A parametric reflectance approximation for rendering Japanese
   lacquerware and Maki-e</title><abstract>This paper proposes a reflectance distribution model that is able to
   express complex reflections from materials such as Japanese lacquerware
   and Maki-e to produce highly realistic computer graphics. Our method
   improves the Ward model for materials with anisotropic reflection, and
   uses the approximate coefficients calculated from measured reflectance
   distribution data to reconstruct the actual reflectance. Our research
   derives a Gaussian distribution summation method and a modified Fresnel
   reflectance approximation function from measured reflectance data. Our
   Fresnel reflectance approximation function adds parameters to Schlick's
   approximation function to control the whole curve and extinction
   component, and is thus closer to the actual Fresnel reflectance. Based
   on these functions, a decision process is developed for the
   approximation coefficients, and this makes it possible to easily
   reconstruct the actual reflectance. The reconstructed reflectance
   distributions obtained using these coefficients are compared with
   measured reflectance data using the L2 norm of the difference. In
   addition, reconstructed Fresnel reflection effects are compared with
   those from actual sample materials. Using these coefficient values, this
   paper shows a sample of simulated Maki-e images under two different
   lighting environments.</abstract><date>NOV 2015</date><author>Yamaguchi, Satoshi</author></paper><paper><title>BMRF-Net: a software tool for identification of protein interaction
   subnetworks by a bagging Markov random field-based method</title><abstract>Identification of protein interaction subnetworks is an important step
   to help us understand complex molecular mechanisms in cancer. In this
   paper, we develop a BMRF-Net package, implemented in Java and C++, to
   identify protein interaction subnetworks based on a bagging Markov
   random field (BMRF) framework. By integrating gene expression data and
   protein-protein interaction data, this software tool can be used to
   identify biologically meaningful subnetworks. A user friendly graphic
   user interface is developed as a Cytoscape plugin for the BMRF-Net
   software to deal with the input/output interface. The detailed structure
   of the identified networks can be visualized in Cytoscape conveniently.
   The BMRF-Net package has been applied to breast cancer data to identify
   significant subnetworks related to breast cancer recurrence.</abstract><date>JUL 15 2015</date><author>Shi, Xu
   Barnes, Robert O.
   Chen, Li
   Shajahan-Haq, Ayesha N.
   Hilakivi-Clarke, Leena
   Clarke, Robert
   Wang, Yue
   Xuan, Jianhua</author></paper><paper><title>A high performance crashworthiness simulation system based on GPU</title><abstract>Crashworthiness simulation system is one of the key computer-aided
   engineering (CAE) tools for the automobile industry and implies two
   potential conflicting requirements: accuracy and efficiency. A parallel
   crashworthiness simulation system based on graphics processing unit
   (GPU) architecture and the explicit finite element (FE) method is
   developed in this work. Implementation details with compute unified
   device architecture (CUDA) are considered. The entire parallel
   simulation system involves a parallel hierarchy-territory
   contact-searching algorithm (HITA) and a parallel penalty contact force
   calculation algorithm. Three basic GPU-based parallel strategies are
   suggested to meet the natural parallelism of the explicit FE algorithm.
   Two free GPU-based numerical calculation libraries, cuBLAS and Thrust,
   are introduced to decrease the difficulty of programming. Furthermore, a
   mixed array and a thread map to element strategy are proposed to improve
   the performance of the test pairs searching. The outer loop of the
   nested loop through the mixed array is unrolled to realize parallel
   searching. An efficient storage strategy based on data sorting is
   presented to realize data transfer between different hierarchies with
   coalesced access during the contact pairs searching. A thread map to
   element pattern is implemented to calculate the penetrations and the
   penetration forces; a double float atomic operation is used to scatter
   contact forces. The simulation results of the three different models
   based on the Intel Core i7-930 and the NVIDIA GeForce GTX 580
   demonstrate the precision and efficiency of this developed parallel
   crashworthiness simulation system. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>AUG 2015</date><author>Cai, Yong
   Wang, Guoping
   Li, Guangyao
   Wang, Hu</author></paper><paper><title>On the possibility of producing true real-time retinal cross-sectional
   images using a graphics processing unit enhanced master-slave optical
   coherence tomography system</title><abstract>In a previous report, we demonstrated master-slave optical coherence
   tomography (MS-OCT), an OCT method that does not need resampling of data
   and can be used to deliver en face images from several depths
   simultaneously. In a separate report, we have also demonstrated MS-OCT's
   capability of producing cross-sectional images of a quality similar to
   those provided by the traditional Fourier domain (FD) OCT technique, but
   at a much slower rate. Here, we demonstrate that by taking advantage of
   the parallel processing capabilities offered by the MS-OCT method,
   cross- sectional OCT images of the human retina can be produced in real
   time. We analyze the conditions that ensure a true real-time B-scan
   imaging operation and demonstrate in vivo real-time images from human
   fovea and the optic nerve, with resolution and sensitivity comparable to
   those produced using the traditional FD-based method, however, without
   the need of data resampling. (C) The Authors. Published by SPIE</abstract><date>JUL 2015</date><author>Bradu, Adrian
   Kapinchev, Konstantin
   Barnes, Frederick
   Podoleanu, Adrian</author></paper><paper><title>Using Presentation Software To Flip an Undergraduate Analytical
   Chemistry Course</title><abstract>An undergraduate analytical chemistry course has been adapted to a
   flipped course format. Course content was provided by video clips, text,
   graphics, audio, and simple animations organized as concept maps using
   the cloud-based presentation platform, Prezi. The advantages of using
   Prezi to present course content in a flipped course format are
   discussed. Results of an American Chemical Society analytical chemistry
   examination were encouraging. Results of pre- and postsurveys of student
   perception of their learning are summarized.</abstract><date>SEP 2015</date><author>Fitzgerald, Neil
   Li, Luisa</author></paper><paper><title>HapFACS 3.0: FACS-Based Facial Expression Generator for 3D Speaking
   Virtual Characters</title><abstract>With the growing number of researchers interested in modeling the inner
   workings of affective social intelligence, the need for tools to easily
   model its associated expressions has emerged. The goal of this article
   is two-fold: 1) we describe HapFACS, a free software and API that we
   developed to provide the affective computing community with a resource
   that produces static and dynamic facial expressions for
   three-dimensional speaking characters; and 2) we discuss results of
   multiple experiments that we conducted in order to scientifically
   validate our facial expressions and head animations in terms of the
   widely accepted Facial Action Coding System (FACS) standard, and its
   Action Units (AU). The result is that users, without any 3D-modeling nor
   computer graphics expertise, can animate speaking virtual characters
   with FACS-based realistic facial expression animations, and embed these
   expressive characters in their own application(s). The HapFACS software
   and API can also be used for generating repertoires of realistic
   FACS-validated facial expressions, useful for testing emotion expression
   generation theories.</abstract><date>OCT-DEC 2015</date><author>Amini, Reza
   Lisetti, Christine
   Ruiz, Guido</author></paper><paper><title>Performance Evaluation of an OpenCL Implementation of the Lattice
   Boltzmann Method on the Intel Xeon Phi</title><abstract>A portable OpenCL implementation of the lattice Boltzmann method
   targeting emerging many-core architectures is described. The main
   purpose of this work is to evaluate and compare the performance of this
   code on three mainstream hardware architectures available today, namely
   an Intel CPU, an Nvidia GPU, and the Intel Xeon Phi. Because of the
   similarities between OpenCL and CUDA, we chose to follow some of the
   strategies devised to implement efficient lattice Boltzmann solvers on
   Nvidia GPU, while remaining as generic as possible. Being fairly
   configurable, this program makes possible to ascertain the best options
   for each hardware platforms. The achieved performance is quite
   satisfactory for both the CPU and the GPU. For the Xeon Phi however, the
   results are below expectations. Nevertheless, comparison with data from
   the literature shows that on this architecture the code seems
   memory-bound.</abstract><date>SEP 2015</date><author>Obrecht, Christian
   Tourancheau, Bernard
   Kuznik, Frederic</author></paper><paper><title>Fast density-based clustering through dataset partition using graphics
   processing units</title><abstract>Graphics processing units (GPUs) have been utilized to improve the
   processing speed of many conventional data mining algorithms. DBSCAN, a
   popular clustering algorithm that has been often used in practice, was
   extended to execute on a GPU. However, existing GPU-based DBSCAN
   extensions still have impediments in that the distances from all objects
   need to be repeatedly computed to find the neighbor objects and the
   objects and intermediate clustering results are stored in costly
   off-chip memory of the GPU. This paper proposes CudaSCAN, a novel
   algorithm that improves the efficiency of DBSCAN by making better use of
   the GPU. CudaSCAN consists of three phases: (1) partitioning the entire
   dataset into sub-regions of size of an integer multiple of the on-chip
   shared memory size in the GPU; (2) local clustering within sub-regions
   in parallel; and (3) merging the local clustering results. CudaSCAN
   allows an overlap between sub-regions to ensure independent, parallel
   local clustering in each sub-region, which in turn enables for objects
   and/or intermediate results to be stored in on-chip shared memory that
   has an access cost a few hundred times cheaper than that of off-chip
   global memory. The independence also enables for merging to be
   parallelized. This paper proves the correctness of CudaSCAN, and
   according to our extensive experiments, CudaSCAN outperforms
   CUDA-DClust, a previous GPU-based DBSCAN extension, by up to 163.6
   times. (C) 2014 Elsevier Inc. All rights reserved.</abstract><date>JUL 1 2015</date><author>Loh, Woong-Kee
   Yu, Hwanjo</author></paper><paper><title>Parallel genetic algorithm based automatic path planning for crane
   lifting in complex environments</title><abstract>Heavy lifting is a common and important task in industrial plants. It is
   conducted frequently during the time of plant construction, maintenance
   shutdown and new equipment installation. To find a safe and cost
   effective way of lifting, a team works for weeks or even months doing
   site investigation, planning and evaluations. This paper considers the
   lifting path planning problem for terrain cranes in complex
   environments. The lifting path planning problem takes inputs such as the
   plant environment, crane mechanical data, crane position, start and end
   lifting configurations to generate the optimal lifting path by
   evaluating costs and safety risks, We formulate the crane lifting path
   planning as a multi-objective nonlinear integer optimization problem
   with implicit constraints. It aims to optimize the energy cost, time
   cost and human operation conformity of the lifting path under
   constraints of collision avoidance and operational limitations. To solve
   the optimization problem, we design a Master-Slave Parallel Genetic
   Algorithm and implement the algorithm on Graphics Processing Units using
   CUDA programming. In order to handle complex plants, we propose a
   collision detection strategy using hybrid configuration spaces based on
   an image-based collision detection algorithm. The results show that the
   method can efficiently generate high quality lifting paths in complex
   environments. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>FEB 2016</date><author>Cai, Panpan
   Cai, Yiyu
   Chandrasekaran, Indhumathi
   Zheng, Jianmin</author></paper><paper><title>GPU-accelerated 3D reconstruction of porous media using multiple-point
   statistics</title><abstract>It is very important for the study of predicting fluid transport
   properties or mechanisms of fluid flow in porous media that the
   characteristics of porous media can be extracted in relatively smaller
   scales and then are copied in a larger or even arbitrary region to
   reconstruct virtual 3D porous media that have similar structures with
   the real porous media. One of multiple-point statistics (MPS) method,
   the single normal equation simulation algorithm (SNESIM), has been
   widely used in reconstructing 3D porous media recently. However, owing
   to its large CPU cost and rigid memory demand, the application of SNESIM
   has been limited in some cases. To overcome this disadvantage,
   parallelization of SNESIM is performed on the compute unified device
   architecture (CUDA) kernels in the graphic processing unit (GPU) to
   reconstruct each node on simulation grids, combined with choosing the
   optimal size of data templates based on the entropy calculation towards
   the training image (TI) to acquire high-quality reconstruction with a
   low CPU cost; meanwhile, the integration of hard data and soft data is
   also included in the processing of CUDA kernels to improve the accuracy.
   Representative elementary volumes (REVs) for porosity, variogram, and
   entropy are analyzed to guarantee that the scale of observation is large
   enough and parameters of concern are constant. This parallel GPU-version
   3D porous media reconstruction only requires relatively small size
   memory and benefits from the tremendous calculating power given by CUDA
   kernels to shorten the CPU time, showing its high efficiency for the
   reconstruction of porous media.</abstract><date>FEB 2015</date><author>Zhang, Ting
   Du, Yi
   Huang, Tao
   Li, Xue</author></paper><paper><title>Parallel Hyperspectral Compressive Sensing Method on GPU</title><abstract>Remote hyperspectral sensors collect large amounts of data per flight
   usually with low spatial resolution. It is known that the bandwidth
   connection between the satellite/airborne platform and the ground
   station is reduced, thus a compression onboard method is desirable to
   reduce the amount of data to be transmitted.This paper presents a
   parallel implementation of an compressive sensing method, called
   parallel hyperspectral coded aperture (P-HYCA), for graphics processing
   units (GPU) using the compute unified device architecture (CUDA). This
   method takes into account two main properties of hyperspectral dataset,
   namely the high correlation existing among the spectral bands and the
   generally low number of endmembers needed to explain the data, which
   largely reduces the number of measurements necessary to correctly
   reconstruct the original data.Experimental results conducted using
   synthetic and real hyperspectral datasets on two different GPU
   architectures by NVIDIA: GeForce GTX 590 and GeForce GTX TITAN, reveal
   that the use of GPUs can provide real-time compressive sensing
   performance. The achieved speedup is up to 20 times when compared with
   the processing time of HYCA running on one core of the Intel i7-2600 CPU
   (3.4GHz), with 16 Gbyte memory.</abstract><date>2015</date><author>Bernabe, Sergio
   Martin, Gabriel
   Nascimento, Jose M. P.</author></paper><paper><title>Big Data Conditional Business Rule Calculations in Multidimensional
   In-GPU-Memory OLAP Databases</title><abstract>The ability to handle Big Data is one of the key requirements of today's
   database systems. Calculating conditional business rules in OLAP
   scenarios means creating virtual cube cells out of previously stored
   database entries and precalculated aggregates based on a given
   condition. It requires passing several steps such as source data
   filtering, aggregation and conditional analysis, each involving storing
   intermediate results which can easily get very large. Therefore,
   algorithms allowing to stream data instead of calculating the results in
   one step are essential to process big sets of data without exceeding the
   hardware limitations. This paper shows how the evaluation of conditional
   business rules can be accelerated using GPUs and massively data-parallel
   streaming-algorithms written in CUDA.</abstract><date>2015</date><author>Haberstroh, Alexander
   Strohm, Peter</author></paper><paper><title>Acceleration of the OpenFOAM-based MHD solver using graphics processing
   units</title><abstract>The pressure-implicit with splitting of operators (PISO)
   magnetohydrodynamics MHD solver of the couple of Navier-Stokes equations
   and Maxwell equations was implemented on Kepler-class graphics
   processing units (GPUs) using the CUDA technology. The solver is
   developed on open source code OpenFOAM based on consistent and
   conservative scheme which is suitable for simulating MHD flow under
   strong magnetic field in fusion liquid metal blanket with structured or
   unstructured mesh. We verified the validity of the implementation on
   several standard cases including the benchmark I of Shercliff and Hunt's
   cases, benchmark II of fully developed circular pipe MHD flow cases and
   benchmark III of KIT experimental case. Computational performance of the
   GPU implementation was examined by comparing its double precision run
   times with those of essentially the same algorithms and meshes. The
   resulted showed that a GPU (GTX 770) can outperform a server-class
   4-core, 8-thread CPU (Intel Core i7-4770k) by a factor of 2 at least.
   (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>DEC 2015</date><author>He, Qingyun
   Chen, Hongli
   Feng, Jingchao</author></paper><paper><title>Drawings of the hand and numerical skills in children of preschool age</title><abstract>This study aims at studying the relationship between manual and digital
   representations and numerical skills through the drawing of a hand.
   Sixty-two children (ages from 4 years old to 5 years and 6 months old)
   were asked to draw a hand and to carry out 2 numerical tasks: first to
   produce 2 equivalent collections and then to use the order-irrelevance
   principle. The statistical implicative analysis shows that more
   elaborated graphic strategies more strongly imply success producing 2
   equivalent collections.</abstract><date>JUL 2015</date><author>Bonneton-Botte, Nathalie
   Hili, Helene
   De La Haye, Fanny
   Noel, Yvonnick</author></paper><paper><title>Assessment of the Renal Function in Potential Donors of Living Kidney
   Transplants: Expanded Study</title><abstract>Introduction. It is very important to determine as accurately as
   possible the renal function in potential living renal transplant donors,
   especially those with limited renal function (CrCl &lt;90 mL/m/1.73 m(2)),
   age older than 50 years, and cardiovascular risk factors that might
   favor the development of long-term kidney diseases.Objective. The
   objective of this study was to compare the direct measurement of
   glomerular filtration rate (GFR) using EDTA-Cr51 and the estimations
   based on creatinine (eGFR): Cr clearance (CCr) with 24-hour urine and
   estimated using Cockroft-Gault (adjusted by using body surface
   area-Mosteller formula-SC), MDRD-4, MDRD-6, and CKD-EPI to determine the
   usefulness of different methods from EDTA-Cr51 to evaluate the kidney
   function.Patients and Methods. The kidney function evaluation has been
   made to 105 potential kidney donors using the EDTA-Cr51 method. The GFR
   obtained through the EDTA-Cr51 is compared with the CCr values in
   24-hour urine and eGFR based on creatinine (Cockcroft-Gault, MDRD4,
   MDRD6, and CKD-EPI).Results. Using the Bland Altman graphic we have
   observed that the most dispersed results are obtained with the eGFR
   using CCr in 24-hour urine and CKD-EPI. By means of Pasing &amp; Bablock, we
   realized that MDRD-4 and MDRD-6 show the highest approximation to the
   reference method proposed to be substituted, whereas CCr shows a high
   dispersion.Conclusions. eGFR using MDRD-4 and MDRD-6 formulas reveal the
   best adjustment to the measure by EDTA-Cr51. This might represent the
   best option if a direct eGFR measure is not available.</abstract><date>NOV 2015</date><author>Macias, L. B.
   Poblet, M. S.
   Perez, N. N.
   Jerez, R. I.
   Gonzalez Roncero, F. M.
   Blanco, G. B.
   Valdivia, M. A. P.
   Benjumea, A. S.
   Gentil Govantes, M. A.</author></paper><paper><title>Automatic analysis and recognition of graphical content in SVG-based
   engineering documents</title><abstract>Graphical engineering documents embody a fundamental piece of
   information used by operators and process experts in every-day plant
   activities. Such documents have been created with computers already for
   decades. In most industrial facilities, however, engineering
   documentation is maintained on paper or in elementary digital formats,
   which prevents their effective management and automatic exploitation by
   control and enterprise systems. In an effort to cope with such
   limitations, this paper presents a novel method for the automatic
   analysis and computer-interpretable description of plant engineering
   documents which are available as Scalable Vector Graphics (SVG).</abstract><date>FEB 2016</date><author>Hoang, Xuan Luu
   Arroyo, Esteban
   Fay, Alexander</author></paper><paper><title>Gravitational search algorithm using CUDA: a case study in
   high-performance metaheuristics</title><abstract>Many scientific and technical problems with massive computation
   requirements could benefit from the graphics processing units (GPUs)
   using compute unified device architecture (CUDA). Gravitational search
   algorithm (GSA) is a population-based metaheuristic which can be
   effectively implemented on GPU to reduce the execution time.
   Nonetheless, the performance improvement depends strongly on the process
   used to adapt the algorithm into CUDA environment. In this paper, we
   discuss possible approaches to parallelize GSA on graphics hardware
   using CUDA. An in-depth study of the computation efficiency of parallel
   algorithms and capability to effectively exploit the architecture of GPU
   is performed. Additionally, a comparative study of parallel and
   sequential GSA was carried out on a set of standard benchmark
   optimization functions. The results show a significant speedup while
   maintaining results quality which re-emphasizes the utility of
   CUDA-based implementation for complex and computationally intensive
   parallel applications.</abstract><date>APR 2015</date><author>Zarrabi, Amirreza
   Samsudin, Khairulmizam
   Karuppiah, Ettikan K.</author></paper><paper><title>Numerical simulations of elastic wave propagation using graphical
   processing units-Comparative study of high-performance computing
   capabilities</title><abstract>High-performance computing is important in many areas of engineering.
   Increasing capabilities of modern workstations open new possibilities in
   scientific computing. This paper demonstrates how graphical processing
   units can be used efficiently for large models of elastic wave
   propagation in complex media. The method is based on the local
   interaction simulation approach and a parallel algorithm architecture.
   The focus of the work presented is on numerical implementation and
   covers aspects related to software modular architecture, computer memory
   organisation and optimisation. A domain decomposition approach allowing
   for calculations using multiple-GPU configurations is proposed,
   implemented and examined. The performance of the proposed simulation
   framework is tested for numerical models of different sizes, various
   computing precisions and hardware platforms. The results obtained are
   discussed in terms of graphical processing unit limitations. Obtained
   results indicate significant speed-up factors comparing to calculations
   using central processing units or different modelling approaches. (C)
   2015 Elsevier B.V. All rights reserved.</abstract><date>JUN 15 2015</date><author>Packo, P.
   Bielak, T.
   Spencer, A. B.
   Uhl, T.
   Staszewski, W. J.
   Worden, K.
   Barszcz, T.
   Russek, P.
   Wiatr, K.</author></paper><paper><title>A Hybrid Parallel Implementation of the Aho-Corasick and Wu-Manber
   Algorithms Using NVIDIA CUDA and MPI Evaluated on a Biological Sequence
   Database</title><abstract>Multiple matching algorithms are used to locate the occurrences of
   patterns from a finite pattern set in a large input string. Aho-Corasick
   and Wu-Manber, two of the most well known algorithms for multiple
   matching require an increased computing power, particularly in cases
   where large-size datasets must be processed, as is common in
   computational biology applications. Over the past years, Graphics
   Processing Units (GPUs) have evolved to powerful parallel processors
   outperforming Central Processing Units (CPUs) in scientific
   calculations. Moreover, multiple GPUs can be used in parallel, forming
   hybrid computer cluster configurations to achieve an even higher
   processing throughput. This paper evaluates the speedup of the parallel
   implementation of the Aho-Corasick and Wu-Manber algorithms on a hybrid
   GPU cluster, when used to process a snapshot of the Expressed Sequence
   Tags of the human genome and for different problem parameters.</abstract><date>FEB 2015</date><author>Kouzinopoulos, Charalampos S.
   Assael, John-Alexander M.
   Pyrgiotis, Themistoklis K.
   Margaritis, Konstantinos G.</author></paper><paper><title>No Sexual Dimorphism Detected in Digit Ratios of the Fire Salamander
   (Salamandra salamandra)</title><abstract>It has been proposed that digit ratio may be used as a biomarker of
   early developmental effects. Specifically, the second-to-fourth digit
   ratio (2D:4D) has been linked to the effects of sex hormones and their
   receptor genes, but other digit ratios have also been investigated.
   Across taxa, patterns of sexual dimorphism in digit ratios are ambiguous
   and a scarcity of studies in basal tetrapods makes it difficult to
   understand how ratios have evolved. Here, we focus on examining sex
   differences in digit ratios (2D:3D, 2D:4D, and 3D:4D) in a common
   amphibian, the fire salamander (Salamandra salamandra). We used graphic
   software to measure soft tissue digit length and digit bone length from
   X-rays. We found a nonsignificant tendency in males to have a lower
   2D:3D than females; however, no sexual differences were detected in the
   other ratios. We discuss our results in the context of other studies of
   digit ratios, and how sex determination systems, as well as other
   factors, might impact patterns of sexual dimorphism, particularly in
   reptiles and in amphibians. Our findings suggest that caution is needed
   when using digit ratios as a potential indicator of prenatal hormonal
   effects in amphibians and highlight the need for more comparative
   studies to elucidate the evolutionary and genetic mechanisms implicated
   in sexually dimorphic patterns across taxonomic groups. Anat Rec,
   298:1786-1795, 2015. (c) 2015 Wiley Periodicals, Inc.</abstract><date>OCT 2015</date><author>Balogova, Monika
   Nelson, Emma
   Uhrin, Marcel
   Figurova, Maria
   Ledecky, Valent
   Zysk, Bartlomiej</author></paper><paper><title>Efficient Computation of Instantons for Multi-Dimensional Turbulent
   Flows with Large Scale Forcing</title><abstract>Extreme events play a crucial role in fluid turbulence. Inspired by
   methods from field theory, these extreme events, their evolution and
   probability can be computed with help of the instanton formalism as
   minimizers of a suitable action functional. Due to the high number of
   degrees of freedom in multi-dimensional fluid flows, traditional global
   minimization techniques quickly become prohibitive in their memory
   requirements. We outline a novel method for finding the minimizing
   trajectory in a wide class of problems that typically occurs in
   turbulence setups, where the underlying dynamical system is a
   non-gradient, non-linear partial differential equation, and the forcing
   is restricted to a limited length scale. We demonstrate the efficiency
   of the algorithm in terms of performance and memory by computing high
   resolution instanton field configurations corresponding to viscous
   shocks for 1D and 2D compressible flows.</abstract><date>SEP 2015</date><author>Grafke, Tobias
   Grauer, Rainer
   Schindel, Stephan</author></paper><paper><title>Parallel performance modeling of irregular applications in cell-centered
   finite volume methods over unstructured tetrahedral meshes</title><abstract>Finite volume methods are widely used numerical strategies for solving
   partial differential equations. This paper aims at obtaining a
   quantitative understanding of the achievable performance of the
   cell-centered finite volume method on 3D unstructured tetrahedral
   meshes, using traditional multicore CPUs as well as modem GPUs. By using
   an optimized implementation and a synthetic connectivity matrix that
   exhibits a perfect structure of equal-sized blocks lying on the main
   diagonal, we can closely relate the achievable computing performance to
   the size of these diagonal blocks. Moreover, we have derived a
   theoretical model for identifying characteristic levels of the
   attainable performance as a function of hardware parameters, based on
   which a realistic upper limit of the performance can be predicted
   accurately. For real-world tetrahedral meshes, the key to high
   performance lies in a reordering of the tetrahedra, such that the
   resulting connectivity matrix resembles a block diagonal form where the
   optimal size of the blocks depends on the hardware. Numerical
   experiments confirm that the achieved performance is close to the
   practically attainable maximum and it reaches 75% of the theoretical
   upper limit, independent of the actual tetrahedral mesh considered. From
   this, we develop a general model capable of identifying bottleneck
   performance of a system's memory hierarchy in irregular applications.
   (C) 2014 Elsevier Inc. All rights reserved.</abstract><date>FEB 2015</date><author>Langguth, J.
   Wu, N.
   Chai, J.
   Cai, X.</author></paper><paper><title>Design and realization of an Image mosaic system on the CCD aerial
   camera</title><abstract>It has long been difficulties in aerial photograph to stitch multi-route
   images into a panoramic image in real time for multi-route flight
   framing CCD camera with very large amount of data, and high accuracy
   requirements. An automatic aerial image mosaic system based on GPU
   development platform is described in this paper. Parallel computing of
   SIFT feature extraction and matching algorithm module is achieved by
   using CUDA technology for motion model parameter estimation on the
   platform, which makes it's possible to stitch multiple CCD images in
   real-time. Aerial tests proved that the mosaic system meets the user's
   requirements with 99% accuracy and 30 to 50 times' speed improvement of
   the normal mosaic system.</abstract><date>2015</date><author>Liu Hai ying
   Wang Peng
   Zhu Hai bin
   Li Yan
   Zhang Shao jun</author></paper><paper><title>A new possibility in thoracoscopic virtual reality simulation training:
   development and testing of a novel virtual reality simulator for
   video-assisted thoracoscopic surgery lobectomy</title><abstract>OBJECTIVES: The aims of this study were to develop virtual reality
   simulation software for video-assisted thoracic surgery (VATS)
   lobectomy, to explore the opinions of thoracic surgeons concerning the
   VATS lobectomy simulator and to test the validity of the simulator
   metrics.METHODS: Experienced VATS surgeons worked with computer
   specialists to develop a VATS lobectomy software for a virtual reality
   simulator. Thoracic surgeons with different degrees of experience in
   VATS were enrolled at the 22nd meeting of the European Society of
   Thoracic Surgeons (ESTS) held in Copenhagen in June 2014. The surgeons
   were divided according to the number of performed VATS lobectomies:
   novices (0 VATS lobectomies), intermediates (1-49 VATS lobectomies) and
   experienced (&gt;50 VATS lobectomies). The participants all performed a
   lobectomy of a right upper lobe on the simulator and answered a
   questionnaire regarding content validity. Metrics were compared between
   the three groups.RESULTS: We succeeded in developing the first version
   of a virtual reality VATS lobectomy simulator. A total of 103 thoracic
   surgeons completed the simulated lobectomy and were distributed as
   follows: novices n = 32, intermediates n = 45 and experienced n = 26.
   All groups rated the overall user realism of the VATS lobectomy scenario
   to a median of 5 on a scale 1-7, with 7 being the best score. The
   experienced surgeons found the graphics and movements realistic and
   rated the scenario high in terms of usefulness as a training tool for
   novice and intermediate experienced thoracic surgeons, but not very
   useful as a training tool for experienced surgeons. The metric scores
   were not statistically significant between groups.CONCLUSIONS: This is
   the first study to describe a commercially available virtual reality
   simulator for a VATS lobectomy. More than 100 thoracic surgeons found
   the simulator realistic, and hence it showed good content validity.
   However, none of the built-in simulator metrics could significantly
   distinguish between novice, intermediate experienced and experienced
   surgeons, and further development of the simulator software is necessary
   to develop valid metrics.</abstract><date>OCT 2015</date><author>Jensen, Katrine
   Bjerrum, Flemming
   Hansen, Henrik Jessen
   Petersen, Rene Horsleben
   Pedersen, Jesper Holst
   Konge, Lars</author></paper><paper><title>Immersive virtual reality to vindicate the application of value stream
   mapping in an US-based SME</title><abstract>Value stream mapping (VSM) assists in identifying opportunities for
   improvement by revealing the inefficiencies in the current state.
   However, several difficulties appear while evaluating such "as-is" state
   for leaner future state. Trial and error method is often employed for
   continuous improvement to accomplish the desired level of future state.
   This causes numerous iterations and improper usage of resources which
   makes lean application costly and inefficient. In order to tackle this,
   an immersive virtual reality (IVR) approach to visualize and interact
   with the image of real models in a computer graphics environment is
   presented in this article. This allows conducting a quick
   experimentation in a virtual world to reach optimal future state without
   exhausting resources or incurring additional cost. In order to reinforce
   applicability and usefulness of the proposed framework, a case study of
   an US-based SME is also discussed. This paper first illustrates the
   implementation procedure of VSM in the manufacturing processes to
   develop current and future states. Data is collected for a year to
   analyse the current state and then IVR is used to validate results for
   future state. A reduction of more than 40 % in lead-time, 41 % in floor
   space and 47 % in manpower is achieved after a period of 3 months of
   implementing the recommendations.</abstract><date>NOV 2015</date><author>Tyagi, Satish
   Vadrevu, Sarat</author></paper><paper><title>Comparison of three artificial models of the magnetohydrodynamic effect
   on the electrocardiogram</title><abstract>The electrocardiogram (ECG) is often acquired during magnetic resonance
   imaging (MRI), but its analysis is restricted by the presence of a
   strong artefact, called magnetohydrodynamic (MHD) effect. MHD effect is
   induced by the flow of electrically charged particles in the blood
   perpendicular to the static magnetic field, which creates a potential of
   the order of magnitude of the ECG and temporally coincident with the
   repolarisation period. In this study, a new MHD model is proposed by
   using MRI-based 4D blood flow measurements made across the aortic arch.
   The model is extended to several cardiac cycles to allow the simulation
   of a realistic ECG acquisition during MRI examination and the quality
   assessment of MHD suppression techniques. A comparison of two existing
   models, based, respectively, on an analytical solution and on a
   numerical method-based solution of the fluids dynamics problem, is made
   with the proposed model and with an estimate of the MHD voltage observed
   during a real MRI scan. Results indicate a moderate agreement between
   the proposed model and the estimated MHD model for most leads, with an
   average correlation factor of 0.47. However, the results demonstrate
   that the proposed model provides a closer approximation to the observed
   MHD effects and a better depiction of the complexity of the MHD effect
   compared with the previously published models, with an improved
   correlation ([GRAPHICS]), coefficient of determination ([GRAPHICS]) and
   fraction of energy ([GRAPHICS]) compared with the best previous model.
   The source code will be made freely available under an open source
   licence to facilitate collaboration and allow more rapid development of
   more accurate models of the MHD effect.</abstract><date>OCT 3 2015</date><author>Oster, Julien
   Llinares, Raul
   Payne, Stephen
   Tse, Zion Tsz Ho
   Schmidt, Ehud Jeruham
   Clifford, Gari D.</author></paper><paper><title>Computer implementations of iterative and non-iterative crystal
   plasticity solvers on high performance graphics hardware</title><abstract>We present parallel implementations of Newton-Raphson iterative and
   spectral based non-iterative solvers for single-crystal visco-plasticity
   models on a specialized computer hardware integrating a
   graphics-processing unit (GPU). We explore two implementations for the
   iterative solver on GPU multiprocessors: one based on a thread per
   crystal parallelization on local memory and another based on multiple
   threads per crystal on shared memory. The non-iterative solver
   implementation on the GPU hardware is based on a divide-conquer approach
   for matrix operations. The reduction of computational time for the
   iterative scheme was found to approach one order of magnitude. From
   detailed performance comparisons of the developed GPU iterative and
   non-iterative implementations, we conclude that the spectral
   non-iterative solver programed on a GPU platform is superior over the
   iterative implementation in terms of runtime as well as ease of
   implementation. It provides remarkable speedup factors exceeding three
   orders of magnitude over the iterative scalar version of the solver.</abstract><date>OCT 2015</date><author>Savage, Daniel J.
   Knezevic, Marko</author></paper><paper><title>CUDA-Based Hybrid Intuitionistic Fuzzy Edge Detection Algorithm</title><abstract>Intuitionistic fuzzy edge detection algorithm has been used for the
   signification or characterization of images. It has been designed by
   experts and the algorithm provides to aim to minimize errors. However,
   it has a fixed value for thresholding. In this paper, a hybrid algorithm
   has been developed using the Otsu method which is calculated a threshold
   value depending on the images. To be applicable in parallel of
   intuitionistic fuzzy edge algorithm is pave the way for accelerating of
   algorithm by performing in the graphics card. Intuitionistic fuzzy logic
   edge detection algorithm has been tested by transferring different size
   images to graphics cards which has different computing capacity via
   Compute Unified Device Architecture (CUDA) programming environment which
   is manufactured by NVIDIA. Parallel model of the algorithm adapted to
   CUDA platform, compared to serial application running on processor, and
   has seen that shortened runtime at least 67 times, most 639 times.</abstract><date>2015</date><author>Yalcin, Eyup
   Badem, Hasan
   Gunes, Mahit</author></paper><paper><title>Generating Grinding Profile between Screw Rotor and Forming Tool by
   Digital Graphic Scanning (DGS) Method</title><abstract>This work presents a digital graphic scanning (DGS) method, based on
   computer scanning graphics, to generate a grinding profile avoiding the
   difficulties appeared from the complex equations of the contact line.
   First the enveloping surface between the forming tool (rotor) profile
   and its corresponding cutting locus was developed, then based on
   Bresenham algorithm, the best possible pixels of the enveloping surface
   in the pixel matrix of screen were demonstrated using a specified color
   Finally, the grinding profile data of the rotor (forming tool) were
   collected by scanning the pixel matrix of screen, capturing the
   coordinates of the indicated color of the best possible pixels.
   Comparing the analytical gearing envelope method and the DGS method, the
   feasibility of the DGS method was indicated The DGS method was shown as
   a precise, rapid, efficient and stable computing tool to generate a
   grinding profile. In addition, such an approach can be applied in
   designing other similarly conjugated products such as gears, perpetual
   screws and milling cutters.</abstract><date>JAN 2016</date><author>Shen, Zhihuang
   Yao, Bin
   Teng, Weibin
   Feng, Wei
   Sun, Weifang</author></paper><paper><title>Adaptive GPU-accelerated force calculation for interactive rigid
   molecular docking using haptics</title><abstract>Molecular docking systems model and simulate in silico the interactions
   of intermolecular binding. Haptics-assisted docking enables the user to
   interact with the simulation via their sense of touch but a stringent
   time constraint on the computation of forces is imposed due to the
   sensitivity of the human haptic system. To simulate high fidelity smooth
   and stable feedback the haptic feedback loop should run at rates of 500
   Hz to 1 kHz. We present an adaptive force calculation approach that can
   be executed in parallel on a wide range of Graphics Processing Units
   (GPUs) for interactive haptics-assisted docking with wider applicability
   to molecular simulations. Prior to the interactive session either a
   regular grid or an octree is selected according to the available GPU
   memory to determine the set of interatomic interactions within a cutoff
   distance. The total force is then calculated from this set. The approach
   can achieve force updates in less than 2 ms for molecular structures
   comprising hundreds of thousands of atoms each, with performance
   improvements of up to 90 times the speed of current CPU-based force
   calculation approaches used in interactive docking. Furthermore, it
   overcomes several computational limitations of previous approaches such
   as pre-computed force grids, and could potentially be used to model
   receptor flexibility at haptic refresh rates. (C) 2015 Elsevier Inc. All
   rights reserved.</abstract><date>SEP 2015</date><author>Iakovou, Georgios
   Hayward, Steven
   Laycock, Stephen D.</author></paper><paper><title>Fast and Exact (Poisson) Solvers on Symmetric Geometries</title><abstract>In computer graphics, numerous geometry processing applications reduce
   to the solution of a Poisson equation. When considering geometries with
   symmetry, a natural question to consider is whether and how the symmetry
   can be leveraged to derive an efficient solver for the underlying system
   of linear equations. In this work we provide a simple
   representation-theoretic analysis that demonstrates how symmetries of
   the geometry translate into block diagonalization of the linear
   operators and we show how this results in efficient linear solvers for
   surfaces of revolution with and without angular boundaries.</abstract><date>AUG 2015</date><author>Kazhdan, M.</author></paper><paper><title>Colloquium: Large scale simulations on GPU clusters</title><abstract>Graphics processing units (GPU) are currently used as a cost-effective
   platform for computer simulations and big-data processing. Large scale
   applications require that multiple GPUs work together but the efficiency
   obtained with cluster of GPUs is, at times, sub-optimal because the GPU
   features are not exploited at their best. We describe how it is possible
   to achieve an excellent efficiency for applications in statistical
   mechanics, particle dynamics and networks analysis by using suitable
   memory access patterns and mechanisms like CUDA streams, profiling
   tools, etc. Similar concepts and techniques may be applied also to other
   problems like the solution of Partial Differential Equations.</abstract><date>JUN 17 2015</date><author>Bernaschi, Massimo
   Bisson, Mauro
   Fatica, Massimiliano</author></paper><paper><title>GPU-Accelerated Quantification Filters for Analytical Queries in
   Multidimensional Databases</title><abstract>In online analytical processing (OLAP), filtering elements of a given
   dimensional attribute according to the value of a measure attribute is
   an essential operation, for example in top-k evaluation. Such filters
   can involve extremely large amounts of data to be processed, in
   particular when the filter condition includes "quantification" such as
   ANY or ALL, where large slices of an OLAP cube have to be computed and
   inspected. Due to the sparsity of OLAP cubes, the slices serving as
   input to the filter are usually sparse as well, presenting a challenge
   for GPU approaches which need to work with a limited amount of memory
   for holding intermediate results. Our CUDA solution involves a hashing
   scheme specifically designed for frequent and parallel updates,
   including several optimizations exploiting architectural features of
   Nvidia's Fermi and Kepler GPUs.</abstract><date>2015</date><author>Strohm, Peter Tim
   Wittmer, Steffen
   Haberstroh, Alexander
   Lauer, Tobias</author></paper><paper><title>Accelerated ray tracing algorithm under urban macro cell</title><abstract>In this study, an ray tracing propagation prediction model, which is
   based on creating a virtual source tree, is used because of their high
   efficiency and reliable prediction accuracy. In addition, several
   acceleration techniques are also adopted to improve the efficiency of
   ray-tracing-based prediction over large areas. However, in the process
   of employing the ray tracing method for coverage zone prediction,
   runtime is linearly proportional to the total number of prediction
   points, leading to large and sometimes prohibitive computation time
   requirements under complex geographical urban macrocell environments. In
   order to overcome this bottleneck, the compute unified device
   architecture ( CUDA), which provides fine-grained data parallelism and
   thread parallelism, is implemented to accelerate the calculation. Taking
   full advantage of tens of thousands of threads in CUDA program, the
   decomposition of the coverage prediction problem is firstly conducted by
   partitioning the image tree and the visible prediction points to
   different sources. Then, we make every thread calculate the
   electromagnetic field of one propagation path and then collect these
   results. Comparing this parallel algorithm with the traditional
   sequential algorithm, it can be found that computational efficiency has
   been improved.</abstract><date>2015</date><author>Liu, Z. -Y.
   Guo, L. -X.
   Guan, X. -W.</author></paper><paper><title>CUDA parallel programming for simulation of epidemiological models based
   on individuals</title><abstract>Mathematical models are of great value in epidemiology to help
   understand the dynamics of the various infectious diseases, as well as
   in the conception of effective control strategies. The classical
   approach is to use differential equations to describe, in a quantitative
   manner, the spread of diseases within a particular population. An
   alternative approach is to represent each individual in the population
   as a string or vector of characteristic data and simulate the contagion
   and recovery processes by computational means. This type of model,
   referred in the literature as MBI (models based on individuals), has the
   advantage of being flexible as the characteristics of each individual
   can be quite complex, involving, for instance, age, sex, pre-existing
   health conditions, environmental factors, social habits, etc. However,
   when it comes to simulations involving large populations, MBI may
   require a large computational effort in terms of memory storage and
   processing time. In order to cope with the problem of heavy
   computational effort, this paper proposes a parallel implementation of
   MBI using a graphics processor unit compatible with CUDA. It was found
   that, even in the case of a simple susceptible-infected-recovered model,
   the computational gains in terms of processing time are significant.
   Copyright (c) 2015John Wiley &amp; Sons, Ltd.</abstract><date>FEB 2016</date><author>Galvao Filho, Arlindo R.
   Martins de Paula, Lauro C.
   Coelho, Clarimar Jose
   de Limab, Telma Woerle
   Soares, Anderson da Silva</author></paper><paper><title>A GPU-Accelerated Parallel Shooting Algorithm for Analysis of Radio
   Frequency and Microwave Integrated Circuits</title><abstract>This paper presents a new parallel shooting-Newton method based on a
   graphic processing unit (GPU)-accelerated periodic Arnoldi shooting
   solver (GAPAS) for fast periodic steady-state analysis of radio
   frequency/millimeter-wave integrated circuits. The new algorithm first
   explores a periodic structure of the state matrix by using a periodic
   Arnoldi algorithm for computing the resulting structured Krylov subspace
   in the generalized minimal residual (GMRES) solver. The resulting
   periodic Arnoldi shooting method is very amenable for massive parallel
   computing, such as GPUs. Second, the periodic Arnoldi-based GMRES solver
   in the shooting-Newton method is parallelized on the recent NVIDIA Tesla
   GPU platforms. We further explore CUDA GPUs features, such as coalesced
   memory access and overlapping transfers with computation to boost the
   efficiency of the resulting parallel GAPAS method. Experimental results
   from several industrial examples show that when compared with the
   state-of-the-art implicit GMRES method under the same accuracy, the new
   parallel shooting-Newton method can lead up to 8x speedup.</abstract><date>MAR 2015</date><author>Liu, Xue-Xin
   Yu, Hao
   Tan, Sheldon X-D</author></paper><paper><title>FAST: framework for heterogeneous medical image computing and
   visualization</title><abstract>Computer systems are becoming increasingly heterogeneous in the sense
   that they consist of different processors, such as multi-core CPUs and
   graphic processing units. As the amount of medical image data increases,
   it is crucial to exploit the computational power of these processors.
   However, this is currently difficult due to several factors, such as
   driver errors, processor differences, and the need for low-level memory
   handling. This paper presents a novel FrAmework for heterogeneouS
   medical image compuTing and visualization (FAST). The framework aims to
   make it easier to simultaneously process and visualize medical images
   efficiently on heterogeneous systems.FAST uses common image processing
   programming paradigms and hides the details of memory handling from the
   user, while enabling the use of all processors and cores on a system.
   The framework is open-source, cross-platform and available online.Code
   examples and performance measurements are presented to show the
   simplicity and efficiency of FAST. The results are compared to the
   insight toolkit (ITK) and the visualization toolkit (VTK) and show that
   the presented framework is faster with up to 20 times speedup on several
   common medical imaging algorithms.FAST enables efficient medical image
   computing and visualization on heterogeneous systems. Code examples and
   performance evaluations have demonstrated that the toolkit is both easy
   to use and performs better than existing frameworks, such as ITK and
   VTK.</abstract><date>NOV 2015</date><author>Smistad, Erik
   Bozorgi, Mohammadmehdi
   Lindseth, Frank</author></paper><paper><title>Comparison of Acceleration Data Structures for Electromagnetic
   Ray-Tracing Purposes on GPUs</title><abstract>We analyze and compare the performances of two acceleration data
   structures for electromagnetic ray-tracing purposes on graphical
   processing units (GPUs) using the CUDA programming language, namely the
   K-Dimensional (KD)-tree and the Split Bounding Volume Hierarchy (SBVH).
   Our implementations have been based on the approach made available by
   Nvidia, which takes into account the programming optimizations made
   possible by the latest version of CUDA and the most recent Nvidia GPU
   architectures. We have tested the two approaches on standard computer
   graphics scenes (conference and bunny) and on a scene of electromagnetic
   interest ("ship"). In all the cases considered, the SBVH has shown to
   perform better in terms of both speed and memory-saving properties.</abstract><date>OCT 2015</date><author>Breglia, Alfonso
   Capozzoli, Amedeo
   Curcio, Claudio
   Liseno, Angelo</author></paper><paper><title>Real-time maximum a-posteriori image reconstruction for fluorescence
   microscopy</title><abstract>Rapid reconstruction of multidimensional image is crucial for enabling
   real-time 3D fluorescence imaging. This becomes a key factor for imaging
   rapidly occurring events in the cellular environment. To facilitate
   real-time imaging, we have developed a graphics processing unit (GPU)
   based real-time maximum a-posteriori (MAP) image reconstruction system.
   The parallel processing capability of GPU device that consists of a
   large number of tiny processing cores and the adaptability of image
   reconstruction algorithm to parallel processing (that employ multiple
   independent computing modules called threads) results in high temporal
   resolution. Moreover, the proposed quadratic potential based MAP
   algorithm effectively deconvolves the images as well as suppresses the
   noise. The multi-node multi-threaded GPU and the Compute Unified Device
   Architecture (CUDA) efficiently execute the iterative image
   reconstruction algorithm that is similar to 200-fold faster (for large
   dataset) when compared to existing CPU based systems. (C) 2015
   Author(s). All article content, except where otherwise noted, is
   licensed under a Creative Commons Attribution 3.0 Unported License.</abstract><date>AUG 2015</date><author>Jabbar, Anwar A.
   Dilipkumar, Shilpa
   Rasmi, C. K.
   Rajan, K.
   Mondal, Partha P.</author></paper><paper><title>Register Efficient Dynamic Memory Allocator for GPUs</title><abstract>We compare five existing dynamic memory allocators optimized for GPUs
   and show their strengths and weaknesses. In the measurements, we use
   three generic evaluation tests proposed in the past and we add one with
   a real workload, where dynamic memory allocation is used in building the
   k-d tree data structure. Following the performance analysis we propose a
   new dynamic memory allocator and its variants that address the
   limitations of the existing dynamic memory allocators. The new dynamic
   memory allocator uses few resources and is targeted towards large and
   variably sized memory allocations on massively parallel hardware
   architectures.</abstract><date>DEC 2015</date><author>Vinkler, M.
   Havran, V.</author></paper><paper><title>DOCK 6: Impact of New Features and Current Docking Performance</title><abstract>This manuscript presents the latest algorithmic and methodological
   developments to the structure-based design program DOCK 6.7 focused on
   an updated internal energy function, new anchor selection control,
   enhanced minimization options, a footprint similarity scoring function,
   a symmetry-corrected root-mean-square deviation algorithm, a database
   filter, and docking forensic tools. An important strategy during
   development involved use of three orthogonal metrics for assessment and
   validation: pose reproduction over a large database of 1043
   protein-ligand complexes (SB2012 test set), cross-docking to 24
   drug-target protein families, and database enrichment using large active
   and decoy datasets (Directory of Useful Decoys [DUD]-E test set) for
   five important proteins including HIV protease and IGF-1R. Relative to
   earlier versions, a key outcome of the work is a significant increase in
   pose reproduction success in going from DOCK 4.0.2 (51.4%) 5.4 (65.2%)
   6.7 (73.3%) as a result of significant decreases in failure arising from
   both sampling 24.1% 13.6% 9.1% and scoring 24.4% 21.1% 17.5%. Companion
   cross-docking and enrichment studies with the new version highlight
   other strengths and remaining areas for improvement, especially for
   systems containing metal ions. The source code for DOCK 6.7 is available
   for download and free for academic users at . (c) 2015 Wiley
   Periodicals, Inc.</abstract><date>JUN 5 2015</date><author>Allen, William J.
   Balius, Trent E.
   Mukherjee, Sudipto
   Brozell, Scott R.
   Moustakas, Demetri T.
   Lang, P. Therese
   Case, David A.
   Kuntz, Irwin D.
   Rizzo, Robert C.</author></paper><paper><title>Implementation of the DWT in a GPU through a Register-based Strategy</title><abstract>The release of the CUDA Kepler architecture in March 2012 has provided
   Nvidia GPUs with a larger register memory space and instructions for the
   communication of registers among threads. This facilitates a new
   programming strategy that utilizes registers for data sharing and
   reusing in detriment of the shared memory. Such a programming strategy
   can significantly improve the performance of applications that reuse
   data heavily. This paper presents a register-based implementation of the
   Discrete Wavelet Transform (DWT), the prevailing data decorrelation
   technique in the field of image coding. Experimental results indicate
   that the proposed method is, at least, four times faster than the best
   GPU implementation of the DWT found in the literature. Furthermore,
   theoretical analysis coincide with experimental tests in proving that
   the execution times achieved by the proposed implementation are close to
   the GPU's performance limits.</abstract><date>DEC 2015</date><author>Enfedaque, Pablo
   Auli-Llinas, Francesc
   Moure, Juan C.</author></paper><paper><title>Organization of a geophysical information space by using an
   event-bush-based collaborative tool</title><abstract>Development of knowledge engineering makes it possible to bring an
   information space relating to an entire domain of knowledge within the
   field of geoscience into a strict form, which is both computer-tractable
   and convenient for collaborative research work. Nevertheless, there are
   issues that seriously hamper this process - the problem of defining key
   terms, which is often not shared by the colleagueship, and interrelation
   of concepts developed by different schools within the colleagueship
   focused on different aspects of this domain. Another issue is the export
   of results to a wider community unfamiliar with the specificity of local
   studies. All these issues can be successfully addressed by a novel
   technique of knowledge engineering, the event bush, brought into the
   COLLA environment for geoscientific collaborative studies. This paper
   demonstrates how the said issues can be resolved by the example of one
   of the most important information domains in the field of seismology,
   the site effects. Text, graphics, tabular data and a physical model
   coming from different sources and different contexts are united in one
   context keeping all the specificity of original understanding and
   allowing the researchers keep on following their own context and
   terminology.</abstract><date>SEP 2015</date><author>Diviacco, Paolo
   Pshenichny, Cyril
   Carniel, Roberto
   Khrabrykh, Zinaida
   Shterkhun, Victoria
   Mouromtsev, Dmitry
   Guzman, Silvina
   Pascolo, Paolo</author></paper><paper><title>Modeling of the three-dimensional flow of polymer melt in a convergent
   channel of rectangular cross-section</title><abstract>The modified rheological Vinogradov-Pokrovskii model is applied to solve
   the problem of the mathematical modeling of three-dimensional flow of a
   nonlinear viscoelastic fluid in a plane-parallel channel with an abrupt
   constriction. The discrete counterparts of the governing equations are
   obtained using the control volume method with the splitting in physical
   processes, while the numerical realization employs graphical processing
   units on the basis of the CUDA technology of parallel computations. The
   velocity and stress fields are calculated for two polyethylene specimens
   and the presence of circulation flow in the vicinity of the slot channel
   entry is noticed. The vortex dimensions considerably depend on the
   rheological parameters of the melt.</abstract><date>MAY 2015</date><author>Koshelev, K. B.
   Pyshnograi, G. V.
   Tolstykh, M. Yu.</author></paper><paper><title>The Light Field Stereoscope Immersive Computer Graphics via Factored
   Near-Eye Light Field Displays with Focus Cues</title><abstract>Over the last few years, virtual reality (VR) has re-emerged as a
   technology that is now feasible at low cost via inexpensive cell-phone
   components. In particular, advances of high-resolution micro displays,
   low-latency orientation trackers, and modern GPUs facilitate immersive
   experiences at low cost. One of the remaining challenges to further
   improve visual comfort in VR experiences is the vergence-accommodation
   conflict inherent to all stereoscopic displays. Accurate reproduction of
   all depth cues is crucial for visual comfort. By combining well-known
   stereoscopic display principles with emerging factored light field
   technology, we present the first wearable VR display supporting high
   image resolution as well as focus cues. A light field is presented to
   each eye, which provides more natural viewing experiences than
   conventional near-eye displays. Since the eye box is just slightly
   larger than the pupil size, rank-1 light field factorizations are
   sufficient to produce correct or nearly-correct focus cues; no
   time-multiplexed image display or gaze tracking is required. We analyze
   lens distortions in 4D light field space and correct them using the
   afforded high-dimensional image formation. We also demonstrate
   significant improvements in resolution and retinal blur quality over
   related near-eye displays. Finally, we analyze diffraction limits of
   these types of displays.</abstract><date>AUG 2015</date><author>Huang, Fu-Chung
   Chen, Kevin
   Wetzstein, Gordon</author></paper><paper><title>CUDA ClustalW: An efficient parallel algorithm for progressive multiple
   sequence alignment on Multi-GPUs</title><abstract>For biological applications, sequence alignment is an important strategy
   to analyze DNA and protein sequences. Multiple sequence alignment is an
   essential methodology to study biological data, such as homology
   modeling, phylogenetic reconstruction and etc. However, multiple
   sequence alignment is a NP-hard problem. In the past decades,
   progressive approach has been proposed to successfully align multiple
   sequences by adopting iterative pairwise alignments. Due to rapid growth
   of the next generation sequencing technologies, a large number of
   sequences can be produced in a short period of time. When the problem
   instance is large, progressive alignment will be time consuming.
   Parallel computing is a suitable solution for such applications, and GPU
   is one of the important architectures for contemporary parallel
   computing researches. Therefore, we proposed a GPU version of ClustalW
   v2.0.11, called CUDA ClustalW v1.0, in this work. From the experiment
   results, it can be seen that the CUDA ClustalW v1.0 can achieve more
   than 33 x speedups for overall execution time by comparing to ClustalW
   v2.0.11. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>OCT 2015</date><author>Hung, Che-Lun
   Lin, Yu-Shiang
   Lin, Chun-Yuan
   Chung, Yeh-Ching
   Chung, Yi-Fang</author></paper><paper><title>Wetbrush: GPU-based 3D Painting Simulation at the Bristle Level</title><abstract>We present a real-time painting system that simulates the interactions
   among brush, paint, and canvas at the bristle level. The key challenge
   is how to model and simulate sub-pixel paint details, given the limited
   computational resource in each time step. To achieve this goal, we
   propose to define paint liquid in a hybrid fashion: the liquid close to
   the brush is modeled by particles, and the liquid away from the brush is
   modeled by a density field. Based on this representation, we develop a
   variety of techniques to ensure the performance and robustness of our
   simulator under large time steps, including brush and particle
   simulations in non-inertial frames, a fixed-point method for
   accelerating Jacobi iterations, and a new Eulerian-Lagrangian approach
   for simulating detailed liquid effects. The resulting system can
   realistically simulate not only the motions of brush bristles and paint
   liquid, but also the liquid transfer processes among different
   representations. We implement the whole system on GPU by CUDA. Our
   experiment shows that artists can use the system to draw realistic and
   vivid digital paintings, by applying the painting techniques that they
   are familiar with but not offered by many existing systems.</abstract><date>NOV 2015</date><author>Chen, Zhili
   Kim, Byungmoon
   Ito, Daichi
   Wang, Huamin</author></paper><paper><title>Research on paralleling compute efficiency of FIR filter</title><abstract>Software radar is a hot direction of radar system research, and FIR
   filter is the basic part of the software radar system. Be restricted by
   the CPU speed, most of the FIR filter is achieved on the DSP. Following
   the development of new technology, GPU perhaps can replace the role of
   the DSP. This paper designs a band pass filter, then uses the GPU to
   fulfill that algorithm, Finally process some typical signals to test the
   speed and research the efficiency of using GPU to accomplish real time
   compute, it's useful for further research on software radar system.</abstract><date>2015</date><author>Wang, Jinbo
   Mao, Lin</author></paper><paper><title>Linear Bregman algorithm implemented in parallel GPU</title><abstract>At present, most compressed sensing (CS) algorithms have poor converging
   speed, thus are difficult to run on PC. To deal with this issue, we use
   a parallel GPU to implement a broadly used compressed sensing algorithm,
   the Linear Bregman algorithm.Linear iterative Bregman algorithm is a
   reconstruction algorithm proposed by Osher and Cai. Compared with other
   CS reconstruction algorithms, the linear Bregman algorithm only involves
   the vector and matrix multiplication and thresholding operation, and is
   simpler and more efficient for programming. We use C as a development
   language and adopt CUDA (Compute Unified Device Architecture) as
   parallel computing architectures.In this paper, we compared the parallel
   Bregman algorithm with traditional CPU realized Bregaman algorithm. In
   addition, we also compared the parallel Bregman algorithm with other CS
   reconstruction algorithms, such as OMP and TwIST algorithms. Compared
   with these two algorithms, the result of this paper shows that, the
   parallel Bregman algorithm needs shorter time, and thus is more
   convenient for real-time object reconstruction, which is important to
   people's fast growing demand to information technology.</abstract><date>2015</date><author>Li, Pengyan
   Ke, Jun
   Sui, Dong
   Wei, Ping</author></paper><paper><title>SPS: A Simulation Tool for Calculating Power of Set-Based Genetic
   Association Tests</title><abstract>Set-based association tests, combining a set of single-nucleotide
   polymorphisms into a unified test, have become important approaches to
   identify weak-effect or low-frequency risk loci of complex diseases.
   However, there is no comprehensive and user-friendly tool to estimate
   power of set-based tests for study design. We developed a simulation
   tool to estimate statistical power of multiple representative set-based
   tests (SPS). SPS has a graphic interface to facilitate parameter
   settings and result visualization. Advanced functions include loading
   real genotypes to define genetic architecture, set-based meta-analysis
   for risk loci with or without heterogeneity, and parallel simulations.
   In proof-of-principle examples, SPS took no more than 3 sec on average
   to estimate the power in a conventional setting. The SPS has been
   integrated into a user-friendly software tool (KGG) as an independent
   functional module and it is freely available at
   http://statgenpro.psychiatry.hku.hk/limx/kgg/. (C) 2015 Wiley
   Periodicals, Inc.</abstract><date>JUL 2015</date><author>Li, Jiang
   Sham, Pak Chung
   Song, Youqiang
   Li, Miaoxin</author></paper><paper><title>Addressing Memory and Speed Problems in Nondestructive Defect
   Characterization: Element-by-Element Processing on a GPU</title><abstract>In nondestructive defect characterization, the genetic algorithm matches
   measurements to computations through finite element optimization of a
   parametrically described defect. This poses a huge computational load. A
   published element-by-element (EbE) conjugate gradients (CG) method for
   the graphics processing unit (GPU) parallelizes thematrix solution
   process. However, in finite element optimization for non-destructive
   evaluation (NDE), thousands of simultaneous finite element matrix
   solutions on parallel genetic algorithm threads on a GPU are required.
   This challenges the memory capabilities of aGPUand results in long
   waiting times making the NDE technique impracticable. To address these
   twin time and memory problems, we revive an old EbE processing by Gauss
   iterations. It yields speedups of 80+ which grow indefinitely with
   matrix size to values much bigger than those reported for EbE CG. For
   the latter the reported figures were less than 4 for the practicable
   matrix size of 91,333, and 96 for a size of over 1 million and falling
   thereafter. Our EbE implementation of CG gives a speedup of 147 for
   first order triangular meshes. No memory issues are faced.</abstract><date>JUN 2015</date><author>Sivasuthan, S.
   Karthik, V. U.
   Rahunanthan, A.
   Jayakumar, P.
   Thyagarajan, R. S.
   Udpa, Lalita
   Hoole, S. R. H.</author></paper><paper><title>Parallel computation of Entropic Lattice Boltzmann method on hybrid
   CPU-GPU accelerated system</title><abstract>Nowadays, an increasing number of researchers have demonstrated a
   tendency to choose hybrid CPU-GPU hybrid computing as a high performance
   computing alternative. Entropic Lattice Boltzmann method (ELBM)
   parallelization, like many parallel algorithms in the field of rapid
   scientific and engineering computing, has given rise to much attention
   for applications of computational fluid dynamics. This study aims to
   present an efficient implementation of ELBM flow simulation for the
   D3Q19 model in a hybrid CPU-GPU computing environment, which consists of
   AMD multi-core CPUs with NVIDIA Graphics Processing Units (GPUs). To
   overcome the GPU memory size limitation and communication overhead, we
   propose a set of techniques for the development of an efficient ELBM
   algorithm for hybrid CPU-GPU computation. Considering the contribution
   of computational capacity for both the CPU and GPU, an efficient load
   balancing model is built. The efficiency and accuracy of the proposed
   approach and established model are tested on a hybrid CPU-GPU
   accelerated system, where the intensive parts of the computation are
   dealt with the software framework OpenMP and CUDA. Finally, we show the
   comparison of resulting computational performance using a hybrid CPU-GPU
   approach against both a single CPU core and a single GPU device. (C)
   2014 Elsevier Ltd. All rights reserved.</abstract><date>MAR 30 2015</date><author>Ye, Yu
   Li, Kenli
   Wang, Yan
   Deng, Tan</author></paper><paper><title>CUDA-optimized cellular automata for diffusion limited processes</title><abstract>In this work diffusion-limited processes are considered in terms of
   cellular automata (CA) approach. This method allows to simulate
   processes in different size scales and levels of accuracy. The
   transition rules allows to take into account main mass transfer
   processes that drive system to thermodynamic equilibrium: diffusion,
   adsorption/desorption and directed flow. CA simulations even in small
   scales show good similarity with real world systems.Proposed model can
   be used for simulations in different processes, such as adsorption in
   porous media and chromatography.Use of parallel computing (process level
   parallelism and GPU-based CUDA technology) in all stages gave from 5x to
   100x speed up in comparison with standard sequential realization.</abstract><date>2015</date><author>Kolnoochenko, Andrey
   Menshutina, Natalia</author></paper><paper><title>Image segmentation using CUDA accelerated non-local means denoising and
   bias correction embedded fuzzy c-means (BCEFCM)</title><abstract>Due to intensity overlaps between interested objects caused by noise and
   intensity inhomogeneity, image segmentation is still an open problem. In
   this paper, we propose a framework to segment images in the well-known
   image model in which intensities of the observed image are viewed as a
   product of the true image and the bias field. In the proposed framework,
   a CUDA accelerated non-local means denoising method is first used to
   remove noise from the image. Then, a bias correction embedded fuzzy
   c-means (BCEFCM) method is proposed to segment the image and correct the
   bias field simultaneously. To ensure the slowly and smoothly varying
   property of the bias field, we convolve it with a normalized kernel as
   soon as it is updated in each iteration. The proposed framework has been
   extensively tested on both selected synthetic and real images and public
   BrainWeb and IBSR datasets. Experimental results and comparison analysis
   demonstrate that the proposed framework is not only able to deal with
   noise and correct the bias field but it is also faster and more accurate
   than state-of-the-art methods. (C) 2015 Elsevier B.V. All rights
   reserved.</abstract><date>MAY 2016</date><author>Feng, Chaolu
   Zhao, Dazhe
   Huang, Min</author></paper><paper><title>Two-stage distributed parallel algorithm with message passing interface
   for maximum flow problem</title><abstract>Maximum flow is one of the important and classical combinatorial
   optimization problems. However, the time complexity of sequential
   maximum flow algorithms remains high. In this paper, we present a
   two-stage distributed parallel algorithm (TSDPA) with message passing
   interface to improve the computational performance. The strategy of
   TSDPA has two stages, which push excess flows separately along cheap and
   expensive paths identified by a new distance estimate function. In
   TSDPA, stage 1 enhances the parallel efficiency by omitting high-cost
   paths and decentralizing calculations, and stage 2 guarantees the
   achievement of an optimal solution through divide-and-conquer method.
   The experimental test demonstrates that TSDPA runs 1.2-15.5 times faster
   than sequential algorithms and is faster than or almost as fast as the
   H_PRF and Q_PRF codes.</abstract><date>FEB 2015</date><author>Jiang, Jincheng
   Wu, Lixin</author></paper><paper><title>Accelerating SWHE Based PIRs Using GPUs</title><abstract>In this work we focus on tailoring and optimizing the computational
   Private Information Retrieval (cPIR) scheme proposed in WAHC 2014 for
   efficient execution on graphics processing units (GPUs). Exploiting the
   mass parallelism inGPUs is a commonly used approach in speeding up
   cPIRs. Our goal is to eliminate the efficiency bottleneck of the Doroz
   et al. construction which would allow us to take advantage of its
   excellent bandwidth performance. To this end, we develop custom code to
   support polynomial ring operations and extend them to realize the
   evaluation functions in an optimized manner on high end GPUs.
   Specifically, we develop optimized CUDA code to support large
   degree/large coefficient polynomial arithmetic operations such
   asmodularmultiplication/reduction, and modulus switching. Moreover, we
   choose same prime numbers for both the CRT domain representation of the
   polynomials and for the modulus switching implementation of the somewhat
   homomorphic encryption scheme. This allows us to combine two arithmetic
   domains, which reduces the number of domain conversions and permits us
   to perform faster arithmetic. Our implementation achieves 14-34 times
   speedup for index comparison and 4-18 times speedup for data aggregation
   compared to a pure CPU software implementation.</abstract><date>2015</date><author>Dai, Wei
   Doroez, Yarkin
   Sunar, Berk</author></paper><paper><title>Time-domain BEM for the wave equation on distributed-heterogeneous
   architectures: A blocking approach</title><abstract>The problem of time-domain BEM for the wave equation in acoustics and
   electromagnetism can be expressed as a sparse linear system composed of
   multiple interaction/convolution matrices. It can be solved by using
   sparse matrix-vector products which are inefficient to achieve high
   Flop-rate neither on CPUs nor GPUs. In this paper we extend the approach
   proposed in a previous work [1] in which we re-order the computation to
   get a special matrix structure with one dense vector per row. This new
   structure is called a slice matrix and is computed with a custom
   matrix/vector product operator. In this study, we present an optimized
   implementation of this operator on Nvidia CPUs based on two blocking
   strategies. We explain how we can obtain multiple block-values from a
   slice and how these can be computed efficiently on CPUs since we target
   heterogeneous nodes composed of CPUs and GPUs. In order to deal with
   different efficiencies of the processing units we use a greedy heuristic
   that dynamically balances work among the workers. We demonstrate the
   performance of our system by studying the quality of the balancing
   heuristic and the sequential Flop-rate of the blocked implementations.
   Finally, we validate our implementation with an industrial test case on
   8 heterogeneous nodes, each composed of 12 CPUs and 3 GPUs. (C) 2015
   Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Bramas, Berenger
   Coulaud, Olivier
   Sylvand, Guillaume</author></paper><paper><title>Rapid Automated Classification of Anesthetic Depth Levels using GPU
   Based Parallelization of Neural Networks</title><abstract>The effect of anesthesia on the patient is referred to as depth of
   anesthesia. Rapid classification of appropriate depth level of
   anesthesia is a matter of great importance in surgical operations.
   Similarly, accelerating classification algorithms is important for the
   rapid solution of problems in the field of biomedical signal processing.
   However numerous, time-consuming mathematical operations are required
   when training and testing stages of the classification algorithms,
   especially in neural networks. In this study, to accelerate the process,
   parallel programming and computing platform (Nvidia CUDA) facilitates
   dramatic increases in computing performance by harnessing the power of
   the graphics processing unit (GPU) was utilized. The system was employed
   to detect anesthetic depth level on related electroencephalogram (EEG)
   data set. This dataset is rather complex and large. Moreover, the
   achieving more anesthetic levels with rapid response is critical in
   anesthesia. The proposed parallelization method yielded high accurate
   classification results in a faster time.</abstract><date>FEB 2015</date><author>Peker, Musa
   Sen, Baha
   Guruler, Huseyin</author></paper><paper><title>Reliability of phantom pain relief in neurorehabilitation using a
   multimodal virtual reality system.</title><abstract>The objective of this study is to demonstrate the reliability of relief
   from phantom limb pain in neurore-habilitation using a multimodal
   virtual reality system. We have developed a virtual reality
   rehabilitation system with multimodal sensory feedback and applied it to
   six patients with brachial plexus avulsion or arm amputation. In an
   experiment, patients executed a reaching task using a virtual phantom
   limb displayed in a three-dimensional computer graphic environment
   manipulated by their real intact limb. The intensity of the phantom limb
   pain was evaluated through a short-form McGill pain questionnaire. The
   experiments were conducted twice on different days at more than
   four-week intervals for each patient. The reliability of our task's
   ability to relieve pain was demonstrated by the test-retest method,
   which checks the degree of the relative similarity between the pain
   reduction rates in two experiments using Fisher's intraclass correlation
   coefficient (ICC). The ICC was 0.737, indicating sufficient
   reproducibility of our task. The average of the reduction rates across
   participants was 50.2%, and it was significantly different from 0 (p &lt;;
   0:001). Overall, our findings indicate that neurorehabilitation using
   our multimodal virtual reality system reduces the phantom limb pain with
   sufficient reliability. </abstract><date>2015-Aug</date><author>Sano, Yuko
   Ichinose, Akimichi
   Wake, Naoki
   Osumi, Michihiro
   Sumitani, Masahiko
   Kumagaya, Shin-Ichiro
   Kuniyoshi, Yasuo</author></paper><paper><title>Parrondo Games with Spatial Dependence, III</title><abstract>We study Toral's Parrondo games with N players and one-dimensional
   spatial dependence as modified by Xie et al. Specifically, we use
   computer graphics to sketch the Parrondo and anti-Parrondo regions for 3
   &lt;= N &lt;= 9. Our work was motivated by a recent paper of Li et al., who
   applied a state space reduction method to this model, reducing the
   number of states from 2(N) to N + 1. We show that their reduced Markov
   chains are inconsistent with the model of Xie et al.</abstract><date>DEC 2015</date><author>Ethier, S. N.
   Lee, Jiyeon</author></paper><paper><title>Regularization Based Iterative Point Match Weighting for Accurate Rigid
   Transformation Estimation</title><abstract>Feature extraction and matching (FEM) for 3D shapes finds numerous
   applications in computer graphics and vision for object modeling,
   retrieval, morphing, and recognition. However, unavoidable incorrect
   matches lead to inaccurate estimation of the transformation relating
   different datasets. Inspired by AdaBoost, this paper proposes a novel
   iterative re-weighting method to tackle the challenging problem of
   evaluating point matches established by typical FEM methods. Weights are
   used to indicate the degree of belief that each point match is correct.
   Our method has three key steps: (i) estimation of the underlying
   transformation using weighted least squares, (ii) penalty parameter
   estimation via minimization of the weighted variance of the matching
   errors, and (iii) weight re-estimation taking into account both matching
   errors and information learnt in previous iterations. A comparative
   study, based on real shapes captured by two laser scanners, shows that
   the proposed method outperforms four other state-of-the-art methods in
   terms of evaluating point matches between overlapping shapes established
   by two typical FEM methods, resulting in more accurate estimates of the
   underlying transformation. This improved transformation can be used to
   better initialize the iterative closest point algorithm and its
   variants, making 3D shape registration more likely to succeed.</abstract><date>SEP 2015</date><author>Liu, Yonghuai
   De Dominicis, Luigi
   Wei, Baogang
   Chen, Liang
   Martin, Ralph R.</author></paper><paper><title>Line recognition algorithm for 3D polygonal model using a parallel
   computing platform</title><abstract>Line recognition-based rendering technique has been used effectively for
   shape transmission of 3D polygon model. Line recognition is defined by
   multifarious forms and characteristics of lines, and has been a
   fundamental key point in expressing shape of 3D polygon model in
   non-photorealistic rendering technique. Line recognition, however,
   requires a long period of calculation time and thus, various methods
   have been studied to accelerate the speed of the operation. This paper
   presents a new method that will accelerate the overall operation
   compared to the standard CPU-based method of extracting ink line. The
   new method will enhance the efficiency of the calculation speed by
   applying the parallel processing technique CUDA (Compute Unified Device
   Architecture) to the complex processes that consume a lot of time such
   as implicit surface calculation and feature point extraction. The
   overall performance will be tested and verified through various types of
   experiments with 3D polygon model.</abstract><date>JAN 2015</date><author>Kang, Ji Hun
   Kang, Shin Jin
   Kim, SooKyun</author></paper><paper><title>GPU accelerated Monte Carlo simulation of Brownian motors dynamics with
   CUDA</title><abstract>This work presents an updated and extended guide on methods of a proper
   acceleration of the Monte Carlo integration of stochastic differential
   equations with the commonly available NVIDIA Graphics Processing Units
   using the CUDA programming environment. We outline the general aspects
   of the scientific computing on graphics cards and demonstrate them with
   two models of a well known phenomenon of the noise induced transport of
   Brownian motors in periodic structures. As a source of fluctuations in
   the considered systems we selected the three most commonly occurring
   noises: the Gaussian white noise, the white Poissonian noise and the
   dichotomous process also known as a random telegraph signal. The
   detailed discussion on various aspects of the applied numerical schemes
   is also presented. The measured speedup can be of the astonishing order
   of about 3000 when compared to a typical CPU. This number significantly
   expands the range of problems solvable by use of stochastic simulations,
   allowing even an interactive research in some cases.Program
   SummaryProgram title: Poisson, dichCatalogue identifier:
   AEVP_v1_0Program summary URL:
   http://cpc.cs.qub.ac.uk/summaries/AEVP_v1_0.htmlProgram obtainable from:
   CPC Program Library, Queen's University, Belfast, N. IrelandLicensing
   provisions: GNU Lesser General Public License. version 3No. of lines in
   distributed program, including test data, etc.: 3338No. of bytes in
   distributed program, including test data, etc.: 45009Distribution
   format: tar.gzProgramming language: CUDA C.Computer: Any with
   CUDA-compliant GPU.Operating system: No limits (tested on Linux).RAM:
   Hundreds of megabytes for typical caseClassification: 4.3, 23.External
   routines: The NVIDIA CUDA Random Number Generation library
   (cuRAND)Nature of problem: Graphics processing unit accelerated
   numerical simulation of stochastic differential equation.Solution
   method: The jump-adapted simplified weak order 2.0 predictor-corrector
   method is employed to integrate the Langevin equation of motion.
   Ensemble-averaged quantities of interest are obtained through averaging
   over multiple independent realizations of the system generated by means
   of the Monte Carlo method.Unusual features: The actual numerical
   simulation runs exclusively on the graphics processing unit using the
   CUDA environment. This allows for a speedup as large as about 3000 when
   compared to a typical CPU.Running time: A few seconds (C) 2015 Elsevier
   B.V. All rights reserved.</abstract><date>JUN 2015</date><author>Spiechowicz, J.
   Kostur, M.
   Machura, L.</author></paper><paper><title>DualSPHysics: Open-source parallel CFD solver based on Smoothed Particle
   Hydrodynamics (SPH)</title><abstract>DualSPHysics is a hardware accelerated Smoothed Particle Hydrodynamics
   code developed to solve freesurface flow problems. DualSPHysics is an
   open-source code developed and released under the terms of GNU General
   Public License (GPLv3). Along with the source code, a complete
   documentation that makes easy the compilation and execution of the
   source files is also distributed. The code has been shown to be
   efficient and reliable. The parallel power computing of Graphics
   Computing Units (CPUs) is used to accelerate DualSPHysics by up to two
   orders of magnitude compared to the performance of the serial
   version.Program summaryProgram title: DualSPHysicsCatalogue identifier:
   AEUS_v1_0Program summary URL:
   http://cpc.cs.qub.ac.uk/summaries/AEUS_v1_0.htmlProgram obtainable from:
   CPC Program Library, Queen's University, Belfast, N. IrelandLicensing
   provisions: GNU General Public LicenseNo. of lines in distributed
   program, including test data, etc.: 121,399No. of bytes in distributed
   program, including test data, etc.: 12,324,308Distribution format:
   tar.gz Programming language: C++ and CUDA.Computer: Tested on CPU Intel
   X5500 and GPUs: GTX 480, GTX 680, Tesla 1(20 and GTX Titan.Operating
   system: Any system with a C++ and NVCC compiler, tested on Linux
   distribution Centos 6.5CUDA: Tested on versions 4.0, 4.1, 4.2, 5.0 and
   5.5 with driver version 331.38.Has the code been vectorised or
   parallelised?: Different threads of CPU or number of cores of GPU.RAM:
   Tens of MB to several GB, depending on problemClassification:
   4.12.Nature of problem:The DualSPHysics code has been developed to study
   free-surface flows requiring high computational cost.Solution
   method:DualSPHysics is an implementation of Smoothed Particle
   Hydrodynamics, which is a Lagrangian meshless particle method.Running
   time:6 h on 8 processors of Intel X5500 (15 min on GTX Titan) for the
   dam-break case with 1 million particles simulating 1.5 s of physical
   time (more than 26,000 steps). (C) 2014 Elsevier B.V. All rights
   reserved.</abstract><date>FEB 2015</date><author>Crespo, A. J. C.
   Dominguez, J. M.
   Rogers, B. D.
   Gomez-Gesteira, M.
   Longshaw, S.
   Canelas, R.
   Vacondio, R.
   Barreiro, A.
   Garcia-Feal, O.</author></paper><paper><title>In vitro and in silico studies of urea-induced denaturation of yeast
   iso-1-cytochrome c and its deletants at pH 6.0 and 25 degrees C</title><abstract>Yeast iso-1-cytochrome c (y-cyt-c) has five extra residues at N-terminus
   in comparison to the horse cytochrome c. These residues are numbered as
   -5 to -1. Here, these extra residues are sequentially removed from
   y-cyt-c to establish their role in folding and stability of the protein.
   We performed urea-induced denaturation of wild-type (WT) y-cyt-c and its
   deletants. Denaturation was followed by observing change in Delta
   epsilon(405) (probe for measuring change in the heme environment within
   the protein), [theta](405) (probe for measuring the change in Phe82 and
   Met80 axial bonding), [theta](222) (probe for measuring change in
   secondary structure) and [theta](416) (probe for measuring change in the
   heme-methionine environment). The urea-induced reversible denaturation
   curves were used to estimate Delta[GRAPHICS], the value of Gibbs free
   energy change (Delta G(D)) in the absence of urea; C-m, the midpoint of
   the denaturation curve, i.e. molar urea concentration ([urea]) at which
   Delta G(D)=0; and m, the slope (= partial differential Delta G(D)/
   partial differential [urea]). Our in vitro results clearly show that
   except Delta(-5/-4) all deletants are less stable than WT protein.
   Coincidence of normalized transition curves of all physical properties
   suggests that unfolding/refolding of WT protein and its deletants is a
   two-state process. To confirm our in vitro observations, we performed
   40ns MD simulation of both WT y-cyt-c and its deletants. MD simulation
   results clearly show that extra N-terminal residues play a role in
   stability but not in folding of the protein.</abstract><date>JUL 3 2015</date><author>Haque, Md Anzarul
   Zaidi, Sobia
   Ubaid-ullah, Shah
   Prakash, Amresh
   Hassan, Md Imtaiyaz
   Islam, Asimul
   Batra, Janendra K.
   Ahmad, Faizan</author></paper><paper><title>L-2 and pointwise a posteriori error estimates for FEM for elliptic PDEs
   on surfaces</title><abstract>Surface finite element methods (SFEMs) are widely used to solve surface
   partial differential equations arising in applications including crystal
   growth, fluid mechanics and computer graphics. A posteriori error
   estimators are computable measures of the error and are used to
   implement adaptive mesh refinement. Previous studies of a posteriori
   error estimation in SFEM have mainly focused on bounding energy norm
   errors. In this work, we derive a posteriori L-2 and pointwise error
   estimates for piecewise linear SFEM for the Laplace-Beltrami equation on
   implicitly defined surfaces. There are two main error sources in SFEM, a
   `Galerkin error' arising in the usual way for finite element methods,
   and a 'geometric error' arising from replacing the continuous surface by
   a discrete approximation when writing the finite element equations. Our
   work includes numerical estimation of the dependence of the error bounds
   on the geometric properties of the surface. We provide also numerical
   experiments where the estimators have been used to implement an adaptive
   FEM over surfaces with different curvatures.</abstract><date>JUL 2015</date><author>Camacho, Fernando
   Demlow, Alan</author></paper><paper><title>EBSDinterp 1.0: A MATLAB((R)) Program to Perform Microstructurally
   Constrained Interpolation of EBSD Data</title><abstract>EBSDinterp is a graphic user interface (GUI)-based MATLAB (R) program to
   perform microstructurally constrained interpolation of nonindexed
   electron backscatter diffraction data points. The area available for
   interpolation is restricted using variations in pattern quality or band
   contrast (BC). Areas of low BC are not available for interpolation, and
   therefore cannot be erroneously filled by adjacent grains growing into
   them. Points with the most indexed neighbors are interpolated first and
   the required number of neighbors is reduced with each successive round
   until a minimum number of neighbors is reached. Further iterations allow
   more data points to be filled by reducing the BC threshold. This method
   ensures that the best quality points (those with high BC and most
   neighbors) are interpolated first, and that the interpolation is
   restricted to grain interiors before adjacent grains are grown together
   to produce a complete microstructure. The algorithm is implemented
   through a GUI, taking advantage of MATLAB (R)'s parallel processing
   toolbox to perform the interpolations rapidly so that a variety of
   parameters can be tested to ensure that the final microstructures are
   robust and artifact-free. The software is freely available through the
   CSIRO Data Access Portal (doi:10.4225/08/5510090C6E620) as both a
   compiled Windows executable and as source code.</abstract><date>AUG 2015</date><author>Pearce, Mark A.</author></paper><paper><title>Case study of multiple trace transform implementations</title><abstract>Scientific algorithms are designed and implemented in a variety of
   programming languages. Depending on the exact application, some
   languages are a better choice than others: some offer a productive
   environment while others focus on performance. Selecting a language is
   often difficult, with poor choices resulting in much higher development
   times. By implementing a case study algorithm in multiple programming
   languages, we compare their pros and cons. As a case study, we selected
   the trace transform, an image processing algorithm from the widely used
   class of integral transforms. We describe each implementation, including
   a highly optimized version for NVIDIA graphics processing units, and
   present a productivity overview and an in-depth performance analysis,
   from which we draw more generic conclusions. We have found that MATLAB
   is still the best choice overall, but Julia proves an interesting
   emerging choice. For realistic images, our compute unified device
   architecture (CUDA) implementation offers the best performance, albeit
   at a high development cost.</abstract><date>NOV 2015</date><author>Besard, Tim
   De Sutter, Bjorn
   Frias-Velazquez, Andres
   Philips, Wilfried</author></paper><paper><title>The informed sampler: A discriminative approach to Bayesian inference in
   generative computer vision models</title><abstract>Computer vision is hard because of a large variability in lighting,
   shape, and texture; in addition the image signal is non-additive due to
   occlusion. Generative models promised to account for this variability by
   accurately modelling the image formation process as a function of latent
   variables with prior beliefs. Bayesian posterior inference could then,
   in principle, explain the observation. While intuitively appealing,
   generative models for computer vision have largely failed to deliver on
   that promise due to the difficulty of posterior inference. As a result
   the community has favoured efficient discriminative approaches. We still
   believe in the usefulness of generative models in computer vision, but
   argue that we need to leverage existing discriminative or even heuristic
   computer vision methods. We implement this idea in a principled way with
   an informed sampler and in careful experiments demonstrate it on
   challenging generative models which contain renderer programs as their
   components. We concentrate on the problem of inverting an existing
   graphics rendering engine, an approach that can be understood as
   "Inverse Graphics". The informed sampler, using simple discriminative
   proposals based on existing computer vision technology, achieves
   significant improvements of inference. (C) 2015 Elsevier Inc. All rights
   reserved.</abstract><date>JUL 2015</date><author>Jampani, Varun
   Nowozin, Sebastian
   Loper, Matthew
   Gehler, Peter V.</author></paper><paper><title>TuCCompi: A Multi-layer Model for Distributed Heterogeneous Computing
   with Tuning Capabilities</title><abstract>During the last decade, parallel processing architectures have become a
   powerful tool to deal with massively-parallel problems that require high
   performance computing (HPC). The last trend of HPC is the use of
   heterogeneous environments, that combine different computational
   processing devices, such as CPU-cores and graphics processing units
   (GPUs). Maximizing the performance of any GPU parallel implementation of
   an algorithm requires an in-depth knowledge about the GPU underlying
   architecture, becoming a tedious manual effort only suited for
   experienced programmers. In this paper, we present TuCCompi, a
   multi-layer abstract model that simplifies the programming on
   heterogeneous systems including hardware accelerators, by hiding the
   details of synchronization, deployment, and tuning. TuCCompi chooses
   optimal values for their configuration parameters using a kernel
   characterization provided by the programmer. This model is very useful
   to tackle problems characterized by independent, high computational-load
   independent tasks, such as embarrassingly-parallel problems. We have
   evaluated TuCCompi in different, real-world, heterogeneous environments
   using the all-pair shortest-path problem as a case study.</abstract><date>OCT 2015</date><author>Ortega-Arranz, Hector
   Torres, Yuri
   Gonzalez-Escribano, Arturo
   Llanos, Diego R.</author></paper><paper><title>Solving Parker's transport equation with stochastic differential
   equations on GPUs</title><abstract>The numerical solution of transport equations for energetic charged
   particles in space is generally very costly in terms of time. Besides
   the use of multi-core CPUs and computer clusters in order to decrease
   the computation times, high performance calculations on graphics
   processing units (CPUs) have become available during the last years. In
   this work we introduce and describe a CPU-accelerated implementation of
   Parker's equation using Stochastic Differential Equations (SDEs) for the
   simulation of the transport of energetic charged particles with the CUDA
   toolkit, which is the focus of this work. We briefly discuss the set of
   SDEs arising from Parker's transport equation and their application to
   boundary value problems such as that of the Jovian magnetosphere. We
   compare the runtimes of the GPU code with a CPU version of the same
   algorithm. Compared to the CPU implementation (using OpenMP and eight
   threads) we find a performance increase of about a factor of 10-60,
   depending on the assumed set of parameters. Furthermore, we benchmark
   our simulation using the results of an existing SDE implementation of
   Parker's transport equation. (C) 2015 Elsevier B.V. All rights reserved.</abstract><date>JUL 2015</date><author>Dunzlaff, P.
   Strauss, R. D.
   Potgieter, M. S.</author></paper><paper><title>Extended Kalman Filter-Based Parallel Dynamic State Estimation</title><abstract>There is a growing need for accurate and efficient real-time state
   estimation with increasing complexity, interconnection, and insertion of
   new devices in power systems. In this paper, a massively parallel
   dynamic state estimator is developed on a graphic processing unit (GPU),
   which is especially designed for processing large data sets. Within the
   massively parallel framework, a lateral two-level dynamic state
   estimator is proposed based on the extended Kalman filter method,
   utilizing both supervisory control and data acquisition, and phasor
   measurement unit (PMU) measurements. The measurements at the buses
   without PMU installations are predicted using previous data. The results
   of the GPU-based dynamic state estimator are compared with a multithread
   CPU-based code. Moreover, the effects of direct and iterative linear
   solvers on the state estimation algorithm are investigated. The
   simulation results show a total speed-up of up to 15 times for a
   4992-bus system.</abstract><date>MAY 2015</date><author>Karimipour, Hadis
   Dinavahi, Venkata</author></paper><paper><title>A localized meshless method for diffusion on folded surfaces</title><abstract>Partial differential equations (PDEs) on surfaces arise in a variety of
   application areas including biological systems, medical imaging, fluid
   dynamics, mathematical physics, image processing and computer graphics.
   In this paper, we propose a radial basis function (RBF) discretization
   of the closest point method. The corresponding localized meshless method
   may be used to approximate diffusion on smooth or folded surfaces. Our
   method has the benefit of having an a priori error bound in terms of
   percentage of the norm of the solution. A stable solver is used to avoid
   the ill-conditioning that arises when the radial basis functions (RBFs)
   become flat. (C) 2015 Elsevier Inc. All rights reserved.</abstract><date>SEP 15 2015</date><author>Cheung, Ka Chun
   Ling, Leevan
   Ruuth, Steven J.</author></paper><paper><title>HIPA(cc) : A Domain-Specific Language and Compiler for Image Processing</title><abstract>Domain-specific languages (DSLs) provide high-level and domain-specific
   abstractions that allow expressive and concise algorithm descriptions.
   Since the description in a DSL hides also the properties of the target
   hardware, DSLs are a promising path to target different parallel and
   heterogeneous hardware from the same algorithm description. In theory,
   the DSL description can capture all characteristics of the algorithm
   that are required to generate highly efficient parallel implementations.
   However, most frameworks do not make use of this knowledge and the
   performance cannot reach that of optimized library implementations. In
   this article, we present the HIPA(cc) framework, a DSL and
   source-to-source compiler for image processing. We show that domain
   knowledge can be captured in the language and that this knowledge
   enables us to generate tailored implementations for a given target
   architecture. Back ends for CUDA, OpenCL, and Renderscript allow us to
   target discrete graphics processing units (GPUs) as well as mobile,
   embedded GPUs. Exploiting the captured domain knowledge, we can generate
   specialized algorithm variants that reach the maximal achievable
   performance due to the peak memory bandwidth. These implementations
   outperform state-of-the-art domain-specific languages and libraries
   significantly.</abstract><date>JAN 2016</date><author>Membarth, Richard
   Reiche, Oliver
   Hannig, Frank
   Teich, Juergen
   Koerner, Mario
   Eckert, Wieland</author></paper><paper><title>Implementation of Massive FDTD Simulation Computing Model Based on MPI
   Cluster for Semi-conductor Process</title><abstract>In the semi-conductor process, a simulation process is performed to
   detect defects by analyzing the behavior of the impurity through the
   physical quantity calculation of the inner element. In order to perform
   the simulation, Finite-Difference Time-Domain(FDTD) algorithm is used.
   The improvement of semiconductor which is composed of nanoscale
   elements, the size of simulation is getting bigger. Problems that a
   processor such as CPU or GPU cannot perform the simulation due to the
   massive size of matrix or a computer consist of multiple processors
   cannot handle a massive FDTD may come up. For those problems, studies
   are performed with parallel/distributed computing. However, in the past,
   only single type of processor was used. In GPU's case, it performs fast,
   but at the same time, it has limited memory. On the other hand, in CPU,
   it performs slower than that of GPU. To solve the problem, we
   implemented a computing model that can handle any FDTD simulation
   regardless of size on the cluster which consist of heterogeneous
   processors. We tested the simulation on processors using MPI libraries
   which is based on ''point to point' communication and verified that it
   operates correctly regardless of the number of node and type. Also, we
   analyzed the performance by measuring the total execution time and
   specific time for the simulation on each test.</abstract><date>2015</date><author>???
   ???
   ???</author></paper><paper><title>NAS Parallel Benchmarks for GPGPUs Using a Directive-Based Programming
   Model</title><abstract>The broad adoption of accelerators boosts the interest in accelerator
   programming. Accelerators such as GPGPUs are optimized for throughput
   and offer high GFLOPS and memory bandwidth. CUDA has been adopted quite
   rapidly but it is proprietary and only applicable to GPUs, and the
   difficulty in writing efficient CUDA code has kindled the necessity to
   create higher-level programming approaches such as OpenACC.
   Directive-based programming models such as OpenMP and OpenACC offer
   programmers an option to rapidly create prototype applications by adding
   annotations to guide compiler optimizations. In this paper we study the
   effectiveness of a high-level directive based programming model,
   OpenACC, for parallelizing NAS Parallel Benchmarks (NPB) on GPGPUs. We
   present the application of techniques such as array privatization,
   memory coalescing, cache optimization and examine their impact on the
   performance of the benchmarks. The right choice or combination of
   techniques/hints are crucial for compilers to generate highly efficient
   codes tuned to a particular type of accelerator. Poorly selected choice
   or combination of techniques can lead to degraded performance. We also
   propose a new clause, 'scan', that handles scan operations for arbitrary
   input array size. We hope that the practices discussed in this paper
   will provide useful guidance to users to effectively migrate their
   sequential/CPU-parallel codes to GPGPU architectures and achieve optimal
   performance.</abstract><date>2015</date><author>Xu, Rengan
   Tian, Xiaonan
   Chandrasekaran, Sunita
   Yan, Yonghong
   Chapman, Barbara</author></paper><paper><title>Aligning the unalignable: bacteriophage whole genome alignments</title><abstract>Background: In recent years, many studies focused on the description and
   comparison of large sets of related bacteriophage genomes. Due to the
   peculiar mosaic structure of these genomes, few informative approaches
   for comparing whole genomes exist: dot plots diagrams give a mostly
   qualitative assessment of the similarity/dissimilarity between two or
   more genomes, and clustering techniques are used to classify genomes.
   Multiple alignments are conspicuously absent from this scene. Indeed,
   whole genome aligners interpret lack of similarity between sequences as
   an indication of rearrangements, insertions, or losses. This behavior
   makes them ill-prepared to align bacteriophage genomes, where even
   closely related strains can accomplish the same biological function with
   highly dissimilar sequences.Results: In this paper, we propose a
   multiple alignment strategy that exploits functional collinearity shared
   by related strains of bacteriophages, and uses partial orders to capture
   mosaicism of sets of genomes. As classical alignments do, the computed
   alignments can be used to predict that genes have the same biological
   function, even in the absence of detectable similarity. The Alpha
   aligner implements these ideas in visual interactive displays, and is
   used to compute several examples of alignments of Staphylococcus aureus
   and Mycobacterium bacteriophages, involving up to 29 genomes. Using
   these datasets, we prove that Alpha alignments are at least as good as
   those computed by standard aligners. Comparison with the progressive
   Mauve aligner - which implements a partial order strategy, but whose
   alignments are linearized - shows a greatly improved interactive graphic
   display, while avoiding misalignments.Conclusions: Multiple alignments
   of whole bacteriophage genomes work, and will become an important
   conceptual and visual tool in comparative genomics of sets of related
   strains. A python implementation of Alpha, along with installation
   instructions for Ubuntu and OSX, is available on bitbucket
   (https://bitbucket.org/thekswenson/alpha).</abstract><date>JAN 13 2016</date><author>Berard, Severine
   Chateau, Annie
   Pompidor, Nicolas
   Guertin, Paul
   Bergeron, Anne
   Swenson, Krister M.</author></paper><paper><title>Speeding up the high-accuracy surface modelling method with GPU</title><abstract>In order to find a solution for accurate, topographic data-demanding
   applications, such as catchment hydrologic modeling and assessments of
   anthropic activities impact on environmental systems, high-accuracy
   surface modeling (HASM) method is developed. Although it can produce a
   digital elevation model (DEM) surface of higher accuracy than classical
   methods, e.g. inverse distance weighted, spline and kriging, HASM
   requires numerous iterations to solve large linear systems, which impede
   its applications in high-resolution and large-scale surface
   interpolation. This paper aims to demonstrate the utilization of
   graphics' processing units (GPUs) device to accelerate HASM in
   constructing large-scale and high-resolution DEM surfaces. We
   parallelized the linear system algorithm for solving HASM with Compute
   Unified Device Architecture, a parallel programming model developed by
   NVIDIA. We designed a memory-saving strategy to enable the HASM
   algorithm to run on GPUs. The speedup ratio of GPU-based algorithm was
   tested and compared with CPU-based algorithm through simulations of both
   ideal Gaussian synthetic surface and real topographic surface in the
   loess plateau of Gansu province. The GPU-parallelized algorithm can
   attain an over 10x speedup ratio with the CPU-based algorithm as a
   reference. The speedup ratio increases with the scale and resolution of
   the dataset. The memory management strategy efficiently reduces the
   memory usage by more than eight times the grid cell number. Implementing
   HASM in the GPUs device enables modeling large-scale and high-resolution
   surfaces in a reasonable time period and implies the potential benefits
   from the use of GPUs as massive, parallel co-processors for
   arithmetic-intensive data-processing applications.</abstract><date>OCT 2015</date><author>Yan, Changqing
   Zhao, Gang
   Yue, Tianxiang
   Chen, Chuanfa
   Liu, Jimin
   Li, Han
   Su, Na</author></paper><paper><title>Toward an Efficient Integral Multi-agent Sensor Network Generic
   Simulation System Design</title><abstract>This research deals with several key issues concerning WSN design
   process. Due to the nature of these networks, debugging after deployment
   is unlikely, thus, a well-organized testing methodology is required. WSN
   simulators could achieve such a task, but still code implementing mote
   sensing and RF behavior consists of layered and/or interacting protocols
   that for the sake of designing accuracy are tested working as a whole,
   running on specific hardware. Simulators that provide cross-layer
   simulation and hardware cross-platform emulation options may be regarded
   as an important milestone for improved WSN design process. The herein
   proposed multi-agent simulation architecture aims at designing a novel
   WSN simulation system independent of specific hardware platforms but
   taking into account all hardware entities and events for testing and
   analyzing the behavior of a pragmatic WSN system. Details of the
   implementation are provided as well as a preliminary validation of the
   simulator in C#.</abstract><date>2016</date><author>Filippou, A.
   Karras, D. A.</author></paper><paper><title>Optimizing the computation of a parallel 3D finite difference algorithm
   for graphics processing units</title><abstract>This paper explores the possibilities of using a graphics processing
   unit for complex 3D finite difference computation via MUSTA-FORCE and
   WENO algorithms. We propose a novel algorithm based on the new
   properties of CUDA surface memory optimized for 2D spatial locality and
   compare it with 3D stencil computations carried out via shared memory,
   which is currently considered to be the best approach. A case study was
   performed for the extensive generation of a time series of 3D grids of
   arbitrary size used in the computation of collisions between heavy
   nuclei in terms of relativistic hydrodynamics. It proved that
   implementation based on surface memory is as much as 23% faster than an
   equivalent implementation using shared memory. Copyright (c) 2014 John
   Wiley &amp; Sons, Ltd.</abstract><date>APR 25 2015</date><author>Porter-Sobieraj, J.
   Cygert, S.
   Kikola, D.
   Sikorski, J.
   Slodkowski, M.</author></paper><paper><title>Combining sigma-lognormal modeling and classical features for analyzing
   graphomotor performances in kindergarten children</title><abstract>This paper investigates the advantage of using the kinematic theory of
   rapid human movements as a complementary approach to those based on
   classical dynamical features to characterize and analyze kindergarten
   children's ability to engage in graphomotor activities as a preparation
   for handwriting learning. This study analyzes nine different movements
   taken from 48 children evenly distributed among three different school
   grades corresponding to pupils aged 3, 4, and 5 years. On the one hand,
   our results show that the ability to perform graphomotor activities
   depends on kindergarten grades. More importantly, this study shows which
   performance criteria, from sophisticated neuromotor modeling as well as
   more classical kinematic parameters, can differentiate children of
   different school grades. These criteria provide a valuable tool for
   studying children's graphomotor control learning strategies. On the
   other hand, from a practical point of view, it is observed that school
   grades do not clearly reflect pupils' graphomotor performances. This
   calls for a large-scale investigation, using a more efficient
   experimental design based on the various observations made throughout
   this study regarding the choice of the graphic shapes, the number of
   repetitions and the features to analyze. (C) 2015 Elsevier B.V. All
   rights reserved.</abstract><date>OCT 2015</date><author>Duval, Theresa
   Remi, Celine
   Plamondon, Rejean
   Valliant, Jean
   O'Reilly, Christian</author></paper><paper><title>Materials Screening Through GPU Accelerated Topological Mapping</title><abstract>Selecting materials with properties tailored to a specific application
   may be accelerated through materials informatics, but kinetic properties
   whose calculations are too computationally intensive to be incorporated
   into materials screening must be replaced with appropriate descriptors.
   Here we present a highly optimized method for general processing on
   graphics processing hardware through which we map the interstitial
   subspace of atomic structures that are used as a qualitative predictor
   for diffusivity. Additionally, analytical methods that determine the
   largest channel diameter and identify the optimal path through a
   material are proposed to characterize this topology. Analysis of the
   interstitial subspace, along with the theoretical capacities for Li
   ions, has lead us to select high-capacity lithium ion battery (LIB)
   materials that display both promising capacities and migration pathways
   able to support lithium insertion and removal. The result is the
   identification of Li2MgSi, a LIB anode material with a theoretical
   capacity of 1023 Ah/kg, from an unfiltered set of 1,754 structures.</abstract><date>APR 3 2015</date><author>Kappes, Branden B.
   Ciobanu, Cristian V.</author></paper><paper><title>GPU-Acceleration of Blowfish Cryptographic Algorithm</title><abstract>The demand for fast and secure cryptographic encryption techniques has
   been growing over the recent years. The introduction of the CUDA
   programming framework by NVIDIA allowed utilizing GPUs for general
   purpose computing. The use of GPUs in the cryptography domain has become
   quite popular ever since. In this paper, we present a CUDA
   implementation of the blowfish encryption algorithm. The algorithm has
   been designed to make use of the unified memory model introduced as a
   part of CUDA 6. Experimental results show that the unified
   implementation of the blowfish algorithm performs better than an
   efficient CPU implementation by a factor of 62X and performs twice
   better than a non-unified CUDA implementation of the algorithm. We also
   present possible solutions to achieve an optimal configuration for the
   target GPU to get the best performance.</abstract><date>2015</date><author>Earanky, Kapil
   Elmiligi, Haytham
   Rahman, Musfiq</author></paper><paper><title>A Parallel Algorithm Using Perlin Noise Superposition Method for Terrain
   Generation Based on CUDA architecture</title><abstract>A parallel algorithm for terrain generation based on CUDA architecture
   is proposed in this paper, which aims to address the problems of high
   computational load and low efficiency when generating large scale
   terrains using the Perlin noise superposition method. The Perlin noise
   superposition method is combined with independent calculation of each
   point based on the characteristics of all adjacent points. The Perlin
   noise value of each terrain grid point is transferred to a GPU thread
   for calculation, so that the terrain generation process is executed in
   completely parallel in the GPU. Experimental results show that The GPU
   algorithm generates a grid of size 25000000 (25 million grid points)
   needs only 0.6355 s, while the original CPU algorithm takes 23.3723 s,
   so, the parallel processing algorithm can improve the efficiency of the
   terrain generation and meet the requirements for large-scale terrain
   generation compared with the original algorithm.</abstract><date>2015</date><author>Li, Huailiang
   Tuo, Xianguo
   Liu, Yao
   Jiang, Xin</author></paper><paper><title>Performance Evaluation of a Human Immune System Simulator on a GPU
   Cluster</title><abstract>The Human Immune System (HIS) is a complex system that protects the body
   against several diseases. Some aspects of such complex system can be
   better understand with the use of mathematical and computational tools.
   Huge computational resources are required to execute simulations of the
   HIS, so the use of parallel environments is mandatory. This work
   presents a parallel implementation of a 3D HIS simulator on a GPU
   cluster that uses CUDA, OpenMP and MPI to speedup the execution of the
   application. A performance evaluation is then carried out, and the
   impact of the use of InfiniBand, a low latency network, and GPU's
   Error-Correcting Code (ECC) are measured. Speedups up to 956 were
   obtained by the parallel version that uses Infiniband and turns off ECC.</abstract><date>2015</date><author>Soares, Thiago M.
   Xavier, Micael P.
   Pigozzo, Alexandre B.
   Campos, Ricardo Silva
   dos Santos, Rodrigo W.
   Lobosco, Marcelo</author></paper><paper><title>Accelerating Search of Protein Sequence Databases Using CUDA-Enabled GPU</title><abstract>Searching databases of protein sequences for those proteins that match
   patterns represented as profile HMMs is a widely performed
   bioinformatics task. The standard tool for the task is HMMER version 3
   from Sean Eddy. HMMER3 achieved significant improvements in performance
   over version 2 through the introduction of a heuristic filter called the
   Multiple Segment Viterbi algorithm (MSV) and the use of native SIMD
   instruction set on modern CPUs. Our objective was to further improve
   performance by using a general-purpose graphical processing unit (GPU)
   and the CUDA software environment from Nvidia.An execution profile of
   HMMER3 identifies the MSV filter as a code hotspot that consumes over
   75% of the total execution time. We applied a number of well-known
   optimization strategies for coding GPUs in order to implement a CUDA
   version of the MSV filter.The results show that our implementation
   achieved 1.8x speedup over the single-threaded HMMER3 CPU SSE2
   implementation on average. The experiments used a modern Kepler
   architecture GPU from Nvidia that has 768 cores running at 811 Mhz and
   an Intel Core i7-3960X 3.3GHz CPU overclocked at 4.6GHz.For HMMER2 there
   was a significant speed-up of an order of magnitude obtained by
   implementations using GPUs. Such gains seem out of reach for HMMER3.</abstract><date>2015</date><author>Cheng, Lin
   Butler, Greg</author></paper><paper><title>ACCELERATION of GENERALIZED ADAPTIVE PULSE COMPRESSION with PARALLEL
   GPUs</title><abstract>Super-computing based on Graphic Processing Unit (GPU) has become a
   booming field both in research and industry. In this paper, GPU is
   applied as the main computing device on traditional RADAR super
   resolution algorithms. Comparison is provided between GPU and CPU as
   computing architecture and MATLAB, as a widely used scientific
   implementation, is also included as well as C++ implementation in
   demonstrations of CPU part in the comparison. Fundamental RADAR
   algorithms as matched filter and least square estimation (LSE) are used
   as standard procedure to measure the efficiency of each implementation.
   Based on the result in this paper, GPU shows an enormous potential to
   expedite the traditional process of RADAR super-resolution applications.</abstract><date>2015</date><author>Cai, Jingxiao
   Zhang, Yan (Rockee)</author></paper><paper><title>Development of a decision support system based on neural networks and a
   genetic algorithm</title><abstract>Given ever increasing information volume and complexity of engineering,
   social and economic systems, it has become more difficult to assess
   incoming data and manage such systems properly. Currently developed
   innovative decision support systems (DSS) aim to achieve optimum results
   while minimizing the risks of serious losses. The purpose of the DSS is
   to help the decision-maker facing the problem of huge amounts of data
   and ambiguous reactions of complicated systems depending on external
   factors. By means of accurate and profound analysis, DSSs are expected
   to provide the user with precisely forecasted indicators and optimal
   decisions.In this paper we suggest a new DSS structure which could be
   used in a wide range of difficult to formalize tasks and achieve a high
   speed of calculation and decision-making.We examine different approaches
   to determining the dependence of a target variable on input data and
   review the most common statistical forecasting methods. The advantages
   of using neural networks for this purpose are described. We suggest
   applying interval neural networks for calculations with underdetermined
   (interval) data, which makes it possible to use our DSS in a wide range
   of complicated tasks. We developed a corresponding learning algorithm
   for the interval neural networks. The advantages of using a genetic
   algorithm (GA) to select the most significant inputs are shown. We
   justify the use of general-purpose computing on graphics processing
   units (GPGPU) to achieve high-speed calculations with the decision
   support system in question. A functional diagram of the system is
   presented and described. The results and samples of the DSS application
   are demonstrated. (C) 2015 Elsevier Ltd. All rights reserved.</abstract><date>SEP 2015</date><author>Bukharov, Oleg E.
   Bogolyubov, Dmitry P.</author></paper><paper><title>GPU accelerated training of image convolution filter weights using
   genetic algorithms</title><abstract>Genetic algorithms (GA) provide an efficient method for training filters
   to find proper weights using a fitness function where the input signal
   is filtered and compared with the desired output. In the case of image
   processing applications, the high computational cost of the fitness
   function that is evaluated repeatedly can cause training time to be
   relatively long. In this study, a new algorithm, called sub-image blocks
   based on graphical processing units (GPU), is developed to accelerate
   the training of mask weights using GA. The method is developed by
   discussing other alternative design considerations, including direct
   method (DM), population-based method (PBM), block-based method (BBM),
   and sub-images-based method (SBM). A comparative performance evaluation
   of the introduced methods is presented using sequential and other GPUs.
   Among the discussed designs, SBM provides the best performance by taking
   advantage of the block shared and thread local memories in GPU.
   According to execution duration and comparative acceleration graphs, SBM
   provides approximately 55-90 times more acceleration using GeForce GTX
   660 over sequential implementation on a 3.5 GHz processor. (C) 2015
   Elsevier B.V. All rights reserved.</abstract><date>MAY 2015</date><author>Akgun, Devrim
   Erdogmus, Pakize</author></paper><paper><title>Progressive Motion Vector Clustering for Motion Estimation and Auxiliary
   Tracking</title><abstract>The motion vector similarity between neighboring blocks is widely used
   in motion estimation algorithms. However, for nonneighboring blocks,
   they may also have similar motions due to close depths or belonging to
   the same object inside the scene. Therefore, the motion vectors usually
   have several kinds of patterns, which reveal a clustering structure. In
   this article, we propose a progressive clustering algorithm, which
   periodically counts the motion vectors of the past blocks to make
   incremental clustering statistics. These statistics are used as the
   motion vector predictors for the following blocks. It is proved to be
   much more efficient for one block to find the best-matching candidate
   with the predictors. We also design the clustering based search with
   CUDA for GPU acceleration. Another interesting application of the
   clustering statistics is persistent static object tracking. Based on the
   statistics, several auxiliary tracking areas are created to guide the
   object tracking. Even when the target object has significant changes in
   appearance or it disappears occasionally, its position still can be
   predicted. The experiments on Xiph.org Video Test Media dataset
   illustrate that our clustering based search algorithm outperforms the
   mainstream and some state-of-the-art motion estimation algorithms. It is
   33 times faster on average than the full search algorithm with only
   slightly higher mean-square error values in the experiments. The
   tracking results show that the auxiliary tracking areas help to locate
   the target object effectively.</abstract><date>JAN 2015</date><author>Chen, Ke
   Zhou, Zhong
   Wu, Wei</author></paper><paper><title>Physically-based smoke simulation for computer graphics: a survey</title><abstract>We present an up-to-date survey on physically-based smoke simulation.
   Physically-based method becomes predominant in smoke simulation in
   computer graphics community. It prevails over traditional methods for
   its plausible visual effect. Significant results have been carried out
   over past two decades. We give a latest overview of state-of-the-art of
   smoke simulation and also compare various techniques according to their
   characteristics. We discuss several issues in terms of computational
   efficiency, numerical stability, numerical dissipation, and runtime
   performance. A number of open challenging problems are also addressed
   for further exploration.</abstract><date>SEP 2015</date><author>Huang, Zhanpeng
   Gong, Guanghong
   Han, Liang</author></paper><paper><title>Tablet and phone applications--A reflection on the experience of
   development.</title><abstract>Tablet devices are now ubiquitous. Medical illustrators have the skills
   to produce a wide range of media content. These devices offer the
   potential of using their creative abilities in new and exciting ways.
   There is much to explore. The primary difficulty lies in understanding
   the necessary computer technical skills to realise a vision. </abstract><date>2015-Jun</date><author>Edwards, Simon
   Winckles, Derek
   Leonard, Mark</author></paper><paper><title>Designing Planar Deployable Objects via Scissor Structures</title><abstract>Scissor structure is used to generate deployable objects for
   space-saving in a variety of applications, from architecture to
   aerospace science. While deployment from a small, regular shape to a
   larger one is easy to design, we focus on a more challenging task:
   designing a planar scissor structure that deploys from a given source
   shape into a specific target shape. We propose a two-step constructive
   method to generate a scissor structure from a high-dimensional parameter
   space. Topology construction of the scissor structure is first performed
   to approximate the two given shapes, as well as to guarantee the
   deployment. Then the geometry of the scissor structure is optimized in
   order to minimize the connection deflections and maximize the shape
   approximation. With the optimized parameters, the deployment can be
   simulated by controlling an anchor scissor unit. Physical deployable
   objects are fabricated according to the designed scissor structures by
   using 3D printing or manual assembly. We show a number of results for
   different shapes to demonstrate that even with fabrication errors, our
   designed structures can deform fluently between the source and target
   shapes.</abstract><date>FEB 2016</date><author>Zhang, Ran
   Wang, Shiwei
   Chen, Xuejin
   Ding, Chao
   Jiang, Luo
   Zhou, Jie
   Liu, Ligang</author></paper><paper><title>Whiteboard: a framework for the programmatic visualization of complex
   biological analyses</title><abstract>A Summary: Whiteboard is a class library implemented in C++ that enables
   visualization to be tightly coupled with computation when analyzing
   large and complex datasets.</abstract><date>JUN 15 2015</date><author>Sundstrom, Gorel
   Zamani, Neda
   Grabherr, Manfred G.
   Mauceli, Evan</author></paper><paper><title>GPU-Accelerated Reconstruction of T2 Maps in Magnetic Resonance Imaging</title><abstract>The main tissue parameters targeted by MR tomography include, among
   others, relaxation times T-1 and T-2. This paper focuses on the
   computation of the relaxation time T-2 measured with the Spin Echo
   method, where the sensing coil of the tomograph provides a multi-echo
   signal. The maxima of these echoes must be interleaved with an
   exponential function, and the T-2 relaxation can be determined directly
   from the exponential waveform. As this procedure needs to be repeated
   for each pixel of the scanned tissue, the processing of large images
   then becomes very intensive. For example, given the common resolution of
   256x256 with 20 slices and five echoes at different times T-E, it is
   necessary to reconstruct 1.3.10(6) exponential functions. At present,
   such computation performed on a regular PC may last even several
   minutes. This paper introduces the results provided by accelerated
   computation based on parallelization and carried out with a graphics
   card. By using the simple method of linear regression, we obtain a
   processing time of less than 36 ms. Another effective option consists in
   the Levenberg-Marquardt algorithm, which enables us to reconstruct the
   same image in 96 ms. This period is at least 900 times shorter than that
   achievable with professional software. In this context, the paper also
   comprises an analysis of the results provided by the above-discussed
   techniques.</abstract><date>AUG 2015</date><author>Mikulka, Jan</author></paper><paper><title>POM.gpu-v1.0: a GPU-based Princeton Ocean Model</title><abstract>Graphics processing units (GPUs) are an attractive solution in many
   scientific applications due to their high performance. However, most
   existing GPU conversions of climate models use GPUs for only a few
   computationally intensive regions. In the present study, we redesign the
   mpiPOM (a parallel version of the Princeton Ocean Model) with GPUs.
   Specifically, we first convert the model from its original Fortran form
   to a new Compute Unified Device Architecture C (CUDA-C) code, then we
   optimize the code on each of the GPUs, the communications between the
   GPUs, and the I / O between the GPUs and the central processing units
   (CPUs). We show that the performance of the new model on a workstation
   containing four GPUs is comparable to that on a powerful cluster with
   408 standard CPU cores, and it reduces the energy consumption by a
   factor of 6.8.</abstract><date>2015</date><author>Xu, S.
   Huang, X.
   Oey, L. -Y.
   Xu, F.
   Fu, H.
   Zhang, Y.
   Yang, G.</author></paper><paper><title>Efficient BSP/CGM Algorithms for the Maximum Subarray Sum and Related
   Problems</title><abstract>Given an n x n array A of integers, with at least one positive value,
   the maximum subarray sum problem consists in finding the maximum sum
   among the sums of all rectangular subarrays of A. The maximum subarray
   problem appears in several scientific applications, particularly in
   Computer Vision. The algorithms that solve this problem have been used
   to help the identification of the brightest regions of the images used
   in astronomy and medical diagnosis. The best known sequential algorithm
   that solves this problem has O(n(3)) time complexity. In this work we
   revisit the BSP/CGM parallel algorithm that solves this problem and we
   present BSP/CGM algorithms for the following related problems: the
   maximum largest subarray sum, the maximum smallest subarray sum, the
   number of subarrays of maximum sum, the selection of the subarray with
   k-maximum sum and the location of the subarray with the maximum relative
   density sum. To the best of our knowledge there are no parallel BSP/CGM
   algorithms for these related problems. Our algorithms use p processors
   and require O (n(3)/p) parallel time with a constant number of
   communication rounds. In order to show the applicability of our
   algorithms, we have implemented them on a cluster of computers using MPI
   and on a machine with GPGPU using CUDA and OpenMP. We have obtained good
   speedup results in both environments. We also tested the maximum
   relative density sum algorithm with a image of the cancer imaging
   archive.</abstract><date>2015</date><author>Lima, Anderson C.
   Branco, Rodrigo G.
   Caceres, Edson N.</author></paper><paper><title>The Potential of a Text-Based Interface as a Design Medium: An
   Experiment in a Computer Animation Environment</title><abstract>Since the birth of the concept of direct manipulation, the graphical
   user interface has been the dominant means of controlling digital
   objects. In this research, we hypothesize that the benefits of a
   text-based interface involve multiple tradeoffs, and we explore the
   potential of text as a medium of design from three perspectives: (i) the
   perceived level of control of the designed object, (ii) a tool for
   realizing creative ideas and (iii) an effective form for a highly
   learnable user interface. Our experiment in a computer animation
   environment shows that (i) participants did feel a high level of control
   of characters, (ii) creativity was both restricted and facilitated
   depending on the task and (iii) natural language expedited the learning
   of a new interface language. Our research provides experimental proof of
   the effect of a text-based interface and offers guidelines for the
   design of future computer-aided design applications.</abstract><date>JAN 2016</date><author>Lee, Sangwon
   Yan, Jin</author></paper><paper><title>The Human Face as a Dynamic Tool for Social Communication</title><abstract>As a highly social species, humans frequently exchange social
   information to support almost all facets of life. One of the richest and
   most powerful tools in social communication is the face, from which
   observers can quickly and easily make a number of inferences about
   identity, gender, sex, age, race, ethnicity, sexual orientation,
   physical health, attractiveness, emotional state, personality traits,
   pain or physical pleasure, deception, and even social status. With the
   advent of the digital economy, increasing globalization and cultural
   integration, understanding precisely which face information supports
   social communication and which produces misunderstanding is central to
   the evolving needs of modern society (for example, in the design of
   socially interactive digital avatars and companion robots). Doing so is
   challenging, however, because the face can be thought of as comprising a
   high-dimensional, dynamic information space, and this impacts cognitive
   science and neuroimaging, and their broader applications in the digital
   economy. New opportunities to address this challenge are arising from
   the development of new methods and technologies, coupled with the
   emergence of a modern scientific culture that embraces
   cross-disciplinary approaches. Here, we briefly review one such approach
   that combines state-of-the-art computer graphics, psychophysics and
   vision science, cultural psychology and social cognition, and highlight
   the main knowledge advances it has generated. In the light of current
   developments, we provide a vision of the future directions in the field
   of human facial communication within and across cultures.</abstract><date>JUL 20 2015</date><author>Jack, Rachael E.
   Schyns, Philippe G.</author></paper><paper><title>VDJtools: Unifying Post-analysis of T Cell Receptor Repertoires</title><abstract>Despite the growing number of immune repertoire sequencing studies, the
   field still lacks software for analysis and comprehension of this
   high-dimensional data. Here we report VDJtools, a complementary software
   suite that solves a wide range of T cell receptor (TCR) repertoires
   post-analysis tasks, provides a detailed tabular output and
   publication-ready graphics, and is built on top of a flexible API. Using
   TCR datasets for a large cohort of unrelated healthy donors, twins, and
   multiple sclerosis patients we demonstrate that VDJtools greatly
   facilitates the analysis and leads to sound biological conclusions.
   VDJtools software and documentation are available at
   https://github.com/mikessh/vdjtools.</abstract><date>NOV 2015</date><author>Shugay, Mikhail
   Bagaev, Dmitriy V.
   Turchaninova, Maria A.
   Bolotin, Dmitriy A.
   Britanova, Olga V.
   Putintseva, Ekaterina V.
   Pogorelyy, Mikhail V.
   Nazarov, Vadim I.
   Zvyagin, Ivan V.
   Kirgizova, Vitalina I.
   Kirgizov, Kirill I.
   Skorobogatova, Elena V.
   Chudakov, Dmitriy M.</author></paper><paper><title>From action icon to knowledge icon: Objective-oriented icon taxonomy in
   computer science</title><abstract>Icon plays a critical role in computer interface design. Studies on icon
   taxonomy explain the way in which various types of icon represent the
   objects and provide designers creation rules by which icons are more in
   line with users' cognitive psychology. However, along with larger and
   larger use of icons, the previous classification criterion causes the
   boundary between categories blur. What's more, Single classification
   standard is not able to well illustrate the icons applied in today's
   computer applications. The purpose of this paper is to present an
   objective-oriented icon taxonomy which proposes to categorize icons into
   action icon and knowledge icon. To assess this proposition, we analyzed
   a sample of icons that applied in computer interface and suggest precise
   application domains to both action icon and knowledge icon categories.
   The results of this practice manifested that action icon and knowledge
   icon implied a high relation with applied environment and explicated the
   development trace of computer icons. This work is one of the first to
   point out the notion of knowledge icon and to highlight the importance
   of objective of icon application. Findings in this paper could enrich
   icon use in computer interface design, especially provides possible way
   to improve online knowledge sharing by visual tool like icon. (C) 2015
   Elsevier B.V. All rights reserved.</abstract><date>OCT 2015</date><author>Ma, Xiaoyue
   Matta, Nada
   Cahier, Jean-Pierre
   Qin, Chunxiu
   Cheng, Yanjie</author></paper><paper><title>Optimizing CUDA code by kernel fusion: application on BLAS</title><abstract>Contemporary GPUs have significantly higher arithmetic throughput than a
   memory throughput. Hence, many GPU kernels are memory bound and cannot
   exploit arithmetic power of the GPU. Examples of memory-bound kernels
   are BLAS-1 (vector-vector) and BLAS-2 (matrix-vector) operations.
   However, when kernels share data, kernel fusion can improve memory
   locality by placing shared data, originally passed via off-chip global
   memory, into a faster, but distributed on-chip memory. In this paper, we
   show how kernels performing map, reduce or their nested combinations can
   be fused automatically by our source-to-source compiler. To demonstrate
   the usability of the compiler, we have implemented several BLAS-1 and
   BLAS-2 routines and show how the performance of their sequences can be
   improved by fusions. Compared with similar sequences using CUBLAS, our
   compiler is able to generate code that is up to 2.24 faster for the
   examples tested.</abstract><date>OCT 2015</date><author>Filipovic, Jiri
   Madzin, Matus
   Fousek, Jan
   Matyska, Ludek</author></paper><paper><title>Introducing parallelism to histogramming functions for GEM systems</title><abstract>This article is an assessment of potential parallelization of
   histogramming algorithms in GEM detector system. Histogramming and
   preprocessing algorithms in MATLAB were analyzed with regard to adding
   parallelism. Preliminary implementation of parallel strip histogramming
   resulted in speedup. Analysis of algorithms parallelizability is
   presented. Overview of potential hardware and software support to
   implement parallel algorithm is discussed.</abstract><date>2015</date><author>Krawczyk, Rafal D.
   Czarski, Tomasz
   Kolasinski, Piotr
   Pozniak, Krzysztof T.
   Linczuk, Maciej
   Byszuk, Adrian
   Chernyshova, Maryna
   Juszczyk, Bartlomiej
   Kasprowicz, Grzegorz
   Wojenski, Andrzej
   Zabolotny, Wojciech</author></paper><paper><title>A low-latency, big database system and browser for storage, querying and
   visualization of 3D genomic data</title><abstract>Recent releases of genome three-dimensional (3D) structures have the
   potential to transform our understanding of genomes. Nonetheless, the
   storage technology and visualization tools need to evolve to offer to
   the scientific community fast and convenient access to these data. We
   introduce simultaneously a database system to store and query 3D genomic
   data (3DBG), and a 3D genome browser to visualize and explore 3D genome
   structures (3DGB). We benchmark 3DBG against state-of-the-art systems
   and demonstrate that it is faster than previous solutions, and
   importantly gracefully scales with the size of data. We also illustrate
   the usefulness of our 3D genome Web browser to explore human genome
   structures. The 3D genome browser is available at
   http://3dgb.cs.mcgill.ca/.</abstract><date>SEP 18 2015</date><author>Butyaev, Alexander
   Mavlyutov, Ruslan
   Blanchette, Mathieu
   Cudre-Mauroux, Philippe
   Waldispuehl, Jerome</author></paper><paper><title>Effects of season on ecological processes in extensive earthen tilapia
   ponds in Southeastern Brazil</title><abstract>In Southeastern Brazil tilapia culture is conducted in extensive and
   semi-intensive flow-through earthen ponds, being water availability and
   flow management different in the rainy and dry seasons. In this region
   lettuce wastes are a potential cheap input for tilapia culture. This
   study examined the ecological processes developing during the rainy and
   dry seasons in three extensive flow-through earthen tilapia ponds
   fertilized with lettuce wastes. Water quality, plankton and sediment
   parameters were sampled monthly during a year. Factor analysis was used
   to identify the ecological processes occurring within the ponds and to
   construct a conceptual graphic model of the pond ecosystem functioning
   during the rainy and dry seasons. Processes related to nitrogen cycling
   presented differences between both seasons while processes related to
   phosphorus cycling did not. Ecological differences among ponds were due
   to effects of wind protection by surrounding vegetation, organic loading
   entering, tilapia density and its grazing pressure on zooplankton.
   Differences in tilapia growth among ponds were related to stocking
   density and ecological process affecting tilapia food availability and
   intraspecific competition. Lettuce wastes addition into the ponds did
   not produce negative effects, thus this practice may be considered a
   disposal option and a low-cost input source for tilapia, at least at the
   amounts applied in this study.</abstract><date>NOV 2015</date><author>Favaro, E. G. P.
   Sipauba-Tavares, L. H.
   Milstein, A.</author></paper><paper><title>Process Tracing Analysis of Hurricane Information Displays</title><abstract>To study people's processing of hurricane forecast advisories, we
   conducted a computer-based experiment that examined 11 research
   questions about the information seeking patterns of students assuming
   the role of a county emergency manager in a sequence of six hurricane
   forecast advisories for each of four different hurricanes. The results
   show that participants considered a variety of different sources of
   information-textual, graphic, and numeric-when tracking hurricanes.
   Click counts and click durations generally gave the same results but
   there were some significant differences. Moreover, participants'
   information search strategies became more efficient over forecast
   advisories and with increased experience tracking the four hurricanes.
   These changes in the search patterns from the first to the fourth
   hurricane suggest that the presentation of abstract principles in a
   training manual was not sufficient for them to learn how to track
   hurricanes efficiently but they were able to significantly improve their
   search efficiency with a modest amount (roughly an hour) of practice.
   Overall, these data indicate that information search patterns are
   complex and deserve greater attention in studies of dynamic decision
   tasks.</abstract><date>DEC 2015</date><author>Wu, Hao-Che
   Lindell, Michael K.
   Prater, Carla S.</author></paper><paper><title>GPU-accelerated ADI-PE method for analysis of EM scatterings</title><abstract>The parabolic equation (PE) can be solved in one dimension by using the
   alternating direction implicit (ADI) scheme. In this way, the reduced
   scattered fields can be calculated line by line in each transverse
   plane. Thus both the CPU time and the memory requirement can be largely
   reduced. In this reported work, the ADI-PE method is adapted to graphics
   processing units (GPUs) implementation with the CUDA. The processes of
   both the matrix-filling and the equation-solving can be accelerated by
   the CUDA-based GPU. A couple of numerical results are given to
   demonstrate that the speedup of over 87 times can be obtained by the GPU
   implementation when compared with the CPU one.</abstract><date>OCT 8 2015</date><author>He, Zi
   Chen, Ru Shan</author></paper><paper><title>Reducing Dose Uncertainty for Spot-Scanning Proton Beam Therapy of
   Moving Tumors by Optimizing the Spot Delivery Sequence</title><abstract>Purpose: To develop and validate a novel delivery strategy for reducing
   the respiratory motion-induced dose uncertainty of spot-scanning proton
   therapy.Methods and Materials: The spot delivery sequence was optimized
   to reduce dose uncertainty. The effectiveness of the delivery sequence
   optimization was evaluated using measurements and patient simulation.
   One hundred ninety-one 2-dimensional measurements using different
   delivery sequences of a single-layer uniform pattern were obtained with
   a detector array on a 1-dimensional moving platform. Intensity modulated
   proton therapy plans were generated for 10 lung cancer patients, and
   dose uncertainties for different delivery sequences were evaluated by
   simulation.Results: Without delivery sequence optimization, the maximum
   absolute dose error can be up to 97.2% in a single measurement, whereas
   the optimized delivery sequence results in a maximum absolute dose error
   of &lt;= 11.8%. In patient simulation, the optimized delivery sequence
   reduces the mean of fractional maximum absolute dose error compared with
   the regular delivery sequence by 3.3% to 10.6% (32.5-68.0% relative
   reduction) for different patients.Conclusions: Optimizing the delivery
   sequence can reduce dose uncertainty due to respiratory motion in
   spot-scanning proton therapy, assuming the 4-dimensional CT is a true
   representation of the patients' breathing patterns. (C) 2015 Elsevier
   Inc. All rights reserved.</abstract><date>NOV 1 2015</date><author>Li, Heng
   Zhu, X. Ronald
   Zhang, Xiaodong</author></paper><paper><title>Fast Detection of Transformed Data Leaks</title><abstract>The leak of sensitive data on computer systems poses a serious threat to
   organizational security. Statistics show that the lack of proper
   encryption on files and communications due to human errors is one of the
   leading causes of data loss. Organizations need tools to identify the
   exposure of sensitive data by screening the content in storage and
   transmission, i.e., to detect sensitive information being stored or
   transmitted in the clear. However, detecting the exposure of sensitive
   information is challenging due to data transformation in the content.
   Transformations (such as insertion and deletion) result in highly
   unpredictable leak patterns. In this paper, we utilize sequence
   alignment techniques for detecting complex data-leak patterns. Our
   algorithm is designed for detecting long and inexact sensitive data
   patterns. This detection is paired with a comparable sampling algorithm,
   which allows one to compare the similarity of two separately sampled
   sequences. Our system achieves good detection accuracy in recognizing
   transformed leaks. We implement a parallelized version of our algorithms
   in graphics processing unit that achieves high analysis throughput. We
   demonstrate the high multithreading scalability of our data leak
   detection method required by a sizable organization.</abstract><date>MAR 2016</date><author>Shu, Xiaokui
   Zhang, Jing
   Yao, Danfeng (Daphne)
   Feng, Wu-Chun</author></paper><paper><title>SequenceCEROSENE: a computational method and web server to visualize
   spatial residue neighborhoods at the sequence level</title><abstract>Background: To understand the molecular function of biopolymers,
   studying their structural characteristics is of central importance.
   Graphics programs are often utilized to conceive these properties, but
   with the increasing number of available structures in databases or
   structure models produced by automated modeling frameworks this process
   requires assistance from tools that allow automated structure
   visualization. In this paper a web server and its underlying method for
   generating graphical sequence representations of molecular structures is
   presented.Results: The method, called SequenceCEROSENE (color encoding
   of residues obtained by spatial neighborhood embedding), retrieves the
   sequence of each amino acid or nucleotide chain in a given structure and
   produces a color coding for each residue based on three-dimensional
   structure information. From this, color-highlighted sequences are
   obtained, where residue coloring represent three-dimensional residue
   locations in the structure. This color encoding thus provides a
   one-dimensional representation, from which spatial interactions,
   proximity and relations between residues or entire chains can be deduced
   quickly and solely from color similarity. Furthermore, additional
   heteroatoms and chemical compounds bound to the structure, like ligands
   or coenzymes, are processed and reported as well.To provide free access
   to SequenceCEROSENE, a web server has been implemented that allows
   generating color codings for structures deposited in the Protein Data
   Bank or structure models uploaded by the user. Besides retrieving
   visualizations in popular graphic formats, underlying raw data can be
   downloaded as well. In addition, the server provides user interactivity
   with generated visualizations and the three-dimensional structure in
   question.Conclusions: Color encoded sequences generated by
   SequenceCEROSENE can aid to quickly perceive the general characteristics
   of a structure of interest (or entire sets of complexes), thus
   supporting the researcher in the initial phase of structure-based
   studies. In this respect, the web server can be a valuable tool, as
   users are allowed to process multiple structures, quickly switch between
   results, and interact with generated visualizations in an intuitive
   manner.The SequenceCEROSENE web server is available at
   https://biosciences.hs-mittweida.de/seqcerosene.</abstract><date>JAN 27 2016</date><author>Heinke, Florian
   Bittrich, Sebastian
   Kaiser, Florian
   Labudde, Dirk</author></paper><paper><title>Acceleration of integration in parametric integral equations system
   using CUDA</title><abstract>Methods of solving boundary value problems are expected to achieve high
   accuracy results in shortest possible time of calculations. In the
   previous papers the authors solved boundary value problems with high
   accuracy using parametric integral equations system. However, time of
   calculations was unsatisfactory. The most time consuming part of the
   method is calculation of integrals. In this paper the authors present
   approach to accelerate numerical integration in PIES by nVidia CUDA. The
   speed of integration increased up to 250 times whereas high accuracy of
   solutions was maintained. Examples included in this paper concern
   solving three-dimensional elasticity problems. (C) 2015 Elsevier Ltd.
   All rights reserved.</abstract><date>MAY 2015</date><author>Kuzelewski, Andrzej
   Zieniuk, Eugeniusz
   Kapturczak, Marta</author></paper><paper><title>Improving CUDA DNA Analysis Software with Genetic Programming</title><abstract>We genetically improve BarraCUDA using a BNF grammar incorporating C
   scoping rules with GP. Barracuda maps next generation DNA sequences to
   the human genome using the Burrows-Wheeler algorithm (BWA) on nVidia
   Tesla parallel graphics hardware (GPUs). GI using phenotypic tabu search
   with manually grown code can graft new features giving more than 100
   fold speed up on a performance critical kernel without loss of accuracy.</abstract><date>2015</date><author>Langdon, William B.
   Lam, Brian Yee Hong
   Petke, Justyna
   Harman, Mark</author></paper><paper><title>DensToolKit: A comprehensive open-source package for analyzing the
   electron density and its derivative scalar and vector fields</title><abstract>DensToolKit is a suite of cross-platform, optionally parallelized,
   programs for analyzing the molecular electron density (p) and several
   fields derived from it. Scalar and vector fields, such as the gradient
   of the electron density (del rho), electron localization function (ELF)
   and its gradient, localized orbital locator (LOL), region of slow
   electrons (RoSE), reduced density gradient, localized electrons detector
   (LED), information entropy, molecular electrostatic potential, kinetic
   energy densities K and G, among others, can be evaluated on zero, one,
   two, and three dimensional grids. The suite includes a program for
   searching critical points and bond paths of the electron density, under
   the framework of Quantum Theory of Atoms in Molecules. DensToolKit also
   evaluates the momentum space electron density on spatial grids, and the
   reduced density matrix of order one along lines joining two arbitrary
   atoms of a molecule. The source code is distributed under the GNU-GPLv3
   license, and we release the code with the intent of establishing an
   open-source collaborative project. The style of DensToolKit's code
   follows some of the guidelines of an object-oriented program. This
   allows us to supply the user with a simple manner for easily implement
   new scalar or vector fields, provided they are derived from any of the
   fields already implemented in the code. In this paper, we present some
   of the most salient features of the programs contained in the suite,
   some examples of how to run them, and the mathematical definitions of
   the implemented fields along with hints of how we optimized their
   evaluation. We benchmarked our suite against both a freely-available
   program and a commercial package. Speed-ups of similar to 2x, and up to
   12x were obtained using a non-parallel compilation of DensToolKit for
   the evaluation of fields. DensToolKit takes similar times for finding
   critical points, compared to a commercial package. Finally, we present
   some perspectives for the future development and growth of the
   suite.Program summaryProgram title: DensToolKitCatalogue identifier:
   AEXI_v1_0 Program summary
   RL:http://cpc.cs.qub.ac.uk/summaries/AEXI_v1_0.htmlProgram obtainable
   from: CPC Program Library, Queen's University, Belfast, N.
   IrelandLicensing provisions: GNU, General Public License, version 3No.
   of lines in distributed program, including test data, etc.: 142037No. of
   bytes in distributed program, including test data, etc.:
   5517409Distribution format: tar.gzProgramming language: C++,
   bash.Computer: Any.Operating system: Linux, MacOSX, Windows
   (cygwin).RAM: The memory requirements grow quadratically with the number
   of primitives describing the wavefunction. A wavefunction with 1,500
   primitives uses similar to 17MB, and 2GB RAM are enough to process
   wavefunctions of around 10,000 primitives. A few more MB may be needed
   by some of the most demanding programs of the package if the number of
   primitives is large.Classification: 6.5, 7.3, 16, 16.1.External
   routines: (optional) gnuplot, povray, epstool, Graphics-Magick,
   epstopdfNature of problem: The study of the electron density of
   molecules, some reactivity indices, and the topology of the electron
   density can be used to analyze the chemical nature, stability and
   reactivity of those molecules. Furthermore, the study of the electron
   density and functionals of it may help us in gaining a better
   understanding of the chemical bond. Reactivity indices and the molecular
   topological properties may also aid in molecular design.Solution method:
   The suite provides several programs in order to compute scalar and
   vector fields derivatives of the electron density. Those fields are
   obtained from a wavefunction file, which is in turn obtained from
   programs such as Nwchem, MolPro, etc. The functions, whereby the fields
   are computed, are implemented following mathematically standard but
   computationally optimized and parallelized algorithms built upon the
   Density Matrix. The suite provides several small but efficient programs,
   easily scriptable, for evaluation of the fields upon spatial grids.
   Regarding the topology analysis, the suite uses the algorithm proposed
   by Popelier, which uses the eigen-values of the Hessian of the electron
   density for locating the critical points. Bond paths are traced using a
   fifth-order Runge-Kutta-Dormand-Prince algorithm. Optional visualization
   of the produced data can be carried out by scripts generated by the
   suite, which can be parsed later to gnuplot, or povray. In addition,
   DensToolKit provides an open door for the user to program new scalar or
   vector fields, with almost complete functionality for evaluating such
   fields upon the same spatial grids as those implemented for the fields
   already provided in the suite.Restrictions: Wavefunctions with more than
   99 nuclei must be input in wfx format. In the current version,
   wavefunctions that use pseudopotentials must have only one Additional
   Electron Density Function (EDF) entry in the wfx file, which in this
   case is the only accepted input format, and pseudopotential support is
   provided only in non-parallel compilation.Additional comments: A simple
   method for implementing new indices (derived from any of the implemented
   fields) is provided. In this manner, the final user may easily program
   his/her own scalar or vector field with a few code lines.Running time:
   Strongly dependent on the number of primitives used for approximating
   the wavefunction (similar to N-p(2)). It also depends on the evaluated
   number of points and type of field. Wavefunctions comprised of 1,500
   primitives may take several hours to complete, while small molecules
   described by two or three hundred primitives take a few seconds. Typical
   running times are at least as fast as the times taken by some commercial
   or freely available codes. In many cases, the programs perform the
   computations with a speed-up of 2x (with respect to other available
   programs), and in some cases 10 x speed-ups or more can be attained. (C)
   2015 Elsevier B.V. All rights reserved.</abstract><date>NOV 2015</date><author>Solano-Altamirano, J. M.
   Hernandez-Perez, Julio M.</author></paper><paper><title>Root growth of maize as studied with minirhizotrons and monolith methods</title><abstract>The root minirhizotron technique (MT) has been used to monitor
   nondestructively the root growth of field crops. The objective of this
   study was to compare the ability of MT to measure the root length
   density (RLD) of maize (Zea mays L.) as compared to the quadrate
   monolith method (QMM). The experiment was conducted at the Gucheng
   Ecological-Meteorological Experiment Station in China during the summer
   of 2007. RLD was estimated using MT and QMM. Results showed that the
   vertical distribution of RLD decreased top-down gradually, starting at
   the top of the root zone. The growth rate of RLD decreased as soil depth
   increased based on both methods. RLD was underestimated by MT at depths
   between 0 and 40cm at the milk and maturity stages and overestimated by
   MT at depths between 0 and 20cm at the jointing and tasseling stages.
   There was a significant correlation (r(2)=0.715,[GRAPHICS]= 0.05)
   between RLD estimates based on QMM and MT. The results of this study
   indicate that properly calibrated MT is a reliable method to screen
   nondestructively the root growth of maize.</abstract><date>OCT 3 2015</date><author>Liao, Rongwei
   Bai, Yueming
   Liang, Hong
   An, Shunqing
   Ren, Sanxue
   Cao, Yujing
   Le, Zhangyan
   Lu, Jianli
   Liu, Jingmiao</author></paper><paper><title>A shape deformation algorithm for constrained multidimensional scaling</title><abstract>We present a new Euclidean embedding technique based on volumetric shape
   registration. Extrinsic representation of the intrinsic geometry of a
   shape is preferable in various computer graphics applications as it
   poses only a small degrees of freedom to deal with during processing. A
   popular Euclidean embedding approach to achieve such a representation is
   multidimensional scaling (MDS), which, however, distorts the original
   geometric details drastically. Our method introduces a constraint on the
   original MDS formulation in order, to preserve the initial geometric
   details while the input shape is pulled towards its MDS pose using the
   perfectly accurate bijection in between. The regularizer of this
   registration framework is chosen in such a way that the system supports
   large deformations yet remains fast. Consequently, we produce a
   detail-preserving MDS pose in 90 s for a 53 K-vertex high-resolution
   mesh on a modest computer. We can also add pairwise point constraints on
   the deforming shape without any additional cost. Detail-preserving MDS
   is superior for non-rigid shape retrieval and useful for shape
   segmentation, as demonstrated. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>DEC 2015</date><author>Sahillioglu, Yusuf</author></paper><paper><title>Discrete element simulation of mill charge in 3D using the BLAZE-DEM GPU
   framework</title><abstract>The Discrete Element Method (DEM) simulation of charge motion in ball,
   semi-autogenous (SAG) and autogenous mills has advanced to a stage where
   the effects of lifter design, power draft and product size can be
   evaluated with sufficient accuracy using either two-dimensional (2D) or
   three-dimensional (3D) codes. While 2D codes may provide a reasonable
   profile of charge distribution in the mill there is a difference in
   power estimations as the anisotropic nature within the mill cannot be
   neglected. Thus 3D codes are preferred as they can provide a more
   accurate estimation of power draw and charge distribution. While 2D
   codes complete a typical industrial simulation in the order of hours, 3D
   codes require computing times in the order of days to weeks on a typical
   multi-threaded desktop computer. A newly developed and recently
   introduced 3D DEM simulation environment is BLAZE-DEM that utilizes the
   Graphical Processor Unit (GPU) via the NVIDIA CUDA programming model.
   Utilizing the parallelism of the GPU a 3D simulation of an industrial
   mill with four million particles takes 1 h to simulate one second (20
   FPS) on a GTX 880 laptop GPU. This new performance level may allow 3D
   simulations to become a routine task for mill designers and researchers.
   This paper makes two notable extensions to the BLAZE-DEM environment.
   Firstly, the sphere-face contact is extended to include a GPU efficient
   sphere-edge contact strategy. Secondly, the world representation is
   extended by an efficient representation of convex geometrical primitives
   that can be combined to form non-convex world boundaries that
   drastically enhances the efficiency of particle world contact. In
   addition to these extensions this paper verifies and validates our GPU
   code by comparing charge profiles and power draw obtained using the CPU
   based code Millsoft and pilot scale experiments. Finally, we conclude
   with plant scale mill simulations. (C) 2015 Elsevier Ltd. All rights
   reserved.</abstract><date>AUG 2015</date><author>Govender, Nicolin
   Rajamani, Raj K.
   Kok, Schalk
   Wilke, Daniel N.</author></paper><paper><title>Master-Slave Optical Coherence Tomography for Parallel Processing,
   Calibration Free and Dispersion Tolerance Operation</title><abstract>We present further improvements on the Master Slave (MS) interferometry
   method since our first communication [1]. In this paper, we present more
   data collection and additionally demonstrate an important feature of the
   MS method, that of tolerance to dispersion. MS interferometry produces
   the interference of a selected point in depth based on principles of
   spectral domain (SD) interferometry, but without the need of a Fast
   Fourier transformation (FFT). The method can be used to directly produce
   en-face optical coherence tomography (OCT) images but also as a tool to
   accurately measure distances in low coherence interferometry for sensing
   applications [1]. In the MS-OCT method, cross-correlation is applied to
   both methods of SD-OCT, spectrometer based (SP) or swept source (SS)
   OCT. The channelled spectrum provided by an OCT system is correlated
   with the signal produced by reading a stored mask. Several such masks
   can be used simultaneously. The masks operate as adaptive filters. Each
   mask (filter) determines recognition in the measured channelled spectrum
   delivered by the interferometer, of the pattern corresponding to each
   optical path difference to be recognized. The method presents net
   advantages in comparison with the classical method of producing axial
   reflectivity profiles by FFT: no need for resampling of data,
   possibility to tailor the trade-off between depth resolution and
   sensitivity. Here, using a swept source, the MS method is used to obtain
   axial reflectivity profiles, which are compared to the axial profiles
   obtained by calibration of data and FFT. The tolerance to dispersion of
   the MS method was assumed in [1] but not demonstrated. Here,
   measurements are performed to demonstrate its axial resolution
   independence on dispersion.</abstract><date>2015</date><author>Bradu, Adrian
   Kapinchev, Konstantin
   Barnes, Fred
   Podoleanu, Adrian Gh.</author></paper><paper><title>An Autotuning Engine for the 3D Fast Wavelet Transform on Clusters with
   Hybrid CPU plus GPU Platforms</title><abstract>This work presents an optimization method to run the 3D-fast wavelet
   transform (3D-FWT) on a CPU + GPU system. The optimization engine
   detects the different computing components in the system, and executes
   the appropriate kernel implemented in both CUDA or OpenCL for GPUs, and
   programmed with pthreads for a CPU. This engine automatically selects
   parameters such as the block size, the work-group size or the number of
   threads to reduce the execution time, and sends proportionally different
   parts of a video sequence to run concurrently in all the computing
   components of the system. An analysis of the development and
   optimization of the 3D-FWT for a hybrid cluster of CPU + GPUs is also
   described. Different parallel programming paradigms (message passing,
   shared memory and GPU SIMD) are combined to fully exploit the computing
   capacity of the different computational elements of the cluster, so
   resulting in an efficient combination of basic codes developed
   previously for individual components (CPUs or GPUs) and an important
   reduction of the compression time of long video sequences.</abstract><date>DEC 2015</date><author>Bernabe, Gregorio
   Cuenca, Javier
   Gimenez, Domingo</author></paper><paper><title>Self-adaptive Multiprecision Preconditioners on Multicore and Manycore
   Architectures</title><abstract>Based on the premise that preconditioners needed for scientific
   computing are not only required to be robust in the numerical sense, but
   also scalable for up to thousands of light-weight cores, we argue that
   this two-fold goal is achieved for the recently developed self-adaptive
   multi-elimination preconditioner. For this purpose, we revise the
   underlying idea and analyze the performance of implementations realized
   in the PARALUTION and MAGMA open-source software libraries on GPU
   architectures (using either CUDA or OpenCL), Intel's Many Integrated
   Core Architecture, and Intel's Sandy Bridge processor. The comparison
   with other well-established preconditioners like multi-coloured
   Gauss-Seidel, ILU(0) and multi-colored ILU(0), shows that the twofold
   goal of a numerically stable cross-platform performant algorithm is
   achieved.</abstract><date>2015</date><author>Anzt, Hartwig
   Lukarski, Dimitar
   Tomov, Stanimire
   Dongarra, Jack</author></paper><paper><title>Improving High-Performance GPU Graph Traversal with Compression</title><abstract>Traversing huge graphs is a crucial part of many real-world problems,
   including graph databases. We show how to apply Fixed Length lightweight
   compression method for traversing graphs stored in the GPU global
   memory. This approach allows for a significant saving of memory space,
   improves data alignment, cache utilization and, in many cases, also
   processing speed. We tested our solution against the state-of-the-art
   implementation of BFS for GPU and obtained very promising results.</abstract><date>2015</date><author>Kaczmarski, Krzysztof
   Przymus, Piotr
   Rzazewski, Pawel</author></paper><paper><title>Triangulating molecular surfaces over a LAN of GPU-enabled computers</title><abstract>Standalone GPU-enabled computers are adequate to triangulate and
   rendering molecular datasets with some tens of thousands of atoms at
   most. But, a standalone GPU-enabled computer has a limited capacity to
   host programable graphics cards, which in turn have also their
   constraints in terms of memory space. Thus, in spite of the huge memory
   space made available and the tremendous processing power of the current
   CPU-based graphics cards, there remains a scalability problem when it is
   necessary to triangulate and render big molecules with hundreds of
   thousands to millions of atoms. In order to overcome this scalability
   problem we use an OpenMPI-OpenMP-CUDA solution that runs on a
   loosely-coupled CPU cluster over a LAN (Local Area Network). More
   specifically, we propose a fast, scalable, parallel triangulation
   algorithm for molecular surfaces that takes advantage of multicore
   processors of CPUs and CPUs available over a local network, with each
   CPU core working as the master of a single GPU. The main contribution of
   this paper is that likely introduces the first marching cubes algorithm
   that triangulates molecular surfaces on CUDA devices over a network of
   CPU-enabled computers. (C) 2014 Elsevier B.V. All rights reserved.</abstract><date>FEB 2015</date><author>Dias, Sergio E. D.
   Gomes, Abel J. P.</author></paper></paperCache>